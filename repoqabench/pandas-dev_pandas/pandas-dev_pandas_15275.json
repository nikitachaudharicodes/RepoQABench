{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "15275",
  "issue_description": "# DataFrameGroupBy.idxmin() returns DataFrame, documentation says Series\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame([[0, 0],\r\n                  [3, 0],\r\n                  [1, 1]], index=list('ABC'), columns=list('ab'))\r\n\r\ngby = df.groupby(by='b')\r\n\r\nprint(type(gby))            # <class 'pandas.core.groupby.DataFrameGroupBy'>\r\nprint(type(gby.idxmin()))   # <class 'pandas.core.frame.DataFrame'>\r\n```\r\n#### Problem description\r\nAccording to [the documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.idxmin.html), this is supposed to output a `pandas.core.Series`. To me, that seems to be what makes sense, but I'm not sure how or why this ended up returning a `DataFrame`. Is this just an issue with the documentation, or is it an issue with the code?\r\n\r\n**Edit**: Ah, I understand why this returns a `DataFrame` now - if you have multiple columns the `idxmin()` might be different for each column. Seems that the documentation needs to be updated. I can make a PR if that's appropriate.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.6-1-ARCH\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 34.0.3\r\nCython: 0.25.2\r\nnumpy: 1.12.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 276664364,
      "user": "jreback",
      "body": "The documentation is actually right. What happens is that ``.groupby`` uses the doc string for certain 'whitelisted' methods, IOW, methods that are effectively called like\r\n\r\n``df.groupby(...).apply(lambda x: getattr(x, method)(...))`` so in this case\r\n``df.groupby(...).apply(lambda x: x.idxmin())``\r\n\r\nthen the results are inferred to be of the appropriate shape. Since this is a groupby, you get a Series for each group which are then stacked.\r\n\r\nSo the result is correct and the method of generating the document is 'correct', but this is a method that generates a Series (and not a scalar return), so ``.apply`` infers the correct output shape.\r\n\r\nSo we would need to update the templates a bit to fix this. A PR is welcome for this. I suspect there are several other methods that have this issue as well, but you would have to scan the doc-strings I think to see.\r\n\r\nwe actually just had some discussion w.r.t. this for ``.describe`` https://github.com/pandas-dev/pandas/pull/15260#issuecomment-276073724 and in #15272 so these are going to be solved differently (but the idea is the same)"
    },
    {
      "id": 276672012,
      "user": "pganssle",
      "body": "OK, I think I understand what you are saying, but I'm a bit confused still about what the \"expected result\" should be here. If I'm reading you correctly, you're saying that the templates need to be changed such that `DataFrameGroupby.idxmin()` (and, presumably, `idxmax()` and possibly a host of other methods where the same thing happens) explicitly states that it returns a `DataFrame`, right?\r\n\r\nI'm just confused because you say that \"the documentation is actually right\", but then go on to explain why the documentation says the wrong thing. I'm guessing that you meant that it's the right documentation for `x.idxmin()`, but that when it gets inherited by things that stack it, the *final* documentation is wrong."
    },
    {
      "id": 276676483,
      "user": "jreback",
      "body": "@pganssle yes, I mean the *groupby* documention is wrong, but the source itself (DataFrame.idxmin/Series.idxmin) is right. We simply use that documentation.\r\n\r\nWe need a slightly more sophisticated strategy of using the source documentation, but changing the return type to what it actually is for *some* groupby methods. A way to do this would be to make the return type a replaceable parameter in the doc-string for Series/Dataframe (and then replace it locally there), and use that here as a template.\r\n\r\nA bit of work, but I think we need to go down this route to avoid copy-pasting doc-strings, IOW, they will be defined in a singular place (on the source method, e.g. Series.idxmin or DataFrame.idxmin / even these *could* combined also), then using that in groupby with some Substitution parameters."
    },
    {
      "id": 276683641,
      "user": "pganssle",
      "body": "OK, yeah, that's what I thought. I agree on the point of documentation, by the way - I've always felt like documentation tools (at least in Python, which is where I'm most familiar with them) were lacking in support for the DRY (don't repeat yourself) principle. You end up having to do some funky stuff like creating metaclasses or dynamically generate docstrings on import (which makes things a bit harder when navigating the code instead of looking at the generated documentation).\r\n\r\nI'll take a look at how the templating looks and see if I can find an easy way to either make the return type a replaceable parameter, infer it from the code or both."
    },
    {
      "id": 276685273,
      "user": "jreback",
      "body": "@pganssle yep, tooling is not great, though using Appender and Substitution we get pretty far.\r\n\r\nthanks for looking into this."
    },
    {
      "id": 303569312,
      "user": "pganssle",
      "body": "Hm... This is quite complicated, I'm not quite sure how `Appender` and `Substitution` are supposed to work in the case of inherited docstrings, because the top-level documentation itself has to be displayed, meaning the substitution has to take place there. In all derived functions, there's nothing to substitute.\r\n\r\nI think the best way to handle this is to modify `Substitution` and `Appender` to preserve the original template versions to allow for function-specific docstring modifications.\r\n"
    },
    {
      "id": 1440037648,
      "user": "Dr-Irv",
      "body": "It turns out that `SeriesGroupBy.idxmin()` and `SeriesGroupBy.idxmax()` are not even showing up in the docs, so that should be fixed as well.\r\n"
    },
    {
      "id": 2466810500,
      "user": "rhshadrach",
      "body": "> It turns out that `SeriesGroupBy.idxmin()` and `SeriesGroupBy.idxmax()` are not even showing up in the docs, so that should be fixed as well.\r\n\r\nThis has been fixed.\r\n\r\nhttps://pandas.pydata.org/docs/dev/reference/api/pandas.core.groupby.SeriesGroupBy.idxmax.html\r\n\r\nThe DataFrameGroupBy.idxmin / max still shows it returning a Series. PRs to fix are welcome!"
    },
    {
      "id": 2477217560,
      "user": "akourieh",
      "body": "Hello I would like to help, it seems the issue is here:\r\n```\r\n        Returns\r\n        -------\r\n        Series\r\n            Indexes of minima along the specified axis.\r\n```\r\nthis is in the code in `pandas/pandas/core/frame.py` for idxmin.\r\n\r\nShould I make a PR by updating the docstring to Dataframe in that file ?"
    },
    {
      "id": 2481255771,
      "user": "rhshadrach",
      "body": "I don't think so - the issue is in `pandas.core.groupby.generic`."
    },
    {
      "id": 2513006757,
      "user": "Axeldnahcram",
      "body": "Take"
    }
  ],
  "text_context": "# DataFrameGroupBy.idxmin() returns DataFrame, documentation says Series\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame([[0, 0],\r\n                  [3, 0],\r\n                  [1, 1]], index=list('ABC'), columns=list('ab'))\r\n\r\ngby = df.groupby(by='b')\r\n\r\nprint(type(gby))            # <class 'pandas.core.groupby.DataFrameGroupBy'>\r\nprint(type(gby.idxmin()))   # <class 'pandas.core.frame.DataFrame'>\r\n```\r\n#### Problem description\r\nAccording to [the documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.idxmin.html), this is supposed to output a `pandas.core.Series`. To me, that seems to be what makes sense, but I'm not sure how or why this ended up returning a `DataFrame`. Is this just an issue with the documentation, or is it an issue with the code?\r\n\r\n**Edit**: Ah, I understand why this returns a `DataFrame` now - if you have multiple columns the `idxmin()` might be different for each column. Seems that the documentation needs to be updated. I can make a PR if that's appropriate.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.6-1-ARCH\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 34.0.3\r\nCython: 0.25.2\r\nnumpy: 1.12.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n\n\nThe documentation is actually right. What happens is that ``.groupby`` uses the doc string for certain 'whitelisted' methods, IOW, methods that are effectively called like\r\n\r\n``df.groupby(...).apply(lambda x: getattr(x, method)(...))`` so in this case\r\n``df.groupby(...).apply(lambda x: x.idxmin())``\r\n\r\nthen the results are inferred to be of the appropriate shape. Since this is a groupby, you get a Series for each group which are then stacked.\r\n\r\nSo the result is correct and the method of generating the document is 'correct', but this is a method that generates a Series (and not a scalar return), so ``.apply`` infers the correct output shape.\r\n\r\nSo we would need to update the templates a bit to fix this. A PR is welcome for this. I suspect there are several other methods that have this issue as well, but you would have to scan the doc-strings I think to see.\r\n\r\nwe actually just had some discussion w.r.t. this for ``.describe`` https://github.com/pandas-dev/pandas/pull/15260#issuecomment-276073724 and in #15272 so these are going to be solved differently (but the idea is the same)\n\nOK, I think I understand what you are saying, but I'm a bit confused still about what the \"expected result\" should be here. If I'm reading you correctly, you're saying that the templates need to be changed such that `DataFrameGroupby.idxmin()` (and, presumably, `idxmax()` and possibly a host of other methods where the same thing happens) explicitly states that it returns a `DataFrame`, right?\r\n\r\nI'm just confused because you say that \"the documentation is actually right\", but then go on to explain why the documentation says the wrong thing. I'm guessing that you meant that it's the right documentation for `x.idxmin()`, but that when it gets inherited by things that stack it, the *final* documentation is wrong.\n\n@pganssle yes, I mean the *groupby* documention is wrong, but the source itself (DataFrame.idxmin/Series.idxmin) is right. We simply use that documentation.\r\n\r\nWe need a slightly more sophisticated strategy of using the source documentation, but changing the return type to what it actually is for *some* groupby methods. A way to do this would be to make the return type a replaceable parameter in the doc-string for Series/Dataframe (and then replace it locally there), and use that here as a template.\r\n\r\nA bit of work, but I think we need to go down this route to avoid copy-pasting doc-strings, IOW, they will be defined in a singular place (on the source method, e.g. Series.idxmin or DataFrame.idxmin / even these *could* combined also), then using that in groupby with some Substitution parameters.\n\nOK, yeah, that's what I thought. I agree on the point of documentation, by the way - I've always felt like documentation tools (at least in Python, which is where I'm most familiar with them) were lacking in support for the DRY (don't repeat yourself) principle. You end up having to do some funky stuff like creating metaclasses or dynamically generate docstrings on import (which makes things a bit harder when navigating the code instead of looking at the generated documentation).\r\n\r\nI'll take a look at how the templating looks and see if I can find an easy way to either make the return type a replaceable parameter, infer it from the code or both.\n\n@pganssle yep, tooling is not great, though using Appender and Substitution we get pretty far.\r\n\r\nthanks for looking into this.\n\nHm... This is quite complicated, I'm not quite sure how `Appender` and `Substitution` are supposed to work in the case of inherited docstrings, because the top-level documentation itself has to be displayed, meaning the substitution has to take place there. In all derived functions, there's nothing to substitute.\r\n\r\nI think the best way to handle this is to modify `Substitution` and `Appender` to preserve the original template versions to allow for function-specific docstring modifications.\r\n\n\nIt turns out that `SeriesGroupBy.idxmin()` and `SeriesGroupBy.idxmax()` are not even showing up in the docs, so that should be fixed as well.\r\n\n\n> It turns out that `SeriesGroupBy.idxmin()` and `SeriesGroupBy.idxmax()` are not even showing up in the docs, so that should be fixed as well.\r\n\r\nThis has been fixed.\r\n\r\nhttps://pandas.pydata.org/docs/dev/reference/api/pandas.core.groupby.SeriesGroupBy.idxmax.html\r\n\r\nThe DataFrameGroupBy.idxmin / max still shows it returning a Series. PRs to fix are welcome!\n\nHello I would like to help, it seems the issue is here:\r\n```\r\n        Returns\r\n        -------\r\n        Series\r\n            Indexes of minima along the specified axis.\r\n```\r\nthis is in the code in `pandas/pandas/core/frame.py` for idxmin.\r\n\r\nShould I make a PR by updating the docstring to Dataframe in that file ?\n\nI don't think so - the issue is in `pandas.core.groupby.generic`.\n\nTake",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/15260",
  "code_context": [
    {
      "filename": "pandas/core/groupby.py",
      "content": "import types\nfrom functools import wraps\nimport numpy as np\nimport datetime\nimport collections\nimport warnings\nimport copy\n\nfrom pandas.compat import (\n    zip, range, long, lzip,\n    callable, map\n)\nfrom pandas import compat\nfrom pandas.compat.numpy import function as nv\nfrom pandas.compat.numpy import _np_version_under1p8\n\nfrom pandas.types.common import (is_numeric_dtype,\n                                 is_timedelta64_dtype, is_datetime64_dtype,\n                                 is_categorical_dtype,\n                                 is_datetimelike,\n                                 is_datetime64_any_dtype,\n                                 is_bool, is_integer_dtype,\n                                 is_complex_dtype,\n                                 is_bool_dtype,\n                                 is_scalar,\n                                 is_list_like,\n                                 needs_i8_conversion,\n                                 _ensure_float64,\n                                 _ensure_platform_int,\n                                 _ensure_int64,\n                                 _ensure_object,\n                                 _ensure_categorical,\n                                 _ensure_float)\nfrom pandas.types.cast import _possibly_downcast_to_dtype\nfrom pandas.types.missing import isnull, notnull, _maybe_fill\n\nfrom pandas.core.common import (_values_from_object, AbstractMethodError,\n                                _default_index)\n\nfrom pandas.core.base import (PandasObject, SelectionMixin, GroupByError,\n                              DataError, SpecificationError)\nfrom pandas.core.categorical import Categorical\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.generic import NDFrame\nfrom pandas.core.index import (Index, MultiIndex, CategoricalIndex,\n                               _ensure_index)\nfrom pandas.core.internals import BlockManager, make_block\nfrom pandas.core.series import Series\nfrom pandas.core.panel import Panel\nfrom pandas.util.decorators import (cache_readonly, Substitution, Appender,\n                                    make_signature, deprecate_kwarg)\nfrom pandas.formats.printing import pprint_thing\nfrom pandas.util.validators import validate_kwargs\n\nimport pandas.core.algorithms as algos\nimport pandas.core.common as com\nfrom pandas.core.config import option_context\nimport pandas.lib as lib\nfrom pandas.lib import Timestamp\nimport pandas.tslib as tslib\nimport pandas.algos as _algos\nimport pandas.hashtable as _hash\n\n_doc_template = \"\"\"\n\n        See also\n        --------\n        pandas.Series.%(name)s\n        pandas.DataFrame.%(name)s\n        pandas.Panel.%(name)s\n\"\"\"\n\n# special case to prevent duplicate plots when catching exceptions when\n# forwarding methods from NDFrames\n_plotting_methods = frozenset(['plot', 'boxplot', 'hist'])\n\n_common_apply_whitelist = frozenset([\n    'last', 'first',\n    'head', 'tail', 'median',\n    'mean', 'sum', 'min', 'max',\n    'cumcount',\n    'resample',\n    'rank', 'quantile',\n    'fillna',\n    'mad',\n    'any', 'all',\n    'take',\n    'idxmax', 'idxmin',\n    'shift', 'tshift',\n    'ffill', 'bfill',\n    'pct_change', 'skew',\n    'corr', 'cov', 'diff',\n]) | _plotting_methods\n\n_series_apply_whitelist = \\\n    (_common_apply_whitelist - set(['boxplot'])) | \\\n    frozenset(['dtype', 'unique'])\n\n_dataframe_apply_whitelist = \\\n    _common_apply_whitelist | frozenset(['dtypes', 'corrwith'])\n\n_cython_transforms = frozenset(['cumprod', 'cumsum', 'shift',\n                                'cummin', 'cummax'])\n\n\ndef _groupby_function(name, alias, npfunc, numeric_only=True,\n                      _convert=False):\n\n    _local_template = \"Compute %(f)s of group values\"\n\n    @Substitution(name='groupby', f=name)\n    @Appender(_doc_template)\n    @Appender(_local_template)\n    def f(self, **kwargs):\n        if 'numeric_only' not in kwargs:\n            kwargs['numeric_only'] = numeric_only\n        self._set_group_selection()\n        try:\n            return self._cython_agg_general(alias, alt=npfunc, **kwargs)\n        except AssertionError as e:\n            raise SpecificationError(str(e))\n        except Exception:\n            result = self.aggregate(lambda x: npfunc(x, axis=self.axis))\n            if _convert:\n                result = result._convert(datetime=True)\n            return result\n\n        f.__name__ = name\n\n    return f\n\n\ndef _first_compat(x, axis=0):\n\n    def _first(x):\n\n        x = np.asarray(x)\n        x = x[notnull(x)]\n        if len(x) == 0:\n            return np.nan\n        return x[0]\n\n    if isinstance(x, DataFrame):\n        return x.apply(_first, axis=axis)\n    else:\n        return _first(x)\n\n\ndef _last_compat(x, axis=0):\n    def _last(x):\n\n        x = np.asarray(x)\n        x = x[notnull(x)]\n        if len(x) == 0:\n            return np.nan\n        return x[-1]\n\n    if isinstance(x, DataFrame):\n        return x.apply(_last, axis=axis)\n    else:\n        return _last(x)\n\n\nclass Grouper(object):\n    \"\"\"\n    A Grouper allows the user to specify a groupby instruction for a target\n    object\n\n    This specification will select a column via the key parameter, or if the\n    level and/or axis parameters are given, a level of the index of the target\n    object.\n\n    These are local specifications and will override 'global' settings,\n    that is the parameters axis and level which are passed to the groupby\n    itself.\n\n    Parameters\n    ----------\n    key : string, defaults to None\n        groupby key, which selects the grouping column of the target\n    level : name/number, defaults to None\n        the level for the target index\n    freq : string / frequency object, defaults to None\n        This will groupby the specified frequency if the target selection\n        (via key or level) is a datetime-like object. For full specification\n        of available frequencies, please see `here\n        <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`_.\n    axis : number/name of the axis, defaults to 0\n    sort : boolean, default to False\n        whether to sort the resulting labels\n\n    additional kwargs to control time-like groupers (when freq is passed)\n\n    closed : closed end of interval; left or right\n    label : interval boundary to use for labeling; left or right\n    convention : {'start', 'end', 'e', 's'}\n        If grouper is PeriodIndex\n\n    Returns\n    -------\n    A specification for a groupby instruction\n\n    Examples\n    --------\n\n    Syntactic sugar for ``df.groupby('A')``\n\n    >>> df.groupby(Grouper(key='A'))\n\n    Specify a resample operation on the column 'date'\n\n    >>> df.groupby(Grouper(key='date', freq='60s'))\n\n    Specify a resample operation on the level 'date' on the columns axis\n    with a frequency of 60s\n\n    >>> df.groupby(Grouper(level='date', freq='60s', axis=1))\n    \"\"\"\n\n    def __new__(cls, *args, **kwargs):\n        if kwargs.get('freq') is not None:\n            from pandas.tseries.resample import TimeGrouper\n            cls = TimeGrouper\n        return super(Grouper, cls).__new__(cls)\n\n    def __init__(self, key=None, level=None, freq=None, axis=0, sort=False):\n        self.key = key\n        self.level = level\n        self.freq = freq\n        self.axis = axis\n        self.sort = sort\n\n        self.grouper = None\n        self.obj = None\n        self.indexer = None\n        self.binner = None\n\n    @property\n    def ax(self):\n        return self.grouper\n\n    def _get_grouper(self, obj):\n        \"\"\"\n        Parameters\n        ----------\n        obj : the subject object\n\n        Returns\n        -------\n        a tuple of binner, grouper, obj (possibly sorted)\n        \"\"\"\n\n        self._set_grouper(obj)\n        self.grouper, exclusions, self.obj = _get_grouper(self.obj, [self.key],\n                                                          axis=self.axis,\n                                                          level=self.level,\n                                                          sort=self.sort)\n        return self.binner, self.grouper, self.obj\n\n    def _set_grouper(self, obj, sort=False):\n        \"\"\"\n        given an object and the specifications, setup the internal grouper\n        for this particular specification\n\n        Parameters\n        ----------\n        obj : the subject object\n        sort : bool, default False\n            whether the resulting grouper should be sorted\n        \"\"\"\n\n        if self.key is not None and self.level is not None:\n            raise ValueError(\n                \"The Grouper cannot specify both a key and a level!\")\n\n        # the key must be a valid info item\n        if self.key is not None:\n            key = self.key\n            if key not in obj._info_axis:\n                raise KeyError(\"The grouper name {0} is not found\".format(key))\n            ax = Index(obj[key], name=key)\n\n        else:\n            ax = obj._get_axis(self.axis)\n            if self.level is not None:\n                level = self.level\n\n                # if a level is given it must be a mi level or\n                # equivalent to the axis name\n                if isinstance(ax, MultiIndex):\n                    level = ax._get_level_number(level)\n                    ax = Index(ax.get_level_values(\n                        level), name=ax.names[level])\n\n                else:\n                    if level not in (0, ax.name):\n                        raise ValueError(\n                            \"The level {0} is not valid\".format(level))\n\n        # possibly sort\n        if (self.sort or sort) and not ax.is_monotonic:\n            # use stable sort to support first, last, nth\n            indexer = self.indexer = ax.argsort(kind='mergesort')\n            ax = ax.take(indexer)\n            obj = obj.take(indexer, axis=self.axis,\n                           convert=False, is_copy=False)\n\n        self.obj = obj\n        self.grouper = ax\n        return self.grouper\n\n    def _get_binner_for_grouping(self, obj):\n        \"\"\" default to the standard binner here \"\"\"\n        group_axis = obj._get_axis(self.axis)\n        return Grouping(group_axis, None, obj=obj, name=self.key,\n                        level=self.level, sort=self.sort, in_axis=False)\n\n    @property\n    def groups(self):\n        return self.grouper.groups\n\n\nclass GroupByPlot(PandasObject):\n    \"\"\"\n    Class implementing the .plot attribute for groupby objects\n    \"\"\"\n\n    def __init__(self, groupby):\n        self._groupby = groupby\n\n    def __call__(self, *args, **kwargs):\n        def f(self):\n            return self.plot(*args, **kwargs)\n        f.__name__ = 'plot'\n        return self._groupby.apply(f)\n\n    def __getattr__(self, name):\n        def attr(*args, **kwargs):\n            def f(self):\n                return getattr(self.plot, name)(*args, **kwargs)\n            return self._groupby.apply(f)\n        return attr\n\n\nclass _GroupBy(PandasObject, SelectionMixin):\n    _group_selection = None\n    _apply_whitelist = frozenset([])\n\n    def __init__(self, obj, keys=None, axis=0, level=None,\n                 grouper=None, exclusions=None, selection=None, as_index=True,\n                 sort=True, group_keys=True, squeeze=False, **kwargs):\n\n        self._selection = selection\n\n        if isinstance(obj, NDFrame):\n            obj._consolidate_inplace()\n\n        self.level = level\n\n        if not as_index:\n            if not isinstance(obj, DataFrame):\n                raise TypeError('as_index=False only valid with DataFrame')\n            if axis != 0:\n                raise ValueError('as_index=False only valid for axis=0')\n\n        self.as_index = as_index\n        self.keys = keys\n        self.sort = sort\n        self.group_keys = group_keys\n        self.squeeze = squeeze\n        self.mutated = kwargs.pop('mutated', False)\n\n        if grouper is None:\n            grouper, exclusions, obj = _get_grouper(obj, keys,\n                                                    axis=axis,\n                                                    level=level,\n                                                    sort=sort,\n                                                    mutated=self.mutated)\n\n        self.obj = obj\n        self.axis = obj._get_axis_number(axis)\n        self.grouper = grouper\n        self.exclusions = set(exclusions) if exclusions else set()\n\n        # we accept no other args\n        validate_kwargs('group', kwargs, {})\n\n    def __len__(self):\n        return len(self.groups)\n\n    def __unicode__(self):\n        # TODO: Better unicode/repr for GroupBy object\n        return object.__repr__(self)\n\n    def _assure_grouper(self):\n        \"\"\"\n        we create the grouper on instantiation\n        sub-classes may have a different policy\n        \"\"\"\n        pass\n\n    @property\n    def groups(self):\n        \"\"\" dict {group name -> group labels} \"\"\"\n        self._assure_grouper()\n        return self.grouper.groups\n\n    @property\n    def ngroups(self):\n        self._assure_grouper()\n        return self.grouper.ngroups\n\n    @property\n    def indices(self):\n        \"\"\" dict {group name -> group indices} \"\"\"\n        self._assure_grouper()\n        return self.grouper.indices\n\n    def _get_indices(self, names):\n        \"\"\"\n        safe get multiple indices, translate keys for\n        datelike to underlying repr\n        \"\"\"\n\n        def get_converter(s):\n            # possibly convert to the actual key types\n            # in the indices, could be a Timestamp or a np.datetime64\n            if isinstance(s, (Timestamp, datetime.datetime)):\n                return lambda key: Timestamp(key)\n            elif isinstance(s, np.datetime64):\n                return lambda key: Timestamp(key).asm8\n            else:\n                return lambda key: key\n\n        if len(names) == 0:\n            return []\n\n        if len(self.indices) > 0:\n            index_sample = next(iter(self.indices))\n        else:\n            index_sample = None     # Dummy sample\n\n        name_sample = names[0]\n        if isinstance(index_sample, tuple):\n            if not isinstance(name_sample, tuple):\n                msg = (\"must supply a tuple to get_group with multiple\"\n                       \" grouping keys\")\n                raise ValueError(msg)\n            if not len(name_sample) == len(index_sample):\n                try:\n                    # If the original grouper was a tuple\n                    return [self.indices[name] for name in names]\n                except KeyError:\n                    # turns out it wasn't a tuple\n                    msg = (\"must supply a a same-length tuple to get_group\"\n                           \" with multiple grouping keys\")\n                    raise ValueError(msg)\n\n            converters = [get_converter(s) for s in index_sample]\n            names = [tuple([f(n) for f, n in zip(converters, name)])\n                     for name in names]\n\n        else:\n            converter = get_converter(index_sample)\n            names = [converter(name) for name in names]\n\n        return [self.indices.get(name, []) for name in names]\n\n    def _get_index(self, name):\n        \"\"\" safe get index, translate keys for datelike to underlying repr \"\"\"\n        return self._get_indices([name])[0]\n\n    @cache_readonly\n    def _selected_obj(self):\n\n        if self._selection is None or isinstance(self.obj, Series):\n            if self._group_selection is not None:\n                return self.obj[self._group_selection]\n            return self.obj\n        else:\n            return self.obj[self._selection]\n\n    def _reset_group_selection(self):\n        \"\"\"\n        Clear group based selection. Used for methods needing to return info on\n        each group regardless of whether a group selection was previously set.\n        \"\"\"\n        if self._group_selection is not None:\n            self._group_selection = None\n            # GH12839 clear cached selection too when changing group selection\n            self._reset_cache('_selected_obj')\n\n    def _set_group_selection(self):\n        \"\"\"\n        Create group based selection. Used when selection is not passed\n        directly but instead via a grouper.\n        \"\"\"\n        grp = self.grouper\n        if self.as_index and getattr(grp, 'groupings', None) is not None and \\\n           self.obj.ndim > 1:\n            ax = self.obj._info_axis\n            groupers = [g.name for g in grp.groupings\n                        if g.level is None and g.in_axis]\n\n            if len(groupers):\n                self._group_selection = ax.difference(Index(groupers)).tolist()\n                # GH12839 clear selected obj cache when group selection changes\n                self._reset_cache('_selected_obj')\n\n    def _set_result_index_ordered(self, result):\n        # set the result index on the passed values object and\n        # return the new object, xref 8046\n\n        # the values/counts are repeated according to the group index\n        # shortcut if we have an already ordered grouper\n        if not self.grouper.is_monotonic:\n            index = Index(np.concatenate(\n                self._get_indices(self.grouper.result_index)))\n            result.set_axis(self.axis, index)\n            result = result.sort_index(axis=self.axis)\n\n        result.set_axis(self.axis, self.obj._get_axis(self.axis))\n        return result\n\n    def _dir_additions(self):\n        return self.obj._dir_additions() | self._apply_whitelist\n\n    def __getattr__(self, attr):\n        if attr in self._internal_names_set:\n            return object.__getattribute__(self, attr)\n        if attr in self.obj:\n            return self[attr]\n        if hasattr(self.obj, attr):\n            return self._make_wrapper(attr)\n\n        raise AttributeError(\"%r object has no attribute %r\" %\n                             (type(self).__name__, attr))\n\n    plot = property(GroupByPlot)\n\n    def _make_wrapper(self, name):\n        if name not in self._apply_whitelist:\n            is_callable = callable(getattr(self._selected_obj, name, None))\n            kind = ' callable ' if is_callable else ' '\n            msg = (\"Cannot access{0}attribute {1!r} of {2!r} objects, try \"\n                   \"using the 'apply' method\".format(kind, name,\n                                                     type(self).__name__))\n            raise AttributeError(msg)\n\n        # need to setup the selection\n        # as are not passed directly but in the grouper\n        self._set_group_selection()\n\n        f = getattr(self._selected_obj, name)\n        if not isinstance(f, types.MethodType):\n            return self.apply(lambda self: getattr(self, name))\n\n        f = getattr(type(self._selected_obj), name)\n\n        def wrapper(*args, **kwargs):\n            # a little trickery for aggregation functions that need an axis\n            # argument\n            kwargs_with_axis = kwargs.copy()\n            if 'axis' not in kwargs_with_axis or \\\n               kwargs_with_axis['axis'] is None:\n                kwargs_with_axis['axis'] = self.axis\n\n            def curried_with_axis(x):\n                return f(x, *args, **kwargs_with_axis)\n\n            def curried(x):\n                return f(x, *args, **kwargs)\n\n            # preserve the name so we can detect it when calling plot methods,\n            # to avoid duplicates\n            curried.__name__ = curried_with_axis.__name__ = name\n\n            # special case otherwise extra plots are created when catching the\n            # exception below\n            if name in _plotting_methods:\n                return self.apply(curried)\n\n            try:\n                return self.apply(curried_with_axis)\n            except Exception:\n                try:\n                    return self.apply(curried)\n                except Exception:\n\n                    # related to : GH3688\n                    # try item-by-item\n                    # this can be called recursively, so need to raise\n                    # ValueError\n                    # if we don't have this method to indicated to aggregate to\n                    # mark this column as an error\n                    try:\n                        return self._aggregate_item_by_item(name,\n                                                            *args, **kwargs)\n                    except (AttributeError):\n                        raise ValueError\n\n        return wrapper\n\n    def get_group(self, name, obj=None):\n        \"\"\"\n        Constructs NDFrame from group with provided name\n\n        Parameters\n        ----------\n        name : object\n            the name of the group to get as a DataFrame\n        obj : NDFrame, default None\n            the NDFrame to take the DataFrame out of.  If\n            it is None, the object groupby was called on will\n            be used\n\n        Returns\n        -------\n        group : type of obj\n        \"\"\"\n        if obj is None:\n            obj = self._selected_obj\n\n        inds = self._get_index(name)\n        if not len(inds):\n            raise KeyError(name)\n\n        return obj.take(inds, axis=self.axis, convert=False)\n\n    def __iter__(self):\n        \"\"\"\n        Groupby iterator\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        return self.grouper.get_iterator(self.obj, axis=self.axis)\n\n    @Substitution(name='groupby')\n    def apply(self, func, *args, **kwargs):\n        \"\"\"\n        Apply function and combine results together in an intelligent way. The\n        split-apply-combine combination rules attempt to be as common sense\n        based as possible. For example:\n\n        case 1:\n        group DataFrame\n        apply aggregation function (f(chunk) -> Series)\n        yield DataFrame, with group axis having group labels\n\n        case 2:\n        group DataFrame\n        apply transform function ((f(chunk) -> DataFrame with same indexes)\n        yield DataFrame with resulting chunks glued together\n\n        case 3:\n        group Series\n        apply function with f(chunk) -> DataFrame\n        yield DataFrame with result of chunks glued together\n\n        Parameters\n        ----------\n        func : function\n\n        Notes\n        -----\n        See online documentation for full exposition on how to use apply.\n\n        In the current implementation apply calls func twice on the\n        first group to decide whether it can take a fast or slow code\n        path. This can lead to unexpected behavior if func has\n        side-effects, as they will take effect twice for the first\n        group.\n\n\n        See also\n        --------\n        aggregate, transform\"\"\"\n\n        func = self._is_builtin_func(func)\n\n        # this is needed so we don't try and wrap strings. If we could\n        # resolve functions to their callable functions prior, this\n        # wouldn't be needed\n        if args or kwargs:\n            if callable(func):\n\n                @wraps(func)\n                def f(g):\n                    with np.errstate(all='ignore'):\n                        return func(g, *args, **kwargs)\n            else:\n                raise ValueError('func must be a callable if args or '\n                                 'kwargs are supplied')\n        else:\n            f = func\n\n        # ignore SettingWithCopy here in case the user mutates\n        with option_context('mode.chained_assignment', None):\n            return self._python_apply_general(f)\n\n    def _python_apply_general(self, f):\n        keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n                                                   self.axis)\n\n        return self._wrap_applied_output(\n            keys,\n            values,\n            not_indexed_same=mutated or self.mutated)\n\n    def _iterate_slices(self):\n        yield self.name, self._selected_obj\n\n    def transform(self, func, *args, **kwargs):\n        raise AbstractMethodError(self)\n\n    def _cumcount_array(self, ascending=True):\n        \"\"\"\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Note\n        ----\n        this is currently implementing sort=False\n        (though the default is sort=True) for groupby in general\n        \"\"\"\n        ids, _, ngroups = self.grouper.group_info\n        sorter = _get_group_index_sorter(ids, ngroups)\n        ids, count = ids[sorter], len(ids)\n\n        if count == 0:\n            return np.empty(0, dtype=np.int64)\n\n        run = np.r_[True, ids[:-1] != ids[1:]]\n        rep = np.diff(np.r_[np.nonzero(run)[0], count])\n        out = (~run).cumsum()\n\n        if ascending:\n            out -= np.repeat(out[run], rep)\n        else:\n            out = np.repeat(out[np.r_[run[1:], True]], rep) - out\n\n        rev = np.empty(count, dtype=np.intp)\n        rev[sorter] = np.arange(count, dtype=np.intp)\n        return out[rev].astype(np.int64, copy=False)\n\n    def _index_with_as_index(self, b):\n        \"\"\"\n        Take boolean mask of index to be returned from apply, if as_index=True\n\n        \"\"\"\n        # TODO perf, it feels like this should already be somewhere...\n        from itertools import chain\n        original = self._selected_obj.index\n        gp = self.grouper\n        levels = chain((gp.levels[i][gp.labels[i][b]]\n                        for i in range(len(gp.groupings))),\n                       (original.get_level_values(i)[b]\n                        for i in range(original.nlevels)))\n        new = MultiIndex.from_arrays(list(levels))\n        new.names = gp.names + original.names\n        return new\n\n    def _try_cast(self, result, obj):\n        \"\"\"\n        try to cast the result to our obj original type,\n        we may have roundtripped thru object in the mean-time\n\n        \"\"\"\n        if obj.ndim > 1:\n            dtype = obj.values.dtype\n        else:\n            dtype = obj.dtype\n\n        if not is_scalar(result):\n            result = _possibly_downcast_to_dtype(result, dtype)\n\n        return result\n\n    def _cython_transform(self, how, numeric_only=True):\n        output = collections.OrderedDict()\n        for name, obj in self._iterate_slices():\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if numeric_only and not is_numeric:\n                continue\n\n            try:\n                result, names = self.grouper.transform(obj.values, how)\n            except NotImplementedError:\n                continue\n            except AssertionError as e:\n                raise GroupByError(str(e))\n            output[name] = self._try_cast(result, obj)\n\n        if len(output) == 0:\n            raise DataError('No numeric types to aggregate')\n\n        return self._wrap_transformed_output(output, names)\n\n    def _cython_agg_general(self, how, alt=None, numeric_only=True):\n        output = {}\n        for name, obj in self._iterate_slices():\n            is_numeric = is_numeric_dtype(obj.dtype)\n            if numeric_only and not is_numeric:\n                continue\n\n            try:\n                result, names = self.grouper.aggregate(obj.values, how)\n            except AssertionError as e:\n                raise GroupByError(str(e))\n            output[name] = self._try_cast(result, obj)\n\n        if len(output) == 0:\n            raise DataError('No numeric types to aggregate')\n\n        return self._wrap_aggregated_output(output, names)\n\n    def _python_agg_general(self, func, *args, **kwargs):\n        func = self._is_builtin_func(func)\n        f = lambda x: func(x, *args, **kwargs)\n\n        # iterate through \"columns\" ex exclusions to populate output dict\n        output = {}\n        for name, obj in self._iterate_slices():\n            try:\n                result, counts = self.grouper.agg_series(obj, f)\n                output[name] = self._try_cast(result, obj)\n            except TypeError:\n                continue\n\n        if len(output) == 0:\n            return self._python_apply_general(f)\n\n        if self.grouper._filter_empty_groups:\n\n            mask = counts.ravel() > 0\n            for name, result in compat.iteritems(output):\n\n                # since we are masking, make sure that we have a float object\n                values = result\n                if is_numeric_dtype(values.dtype):\n                    values = _ensure_float(values)\n\n                output[name] = self._try_cast(values[mask], result)\n\n        return self._wrap_aggregated_output(output)\n\n    def _wrap_applied_output(self, *args, **kwargs):\n        raise AbstractMethodError(self)\n\n    def _concat_objects(self, keys, values, not_indexed_same=False):\n        from pandas.tools.merge import concat\n\n        def reset_identity(values):\n            # reset the identities of the components\n            # of the values to prevent aliasing\n            for v in values:\n                if v is not None:\n                    ax = v._get_axis(self.axis)\n                    ax._reset_identity()\n            return values\n\n        if not not_indexed_same:\n            result = concat(values, axis=self.axis)\n            ax = self._selected_obj._get_axis(self.axis)\n\n            if isinstance(result, Series):\n                result = result.reindex(ax)\n            else:\n\n                # this is a very unfortunate situation\n                # we have a multi-index that is NOT lexsorted\n                # and we have a result which is duplicated\n                # we can't reindex, so we resort to this\n                # GH 14776\n                if isinstance(ax, MultiIndex) and not ax.is_unique:\n                    result = result.take(result.index.get_indexer_for(\n                        ax.values).unique(), axis=self.axis)\n                else:\n                    result = result.reindex_axis(ax, axis=self.axis)\n\n        elif self.group_keys:\n\n            values = reset_identity(values)\n            if self.as_index:\n\n                # possible MI return case\n                group_keys = keys\n                group_levels = self.grouper.levels\n                group_names = self.grouper.names\n\n                result = concat(values, axis=self.axis, keys=group_keys,\n                                levels=group_levels, names=group_names)\n            else:\n\n                # GH5610, returns a MI, with the first level being a\n                # range index\n                keys = list(range(len(values)))\n                result = concat(values, axis=self.axis, keys=keys)\n        else:\n            values = reset_identity(values)\n            result = concat(values, axis=self.axis)\n\n        if (isinstance(result, Series) and\n                getattr(self, 'name', None) is not None):\n\n            result.name = self.name\n\n        return result\n\n    def _apply_filter(self, indices, dropna):\n        if len(indices) == 0:\n            indices = np.array([], dtype='int64')\n        else:\n            indices = np.sort(np.concatenate(indices))\n        if dropna:\n            filtered = self._selected_obj.take(indices, axis=self.axis)\n        else:\n            mask = np.empty(len(self._selected_obj.index), dtype=bool)\n            mask.fill(False)\n            mask[indices.astype(int)] = True\n            # mask fails to broadcast when passed to where; broadcast manually.\n            mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T\n            filtered = self._selected_obj.where(mask)  # Fill with NaNs.\n        return filtered\n\n\nclass GroupBy(_GroupBy):\n\n    \"\"\"\n    Class for grouping and aggregating relational data. See aggregate,\n    transform, and apply functions on this object.\n\n    It's easiest to use obj.groupby(...) to use GroupBy, but you can also do:\n\n    ::\n\n        grouped = groupby(obj, ...)\n\n    Parameters\n    ----------\n    obj : pandas object\n    axis : int, default 0\n    level : int, default None\n        Level of MultiIndex\n    groupings : list of Grouping objects\n        Most users should ignore this\n    exclusions : array-like, optional\n        List of columns to exclude\n    name : string\n        Most users should ignore this\n\n    Notes\n    -----\n    After grouping, see aggregate, apply, and transform functions. Here are\n    some other brief notes about usage. When grouping by multiple groups, the\n    result index will be a MultiIndex (hierarchical) by default.\n\n    Iteration produces (key, group) tuples, i.e. chunking the data by group. So\n    you can write code like:\n\n    ::\n\n        grouped = obj.groupby(keys, axis=axis)\n        for key, group in grouped:\n            # do something with the data\n\n    Function calls on GroupBy, if not specially implemented, \"dispatch\" to the\n    grouped data. So if you group a DataFrame and wish to invoke the std()\n    method on each group, you can simply do:\n\n    ::\n\n        df.groupby(mapper).std()\n\n    rather than\n\n    ::\n\n        df.groupby(mapper).aggregate(np.std)\n\n    You can pass arguments to these \"wrapped\" functions, too.\n\n    See the online documentation for full exposition on these topics and much\n    more\n\n    Returns\n    -------\n    **Attributes**\n    groups : dict\n        {group name -> group labels}\n    len(grouped) : int\n        Number of groups\n    \"\"\"\n    _apply_whitelist = _common_apply_whitelist\n\n    def irow(self, i):\n        \"\"\"\n        DEPRECATED. Use ``.nth(i)`` instead\n        \"\"\"\n\n        # 10177\n        warnings.warn(\"irow(i) is deprecated. Please use .nth(i)\",\n                      FutureWarning, stacklevel=2)\n        return self.nth(i)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def count(self):\n        \"\"\"Compute count of group, excluding missing values\"\"\"\n\n        # defined here for API doc\n        raise NotImplementedError\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def mean(self, *args, **kwargs):\n        \"\"\"\n        Compute mean of groups, excluding missing values\n\n        For multiple groupings, the result index will be a MultiIndex\n        \"\"\"\n        nv.validate_groupby_func('mean', args, kwargs, ['numeric_only'])\n        try:\n            return self._cython_agg_general('mean', **kwargs)\n        except GroupByError:\n            raise\n        except Exception:  # pragma: no cover\n            self._set_group_selection()\n            f = lambda x: x.mean(axis=self.axis, **kwargs)\n            return self._python_agg_general(f)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def median(self, **kwargs):\n        \"\"\"\n        Compute median of groups, excluding missing values\n\n        For multiple groupings, the result index will be a MultiIndex\n        \"\"\"\n        try:\n            return self._cython_agg_general('median', **kwargs)\n        except GroupByError:\n            raise\n        except Exception:  # pragma: no cover\n\n            self._set_group_selection()\n\n            def f(x):\n                if isinstance(x, np.ndarray):\n                    x = Series(x)\n                return x.median(axis=self.axis, **kwargs)\n            return self._python_agg_general(f)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def std(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute standard deviation of groups, excluding missing values\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom\n        \"\"\"\n\n        # TODO: implement at Cython level?\n        nv.validate_groupby_func('std', args, kwargs)\n        return np.sqrt(self.var(ddof=ddof, **kwargs))\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def var(self, ddof=1, *args, **kwargs):\n        \"\"\"\n        Compute variance of groups, excluding missing values\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom\n        \"\"\"\n        nv.validate_groupby_func('var', args, kwargs)\n        if ddof == 1:\n            return self._cython_agg_general('var', **kwargs)\n        else:\n            self._set_group_selection()\n            f = lambda x: x.var(ddof=ddof, **kwargs)\n            return self._python_agg_general(f)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def sem(self, ddof=1):\n        \"\"\"\n        Compute standard error of the mean of groups, excluding missing values\n\n        For multiple groupings, the result index will be a MultiIndex\n\n        Parameters\n        ----------\n        ddof : integer, default 1\n            degrees of freedom\n        \"\"\"\n\n        return self.std(ddof=ddof) / np.sqrt(self.count())\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def size(self):\n        \"\"\"Compute group sizes\"\"\"\n        return self.grouper.size()\n\n    sum = _groupby_function('sum', 'add', np.sum)\n    prod = _groupby_function('prod', 'prod', np.prod)\n    min = _groupby_function('min', 'min', np.min, numeric_only=False)\n    max = _groupby_function('max', 'max', np.max, numeric_only=False)\n    first = _groupby_function('first', 'first', _first_compat,\n                              numeric_only=False, _convert=True)\n    last = _groupby_function('last', 'last', _last_compat, numeric_only=False,\n                             _convert=True)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def ohlc(self):\n        \"\"\"\n        Compute sum of values, excluding missing values\n        For multiple groupings, the result index will be a MultiIndex\n        \"\"\"\n\n        return self._apply_to_column_groupbys(\n            lambda x: x._cython_agg_general('ohlc'))\n\n    @Appender(DataFrame.describe.__doc__)\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def describe(self, **kwargs):\n        self._set_group_selection()\n        result = self.apply(lambda x: x.describe(**kwargs))\n        if self.axis == 1:\n            return result.T\n        return result.unstack()\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def resample(self, rule, *args, **kwargs):\n        \"\"\"\n        Provide resampling when using a TimeGrouper\n        Return a new grouper with our resampler appended\n        \"\"\"\n        from pandas.tseries.resample import get_resampler_for_grouping\n        return get_resampler_for_grouping(self, rule, *args, **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def rolling(self, *args, **kwargs):\n        \"\"\"\n        Return a rolling grouper, providing rolling\n        functionaility per group\n\n        \"\"\"\n        from pandas.core.window import RollingGroupby\n        return RollingGroupby(self, *args, **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def expanding(self, *args, **kwargs):\n        \"\"\"\n        Return an expanding grouper, providing expanding\n        functionaility per group\n\n        \"\"\"\n        from pandas.core.window import ExpandingGroupby\n        return ExpandingGroupby(self, *args, **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def pad(self, limit=None):\n        \"\"\"\n        Forward fill the values\n\n        Parameters\n        ----------\n        limit : integer, optional\n            limit of how many values to fill\n\n        See Also\n        --------\n        Series.fillna\n        DataFrame.fillna\n        \"\"\"\n        return self.apply(lambda x: x.ffill(limit=limit))\n    ffill = pad\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def backfill(self, limit=None):\n        \"\"\"\n        Backward fill the values\n\n        Parameters\n        ----------\n        limit : integer, optional\n            limit of how many values to fill\n\n        See Also\n        --------\n        Series.fillna\n        DataFrame.fillna\n        \"\"\"\n        return self.apply(lambda x: x.bfill(limit=limit))\n    bfill = backfill\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def nth(self, n, dropna=None):\n        \"\"\"\n        Take the nth row from each group if n is an int, or a subset of rows\n        if n is a list of ints.\n\n        If dropna, will take the nth non-null row, dropna is either\n        Truthy (if a Series) or 'all', 'any' (if a DataFrame);\n        this is equivalent to calling dropna(how=dropna) before the\n        groupby.\n\n        Parameters\n        ----------\n        n : int or list of ints\n            a single nth value for the row or a list of nth values\n        dropna : None or str, optional\n            apply the specified dropna operation before counting which row is\n            the nth row. Needs to be None, 'any' or 'all'\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n        ...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])\n        >>> g = df.groupby('A')\n        >>> g.nth(0)\n             B\n        A\n        1  NaN\n        2  3.0\n        >>> g.nth(1)\n             B\n        A\n        1  2.0\n        2  5.0\n        >>> g.nth(-1)\n             B\n        A\n        1  4.0\n        2  5.0\n        >>> g.nth([0, 1])\n             B\n        A\n        1  NaN\n        1  2.0\n        2  3.0\n        2  5.0\n\n        Specifying ``dropna`` allows count ignoring NaN\n\n        >>> g.nth(0, dropna='any')\n             B\n        A\n        1  2.0\n        2  3.0\n\n        NaNs denote group exhausted when using dropna\n\n        >>> g.nth(3, dropna='any')\n            B\n        A\n        1 NaN\n        2 NaN\n\n        Specifying ``as_index=False`` in ``groupby`` keeps the original index.\n\n        >>> df.groupby('A', as_index=False).nth(1)\n           A    B\n        1  1  2.0\n        4  2  5.0\n        \"\"\"\n\n        if isinstance(n, int):\n            nth_values = [n]\n        elif isinstance(n, (set, list, tuple)):\n            nth_values = list(set(n))\n            if dropna is not None:\n                raise ValueError(\n                    \"dropna option with a list of nth values is not supported\")\n        else:\n            raise TypeError(\"n needs to be an int or a list/set/tuple of ints\")\n\n        nth_values = np.array(nth_values, dtype=np.intp)\n        self._set_group_selection()\n\n        if not dropna:\n            mask = np.in1d(self._cumcount_array(), nth_values) | \\\n                np.in1d(self._cumcount_array(ascending=False) + 1, -nth_values)\n\n            out = self._selected_obj[mask]\n            if not self.as_index:\n                return out\n\n            ids, _, _ = self.grouper.group_info\n            out.index = self.grouper.result_index[ids[mask]]\n\n            return out.sort_index() if self.sort else out\n\n        if isinstance(self._selected_obj, DataFrame) and \\\n           dropna not in ['any', 'all']:\n            # Note: when agg-ing picker doesn't raise this, just returns NaN\n            raise ValueError(\"For a DataFrame groupby, dropna must be \"\n                             \"either None, 'any' or 'all', \"\n                             \"(was passed %s).\" % (dropna),)\n\n        # old behaviour, but with all and any support for DataFrames.\n        # modified in GH 7559 to have better perf\n        max_len = n if n >= 0 else - 1 - n\n        dropped = self.obj.dropna(how=dropna, axis=self.axis)\n\n        # get a new grouper for our dropped obj\n        if self.keys is None and self.level is None:\n\n            # we don't have the grouper info available\n            # (e.g. we have selected out\n            # a column that is not in the current object)\n            axis = self.grouper.axis\n            grouper = axis[axis.isin(dropped.index)]\n\n        else:\n\n            # create a grouper with the original parameters, but on the dropped\n            # object\n            grouper, _, _ = _get_grouper(dropped, key=self.keys,\n                                         axis=self.axis, level=self.level,\n                                         sort=self.sort,\n                                         mutated=self.mutated)\n\n        grb = dropped.groupby(grouper, as_index=self.as_index, sort=self.sort)\n        sizes, result = grb.size(), grb.nth(n)\n        mask = (sizes < max_len).values\n\n        # set the results which don't meet the criteria\n        if len(result) and mask.any():\n            result.loc[mask] = np.nan\n\n        # reset/reindex to the original groups\n        if len(self.obj) == len(dropped) or \\\n           len(result) == len(self.grouper.result_index):\n            result.index = self.grouper.result_index\n        else:\n            result = result.reindex(self.grouper.result_index)\n\n        return result\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def cumcount(self, ascending=True):\n        \"\"\"\n        Number each item in each group from 0 to the length of that group - 1.\n\n        Essentially this is equivalent to\n\n        >>> self.apply(lambda x: Series(np.arange(len(x)), x.index))\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Examples\n        --------\n\n        >>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n        ...                   columns=['A'])\n        >>> df\n           A\n        0  a\n        1  a\n        2  a\n        3  b\n        4  b\n        5  a\n        >>> df.groupby('A').cumcount()\n        0    0\n        1    1\n        2    2\n        3    0\n        4    1\n        5    3\n        dtype: int64\n        >>> df.groupby('A').cumcount(ascending=False)\n        0    3\n        1    2\n        2    1\n        3    1\n        4    0\n        5    0\n        dtype: int64\n        \"\"\"\n\n        self._set_group_selection()\n\n        index = self._selected_obj.index\n        cumcounts = self._cumcount_array(ascending=ascending)\n        return Series(cumcounts, index)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def cumprod(self, axis=0, *args, **kwargs):\n        \"\"\"Cumulative product for each group\"\"\"\n        nv.validate_groupby_func('cumprod', args, kwargs, ['numeric_only'])\n        if axis != 0:\n            return self.apply(lambda x: x.cumprod(axis=axis, **kwargs))\n\n        return self._cython_transform('cumprod', **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"Cumulative sum for each group\"\"\"\n        nv.validate_groupby_func('cumsum', args, kwargs, ['numeric_only'])\n        if axis != 0:\n            return self.apply(lambda x: x.cumsum(axis=axis, **kwargs))\n\n        return self._cython_transform('cumsum', **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def cummin(self, axis=0, **kwargs):\n        \"\"\"Cumulative min for each group\"\"\"\n        if axis != 0:\n            return self.apply(lambda x: np.minimum.accumulate(x, axis))\n\n        return self._cython_transform('cummin', **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def cummax(self, axis=0, **kwargs):\n        \"\"\"Cumulative max for each group\"\"\"\n        if axis != 0:\n            return self.apply(lambda x: np.maximum.accumulate(x, axis))\n\n        return self._cython_transform('cummax', **kwargs)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def shift(self, periods=1, freq=None, axis=0):\n        \"\"\"\n        Shift each group by periods observations\n\n        Parameters\n        ----------\n        periods : integer, default 1\n            number of periods to shift\n        freq : frequency string\n        axis : axis to shift, default 0\n        \"\"\"\n\n        if freq is not None or axis != 0:\n            return self.apply(lambda x: x.shift(periods, freq, axis))\n\n        labels, _, ngroups = self.grouper.group_info\n\n        # filled in by Cython\n        indexer = np.zeros_like(labels)\n        _algos.group_shift_indexer(indexer, labels, ngroups, periods)\n\n        output = {}\n        for name, obj in self._iterate_slices():\n            output[name] = algos.take_nd(obj.values, indexer)\n\n        return self._wrap_transformed_output(output)\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def head(self, n=5):\n        \"\"\"\n        Returns first n rows of each group.\n\n        Essentially equivalent to ``.apply(lambda x: x.head(n))``,\n        except ignores as_index flag.\n\n        Examples\n        --------\n\n        >>> df = DataFrame([[1, 2], [1, 4], [5, 6]],\n                           columns=['A', 'B'])\n        >>> df.groupby('A', as_index=False).head(1)\n           A  B\n        0  1  2\n        2  5  6\n        >>> df.groupby('A').head(1)\n           A  B\n        0  1  2\n        2  5  6\n        \"\"\"\n        self._reset_group_selection()\n        mask = self._cumcount_array() < n\n        return self._selected_obj[mask]\n\n    @Substitution(name='groupby')\n    @Appender(_doc_template)\n    def tail(self, n=5):\n        \"\"\"\n        Returns last n rows of each group\n\n        Essentially equivalent to ``.apply(lambda x: x.tail(n))``,\n        except ignores as_index flag.\n\n        Examples\n        --------\n\n        >>> df = DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n                           columns=['A', 'B'])\n        >>> df.groupby('A').tail(1)\n           A  B\n        1  a  2\n        3  b  2\n        >>> df.groupby('A').head(1)\n           A  B\n        0  a  1\n        2  b  1\n        \"\"\"\n        self._reset_group_selection()\n        mask = self._cumcount_array(ascending=False) < n\n        return self._selected_obj[mask]\n\n\n@Appender(GroupBy.__doc__)\ndef groupby(obj, by, **kwds):\n    if isinstance(obj, Series):\n        klass = SeriesGroupBy\n    elif isinstance(obj, DataFrame):\n        klass = DataFrameGroupBy\n    else:  # pragma: no cover\n        raise TypeError('invalid type: %s' % type(obj))\n\n    return klass(obj, by, **kwds)\n\n\ndef _get_axes(group):\n    if isinstance(group, Series):\n        return [group.index]\n    else:\n        return group.axes\n\n\ndef _is_indexed_like(obj, axes):\n    if isinstance(obj, Series):\n        if len(axes) > 1:\n            return False\n        return obj.index.equals(axes[0])\n    elif isinstance(obj, DataFrame):\n        return obj.index.equals(axes[0])\n\n    return False\n\n\nclass BaseGrouper(object):\n    \"\"\"\n    This is an internal Grouper class, which actually holds\n    the generated groups\n    \"\"\"\n\n    def __init__(self, axis, groupings, sort=True, group_keys=True,\n                 mutated=False):\n        self._filter_empty_groups = self.compressed = len(groupings) != 1\n        self.axis = axis\n        self.groupings = groupings\n        self.sort = sort\n        self.group_keys = group_keys\n        self.mutated = mutated\n\n    @property\n    def shape(self):\n        return tuple(ping.ngroups for ping in self.groupings)\n\n    def __iter__(self):\n        return iter(self.indices)\n\n    @property\n    def nkeys(self):\n        return len(self.groupings)\n\n    def get_iterator(self, data, axis=0):\n        \"\"\"\n        Groupby iterator\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        splitter = self._get_splitter(data, axis=axis)\n        keys = self._get_group_keys()\n        for key, (i, group) in zip(keys, splitter):\n            yield key, group\n\n    def _get_splitter(self, data, axis=0):\n        comp_ids, _, ngroups = self.group_info\n        return get_splitter(data, comp_ids, ngroups, axis=axis)\n\n    def _get_group_keys(self):\n        if len(self.groupings) == 1:\n            return self.levels[0]\n        else:\n            comp_ids, _, ngroups = self.group_info\n            # provide \"flattened\" iterator for multi-group setting\n            mapper = _KeyMapper(comp_ids, ngroups, self.labels, self.levels)\n            return [mapper.get_key(i) for i in range(ngroups)]\n\n    def apply(self, f, data, axis=0):\n        mutated = self.mutated\n        splitter = self._get_splitter(data, axis=axis)\n        group_keys = self._get_group_keys()\n\n        # oh boy\n        f_name = com._get_callable_name(f)\n        if (f_name not in _plotting_methods and\n                hasattr(splitter, 'fast_apply') and axis == 0):\n            try:\n                values, mutated = splitter.fast_apply(f, group_keys)\n                return group_keys, values, mutated\n            except (lib.InvalidApply):\n                # we detect a mutation of some kind\n                # so take slow path\n                pass\n            except Exception:\n                # raise this error to the caller\n                pass\n\n        result_values = []\n        for key, (i, group) in zip(group_keys, splitter):\n            object.__setattr__(group, 'name', key)\n\n            # group might be modified\n            group_axes = _get_axes(group)\n            res = f(group)\n            if not _is_indexed_like(res, group_axes):\n                mutated = True\n            result_values.append(res)\n\n        return group_keys, result_values, mutated\n\n    @cache_readonly\n    def indices(self):\n        \"\"\" dict {group name -> group indices} \"\"\"\n        if len(self.groupings) == 1:\n            return self.groupings[0].indices\n        else:\n            label_list = [ping.labels for ping in self.groupings]\n            keys = [_values_from_object(ping.group_index)\n                    for ping in self.groupings]\n            return _get_indices_dict(label_list, keys)\n\n    @property\n    def labels(self):\n        return [ping.labels for ping in self.groupings]\n\n    @property\n    def levels(self):\n        return [ping.group_index for ping in self.groupings]\n\n    @property\n    def names(self):\n        return [ping.name for ping in self.groupings]\n\n    def size(self):\n        \"\"\"\n        Compute group sizes\n\n        \"\"\"\n        ids, _, ngroup = self.group_info\n        ids = _ensure_platform_int(ids)\n        out = np.bincount(ids[ids != -1], minlength=ngroup or None)\n        return Series(out, index=self.result_index, dtype='int64')\n\n    @cache_readonly\n    def _max_groupsize(self):\n        \"\"\"\n        Compute size of largest group\n        \"\"\"\n        # For many items in each group this is much faster than\n        # self.size().max(), in worst case marginally slower\n        if self.indices:\n            return max(len(v) for v in self.indices.values())\n        else:\n            return 0\n\n    @cache_readonly\n    def groups(self):\n        \"\"\" dict {group name -> group labels} \"\"\"\n        if len(self.groupings) == 1:\n            return self.groupings[0].groups\n        else:\n            to_groupby = lzip(*(ping.grouper for ping in self.groupings))\n            to_groupby = Index(to_groupby)\n            return self.axis.groupby(to_groupby)\n\n    @cache_readonly\n    def is_monotonic(self):\n        # return if my group orderings are monotonic\n        return Index(self.group_info[0]).is_monotonic\n\n    @cache_readonly\n    def group_info(self):\n        comp_ids, obs_group_ids = self._get_compressed_labels()\n\n        ngroups = len(obs_group_ids)\n        comp_ids = _ensure_int64(comp_ids)\n        return comp_ids, obs_group_ids, ngroups\n\n    def _get_compressed_labels(self):\n        all_labels = [ping.labels for ping in self.groupings]\n        if len(all_labels) > 1:\n            group_index = get_group_index(all_labels, self.shape,\n                                          sort=True, xnull=True)\n            return _compress_group_index(group_index, sort=self.sort)\n\n        ping = self.groupings[0]\n        return ping.labels, np.arange(len(ping.group_index))\n\n    @cache_readonly\n    def ngroups(self):\n        return len(self.result_index)\n\n    @property\n    def recons_labels(self):\n        comp_ids, obs_ids, _ = self.group_info\n        labels = (ping.labels for ping in self.groupings)\n        return decons_obs_group_ids(comp_ids,\n                                    obs_ids, self.shape, labels, xnull=True)\n\n    @cache_readonly\n    def result_index(self):\n        if not self.compressed and len(self.groupings) == 1:\n            return self.groupings[0].group_index.rename(self.names[0])\n\n        return MultiIndex(levels=[ping.group_index for ping in self.groupings],\n                          labels=self.recons_labels,\n                          verify_integrity=False,\n                          names=self.names)\n\n    def get_group_levels(self):\n        if not self.compressed and len(self.groupings) == 1:\n            return [self.groupings[0].group_index]\n\n        name_list = []\n        for ping, labels in zip(self.groupings, self.recons_labels):\n            labels = _ensure_platform_int(labels)\n            levels = ping.group_index.take(labels)\n\n            name_list.append(levels)\n\n        return name_list\n\n    # ------------------------------------------------------------\n    # Aggregation functions\n\n    _cython_functions = {\n        'aggregate': {\n            'add': 'group_add',\n            'prod': 'group_prod',\n            'min': 'group_min',\n            'max': 'group_max',\n            'mean': 'group_mean',\n            'median': {\n                'name': 'group_median'\n            },\n            'var': 'group_var',\n            'first': {\n                'name': 'group_nth',\n                'f': lambda func, a, b, c, d: func(a, b, c, d, 1)\n            },\n            'last': 'group_last',\n            'ohlc': 'group_ohlc',\n        },\n\n        'transform': {\n            'cumprod': 'group_cumprod',\n            'cumsum': 'group_cumsum',\n            'cummin': 'group_cummin',\n            'cummax': 'group_cummax',\n        }\n    }\n\n    _cython_arity = {\n        'ohlc': 4,  # OHLC\n    }\n\n    _name_functions = {\n        'ohlc': lambda *args: ['open', 'high', 'low', 'close']\n    }\n\n    def _get_cython_function(self, kind, how, values, is_numeric):\n\n        dtype_str = values.dtype.name\n\n        def get_func(fname):\n            # see if there is a fused-type version of function\n            # only valid for numeric\n            f = getattr(_algos, fname, None)\n            if f is not None and is_numeric:\n                return f\n\n            # otherwise find dtype-specific version, falling back to object\n            for dt in [dtype_str, 'object']:\n                f = getattr(_algos, \"%s_%s\" % (fname, dtype_str), None)\n                if f is not None:\n                    return f\n\n        ftype = self._cython_functions[kind][how]\n\n        if isinstance(ftype, dict):\n            func = afunc = get_func(ftype['name'])\n\n            # a sub-function\n            f = ftype.get('f')\n            if f is not None:\n\n                def wrapper(*args, **kwargs):\n                    return f(afunc, *args, **kwargs)\n\n                # need to curry our sub-function\n                func = wrapper\n\n        else:\n            func = get_func(ftype)\n\n        if func is None:\n            raise NotImplementedError(\"function is not implemented for this\"\n                                      \"dtype: [how->%s,dtype->%s]\" %\n                                      (how, dtype_str))\n        return func, dtype_str\n\n    def _cython_operation(self, kind, values, how, axis):\n        assert kind in ['transform', 'aggregate']\n\n        # can we do this operation with our cython functions\n        # if not raise NotImplementedError\n\n        # we raise NotImplemented if this is an invalid operation\n        # entirely, e.g. adding datetimes\n\n        # categoricals are only 1d, so we\n        # are not setup for dim transforming\n        if is_categorical_dtype(values):\n            raise NotImplementedError(\n                \"categoricals are not support in cython ops ATM\")\n        elif is_datetime64_any_dtype(values):\n            if how in ['add', 'prod', 'cumsum', 'cumprod']:\n                raise NotImplementedError(\n                    \"datetime64 type does not support {} \"\n                    \"operations\".format(how))\n        elif is_timedelta64_dtype(values):\n            if how in ['prod', 'cumprod']:\n                raise NotImplementedError(\n                    \"timedelta64 type does not support {} \"\n                    \"operations\".format(how))\n\n        arity = self._cython_arity.get(how, 1)\n\n        vdim = values.ndim\n        swapped = False\n        if vdim == 1:\n            values = values[:, None]\n            out_shape = (self.ngroups, arity)\n        else:\n            if axis > 0:\n                swapped = True\n                values = values.swapaxes(0, axis)\n            if arity > 1:\n                raise NotImplementedError(\"arity of more than 1 is not \"\n                                          \"supported for the 'how' argument\")\n            out_shape = (self.ngroups,) + values.shape[1:]\n\n        is_datetimelike = needs_i8_conversion(values.dtype)\n        is_numeric = is_numeric_dtype(values.dtype)\n\n        if is_datetimelike:\n            values = values.view('int64')\n            is_numeric = True\n        elif is_bool_dtype(values.dtype):\n            values = _ensure_float64(values)\n        elif is_integer_dtype(values):\n            # we use iNaT for the missing value on ints\n            # so pre-convert to guard this condition\n            if (values == tslib.iNaT).any():\n                values = _ensure_float64(values)\n            else:\n                values = values.astype('int64', copy=False)\n        elif is_numeric and not is_complex_dtype(values):\n            values = _ensure_float64(values)\n        else:\n            values = values.astype(object)\n\n        try:\n            func, dtype_str = self._get_cython_function(\n                kind, how, values, is_numeric)\n        except NotImplementedError:\n            if is_numeric:\n                values = _ensure_float64(values)\n                func, dtype_str = self._get_cython_function(\n                    kind, how, values, is_numeric)\n            else:\n                raise\n\n        if is_numeric:\n            out_dtype = '%s%d' % (values.dtype.kind, values.dtype.itemsize)\n        else:\n            out_dtype = 'object'\n\n        labels, _, _ = self.group_info\n\n        if kind == 'aggregate':\n            result = _maybe_fill(np.empty(out_shape, dtype=out_dtype),\n                                 fill_value=np.nan)\n            counts = np.zeros(self.ngroups, dtype=np.int64)\n            result = self._aggregate(\n                result, counts, values, labels, func, is_numeric,\n                is_datetimelike)\n        elif kind == 'transform':\n            result = _maybe_fill(np.empty_like(values, dtype=out_dtype),\n                                 fill_value=np.nan)\n\n            result = self._transform(\n                result, values, labels, func, is_numeric, is_datetimelike)\n\n        if is_integer_dtype(result):\n            mask = result == tslib.iNaT\n            if mask.any():\n                result = result.astype('float64')\n                result[mask] = np.nan\n\n        if kind == 'aggregate' and \\\n           self._filter_empty_groups and not counts.all():\n            if result.ndim == 2:\n                try:\n                    result = lib.row_bool_subset(\n                        result, (counts > 0).view(np.uint8))\n                except ValueError:\n                    result = lib.row_bool_subset_object(\n                        _ensure_object(result),\n                        (counts > 0).view(np.uint8))\n            else:\n                result = result[counts > 0]\n\n        if vdim == 1 and arity == 1:\n            result = result[:, 0]\n\n        if how in self._name_functions:\n            # TODO\n            names = self._name_functions[how]()\n        else:\n            names = None\n\n        if swapped:\n            result = result.swapaxes(0, axis)\n\n        return result, names\n\n    def aggregate(self, values, how, axis=0):\n        return self._cython_operation('aggregate', values, how, axis)\n\n    def transform(self, values, how, axis=0):\n        return self._cython_operation('transform', values, how, axis)\n\n    def _aggregate(self, result, counts, values, comp_ids, agg_func,\n                   is_numeric, is_datetimelike):\n        if values.ndim > 3:\n            # punting for now\n            raise NotImplementedError(\"number of dimensions is currently \"\n                                      \"limited to 3\")\n        elif values.ndim > 2:\n            for i, chunk in enumerate(values.transpose(2, 0, 1)):\n\n                chunk = chunk.squeeze()\n                agg_func(result[:, :, i], counts, chunk, comp_ids)\n        else:\n            agg_func(result, counts, values, comp_ids)\n\n        return result\n\n    def _transform(self, result, values, comp_ids, transform_func,\n                   is_numeric, is_datetimelike):\n\n        comp_ids, _, ngroups = self.group_info\n        if values.ndim > 3:\n            # punting for now\n            raise NotImplementedError(\"number of dimensions is currently \"\n                                      \"limited to 3\")\n        elif values.ndim > 2:\n            for i, chunk in enumerate(values.transpose(2, 0, 1)):\n\n                chunk = chunk.squeeze()\n                transform_func(result[:, :, i], values,\n                               comp_ids, is_datetimelike)\n        else:\n            transform_func(result, values, comp_ids, is_datetimelike)\n\n        return result\n\n    def agg_series(self, obj, func):\n        try:\n            return self._aggregate_series_fast(obj, func)\n        except Exception:\n            return self._aggregate_series_pure_python(obj, func)\n\n    def _aggregate_series_fast(self, obj, func):\n        func = self._is_builtin_func(func)\n\n        if obj.index._has_complex_internals:\n            raise TypeError('Incompatible index for Cython grouper')\n\n        group_index, _, ngroups = self.group_info\n\n        # avoids object / Series creation overhead\n        dummy = obj._get_values(slice(None, 0)).to_dense()\n        indexer = _get_group_index_sorter(group_index, ngroups)\n        obj = obj.take(indexer, convert=False)\n        group_index = algos.take_nd(group_index, indexer, allow_fill=False)\n        grouper = lib.SeriesGrouper(obj, func, group_index, ngroups,\n                                    dummy)\n        result, counts = grouper.get_result()\n        return result, counts\n\n    def _aggregate_series_pure_python(self, obj, func):\n\n        group_index, _, ngroups = self.group_info\n\n        counts = np.zeros(ngroups, dtype=int)\n        result = None\n\n        splitter = get_splitter(obj, group_index, ngroups, axis=self.axis)\n\n        for label, group in splitter:\n            res = func(group)\n            if result is None:\n                if (isinstance(res, (Series, Index, np.ndarray)) or\n                        isinstance(res, list)):\n                    raise ValueError('Function does not reduce')\n                result = np.empty(ngroups, dtype='O')\n\n            counts[label] = group.shape[0]\n            result[label] = res\n\n        result = lib.maybe_convert_objects(result, try_float=0)\n        return result, counts\n\n\ndef generate_bins_generic(values, binner, closed):\n    \"\"\"\n    Generate bin edge offsets and bin labels for one array using another array\n    which has bin edge values. Both arrays must be sorted.\n\n    Parameters\n    ----------\n    values : array of values\n    binner : a comparable array of values representing bins into which to bin\n        the first array. Note, 'values' end-points must fall within 'binner'\n        end-points.\n    closed : which end of bin is closed; left (default), right\n\n    Returns\n    -------\n    bins : array of offsets (into 'values' argument) of bins.\n        Zero and last edge are excluded in result, so for instance the first\n        bin is values[0:bin[0]] and the last is values[bin[-1]:]\n    \"\"\"\n    lenidx = len(values)\n    lenbin = len(binner)\n\n    if lenidx <= 0 or lenbin <= 0:\n        raise ValueError(\"Invalid length for values or for binner\")\n\n    # check binner fits data\n    if values[0] < binner[0]:\n        raise ValueError(\"Values falls before first bin\")\n\n    if values[lenidx - 1] > binner[lenbin - 1]:\n        raise ValueError(\"Values falls after last bin\")\n\n    bins = np.empty(lenbin - 1, dtype=np.int64)\n\n    j = 0  # index into values\n    bc = 0  # bin count\n\n    # linear scan, presume nothing about values/binner except that it fits ok\n    for i in range(0, lenbin - 1):\n        r_bin = binner[i + 1]\n\n        # count values in current bin, advance to next bin\n        while j < lenidx and (values[j] < r_bin or\n                              (closed == 'right' and values[j] == r_bin)):\n            j += 1\n\n        bins[bc] = j\n        bc += 1\n\n    return bins\n\n\nclass BinGrouper(BaseGrouper):\n\n    def __init__(self, bins, binlabels, filter_empty=False, mutated=False):\n        self.bins = _ensure_int64(bins)\n        self.binlabels = _ensure_index(binlabels)\n        self._filter_empty_groups = filter_empty\n        self.mutated = mutated\n\n    @cache_readonly\n    def groups(self):\n        \"\"\" dict {group name -> group labels} \"\"\"\n\n        # this is mainly for compat\n        # GH 3881\n        result = {}\n        for key, value in zip(self.binlabels, self.bins):\n            if key is not tslib.NaT:\n                result[key] = value\n        return result\n\n    @property\n    def nkeys(self):\n        return 1\n\n    def get_iterator(self, data, axis=0):\n        \"\"\"\n        Groupby iterator\n\n        Returns\n        -------\n        Generator yielding sequence of (name, subsetted object)\n        for each group\n        \"\"\"\n        if isinstance(data, NDFrame):\n            slicer = lambda start, edge: data._slice(\n                slice(start, edge), axis=axis)\n            length = len(data.axes[axis])\n        else:\n            slicer = lambda start, edge: data[slice(start, edge)]\n            length = len(data)\n\n        start = 0\n        for edge, label in zip(self.bins, self.binlabels):\n            if label is not tslib.NaT:\n                yield label, slicer(start, edge)\n            start = edge\n\n        if start < length:\n            yield self.binlabels[-1], slicer(start, None)\n\n    @cache_readonly\n    def indices(self):\n        indices = collections.defaultdict(list)\n\n        i = 0\n        for label, bin in zip(self.binlabels, self.bins):\n            if i < bin:\n                if label is not tslib.NaT:\n                    indices[label] = list(range(i, bin))\n                i = bin\n        return indices\n\n    @cache_readonly\n    def group_info(self):\n        ngroups = self.ngroups\n        obs_group_ids = np.arange(ngroups)\n        rep = np.diff(np.r_[0, self.bins])\n\n        rep = _ensure_platform_int(rep)\n        if ngroups == len(self.bins):\n            comp_ids = np.repeat(np.arange(ngroups), rep)\n        else:\n            comp_ids = np.repeat(np.r_[-1, np.arange(ngroups)], rep)\n\n        return comp_ids.astype('int64', copy=False), \\\n            obs_group_ids.astype('int64', copy=False), ngroups\n\n    @cache_readonly\n    def ngroups(self):\n        return len(self.result_index)\n\n    @cache_readonly\n    def result_index(self):\n        if len(self.binlabels) != 0 and isnull(self.binlabels[0]):\n            return self.binlabels[1:]\n\n        return self.binlabels\n\n    @property\n    def levels(self):\n        return [self.binlabels]\n\n    @property\n    def names(self):\n        return [self.binlabels.name]\n\n    @property\n    def groupings(self):\n        return [Grouping(lvl, lvl, in_axis=False, level=None, name=name)\n                for lvl, name in zip(self.levels, self.names)]\n\n    def agg_series(self, obj, func):\n        dummy = obj[:0]\n        grouper = lib.SeriesBinGrouper(obj, func, self.bins, dummy)\n        return grouper.get_result()\n\n    # ----------------------------------------------------------------------\n    # cython aggregation\n\n    _cython_functions = copy.deepcopy(BaseGrouper._cython_functions)\n\n\nclass Grouping(object):\n\n    \"\"\"\n    Holds the grouping information for a single key\n\n    Parameters\n    ----------\n    index : Index\n    grouper :\n    obj :\n    name :\n    level :\n    in_axis : if the Grouping is a column in self.obj and hence among\n        Groupby.exclusions list\n\n    Returns\n    -------\n    **Attributes**:\n      * indices : dict of {group -> index_list}\n      * labels : ndarray, group labels\n      * ids : mapping of label -> group\n      * counts : array of group counts\n      * group_index : unique groups\n      * groups : dict of {group -> label_list}\n    \"\"\"\n\n    def __init__(self, index, grouper=None, obj=None, name=None, level=None,\n                 sort=True, in_axis=False):\n\n        self.name = name\n        self.level = level\n        self.grouper = _convert_grouper(index, grouper)\n        self.index = index\n        self.sort = sort\n        self.obj = obj\n        self.in_axis = in_axis\n\n        # right place for this?\n        if isinstance(grouper, (Series, Index)) and name is None:\n            self.name = grouper.name\n\n        if isinstance(grouper, MultiIndex):\n            self.grouper = grouper.values\n\n        # pre-computed\n        self._should_compress = True\n\n        # we have a single grouper which may be a myriad of things,\n        # some of which are dependent on the passing in level\n\n        if level is not None:\n            if not isinstance(level, int):\n                if level not in index.names:\n                    raise AssertionError('Level %s not in index' % str(level))\n                level = index.names.index(level)\n\n            if self.name is None:\n                self.name = index.names[level]\n\n            self.grouper, self._labels, self._group_index = \\\n                index._get_grouper_for_level(self.grouper, level)\n\n        else:\n            if self.grouper is None and self.name is not None:\n                self.grouper = self.obj[self.name]\n\n            elif isinstance(self.grouper, (list, tuple)):\n                self.grouper = com._asarray_tuplesafe(self.grouper)\n\n            # a passed Categorical\n            elif is_categorical_dtype(self.grouper):\n\n                # must have an ordered categorical\n                if self.sort:\n                    if not self.grouper.ordered:\n\n                        # technically we cannot group on an unordered\n                        # Categorical\n                        # but this a user convenience to do so; the ordering\n                        # is preserved and if it's a reduction it doesn't make\n                        # any difference\n                        pass\n\n                # fix bug #GH8868 sort=False being ignored in categorical\n                # groupby\n                else:\n                    cat = self.grouper.unique()\n                    self.grouper = self.grouper.reorder_categories(\n                        cat.categories)\n\n                # we make a CategoricalIndex out of the cat grouper\n                # preserving the categories / ordered attributes\n                self._labels = self.grouper.codes\n\n                c = self.grouper.categories\n                self._group_index = CategoricalIndex(\n                    Categorical.from_codes(np.arange(len(c)),\n                                           categories=c,\n                                           ordered=self.grouper.ordered))\n\n            # a passed Grouper like\n            elif isinstance(self.grouper, Grouper):\n\n                # get the new grouper\n                grouper = self.grouper._get_binner_for_grouping(self.obj)\n                self.obj = self.grouper.obj\n                self.grouper = grouper\n                if self.name is None:\n                    self.name = grouper.name\n\n            # we are done\n            if isinstance(self.grouper, Grouping):\n                self.grouper = self.grouper.grouper\n\n            # no level passed\n            elif not isinstance(self.grouper,\n                                (Series, Index, Categorical, np.ndarray)):\n                if getattr(self.grouper, 'ndim', 1) != 1:\n                    t = self.name or str(type(self.grouper))\n                    raise ValueError(\"Grouper for '%s' not 1-dimensional\" % t)\n                self.grouper = self.index.map(self.grouper)\n                if not (hasattr(self.grouper, \"__len__\") and\n                        len(self.grouper) == len(self.index)):\n                    errmsg = ('Grouper result violates len(labels) == '\n                              'len(data)\\nresult: %s' %\n                              pprint_thing(self.grouper))\n                    self.grouper = None  # Try for sanity\n                    raise AssertionError(errmsg)\n\n        # if we have a date/time-like grouper, make sure that we have\n        # Timestamps like\n        if getattr(self.grouper, 'dtype', None) is not None:\n            if is_datetime64_dtype(self.grouper):\n                from pandas import to_datetime\n                self.grouper = to_datetime(self.grouper)\n            elif is_timedelta64_dtype(self.grouper):\n                from pandas import to_timedelta\n                self.grouper = to_timedelta(self.grouper)\n\n    def __repr__(self):\n        return 'Grouping({0})'.format(self.name)\n\n    def __iter__(self):\n        return iter(self.indices)\n\n    _labels = None\n    _group_index = None\n\n    @property\n    def ngroups(self):\n        return len(self.group_index)\n\n    @cache_readonly\n    def indices(self):\n        values = _ensure_categorical(self.grouper)\n        return values._reverse_indexer()\n\n    @property\n    def labels(self):\n        if self._labels is None:\n            self._make_labels()\n        return self._labels\n\n    @property\n    def group_index(self):\n        if self._group_index is None:\n            self._make_labels()\n        return self._group_index\n\n    def _make_labels(self):\n        if self._labels is None or self._group_index is None:\n            labels, uniques = algos.factorize(self.grouper, sort=self.sort)\n            uniques = Index(uniques, name=self.name)\n            self._labels = labels\n            self._group_index = uniques\n\n    @cache_readonly\n    def groups(self):\n        return self.index.groupby(Categorical.from_codes(self.labels,\n                                                         self.group_index))\n\n\ndef _get_grouper(obj, key=None, axis=0, level=None, sort=True,\n                 mutated=False):\n    \"\"\"\n    create and return a BaseGrouper, which is an internal\n    mapping of how to create the grouper indexers.\n    This may be composed of multiple Grouping objects, indicating\n    multiple groupers\n\n    Groupers are ultimately index mappings. They can originate as:\n    index mappings, keys to columns, functions, or Groupers\n\n    Groupers enable local references to axis,level,sort, while\n    the passed in axis, level, and sort are 'global'.\n\n    This routine tries to figure out what the passing in references\n    are and then creates a Grouping for each one, combined into\n    a BaseGrouper.\n\n    \"\"\"\n\n    group_axis = obj._get_axis(axis)\n\n    # validate that the passed level is compatible with the passed\n    # axis of the object\n    if level is not None:\n        if not isinstance(group_axis, MultiIndex):\n            # allow level to be a length-one list-like object\n            # (e.g., level=[0])\n            # GH 13901\n            if is_list_like(level):\n                nlevels = len(level)\n                if nlevels == 1:\n                    level = level[0]\n                elif nlevels == 0:\n                    raise ValueError('No group keys passed!')\n                else:\n                    raise ValueError('multiple levels only valid with '\n                                     'MultiIndex')\n\n            if isinstance(level, compat.string_types):\n                if obj.index.name != level:\n                    raise ValueError('level name %s is not the name of the '\n                                     'index' % level)\n            elif level > 0 or level < -1:\n                raise ValueError('level > 0 or level < -1 only valid with '\n                                 ' MultiIndex')\n\n            level = None\n            key = group_axis\n\n    # a passed-in Grouper, directly convert\n    if isinstance(key, Grouper):\n        binner, grouper, obj = key._get_grouper(obj)\n        if key.key is None:\n            return grouper, [], obj\n        else:\n            return grouper, set([key.key]), obj\n\n    # already have a BaseGrouper, just return it\n    elif isinstance(key, BaseGrouper):\n        return key, [], obj\n\n    if not isinstance(key, (tuple, list)):\n        keys = [key]\n        match_axis_length = False\n    else:\n        keys = key\n        match_axis_length = len(keys) == len(group_axis)\n\n    # what are we after, exactly?\n    any_callable = any(callable(g) or isinstance(g, dict) for g in keys)\n    any_groupers = any(isinstance(g, Grouper) for g in keys)\n    any_arraylike = any(isinstance(g, (list, tuple, Series, Index, np.ndarray))\n                        for g in keys)\n\n    try:\n        if isinstance(obj, DataFrame):\n            all_in_columns = all(g in obj.columns for g in keys)\n        else:\n            all_in_columns = False\n    except Exception:\n        all_in_columns = False\n\n    if not any_callable and not all_in_columns and \\\n       not any_arraylike and not any_groupers and \\\n       match_axis_length and level is None:\n        keys = [com._asarray_tuplesafe(keys)]\n\n    if isinstance(level, (tuple, list)):\n        if key is None:\n            keys = [None] * len(level)\n        levels = level\n    else:\n        levels = [level] * len(keys)\n\n    groupings = []\n    exclusions = []\n\n    # if the actual grouper should be obj[key]\n    def is_in_axis(key):\n        if not _is_label_like(key):\n            try:\n                obj._data.items.get_loc(key)\n            except Exception:\n                return False\n\n        return True\n\n    # if the the grouper is obj[name]\n    def is_in_obj(gpr):\n        try:\n            return id(gpr) == id(obj[gpr.name])\n        except Exception:\n            return False\n\n    for i, (gpr, level) in enumerate(zip(keys, levels)):\n\n        if is_in_obj(gpr):  # df.groupby(df['name'])\n            in_axis, name = True, gpr.name\n            exclusions.append(name)\n\n        elif is_in_axis(gpr):  # df.groupby('name')\n            if gpr in obj:\n                if gpr in obj.index.names:\n                    warnings.warn(\n                        (\"'%s' is both a column name and an index level.\\n\"\n                         \"Defaulting to column but \"\n                         \"this will raise an ambiguity error in a \"\n                         \"future version\") % gpr,\n                        FutureWarning, stacklevel=5)\n                in_axis, name, gpr = True, gpr, obj[gpr]\n                exclusions.append(name)\n            elif gpr in obj.index.names:\n                in_axis, name, level, gpr = False, None, gpr, None\n            else:\n                raise KeyError(gpr)\n        elif isinstance(gpr, Grouper) and gpr.key is not None:\n            # Add key to exclusions\n            exclusions.append(gpr.key)\n            in_axis, name = False, None\n        else:\n            in_axis, name = False, None\n\n        if is_categorical_dtype(gpr) and len(gpr) != len(obj):\n            raise ValueError(\"Categorical dtype grouper must \"\n                             \"have len(grouper) == len(data)\")\n\n        # create the Grouping\n        # allow us to passing the actual Grouping as the gpr\n        ping = Grouping(group_axis,\n                        gpr,\n                        obj=obj,\n                        name=name,\n                        level=level,\n                        sort=sort,\n                        in_axis=in_axis) \\\n            if not isinstance(gpr, Grouping) else gpr\n\n        groupings.append(ping)\n\n    if len(groupings) == 0:\n        raise ValueError('No group keys passed!')\n\n    # create the internals grouper\n    grouper = BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)\n\n    return grouper, exclusions, obj\n\n\ndef _is_label_like(val):\n    return (isinstance(val, compat.string_types) or\n            (val is not None and is_scalar(val)))\n\n\ndef _convert_grouper(axis, grouper):\n    if isinstance(grouper, dict):\n        return grouper.get\n    elif isinstance(grouper, Series):\n        if grouper.index.equals(axis):\n            return grouper._values\n        else:\n            return grouper.reindex(axis)._values\n    elif isinstance(grouper, (list, Series, Index, np.ndarray)):\n        if len(grouper) != len(axis):\n            raise AssertionError('Grouper and axis must be same length')\n        return grouper\n    else:\n        return grouper\n\n\ndef _whitelist_method_generator(klass, whitelist):\n    \"\"\"\n    Yields all GroupBy member defs for DataFrame/Series names in _whitelist.\n\n    Parameters\n    ----------\n    klass - class where members are defined.  Should be Series or DataFrame\n\n    whitelist - list of names of klass methods to be constructed\n\n    Returns\n    -------\n    The generator yields a sequence of strings, each suitable for exec'ing,\n    that define implementations of the named methods for DataFrameGroupBy\n    or SeriesGroupBy.\n\n    Since we don't want to override methods explicitly defined in the\n    base class, any such name is skipped.\n    \"\"\"\n\n    method_wrapper_template = \\\n        \"\"\"def %(name)s(%(sig)s) :\n    \\\"\"\"\n    %(doc)s\n    \\\"\"\"\n    f = %(self)s.__getattr__('%(name)s')\n    return f(%(args)s)\"\"\"\n    property_wrapper_template = \\\n        \"\"\"@property\ndef %(name)s(self) :\n    \\\"\"\"\n    %(doc)s\n    \\\"\"\"\n    return self.__getattr__('%(name)s')\"\"\"\n    for name in whitelist:\n        # don't override anything that was explicitly defined\n        # in the base class\n        if hasattr(GroupBy, name):\n            continue\n        # ugly, but we need the name string itself in the method.\n        f = getattr(klass, name)\n        doc = f.__doc__\n        doc = doc if type(doc) == str else ''\n        if isinstance(f, types.MethodType):\n            wrapper_template = method_wrapper_template\n            decl, args = make_signature(f)\n            # pass args by name to f because otherwise\n            # GroupBy._make_wrapper won't know whether\n            # we passed in an axis parameter.\n            args_by_name = ['{0}={0}'.format(arg) for arg in args[1:]]\n            params = {'name': name,\n                      'doc': doc,\n                      'sig': ','.join(decl),\n                      'self': args[0],\n                      'args': ','.join(args_by_name)}\n        else:\n            wrapper_template = property_wrapper_template\n            params = {'name': name, 'doc': doc}\n        yield wrapper_template % params\n\n\nclass SeriesGroupBy(GroupBy):\n    #\n    # Make class defs of attributes on SeriesGroupBy whitelist\n    _apply_whitelist = _series_apply_whitelist\n    for _def_str in _whitelist_method_generator(Series,\n                                                _series_apply_whitelist):\n        exec(_def_str)\n\n    @property\n    def name(self):\n        \"\"\"\n        since we are a series, we by definition only have\n        a single name, but may be the result of a selection or\n        the name of our object\n        \"\"\"\n        if self._selection is None:\n            return self.obj.name\n        else:\n            return self._selection\n\n    def aggregate(self, func_or_funcs, *args, **kwargs):\n        \"\"\"\n        Apply aggregation function or functions to groups, yielding most likely\n        Series but in some cases DataFrame depending on the output of the\n        aggregation function\n\n        Parameters\n        ----------\n        func_or_funcs : function or list / dict of functions\n            List/dict of functions will produce DataFrame with column names\n            determined by the function names themselves (list) or the keys in\n            the dict\n\n        Notes\n        -----\n        agg is an alias for aggregate. Use it.\n\n        Examples\n        --------\n        >>> series\n        bar    1.0\n        baz    2.0\n        qot    3.0\n        qux    4.0\n\n        >>> mapper = lambda x: x[0] # first letter\n        >>> grouped = series.groupby(mapper)\n\n        >>> grouped.aggregate(np.sum)\n        b    3.0\n        q    7.0\n\n        >>> grouped.aggregate([np.sum, np.mean, np.std])\n           mean  std  sum\n        b  1.5   0.5  3\n        q  3.5   0.5  7\n\n        >>> grouped.agg({'result' : lambda x: x.mean() / x.std(),\n        ...              'total' : np.sum})\n           result  total\n        b  2.121   3\n        q  4.95    7\n\n        See also\n        --------\n        apply, transform\n\n        Returns\n        -------\n        Series or DataFrame\n        \"\"\"\n        _level = kwargs.pop('_level', None)\n        if isinstance(func_or_funcs, compat.string_types):\n            return getattr(self, func_or_funcs)(*args, **kwargs)\n\n        if hasattr(func_or_funcs, '__iter__'):\n            ret = self._aggregate_multiple_funcs(func_or_funcs,\n                                                 (_level or 0) + 1)\n        else:\n            cyfunc = self._is_cython_func(func_or_funcs)\n            if cyfunc and not args and not kwargs:\n                return getattr(self, cyfunc)()\n\n            if self.grouper.nkeys > 1:\n                return self._python_agg_general(func_or_funcs, *args, **kwargs)\n\n            try:\n                return self._python_agg_general(func_or_funcs, *args, **kwargs)\n            except Exception:\n                result = self._aggregate_named(func_or_funcs, *args, **kwargs)\n\n            index = Index(sorted(result), name=self.grouper.names[0])\n            ret = Series(result, index=index)\n\n        if not self.as_index:  # pragma: no cover\n            print('Warning, ignoring as_index=True')\n\n        # _level handled at higher\n        if not _level and isinstance(ret, dict):\n            from pandas import concat\n            ret = concat(ret, axis=1)\n        return ret\n\n    agg = aggregate\n\n    def _aggregate_multiple_funcs(self, arg, _level):\n        if isinstance(arg, dict):\n            columns = list(arg.keys())\n            arg = list(arg.items())\n        elif any(isinstance(x, (tuple, list)) for x in arg):\n            arg = [(x, x) if not isinstance(x, (tuple, list)) else x\n                   for x in arg]\n\n            # indicated column order\n            columns = lzip(*arg)[0]\n        else:\n            # list of functions / function names\n            columns = []\n            for f in arg:\n                if isinstance(f, compat.string_types):\n                    columns.append(f)\n                else:\n                    # protect against callables without names\n                    columns.append(com._get_callable_name(f))\n            arg = lzip(columns, arg)\n\n        results = {}\n        for name, func in arg:\n            obj = self\n            if name in results:\n                raise SpecificationError('Function names must be unique, '\n                                         'found multiple named %s' % name)\n\n            # reset the cache so that we\n            # only include the named selection\n            if name in self._selected_obj:\n                obj = copy.copy(obj)\n                obj._reset_cache()\n                obj._selection = name\n            results[name] = obj.aggregate(func)\n\n        if isinstance(list(compat.itervalues(results))[0],\n                      DataFrame):\n\n            # let higher level handle\n            if _level:\n                return results\n            return list(compat.itervalues(results))[0]\n        return DataFrame(results, columns=columns)\n\n    def _wrap_output(self, output, index, names=None):\n        \"\"\" common agg/transform wrapping logic \"\"\"\n        output = output[self.name]\n\n        if names is not None:\n            return DataFrame(output, index=index, columns=names)\n        else:\n            name = self.name\n            if name is None:\n                name = self._selected_obj.name\n            return Series(output, index=index, name=name)\n\n    def _wrap_aggregated_output(self, output, names=None):\n        return self._wrap_output(output=output,\n                                 index=self.grouper.result_index,\n                                 names=names)\n\n    def _wrap_transformed_output(self, output, names=None):\n        return self._wrap_output(output=output,\n                                 index=self.obj.index,\n                                 names=names)\n\n    def _wrap_applied_output(self, keys, values, not_indexed_same=False):\n        if len(keys) == 0:\n            # GH #6265\n            return Series([], name=self.name, index=keys)\n\n        def _get_index():\n            if self.grouper.nkeys > 1:\n                index = MultiIndex.from_tuples(keys, names=self.grouper.names)\n            else:\n                index = Index(keys, name=self.grouper.names[0])\n            return index\n\n        if isinstance(values[0], dict):\n            # GH #823\n            index = _get_index()\n            result = DataFrame(values, index=index).stack()\n            result.name = self.name\n            return result\n\n        if isinstance(values[0], (Series, dict)):\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n        elif isinstance(values[0], DataFrame):\n            # possible that Series -> DataFrame by applied function\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n        else:\n            # GH #6265\n            return Series(values, index=_get_index(), name=self.name)\n\n    def _aggregate_named(self, func, *args, **kwargs):\n        result = {}\n\n        for name, group in self:\n            group.name = name\n            output = func(group, *args, **kwargs)\n            if isinstance(output, (Series, Index, np.ndarray)):\n                raise Exception('Must produce aggregated value')\n            result[name] = self._try_cast(output, group)\n\n        return result\n\n    def transform(self, func, *args, **kwargs):\n        \"\"\"\n        Call function producing a like-indexed Series on each group and return\n        a Series with the transformed values\n\n        Parameters\n        ----------\n        func : function\n            To apply to each group. Should return a Series with the same index\n\n        Examples\n        --------\n        >>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n\n        Returns\n        -------\n        transformed : Series\n        \"\"\"\n\n        func = self._is_cython_func(func) or func\n\n        # if string function\n        if isinstance(func, compat.string_types):\n            if func in _cython_transforms:\n                # cythonized transform\n                return getattr(self, func)(*args, **kwargs)\n            else:\n                # cythonized aggregation and merge\n                return self._transform_fast(\n                    lambda: getattr(self, func)(*args, **kwargs))\n\n        # reg transform\n        dtype = self._selected_obj.dtype\n        result = self._selected_obj.values.copy()\n\n        wrapper = lambda x: func(x, *args, **kwargs)\n        for i, (name, group) in enumerate(self):\n            object.__setattr__(group, 'name', name)\n            res = wrapper(group)\n\n            if hasattr(res, 'values'):\n                res = res.values\n\n            # may need to astype\n            try:\n                common_type = np.common_type(np.array(res), result)\n                if common_type != result.dtype:\n                    result = result.astype(common_type)\n            except:\n                pass\n\n            indexer = self._get_index(name)\n            result[indexer] = res\n\n        result = _possibly_downcast_to_dtype(result, dtype)\n        return self._selected_obj.__class__(result,\n                                            index=self._selected_obj.index,\n                                            name=self._selected_obj.name)\n\n    def _transform_fast(self, func):\n        \"\"\"\n        fast version of transform, only applicable to\n        builtin/cythonizable functions\n        \"\"\"\n        if isinstance(func, compat.string_types):\n            func = getattr(self, func)\n\n        ids, _, ngroup = self.grouper.group_info\n        cast = (self.size().fillna(0) > 0).any()\n        out = algos.take_1d(func().values, ids)\n        if cast:\n            out = self._try_cast(out, self.obj)\n        return Series(out, index=self.obj.index, name=self.obj.name)\n\n    def filter(self, func, dropna=True, *args, **kwargs):  # noqa\n        \"\"\"\n        Return a copy of a Series excluding elements from groups that\n        do not satisfy the boolean criterion specified by func.\n\n        Parameters\n        ----------\n        func : function\n            To apply to each group. Should return True or False.\n        dropna : Drop groups that do not pass the filter. True by default;\n            if False, groups that evaluate False are filled with NaNs.\n\n        Examples\n        --------\n        >>> grouped.filter(lambda x: x.mean() > 0)\n\n        Returns\n        -------\n        filtered : Series\n        \"\"\"\n        if isinstance(func, compat.string_types):\n            wrapper = lambda x: getattr(x, func)(*args, **kwargs)\n        else:\n            wrapper = lambda x: func(x, *args, **kwargs)\n\n        # Interpret np.nan as False.\n        def true_and_notnull(x, *args, **kwargs):\n            b = wrapper(x, *args, **kwargs)\n            return b and notnull(b)\n\n        try:\n            indices = [self._get_index(name) for name, group in self\n                       if true_and_notnull(group)]\n        except ValueError:\n            raise TypeError(\"the filter must return a boolean result\")\n        except TypeError:\n            raise TypeError(\"the filter must return a boolean result\")\n\n        filtered = self._apply_filter(indices, dropna)\n        return filtered\n\n    def nunique(self, dropna=True):\n        \"\"\" Returns number of unique elements in the group \"\"\"\n        ids, _, _ = self.grouper.group_info\n\n        val = self.obj.get_values()\n\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:  # catches object dtypes\n            assert val.dtype == object, \\\n                'val.dtype must be object, got %s' % val.dtype\n            val, _ = algos.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isnull = lambda a: a == -1\n        else:\n            _isnull = isnull\n\n        ids, val = ids[sorter], val[sorter]\n\n        # group boundaries are where group ids change\n        # unique observations are where sorted values change\n        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n\n        # 1st item of each group is a new unique observation\n        mask = _isnull(val)\n        if dropna:\n            inc[idx] = 1\n            inc[mask] = 0\n        else:\n            inc[mask & np.r_[False, mask[:-1]]] = 0\n            inc[idx] = 1\n\n        out = np.add.reduceat(inc, idx).astype('int64', copy=False)\n        if len(ids):\n            res = out if ids[0] != -1 else out[1:]\n        else:\n            res = out[1:]\n        ri = self.grouper.result_index\n\n        # we might have duplications among the bins\n        if len(res) != len(ri):\n            res, out = np.zeros(len(ri), dtype=out.dtype), res\n            res[ids] = out\n\n        return Series(res,\n                      index=ri,\n                      name=self.name)\n\n    @deprecate_kwarg('take_last', 'keep',\n                     mapping={True: 'last', False: 'first'})\n    @Appender(Series.nlargest.__doc__)\n    def nlargest(self, n=5, keep='first'):\n        # ToDo: When we remove deprecate_kwargs, we can remote these methods\n        # and include nlargest and nsmallest to _series_apply_whitelist\n        return self.apply(lambda x: x.nlargest(n=n, keep=keep))\n\n    @deprecate_kwarg('take_last', 'keep',\n                     mapping={True: 'last', False: 'first'})\n    @Appender(Series.nsmallest.__doc__)\n    def nsmallest(self, n=5, keep='first'):\n        return self.apply(lambda x: x.nsmallest(n=n, keep=keep))\n\n    @Appender(Series.describe.__doc__)\n    def describe(self, **kwargs):\n        self._set_group_selection()\n        result = self.apply(lambda x: x.describe(**kwargs))\n        if self.axis == 1:\n            return result.T\n        return result.unstack()\n\n    def value_counts(self, normalize=False, sort=True, ascending=False,\n                     bins=None, dropna=True):\n\n        from functools import partial\n        from pandas.tools.tile import cut\n        from pandas.tools.merge import _get_join_indexers\n\n        if bins is not None and not np.iterable(bins):\n            # scalar bins cannot be done at top level\n            # in a backward compatible way\n            return self.apply(Series.value_counts,\n                              normalize=normalize,\n                              sort=sort,\n                              ascending=ascending,\n                              bins=bins)\n\n        ids, _, _ = self.grouper.group_info\n        val = self.obj.get_values()\n\n        # groupby removes null keys from groupings\n        mask = ids != -1\n        ids, val = ids[mask], val[mask]\n\n        if bins is None:\n            lab, lev = algos.factorize(val, sort=True)\n        else:\n            cat, bins = cut(val, bins, retbins=True)\n            # bins[:-1] for backward compat;\n            # o.w. cat.categories could be better\n            lab, lev, dropna = cat.codes, bins[:-1], False\n\n        sorter = np.lexsort((lab, ids))\n        ids, lab = ids[sorter], lab[sorter]\n\n        # group boundaries are where group ids change\n        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n\n        # new values are where sorted labels change\n        inc = np.r_[True, lab[1:] != lab[:-1]]\n        inc[idx] = True  # group boundaries are also new values\n        out = np.diff(np.nonzero(np.r_[inc, True])[0])  # value counts\n\n        # num. of times each group should be repeated\n        rep = partial(np.repeat, repeats=np.add.reduceat(inc, idx))\n\n        # multi-index components\n        labels = list(map(rep, self.grouper.recons_labels)) + [lab[inc]]\n        levels = [ping.group_index for ping in self.grouper.groupings] + [lev]\n        names = self.grouper.names + [self.name]\n\n        if dropna:\n            mask = labels[-1] != -1\n            if mask.all():\n                dropna = False\n            else:\n                out, labels = out[mask], [label[mask] for label in labels]\n\n        if normalize:\n            out = out.astype('float')\n            d = np.diff(np.r_[idx, len(ids)])\n            if dropna:\n                m = ids[lab == -1]\n                if _np_version_under1p8:\n                    mi, ml = algos.factorize(m)\n                    d[ml] = d[ml] - np.bincount(mi)\n                else:\n                    np.add.at(d, m, -1)\n                acc = rep(d)[mask]\n            else:\n                acc = rep(d)\n            out /= acc\n\n        if sort and bins is None:\n            cat = ids[inc][mask] if dropna else ids[inc]\n            sorter = np.lexsort((out if ascending else -out, cat))\n            out, labels[-1] = out[sorter], labels[-1][sorter]\n\n        if bins is None:\n            mi = MultiIndex(levels=levels, labels=labels, names=names,\n                            verify_integrity=False)\n\n            if is_integer_dtype(out):\n                out = _ensure_int64(out)\n            return Series(out, index=mi, name=self.name)\n\n        # for compat. with algos.value_counts need to ensure every\n        # bin is present at every index level, null filled with zeros\n        diff = np.zeros(len(out), dtype='bool')\n        for lab in labels[:-1]:\n            diff |= np.r_[True, lab[1:] != lab[:-1]]\n\n        ncat, nbin = diff.sum(), len(levels[-1])\n\n        left = [np.repeat(np.arange(ncat), nbin),\n                np.tile(np.arange(nbin), ncat)]\n\n        right = [diff.cumsum() - 1, labels[-1]]\n\n        _, idx = _get_join_indexers(left, right, sort=False, how='left')\n        out = np.where(idx != -1, out[idx], 0)\n\n        if sort:\n            sorter = np.lexsort((out if ascending else -out, left[0]))\n            out, left[-1] = out[sorter], left[-1][sorter]\n\n        # build the multi-index w/ full levels\n        labels = list(map(lambda lab: np.repeat(lab[diff], nbin), labels[:-1]))\n        labels.append(left[-1])\n\n        mi = MultiIndex(levels=levels, labels=labels, names=names,\n                        verify_integrity=False)\n\n        if is_integer_dtype(out):\n            out = _ensure_int64(out)\n        return Series(out, index=mi, name=self.name)\n\n    def count(self):\n        \"\"\" Compute count of group, excluding missing values \"\"\"\n        ids, _, ngroups = self.grouper.group_info\n        val = self.obj.get_values()\n\n        mask = (ids != -1) & ~isnull(val)\n        ids = _ensure_platform_int(ids)\n        out = np.bincount(ids[mask], minlength=ngroups or None)\n\n        return Series(out,\n                      index=self.grouper.result_index,\n                      name=self.name,\n                      dtype='int64')\n\n    def _apply_to_column_groupbys(self, func):\n        \"\"\" return a pass thru \"\"\"\n        return func(self)\n\n\nclass NDFrameGroupBy(GroupBy):\n\n    def _iterate_slices(self):\n        if self.axis == 0:\n            # kludge\n            if self._selection is None:\n                slice_axis = self.obj.columns\n            else:\n                slice_axis = self._selection_list\n            slicer = lambda x: self.obj[x]\n        else:\n            slice_axis = self.obj.index\n            slicer = self.obj.xs\n\n        for val in slice_axis:\n            if val in self.exclusions:\n                continue\n            yield val, slicer(val)\n\n    def _cython_agg_general(self, how, alt=None, numeric_only=True):\n        new_items, new_blocks = self._cython_agg_blocks(\n            how, alt=alt, numeric_only=numeric_only)\n        return self._wrap_agged_blocks(new_items, new_blocks)\n\n    def _wrap_agged_blocks(self, items, blocks):\n        obj = self._obj_with_exclusions\n\n        new_axes = list(obj._data.axes)\n\n        # more kludge\n        if self.axis == 0:\n            new_axes[0], new_axes[1] = new_axes[1], self.grouper.result_index\n        else:\n            new_axes[self.axis] = self.grouper.result_index\n\n        # Make sure block manager integrity check passes.\n        assert new_axes[0].equals(items)\n        new_axes[0] = items\n\n        mgr = BlockManager(blocks, new_axes)\n\n        new_obj = type(obj)(mgr)\n\n        return self._post_process_cython_aggregate(new_obj)\n\n    _block_agg_axis = 0\n\n    def _cython_agg_blocks(self, how, alt=None, numeric_only=True):\n        # TODO: the actual managing of mgr_locs is a PITA\n        # here, it should happen via BlockManager.combine\n\n        data, agg_axis = self._get_data_to_aggregate()\n\n        if numeric_only:\n            data = data.get_numeric_data(copy=False)\n\n        new_blocks = []\n        new_items = []\n        deleted_items = []\n        for block in data.blocks:\n\n            locs = block.mgr_locs.as_array\n            try:\n                result, _ = self.grouper.aggregate(\n                    block.values, how, axis=agg_axis)\n            except NotImplementedError:\n                # generally if we have numeric_only=False\n                # and non-applicable functions\n                # try to python agg\n\n                if alt is None:\n                    # we cannot perform the operation\n                    # in an alternate way, exclude the block\n                    deleted_items.append(locs)\n                    continue\n\n                # call our grouper again with only this block\n                obj = self.obj[data.items[locs]]\n                s = groupby(obj, self.grouper)\n                result = s.aggregate(lambda x: alt(x, axis=self.axis))\n                result = result._data.blocks[0]\n\n            # see if we can cast the block back to the original dtype\n            result = block._try_coerce_and_cast_result(result)\n\n            new_items.append(locs)\n            newb = block.make_block_same_class(result)\n            new_blocks.append(newb)\n\n        if len(new_blocks) == 0:\n            raise DataError('No numeric types to aggregate')\n\n        # reset the locs in the blocks to correspond to our\n        # current ordering\n        indexer = np.concatenate(new_items)\n        new_items = data.items.take(np.sort(indexer))\n\n        if len(deleted_items):\n\n            # we need to adjust the indexer to account for the\n            # items we have removed\n            # really should be done in internals :<\n\n            deleted = np.concatenate(deleted_items)\n            ai = np.arange(len(data))\n            mask = np.zeros(len(data))\n            mask[deleted] = 1\n            indexer = (ai - mask.cumsum())[indexer]\n\n        offset = 0\n        for b in new_blocks:\n            l = len(b.mgr_locs)\n            b.mgr_locs = indexer[offset:(offset + l)]\n            offset += l\n\n        return new_items, new_blocks\n\n    def _get_data_to_aggregate(self):\n        obj = self._obj_with_exclusions\n        if self.axis == 0:\n            return obj.swapaxes(0, 1)._data, 1\n        else:\n            return obj._data, self.axis\n\n    def _post_process_cython_aggregate(self, obj):\n        # undoing kludge from below\n        if self.axis == 0:\n            obj = obj.swapaxes(0, 1)\n        return obj\n\n    def aggregate(self, arg, *args, **kwargs):\n\n        _level = kwargs.pop('_level', None)\n        result, how = self._aggregate(arg, _level=_level, *args, **kwargs)\n        if how is None:\n            return result\n\n        if result is None:\n\n            # grouper specific aggregations\n            if self.grouper.nkeys > 1:\n                return self._python_agg_general(arg, *args, **kwargs)\n            else:\n\n                # try to treat as if we are passing a list\n                try:\n                    assert not args and not kwargs\n                    result = self._aggregate_multiple_funcs(\n                        [arg], _level=_level)\n                    result.columns = Index(\n                        result.columns.levels[0],\n                        name=self._selected_obj.columns.name)\n                except:\n                    result = self._aggregate_generic(arg, *args, **kwargs)\n\n        if not self.as_index:\n            self._insert_inaxis_grouper_inplace(result)\n            result.index = np.arange(len(result))\n\n        return result._convert(datetime=True)\n\n    agg = aggregate\n\n    def _aggregate_generic(self, func, *args, **kwargs):\n        if self.grouper.nkeys != 1:\n            raise AssertionError('Number of keys must be 1')\n\n        axis = self.axis\n        obj = self._obj_with_exclusions\n\n        result = {}\n        if axis != obj._info_axis_number:\n            try:\n                for name, data in self:\n                    result[name] = self._try_cast(func(data, *args, **kwargs),\n                                                  data)\n            except Exception:\n                return self._aggregate_item_by_item(func, *args, **kwargs)\n        else:\n            for name in self.indices:\n                try:\n                    data = self.get_group(name, obj=obj)\n                    result[name] = self._try_cast(func(data, *args, **kwargs),\n                                                  data)\n                except Exception:\n                    wrapper = lambda x: func(x, *args, **kwargs)\n                    result[name] = data.apply(wrapper, axis=axis)\n\n        return self._wrap_generic_output(result, obj)\n\n    def _wrap_aggregated_output(self, output, names=None):\n        raise AbstractMethodError(self)\n\n    def _aggregate_item_by_item(self, func, *args, **kwargs):\n        # only for axis==0\n\n        obj = self._obj_with_exclusions\n        result = {}\n        cannot_agg = []\n        errors = None\n        for item in obj:\n            try:\n                data = obj[item]\n                colg = SeriesGroupBy(data, selection=item,\n                                     grouper=self.grouper)\n                result[item] = self._try_cast(\n                    colg.aggregate(func, *args, **kwargs), data)\n            except ValueError:\n                cannot_agg.append(item)\n                continue\n            except TypeError as e:\n                cannot_agg.append(item)\n                errors = e\n                continue\n\n        result_columns = obj.columns\n        if cannot_agg:\n            result_columns = result_columns.drop(cannot_agg)\n\n            # GH6337\n            if not len(result_columns) and errors is not None:\n                raise errors\n\n        return DataFrame(result, columns=result_columns)\n\n    def _decide_output_index(self, output, labels):\n        if len(output) == len(labels):\n            output_keys = labels\n        else:\n            output_keys = sorted(output)\n            try:\n                output_keys.sort()\n            except Exception:  # pragma: no cover\n                pass\n\n            if isinstance(labels, MultiIndex):\n                output_keys = MultiIndex.from_tuples(output_keys,\n                                                     names=labels.names)\n\n        return output_keys\n\n    def _wrap_applied_output(self, keys, values, not_indexed_same=False):\n        from pandas.core.index import _all_indexes_same\n\n        if len(keys) == 0:\n            return DataFrame(index=keys)\n\n        key_names = self.grouper.names\n\n        # GH12824.\n        def first_non_None_value(values):\n            try:\n                v = next(v for v in values if v is not None)\n            except StopIteration:\n                return None\n            return v\n\n        v = first_non_None_value(values)\n\n        if v is None:\n            # GH9684. If all values are None, then this will throw an error.\n            # We'd prefer it return an empty dataframe.\n            return DataFrame()\n        elif isinstance(v, DataFrame):\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n        elif self.grouper.groupings is not None:\n            if len(self.grouper.groupings) > 1:\n                key_index = MultiIndex.from_tuples(keys, names=key_names)\n\n            else:\n                ping = self.grouper.groupings[0]\n                if len(keys) == ping.ngroups:\n                    key_index = ping.group_index\n                    key_index.name = key_names[0]\n\n                    key_lookup = Index(keys)\n                    indexer = key_lookup.get_indexer(key_index)\n\n                    # reorder the values\n                    values = [values[i] for i in indexer]\n                else:\n\n                    key_index = Index(keys, name=key_names[0])\n\n                # don't use the key indexer\n                if not self.as_index:\n                    key_index = None\n\n            # make Nones an empty object\n            v = first_non_None_value(values)\n            if v is None:\n                return DataFrame()\n            elif isinstance(v, NDFrame):\n                values = [\n                    x if x is not None else\n                    v._constructor(**v._construct_axes_dict())\n                    for x in values\n                ]\n\n            v = values[0]\n\n            if isinstance(v, (np.ndarray, Index, Series)):\n                if isinstance(v, Series):\n                    applied_index = self._selected_obj._get_axis(self.axis)\n                    all_indexed_same = _all_indexes_same([\n                        x.index for x in values\n                    ])\n                    singular_series = (len(values) == 1 and\n                                       applied_index.nlevels == 1)\n\n                    # GH3596\n                    # provide a reduction (Frame -> Series) if groups are\n                    # unique\n                    if self.squeeze:\n\n                        # assign the name to this series\n                        if singular_series:\n                            values[0].name = keys[0]\n\n                            # GH2893\n                            # we have series in the values array, we want to\n                            # produce a series:\n                            # if any of the sub-series are not indexed the same\n                            # OR we don't have a multi-index and we have only a\n                            # single values\n                            return self._concat_objects(\n                                keys, values, not_indexed_same=not_indexed_same\n                            )\n\n                        # still a series\n                        # path added as of GH 5545\n                        elif all_indexed_same:\n                            from pandas.tools.merge import concat\n                            return concat(values)\n\n                    if not all_indexed_same:\n                        # GH 8467\n                        return self._concat_objects(\n                            keys, values, not_indexed_same=True,\n                        )\n\n                try:\n                    if self.axis == 0:\n                        # GH6124 if the list of Series have a consistent name,\n                        # then propagate that name to the result.\n                        index = v.index.copy()\n                        if index.name is None:\n                            # Only propagate the series name to the result\n                            # if all series have a consistent name.  If the\n                            # series do not have a consistent name, do\n                            # nothing.\n                            names = set(v.name for v in values)\n                            if len(names) == 1:\n                                index.name = list(names)[0]\n\n                        # normally use vstack as its faster than concat\n                        # and if we have mi-columns\n                        if isinstance(v.index,\n                                      MultiIndex) or key_index is None:\n                            stacked_values = np.vstack(map(np.asarray, values))\n                            result = DataFrame(stacked_values, index=key_index,\n                                               columns=index)\n                        else:\n                            # GH5788 instead of stacking; concat gets the\n                            # dtypes correct\n                            from pandas.tools.merge import concat\n                            result = concat(values, keys=key_index,\n                                            names=key_index.names,\n                                            axis=self.axis).unstack()\n                            result.columns = index\n                    else:\n                        stacked_values = np.vstack(map(np.asarray, values))\n                        result = DataFrame(stacked_values.T, index=v.index,\n                                           columns=key_index)\n\n                except (ValueError, AttributeError):\n                    # GH1738: values is list of arrays of unequal lengths fall\n                    # through to the outer else caluse\n                    return Series(values, index=key_index, name=self.name)\n\n                # if we have date/time like in the original, then coerce dates\n                # as we are stacking can easily have object dtypes here\n                so = self._selected_obj\n                if (so.ndim == 2 and so.dtypes.apply(is_datetimelike).any()):\n                    result = result._convert(numeric=True)\n                    date_cols = self._selected_obj.select_dtypes(\n                        include=['datetime', 'timedelta']).columns\n                    date_cols = date_cols.intersection(result.columns)\n                    result[date_cols] = (result[date_cols]\n                                         ._convert(datetime=True,\n                                                   coerce=True))\n                else:\n                    result = result._convert(datetime=True)\n\n                return self._reindex_output(result)\n\n            # values are not series or array-like but scalars\n            else:\n                # only coerce dates if we find at least 1 datetime\n                coerce = True if any([isinstance(x, Timestamp)\n                                      for x in values]) else False\n                # self.name not passed through to Series as the result\n                # should not take the name of original selection of columns\n                return (Series(values, index=key_index)\n                        ._convert(datetime=True,\n                                  coerce=coerce))\n\n        else:\n            # Handle cases like BinGrouper\n            return self._concat_objects(keys, values,\n                                        not_indexed_same=not_indexed_same)\n\n    def _transform_general(self, func, *args, **kwargs):\n        from pandas.tools.merge import concat\n\n        applied = []\n        obj = self._obj_with_exclusions\n        gen = self.grouper.get_iterator(obj, axis=self.axis)\n        fast_path, slow_path = self._define_paths(func, *args, **kwargs)\n\n        path = None\n        for name, group in gen:\n            object.__setattr__(group, 'name', name)\n\n            if path is None:\n                # Try slow path and fast path.\n                try:\n                    path, res = self._choose_path(fast_path, slow_path, group)\n                except TypeError:\n                    return self._transform_item_by_item(obj, fast_path)\n                except ValueError:\n                    msg = 'transform must return a scalar value for each group'\n                    raise ValueError(msg)\n            else:\n                res = path(group)\n\n            if isinstance(res, Series):\n\n                # we need to broadcast across the\n                # other dimension; this will preserve dtypes\n                # GH14457\n                if not np.prod(group.shape):\n                    continue\n                elif res.index.is_(obj.index):\n                    r = concat([res] * len(group.columns), axis=1)\n                    r.columns = group.columns\n                    r.index = group.index\n                else:\n                    r = DataFrame(\n                        np.concatenate([res.values] * len(group.index)\n                                       ).reshape(group.shape),\n                        columns=group.columns, index=group.index)\n\n                applied.append(r)\n            else:\n                applied.append(res)\n\n        concat_index = obj.columns if self.axis == 0 else obj.index\n        concatenated = concat(applied, join_axes=[concat_index],\n                              axis=self.axis, verify_integrity=False)\n        return self._set_result_index_ordered(concatenated)\n\n    def transform(self, func, *args, **kwargs):\n        \"\"\"\n        Call function producing a like-indexed DataFrame on each group and\n        return a DataFrame having the same indexes as the original object\n        filled with the transformed values\n\n        Parameters\n        ----------\n        f : function\n            Function to apply to each subframe\n\n        Notes\n        -----\n        Each subframe is endowed the attribute 'name' in case you need to know\n        which group you are working on.\n\n        Examples\n        --------\n        >>> grouped = df.groupby(lambda x: mapping[x])\n        >>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n        \"\"\"\n\n        # optimized transforms\n        func = self._is_cython_func(func) or func\n        if isinstance(func, compat.string_types):\n            if func in _cython_transforms:\n                # cythonized transform\n                return getattr(self, func)(*args, **kwargs)\n            else:\n                # cythonized aggregation and merge\n                result = getattr(self, func)(*args, **kwargs)\n        else:\n            return self._transform_general(func, *args, **kwargs)\n\n        # a reduction transform\n        if not isinstance(result, DataFrame):\n            return self._transform_general(func, *args, **kwargs)\n\n        obj = self._obj_with_exclusions\n        # nuiscance columns\n        if not result.columns.equals(obj.columns):\n            return self._transform_general(func, *args, **kwargs)\n\n        return self._transform_fast(result, obj)\n\n    def _transform_fast(self, result, obj):\n        \"\"\"\n        Fast transform path for aggregations\n        \"\"\"\n        # if there were groups with no observations (Categorical only?)\n        # try casting data to original dtype\n        cast = (self.size().fillna(0) > 0).any()\n\n        # for each col, reshape to to size of original frame\n        # by take operation\n        ids, _, ngroup = self.grouper.group_info\n        output = []\n        for i, _ in enumerate(result.columns):\n            res = algos.take_1d(result.iloc[:, i].values, ids)\n            if cast:\n                res = self._try_cast(res, obj.iloc[:, i])\n            output.append(res)\n\n        return DataFrame._from_arrays(output, columns=result.columns,\n                                      index=obj.index)\n\n    def _define_paths(self, func, *args, **kwargs):\n        if isinstance(func, compat.string_types):\n            fast_path = lambda group: getattr(group, func)(*args, **kwargs)\n            slow_path = lambda group: group.apply(\n                lambda x: getattr(x, func)(*args, **kwargs), axis=self.axis)\n        else:\n            fast_path = lambda group: func(group, *args, **kwargs)\n            slow_path = lambda group: group.apply(\n                lambda x: func(x, *args, **kwargs), axis=self.axis)\n        return fast_path, slow_path\n\n    def _choose_path(self, fast_path, slow_path, group):\n        path = slow_path\n        res = slow_path(group)\n\n        # if we make it here, test if we can use the fast path\n        try:\n            res_fast = fast_path(group)\n\n            # compare that we get the same results\n            if res.shape == res_fast.shape:\n                res_r = res.values.ravel()\n                res_fast_r = res_fast.values.ravel()\n                mask = notnull(res_r)\n            if (res_r[mask] == res_fast_r[mask]).all():\n                path = fast_path\n\n        except:\n            pass\n        return path, res\n\n    def _transform_item_by_item(self, obj, wrapper):\n        # iterate through columns\n        output = {}\n        inds = []\n        for i, col in enumerate(obj):\n            try:\n                output[col] = self[col].transform(wrapper)\n                inds.append(i)\n            except Exception:\n                pass\n\n        if len(output) == 0:  # pragma: no cover\n            raise TypeError('Transform function invalid for data types')\n\n        columns = obj.columns\n        if len(output) < len(obj.columns):\n            columns = columns.take(inds)\n\n        return DataFrame(output, index=obj.index, columns=columns)\n\n    def filter(self, func, dropna=True, *args, **kwargs):  # noqa\n        \"\"\"\n        Return a copy of a DataFrame excluding elements from groups that\n        do not satisfy the boolean criterion specified by func.\n\n        Parameters\n        ----------\n        f : function\n            Function to apply to each subframe. Should return True or False.\n        dropna : Drop groups that do not pass the filter. True by default;\n            if False, groups that evaluate False are filled with NaNs.\n\n        Notes\n        -----\n        Each subframe is endowed the attribute 'name' in case you need to know\n        which group you are working on.\n\n        Examples\n        --------\n        >>> grouped = df.groupby(lambda x: mapping[x])\n        >>> grouped.filter(lambda x: x['A'].sum() + x['B'].sum() > 0)\n        \"\"\"\n\n        indices = []\n\n        obj = self._selected_obj\n        gen = self.grouper.get_iterator(obj, axis=self.axis)\n\n        for name, group in gen:\n            object.__setattr__(group, 'name', name)\n\n            res = func(group, *args, **kwargs)\n\n            try:\n                res = res.squeeze()\n            except AttributeError:  # allow e.g., scalars and frames to pass\n                pass\n\n            # interpret the result of the filter\n            if is_bool(res) or (is_scalar(res) and isnull(res)):\n                if res and notnull(res):\n                    indices.append(self._get_index(name))\n            else:\n                # non scalars aren't allowed\n                raise TypeError(\"filter function returned a %s, \"\n                                \"but expected a scalar bool\" %\n                                type(res).__name__)\n\n        return self._apply_filter(indices, dropna)\n\n\nclass DataFrameGroupBy(NDFrameGroupBy):\n    _apply_whitelist = _dataframe_apply_whitelist\n    #\n    # Make class defs of attributes on DataFrameGroupBy whitelist.\n    for _def_str in _whitelist_method_generator(DataFrame, _apply_whitelist):\n        exec(_def_str)\n\n    _block_agg_axis = 1\n\n    @Substitution(name='groupby')\n    @Appender(SelectionMixin._see_also_template)\n    @Appender(SelectionMixin._agg_doc)\n    def aggregate(self, arg, *args, **kwargs):\n        return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n\n    agg = aggregate\n\n    def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n\n        if ndim == 2:\n            if subset is None:\n                subset = self.obj\n            return DataFrameGroupBy(subset, self.grouper, selection=key,\n                                    grouper=self.grouper,\n                                    exclusions=self.exclusions,\n                                    as_index=self.as_index)\n        elif ndim == 1:\n            if subset is None:\n                subset = self.obj[key]\n            return SeriesGroupBy(subset, selection=key,\n                                 grouper=self.grouper)\n\n        raise AssertionError(\"invalid ndim for _gotitem\")\n\n    def _wrap_generic_output(self, result, obj):\n        result_index = self.grouper.levels[0]\n\n        if self.axis == 0:\n            return DataFrame(result, index=obj.columns,\n                             columns=result_index).T\n        else:\n            return DataFrame(result, index=obj.index,\n                             columns=result_index)\n\n    def _get_data_to_aggregate(self):\n        obj = self._obj_with_exclusions\n        if self.axis == 1:\n            return obj.T._data, 1\n        else:\n            return obj._data, 1\n\n    def _insert_inaxis_grouper_inplace(self, result):\n        # zip in reverse so we can always insert at loc 0\n        izip = zip(* map(reversed, (\n            self.grouper.names,\n            self.grouper.get_group_levels(),\n            [grp.in_axis for grp in self.grouper.groupings])))\n\n        for name, lev, in_axis in izip:\n            if in_axis:\n                result.insert(0, name, lev)\n\n    def _wrap_aggregated_output(self, output, names=None):\n        agg_axis = 0 if self.axis == 1 else 1\n        agg_labels = self._obj_with_exclusions._get_axis(agg_axis)\n\n        output_keys = self._decide_output_index(output, agg_labels)\n\n        if not self.as_index:\n            result = DataFrame(output, columns=output_keys)\n            self._insert_inaxis_grouper_inplace(result)\n            result = result.consolidate()\n        else:\n            index = self.grouper.result_index\n            result = DataFrame(output, index=index, columns=output_keys)\n\n        if self.axis == 1:\n            result = result.T\n\n        return self._reindex_output(result)._convert(datetime=True)\n\n    def _wrap_transformed_output(self, output, names=None):\n        return DataFrame(output, index=self.obj.index)\n\n    def _wrap_agged_blocks(self, items, blocks):\n        if not self.as_index:\n            index = np.arange(blocks[0].values.shape[1])\n            mgr = BlockManager(blocks, [items, index])\n            result = DataFrame(mgr)\n\n            self._insert_inaxis_grouper_inplace(result)\n            result = result.consolidate()\n        else:\n            index = self.grouper.result_index\n            mgr = BlockManager(blocks, [items, index])\n            result = DataFrame(mgr)\n\n        if self.axis == 1:\n            result = result.T\n\n        return self._reindex_output(result)._convert(datetime=True)\n\n    def _reindex_output(self, result):\n        \"\"\"\n        if we have categorical groupers, then we want to make sure that\n        we have a fully reindex-output to the levels. These may have not\n        participated in the groupings (e.g. may have all been\n        nan groups)\n\n        This can re-expand the output space\n        \"\"\"\n        groupings = self.grouper.groupings\n        if groupings is None:\n            return result\n        elif len(groupings) == 1:\n            return result\n        elif not any([isinstance(ping.grouper, (Categorical, CategoricalIndex))\n                      for ping in groupings]):\n            return result\n\n        levels_list = [ping.group_index for ping in groupings]\n        index, _ = MultiIndex.from_product(\n            levels_list, names=self.grouper.names).sortlevel()\n\n        if self.as_index:\n            d = {self.obj._get_axis_name(self.axis): index, 'copy': False}\n            return result.reindex(**d)\n\n        # GH 13204\n        # Here, the categorical in-axis groupers, which need to be fully\n        # expanded, are columns in `result`. An idea is to do:\n        # result = result.set_index(self.grouper.names)\n        #                .reindex(index).reset_index()\n        # but special care has to be taken because of possible not-in-axis\n        # groupers.\n        # So, we manually select and drop the in-axis grouper columns,\n        # reindex `result`, and then reset the in-axis grouper columns.\n\n        # Select in-axis groupers\n        in_axis_grps = [(i, ping.name) for (i, ping)\n                        in enumerate(groupings) if ping.in_axis]\n        g_nums, g_names = zip(*in_axis_grps)\n\n        result = result.drop(labels=list(g_names), axis=1)\n\n        # Set a temp index and reindex (possibly expanding)\n        result = result.set_index(self.grouper.result_index\n                                  ).reindex(index, copy=False)\n\n        # Reset in-axis grouper columns\n        # (using level numbers `g_nums` because level names may not be unique)\n        result = result.reset_index(level=g_nums)\n\n        return result.reset_index(drop=True)\n\n    def _iterate_column_groupbys(self):\n        for i, colname in enumerate(self._selected_obj.columns):\n            yield colname, SeriesGroupBy(self._selected_obj.iloc[:, i],\n                                         selection=colname,\n                                         grouper=self.grouper,\n                                         exclusions=self.exclusions)\n\n    def _apply_to_column_groupbys(self, func):\n        from pandas.tools.merge import concat\n        return concat(\n            (func(col_groupby) for _, col_groupby\n             in self._iterate_column_groupbys()),\n            keys=self._selected_obj.columns, axis=1)\n\n    def count(self):\n        \"\"\" Compute count of group, excluding missing values \"\"\"\n        from functools import partial\n        from pandas.lib import count_level_2d\n        from pandas.types.missing import _isnull_ndarraylike as isnull\n\n        data, _ = self._get_data_to_aggregate()\n        ids, _, ngroups = self.grouper.group_info\n        mask = ids != -1\n\n        val = ((mask & ~isnull(blk.get_values())) for blk in data.blocks)\n        loc = (blk.mgr_locs for blk in data.blocks)\n\n        counter = partial(count_level_2d, labels=ids, max_bin=ngroups, axis=1)\n        blk = map(make_block, map(counter, val), loc)\n\n        return self._wrap_agged_blocks(data.items, list(blk))\n\n    def nunique(self, dropna=True):\n        \"\"\"\n        Return DataFrame with number of distinct observations per group for\n        each column.\n\n        .. versionadded:: 0.20.0\n\n        Parameters\n        ----------\n        dropna : boolean, default True\n            Don't include NaN in the counts.\n\n        Returns\n        -------\n        nunique: DataFrame\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n        ...                           'ham', 'ham'],\n        ...                    'value1': [1, 5, 5, 2, 5, 5],\n        ...                    'value2': list('abbaxy')})\n        >>> df\n             id  value1 value2\n        0  spam       1      a\n        1   egg       5      b\n        2   egg       5      b\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n\n        >>> df.groupby('id').nunique()\n            id  value1  value2\n        id\n        egg    1       1       1\n        ham    1       1       2\n        spam   1       2       1\n\n        # check for rows with the same id but conflicting values\n        >>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n             id  value1 value2\n        0  spam       1      a\n        3  spam       2      a\n        4   ham       5      x\n        5   ham       5      y\n        \"\"\"\n\n        obj = self._selected_obj\n\n        def groupby_series(obj, col=None):\n            return SeriesGroupBy(obj,\n                                 selection=col,\n                                 grouper=self.grouper).nunique(dropna=dropna)\n\n        if isinstance(obj, Series):\n            results = groupby_series(obj)\n        else:\n            from pandas.tools.merge import concat\n            results = [groupby_series(obj[col], col) for col in obj.columns]\n            results = concat(results, axis=1)\n\n        if not self.as_index:\n            results.index = _default_index(len(results))\n        return results\n\n\nfrom pandas.tools.plotting import boxplot_frame_groupby  # noqa\nDataFrameGroupBy.boxplot = boxplot_frame_groupby\n\n\nclass PanelGroupBy(NDFrameGroupBy):\n\n    @Substitution(name='groupby')\n    @Appender(SelectionMixin._see_also_template)\n    @Appender(SelectionMixin._agg_doc)\n    def aggregate(self, arg, *args, **kwargs):\n        return super(PanelGroupBy, self).aggregate(arg, *args, **kwargs)\n\n    agg = aggregate\n\n    def _iterate_slices(self):\n        if self.axis == 0:\n            # kludge\n            if self._selection is None:\n                slice_axis = self._selected_obj.items\n            else:\n                slice_axis = self._selection_list\n            slicer = lambda x: self._selected_obj[x]\n        else:\n            raise NotImplementedError(\"axis other than 0 is not supported\")\n\n        for val in slice_axis:\n            if val in self.exclusions:\n                continue\n\n            yield val, slicer(val)\n\n    def aggregate(self, arg, *args, **kwargs):\n        \"\"\"\n        Aggregate using input function or dict of {column -> function}\n\n        Parameters\n        ----------\n        arg : function or dict\n            Function to use for aggregating groups. If a function, must either\n            work when passed a Panel or when passed to Panel.apply. If\n            pass a dict, the keys must be DataFrame column names\n\n        Returns\n        -------\n        aggregated : Panel\n        \"\"\"\n        if isinstance(arg, compat.string_types):\n            return getattr(self, arg)(*args, **kwargs)\n\n        return self._aggregate_generic(arg, *args, **kwargs)\n\n    def _wrap_generic_output(self, result, obj):\n        if self.axis == 0:\n            new_axes = list(obj.axes)\n            new_axes[0] = self.grouper.result_index\n        elif self.axis == 1:\n            x, y, z = obj.axes\n            new_axes = [self.grouper.result_index, z, x]\n        else:\n            x, y, z = obj.axes\n            new_axes = [self.grouper.result_index, y, x]\n\n        result = Panel._from_axes(result, new_axes)\n\n        if self.axis == 1:\n            result = result.swapaxes(0, 1).swapaxes(0, 2)\n        elif self.axis == 2:\n            result = result.swapaxes(0, 2)\n\n        return result\n\n    def _aggregate_item_by_item(self, func, *args, **kwargs):\n        obj = self._obj_with_exclusions\n        result = {}\n\n        if self.axis > 0:\n            for item in obj:\n                try:\n                    itemg = DataFrameGroupBy(obj[item],\n                                             axis=self.axis - 1,\n                                             grouper=self.grouper)\n                    result[item] = itemg.aggregate(func, *args, **kwargs)\n                except (ValueError, TypeError):\n                    raise\n            new_axes = list(obj.axes)\n            new_axes[self.axis] = self.grouper.result_index\n            return Panel._from_axes(result, new_axes)\n        else:\n            raise ValueError(\"axis value must be greater than 0\")\n\n    def _wrap_aggregated_output(self, output, names=None):\n        raise AbstractMethodError(self)\n\n\nclass NDArrayGroupBy(GroupBy):\n    pass\n\n\n# ----------------------------------------------------------------------\n# Splitting / application\n\n\nclass DataSplitter(object):\n\n    def __init__(self, data, labels, ngroups, axis=0):\n        self.data = data\n        self.labels = _ensure_int64(labels)\n        self.ngroups = ngroups\n\n        self.axis = axis\n\n    @cache_readonly\n    def slabels(self):\n        # Sorted labels\n        return algos.take_nd(self.labels, self.sort_idx, allow_fill=False)\n\n    @cache_readonly\n    def sort_idx(self):\n        # Counting sort indexer\n        return _get_group_index_sorter(self.labels, self.ngroups)\n\n    def __iter__(self):\n        sdata = self._get_sorted_data()\n\n        if self.ngroups == 0:\n            # we are inside a generator, rather than raise StopIteration\n            # we merely return signal the end\n            return\n\n        starts, ends = lib.generate_slices(self.slabels, self.ngroups)\n\n        for i, (start, end) in enumerate(zip(starts, ends)):\n            # Since I'm now compressing the group ids, it's now not \"possible\"\n            # to produce empty slices because such groups would not be observed\n            # in the data\n            # if start >= end:\n            #     raise AssertionError('Start %s must be less than end %s'\n            #                          % (str(start), str(end)))\n            yield i, self._chop(sdata, slice(start, end))\n\n    def _get_sorted_data(self):\n        return self.data.take(self.sort_idx, axis=self.axis, convert=False)\n\n    def _chop(self, sdata, slice_obj):\n        return sdata.iloc[slice_obj]\n\n    def apply(self, f):\n        raise AbstractMethodError(self)\n\n\nclass ArraySplitter(DataSplitter):\n    pass\n\n\nclass SeriesSplitter(DataSplitter):\n\n    def _chop(self, sdata, slice_obj):\n        return sdata._get_values(slice_obj).to_dense()\n\n\nclass FrameSplitter(DataSplitter):\n\n    def __init__(self, data, labels, ngroups, axis=0):\n        super(FrameSplitter, self).__init__(data, labels, ngroups, axis=axis)\n\n    def fast_apply(self, f, names):\n        # must return keys::list, values::list, mutated::bool\n        try:\n            starts, ends = lib.generate_slices(self.slabels, self.ngroups)\n        except:\n            # fails when all -1\n            return [], True\n\n        sdata = self._get_sorted_data()\n        results, mutated = lib.apply_frame_axis0(sdata, f, names, starts, ends)\n\n        return results, mutated\n\n    def _chop(self, sdata, slice_obj):\n        if self.axis == 0:\n            return sdata.iloc[slice_obj]\n        else:\n            return sdata._slice(slice_obj, axis=1)  # .loc[:, slice_obj]\n\n\nclass NDFrameSplitter(DataSplitter):\n\n    def __init__(self, data, labels, ngroups, axis=0):\n        super(NDFrameSplitter, self).__init__(data, labels, ngroups, axis=axis)\n\n        self.factory = data._constructor\n\n    def _get_sorted_data(self):\n        # this is the BlockManager\n        data = self.data._data\n\n        # this is sort of wasteful but...\n        sorted_axis = data.axes[self.axis].take(self.sort_idx)\n        sorted_data = data.reindex_axis(sorted_axis, axis=self.axis)\n\n        return sorted_data\n\n    def _chop(self, sdata, slice_obj):\n        return self.factory(sdata.get_slice(slice_obj, axis=self.axis))\n\n\ndef get_splitter(data, *args, **kwargs):\n    if isinstance(data, Series):\n        klass = SeriesSplitter\n    elif isinstance(data, DataFrame):\n        klass = FrameSplitter\n    else:\n        klass = NDFrameSplitter\n\n    return klass(data, *args, **kwargs)\n\n\n# ----------------------------------------------------------------------\n# Misc utilities\n\n\ndef get_group_index(labels, shape, sort, xnull):\n    \"\"\"\n    For the particular label_list, gets the offsets into the hypothetical list\n    representing the totally ordered cartesian product of all possible label\n    combinations, *as long as* this space fits within int64 bounds;\n    otherwise, though group indices identify unique combinations of\n    labels, they cannot be deconstructed.\n    - If `sort`, rank of returned ids preserve lexical ranks of labels.\n      i.e. returned id's can be used to do lexical sort on labels;\n    - If `xnull` nulls (-1 labels) are passed through.\n\n    Parameters\n    ----------\n    labels: sequence of arrays\n        Integers identifying levels at each location\n    shape: sequence of ints same length as labels\n        Number of unique levels at each location\n    sort: boolean\n        If the ranks of returned ids should match lexical ranks of labels\n    xnull: boolean\n        If true nulls are excluded. i.e. -1 values in the labels are\n        passed through\n    Returns\n    -------\n    An array of type int64 where two elements are equal if their corresponding\n    labels are equal at all location.\n    \"\"\"\n    def _int64_cut_off(shape):\n        acc = long(1)\n        for i, mul in enumerate(shape):\n            acc *= long(mul)\n            if not acc < _INT64_MAX:\n                return i\n        return len(shape)\n\n    def loop(labels, shape):\n        # how many levels can be done without overflow:\n        nlev = _int64_cut_off(shape)\n\n        # compute flat ids for the first `nlev` levels\n        stride = np.prod(shape[1:nlev], dtype='i8')\n        out = stride * labels[0].astype('i8', subok=False, copy=False)\n\n        for i in range(1, nlev):\n            if shape[i] == 0:\n                stride = 0\n            else:\n                stride //= shape[i]\n            out += labels[i] * stride\n\n        if xnull:  # exclude nulls\n            mask = labels[0] == -1\n            for lab in labels[1:nlev]:\n                mask |= lab == -1\n            out[mask] = -1\n\n        if nlev == len(shape):  # all levels done!\n            return out\n\n        # compress what has been done so far in order to avoid overflow\n        # to retain lexical ranks, obs_ids should be sorted\n        comp_ids, obs_ids = _compress_group_index(out, sort=sort)\n\n        labels = [comp_ids] + labels[nlev:]\n        shape = [len(obs_ids)] + shape[nlev:]\n\n        return loop(labels, shape)\n\n    def maybe_lift(lab, size):  # pormote nan values\n        return (lab + 1, size + 1) if (lab == -1).any() else (lab, size)\n\n    labels = map(_ensure_int64, labels)\n    if not xnull:\n        labels, shape = map(list, zip(*map(maybe_lift, labels, shape)))\n\n    return loop(list(labels), list(shape))\n\n\n_INT64_MAX = np.iinfo(np.int64).max\n\n\ndef _int64_overflow_possible(shape):\n    the_prod = long(1)\n    for x in shape:\n        the_prod *= long(x)\n\n    return the_prod >= _INT64_MAX\n\n\ndef decons_group_index(comp_labels, shape):\n    # reconstruct labels\n    if _int64_overflow_possible(shape):\n        # at some point group indices are factorized,\n        # and may not be deconstructed here! wrong path!\n        raise ValueError('cannot deconstruct factorized group indices!')\n\n    label_list = []\n    factor = 1\n    y = 0\n    x = comp_labels\n    for i in reversed(range(len(shape))):\n        labels = (x - y) % (factor * shape[i]) // factor\n        np.putmask(labels, comp_labels < 0, -1)\n        label_list.append(labels)\n        y = labels * factor\n        factor *= shape[i]\n    return label_list[::-1]\n\n\ndef decons_obs_group_ids(comp_ids, obs_ids, shape, labels, xnull):\n    \"\"\"\n    reconstruct labels from observed group ids\n\n    Parameters\n    ----------\n    xnull: boolean,\n        if nulls are excluded; i.e. -1 labels are passed through\n    \"\"\"\n    from pandas.hashtable import unique_label_indices\n\n    if not xnull:\n        lift = np.fromiter(((a == -1).any() for a in labels), dtype='i8')\n        shape = np.asarray(shape, dtype='i8') + lift\n\n    if not _int64_overflow_possible(shape):\n        # obs ids are deconstructable! take the fast route!\n        out = decons_group_index(obs_ids, shape)\n        return out if xnull or not lift.any() \\\n            else [x - y for x, y in zip(out, lift)]\n\n    i = unique_label_indices(comp_ids)\n    i8copy = lambda a: a.astype('i8', subok=False, copy=True)\n    return [i8copy(lab[i]) for lab in labels]\n\n\ndef _indexer_from_factorized(labels, shape, compress=True):\n    ids = get_group_index(labels, shape, sort=True, xnull=False)\n\n    if not compress:\n        ngroups = (ids.size and ids.max()) + 1\n    else:\n        ids, obs = _compress_group_index(ids, sort=True)\n        ngroups = len(obs)\n\n    return _get_group_index_sorter(ids, ngroups)\n\n\ndef _lexsort_indexer(keys, orders=None, na_position='last'):\n    labels = []\n    shape = []\n    if isinstance(orders, bool):\n        orders = [orders] * len(keys)\n    elif orders is None:\n        orders = [True] * len(keys)\n\n    for key, order in zip(keys, orders):\n\n        # we are already a Categorical\n        if is_categorical_dtype(key):\n            c = key\n\n        # create the Categorical\n        else:\n            c = Categorical(key, ordered=True)\n\n        if na_position not in ['last', 'first']:\n            raise ValueError('invalid na_position: {!r}'.format(na_position))\n\n        n = len(c.categories)\n        codes = c.codes.copy()\n\n        mask = (c.codes == -1)\n        if order:  # ascending\n            if na_position == 'last':\n                codes = np.where(mask, n, codes)\n            elif na_position == 'first':\n                codes += 1\n        else:  # not order means descending\n            if na_position == 'last':\n                codes = np.where(mask, n, n - codes - 1)\n            elif na_position == 'first':\n                codes = np.where(mask, 0, n - codes)\n        if mask.any():\n            n += 1\n\n        shape.append(n)\n        labels.append(codes)\n\n    return _indexer_from_factorized(labels, shape)\n\n\ndef _nargsort(items, kind='quicksort', ascending=True, na_position='last'):\n    \"\"\"\n    This is intended to be a drop-in replacement for np.argsort which\n    handles NaNs. It adds ascending and na_position parameters.\n    GH #6399, #5231\n    \"\"\"\n\n    # specially handle Categorical\n    if is_categorical_dtype(items):\n        return items.argsort(ascending=ascending)\n\n    items = np.asanyarray(items)\n    idx = np.arange(len(items))\n    mask = isnull(items)\n    non_nans = items[~mask]\n    non_nan_idx = idx[~mask]\n    nan_idx = np.nonzero(mask)[0]\n    if not ascending:\n        non_nans = non_nans[::-1]\n        non_nan_idx = non_nan_idx[::-1]\n    indexer = non_nan_idx[non_nans.argsort(kind=kind)]\n    if not ascending:\n        indexer = indexer[::-1]\n    # Finally, place the NaNs at the end or the beginning according to\n    # na_position\n    if na_position == 'last':\n        indexer = np.concatenate([indexer, nan_idx])\n    elif na_position == 'first':\n        indexer = np.concatenate([nan_idx, indexer])\n    else:\n        raise ValueError('invalid na_position: {!r}'.format(na_position))\n    return indexer\n\n\nclass _KeyMapper(object):\n\n    \"\"\"\n    Ease my suffering. Map compressed group id -> key tuple\n    \"\"\"\n\n    def __init__(self, comp_ids, ngroups, labels, levels):\n        self.levels = levels\n        self.labels = labels\n        self.comp_ids = comp_ids.astype(np.int64)\n\n        self.k = len(labels)\n        self.tables = [_hash.Int64HashTable(ngroups) for _ in range(self.k)]\n\n        self._populate_tables()\n\n    def _populate_tables(self):\n        for labs, table in zip(self.labels, self.tables):\n            table.map(self.comp_ids, labs.astype(np.int64))\n\n    def get_key(self, comp_id):\n        return tuple(level[table.get_item(comp_id)]\n                     for table, level in zip(self.tables, self.levels))\n\n\ndef _get_indices_dict(label_list, keys):\n    shape = list(map(len, keys))\n\n    group_index = get_group_index(label_list, shape, sort=True, xnull=True)\n    ngroups = ((group_index.size and group_index.max()) + 1) \\\n        if _int64_overflow_possible(shape) \\\n        else np.prod(shape, dtype='i8')\n\n    sorter = _get_group_index_sorter(group_index, ngroups)\n\n    sorted_labels = [lab.take(sorter) for lab in label_list]\n    group_index = group_index.take(sorter)\n\n    return lib.indices_fast(sorter, group_index, keys, sorted_labels)\n\n\n# ----------------------------------------------------------------------\n# sorting levels...cleverly?\n\ndef _get_group_index_sorter(group_index, ngroups):\n    \"\"\"\n    _algos.groupsort_indexer implements `counting sort` and it is at least\n    O(ngroups), where\n        ngroups = prod(shape)\n        shape = map(len, keys)\n    that is, linear in the number of combinations (cartesian product) of unique\n    values of groupby keys. This can be huge when doing multi-key groupby.\n    np.argsort(kind='mergesort') is O(count x log(count)) where count is the\n    length of the data-frame;\n    Both algorithms are `stable` sort and that is necessary for correctness of\n    groupby operations. e.g. consider:\n        df.groupby(key)[col].transform('first')\n    \"\"\"\n    count = len(group_index)\n    alpha = 0.0  # taking complexities literally; there may be\n    beta = 1.0  # some room for fine-tuning these parameters\n    do_groupsort = (count > 0 and ((alpha + beta * ngroups) <\n                                   (count * np.log(count))))\n    if do_groupsort:\n        sorter, _ = _algos.groupsort_indexer(_ensure_int64(group_index),\n                                             ngroups)\n        return _ensure_platform_int(sorter)\n    else:\n        return group_index.argsort(kind='mergesort')\n\n\ndef _compress_group_index(group_index, sort=True):\n    \"\"\"\n    Group_index is offsets into cartesian product of all possible labels. This\n    space can be huge, so this function compresses it, by computing offsets\n    (comp_ids) into the list of unique labels (obs_group_ids).\n    \"\"\"\n\n    size_hint = min(len(group_index), _hash._SIZE_HINT_LIMIT)\n    table = _hash.Int64HashTable(size_hint)\n\n    group_index = _ensure_int64(group_index)\n\n    # note, group labels come out ascending (ie, 1,2,3 etc)\n    comp_ids, obs_group_ids = table.get_labels_groupby(group_index)\n\n    if sort and len(obs_group_ids) > 0:\n        obs_group_ids, comp_ids = _reorder_by_uniques(obs_group_ids, comp_ids)\n\n    return comp_ids, obs_group_ids\n\n\ndef _reorder_by_uniques(uniques, labels):\n    # sorter is index where elements ought to go\n    sorter = uniques.argsort()\n\n    # reverse_indexer is where elements came from\n    reverse_indexer = np.empty(len(sorter), dtype=np.int64)\n    reverse_indexer.put(sorter, np.arange(len(sorter)))\n\n    mask = labels < 0\n\n    # move labels to right locations (ie, unsort ascending labels)\n    labels = algos.take_nd(reverse_indexer, labels, allow_fill=False)\n    np.putmask(labels, mask, -1)\n\n    # sort observed ids\n    uniques = algos.take_nd(uniques, sorter, allow_fill=False)\n\n    return uniques, labels\n\n\ndef numpy_groupby(data, labels, axis=0):\n    s = np.argsort(labels)\n    keys, inv = np.unique(labels, return_inverse=True)\n    i = inv.take(s)\n    groups_at = np.where(i != np.concatenate(([-1], i[:-1])))[0]\n    ordered_data = data.take(s, axis=axis)\n    group_sums = np.add.reduceat(ordered_data, groups_at, axis=axis)\n\n    return group_sums\n"
    },
    {
      "filename": "pandas/tests/formats/test_format.py",
      "content": "# -*- coding: utf-8 -*-\n\n# TODO(wesm): lots of issues making flake8 hard\n# flake8: noqa\n\nfrom __future__ import print_function\nfrom distutils.version import LooseVersion\nimport re\n\nfrom pandas.compat import (range, zip, lrange, StringIO, PY3,\n                           u, lzip, is_platform_windows,\n                           is_platform_32bit)\nimport pandas.compat as compat\nimport itertools\nfrom operator import methodcaller\nimport os\nimport sys\nfrom textwrap import dedent\nimport warnings\n\nfrom numpy import nan\nfrom numpy.random import randn\nimport numpy as np\n\nimport codecs\n\ndiv_style = ''\ntry:\n    import IPython\n    if IPython.__version__ < LooseVersion('3.0.0'):\n        div_style = ' style=\"max-width:1500px;overflow:auto;\"'\nexcept (ImportError, AttributeError):\n    pass\n\nfrom pandas import DataFrame, Series, Index, Timestamp, MultiIndex, date_range, NaT\n\nimport pandas.formats.format as fmt\nimport pandas.util.testing as tm\nimport pandas.core.common as com\nimport pandas.formats.printing as printing\nfrom pandas.util.terminal import get_terminal_size\nimport pandas as pd\nfrom pandas.core.config import (set_option, get_option, option_context,\n                                reset_option)\nfrom datetime import datetime\n\nimport nose\n\nuse_32bit_repr = is_platform_windows() or is_platform_32bit()\n\n_frame = DataFrame(tm.getSeriesData())\n\n\ndef curpath():\n    pth, _ = os.path.split(os.path.abspath(__file__))\n    return pth\n\n\ndef has_info_repr(df):\n    r = repr(df)\n    c1 = r.split('\\n')[0].startswith(\"<class\")\n    c2 = r.split('\\n')[0].startswith(r\"&lt;class\")  # _repr_html_\n    return c1 or c2\n\n\ndef has_non_verbose_info_repr(df):\n    has_info = has_info_repr(df)\n    r = repr(df)\n    nv = len(r.split(\n        '\\n')) == 6  # 1. <class>, 2. Index, 3. Columns, 4. dtype, 5. memory usage, 6. trailing newline\n    return has_info and nv\n\n\ndef has_horizontally_truncated_repr(df):\n    try:  # Check header row\n        fst_line = np.array(repr(df).splitlines()[0].split())\n        cand_col = np.where(fst_line == '...')[0][0]\n    except:\n        return False\n    # Make sure each row has this ... in the same place\n    r = repr(df)\n    for ix, l in enumerate(r.splitlines()):\n        if not r.split()[cand_col] == '...':\n            return False\n    return True\n\n\ndef has_vertically_truncated_repr(df):\n    r = repr(df)\n    only_dot_row = False\n    for row in r.splitlines():\n        if re.match(r'^[\\.\\ ]+$', row):\n            only_dot_row = True\n    return only_dot_row\n\n\ndef has_truncated_repr(df):\n    return has_horizontally_truncated_repr(\n        df) or has_vertically_truncated_repr(df)\n\n\ndef has_doubly_truncated_repr(df):\n    return has_horizontally_truncated_repr(\n        df) and has_vertically_truncated_repr(df)\n\n\ndef has_expanded_repr(df):\n    r = repr(df)\n    for line in r.split('\\n'):\n        if line.endswith('\\\\'):\n            return True\n    return False\n\n\nclass TestDataFrameFormatting(tm.TestCase):\n\n    def setUp(self):\n        self.warn_filters = warnings.filters\n        warnings.filterwarnings('ignore', category=FutureWarning,\n                                module=\".*format\")\n\n        self.frame = _frame.copy()\n\n    def tearDown(self):\n        warnings.filters = self.warn_filters\n\n    def test_repr_embedded_ndarray(self):\n        arr = np.empty(10, dtype=[('err', object)])\n        for i in range(len(arr)):\n            arr['err'][i] = np.random.randn(i)\n\n        df = DataFrame(arr)\n        repr(df['err'])\n        repr(df)\n        df.to_string()\n\n    def test_eng_float_formatter(self):\n        self.frame.loc[5] = 0\n\n        fmt.set_eng_float_format()\n        repr(self.frame)\n\n        fmt.set_eng_float_format(use_eng_prefix=True)\n        repr(self.frame)\n\n        fmt.set_eng_float_format(accuracy=0)\n        repr(self.frame)\n        self.reset_display_options()\n\n    def test_show_null_counts(self):\n\n        df = DataFrame(1, columns=range(10), index=range(10))\n        df.iloc[1, 1] = np.nan\n\n        def check(null_counts, result):\n            buf = StringIO()\n            df.info(buf=buf, null_counts=null_counts)\n            self.assertTrue(('non-null' in buf.getvalue()) is result)\n\n        with option_context('display.max_info_rows', 20,\n                            'display.max_info_columns', 20):\n            check(None, True)\n            check(True, True)\n            check(False, False)\n\n        with option_context('display.max_info_rows', 5,\n                            'display.max_info_columns', 5):\n            check(None, False)\n            check(True, False)\n            check(False, False)\n\n    def test_repr_tuples(self):\n        buf = StringIO()\n\n        df = DataFrame({'tups': lzip(range(10), range(10))})\n        repr(df)\n        df.to_string(col_space=10, buf=buf)\n\n    def test_repr_truncation(self):\n        max_len = 20\n        with option_context(\"display.max_colwidth\", max_len):\n            df = DataFrame({'A': np.random.randn(10),\n                            'B': [tm.rands(np.random.randint(\n                                max_len - 1, max_len + 1)) for i in range(10)\n            ]})\n            r = repr(df)\n            r = r[r.find('\\n') + 1:]\n\n            adj = fmt._get_adjustment()\n\n            for line, value in lzip(r.split('\\n'), df['B']):\n                if adj.len(value) + 1 > max_len:\n                    self.assertIn('...', line)\n                else:\n                    self.assertNotIn('...', line)\n\n        with option_context(\"display.max_colwidth\", 999999):\n            self.assertNotIn('...', repr(df))\n\n        with option_context(\"display.max_colwidth\", max_len + 2):\n            self.assertNotIn('...', repr(df))\n\n    def test_repr_chop_threshold(self):\n        df = DataFrame([[0.1, 0.5], [0.5, -0.1]])\n        pd.reset_option(\"display.chop_threshold\")  # default None\n        self.assertEqual(repr(df), '     0    1\\n0  0.1  0.5\\n1  0.5 -0.1')\n\n        with option_context(\"display.chop_threshold\", 0.2):\n            self.assertEqual(repr(df), '     0    1\\n0  0.0  0.5\\n1  0.5  0.0')\n\n        with option_context(\"display.chop_threshold\", 0.6):\n            self.assertEqual(repr(df), '     0    1\\n0  0.0  0.0\\n1  0.0  0.0')\n\n        with option_context(\"display.chop_threshold\", None):\n            self.assertEqual(repr(df), '     0    1\\n0  0.1  0.5\\n1  0.5 -0.1')\n\n    def test_repr_obeys_max_seq_limit(self):\n        with option_context(\"display.max_seq_items\", 2000):\n            self.assertTrue(len(printing.pprint_thing(lrange(1000))) > 1000)\n\n        with option_context(\"display.max_seq_items\", 5):\n            self.assertTrue(len(printing.pprint_thing(lrange(1000))) < 100)\n\n    def test_repr_set(self):\n        self.assertEqual(printing.pprint_thing(set([1])), '{1}')\n\n    def test_repr_is_valid_construction_code(self):\n        # for the case of Index, where the repr is traditional rather then\n        # stylized\n        idx = Index(['a', 'b'])\n        res = eval(\"pd.\" + repr(idx))\n        tm.assert_series_equal(Series(res), Series(idx))\n\n    def test_repr_should_return_str(self):\n        # http://docs.python.org/py3k/reference/datamodel.html#object.__repr__\n        # http://docs.python.org/reference/datamodel.html#object.__repr__\n        # \"...The return value must be a string object.\"\n\n        # (str on py2.x, str (unicode) on py3)\n\n        data = [8, 5, 3, 5]\n        index1 = [u(\"\\u03c3\"), u(\"\\u03c4\"), u(\"\\u03c5\"), u(\"\\u03c6\")]\n        cols = [u(\"\\u03c8\")]\n        df = DataFrame(data, columns=cols, index=index1)\n        self.assertTrue(type(df.__repr__()) == str)  # both py2 / 3\n\n    def test_repr_no_backslash(self):\n        with option_context('mode.sim_interactive', True):\n            df = DataFrame(np.random.randn(10, 4))\n            self.assertTrue('\\\\' not in repr(df))\n\n    def test_expand_frame_repr(self):\n        df_small = DataFrame('hello', [0], [0])\n        df_wide = DataFrame('hello', [0], lrange(10))\n        df_tall = DataFrame('hello', lrange(30), lrange(5))\n\n        with option_context('mode.sim_interactive', True):\n            with option_context('display.max_columns', 10, 'display.width', 20,\n                                'display.max_rows', 20,\n                                'display.show_dimensions', True):\n                with option_context('display.expand_frame_repr', True):\n                    self.assertFalse(has_truncated_repr(df_small))\n                    self.assertFalse(has_expanded_repr(df_small))\n                    self.assertFalse(has_truncated_repr(df_wide))\n                    self.assertTrue(has_expanded_repr(df_wide))\n                    self.assertTrue(has_vertically_truncated_repr(df_tall))\n                    self.assertTrue(has_expanded_repr(df_tall))\n\n                with option_context('display.expand_frame_repr', False):\n                    self.assertFalse(has_truncated_repr(df_small))\n                    self.assertFalse(has_expanded_repr(df_small))\n                    self.assertFalse(has_horizontally_truncated_repr(df_wide))\n                    self.assertFalse(has_expanded_repr(df_wide))\n                    self.assertTrue(has_vertically_truncated_repr(df_tall))\n                    self.assertFalse(has_expanded_repr(df_tall))\n\n    def test_repr_non_interactive(self):\n        # in non interactive mode, there can be no dependency on the\n        # result of terminal auto size detection\n        df = DataFrame('hello', lrange(1000), lrange(5))\n\n        with option_context('mode.sim_interactive', False, 'display.width', 0,\n                            'display.height', 0, 'display.max_rows', 5000):\n            self.assertFalse(has_truncated_repr(df))\n            self.assertFalse(has_expanded_repr(df))\n\n    def test_repr_max_columns_max_rows(self):\n        term_width, term_height = get_terminal_size()\n        if term_width < 10 or term_height < 10:\n            raise nose.SkipTest(\"terminal size too small, \"\n                                \"{0} x {1}\".format(term_width, term_height))\n\n        def mkframe(n):\n            index = ['%05d' % i for i in range(n)]\n            return DataFrame(0, index, index)\n\n        df6 = mkframe(6)\n        df10 = mkframe(10)\n        with option_context('mode.sim_interactive', True):\n            with option_context('display.width', term_width * 2):\n                with option_context('display.max_rows', 5,\n                                    'display.max_columns', 5):\n                    self.assertFalse(has_expanded_repr(mkframe(4)))\n                    self.assertFalse(has_expanded_repr(mkframe(5)))\n                    self.assertFalse(has_expanded_repr(df6))\n                    self.assertTrue(has_doubly_truncated_repr(df6))\n\n                with option_context('display.max_rows', 20,\n                                    'display.max_columns', 10):\n                    # Out off max_columns boundary, but no extending\n                    # since not exceeding width\n                    self.assertFalse(has_expanded_repr(df6))\n                    self.assertFalse(has_truncated_repr(df6))\n\n                with option_context('display.max_rows', 9,\n                                    'display.max_columns', 10):\n                    # out vertical bounds can not result in exanded repr\n                    self.assertFalse(has_expanded_repr(df10))\n                    self.assertTrue(has_vertically_truncated_repr(df10))\n\n            # width=None in terminal, auto detection\n            with option_context('display.max_columns', 100, 'display.max_rows',\n                                term_width * 20, 'display.width', None):\n                df = mkframe((term_width // 7) - 2)\n                self.assertFalse(has_expanded_repr(df))\n                df = mkframe((term_width // 7) + 2)\n                printing.pprint_thing(df._repr_fits_horizontal_())\n                self.assertTrue(has_expanded_repr(df))\n\n    def test_str_max_colwidth(self):\n        # GH 7856\n        df = pd.DataFrame([{'a': 'foo',\n                            'b': 'bar',\n                            'c': 'uncomfortably long line with lots of stuff',\n                            'd': 1}, {'a': 'foo',\n                                      'b': 'bar',\n                                      'c': 'stuff',\n                                      'd': 1}])\n        df.set_index(['a', 'b', 'c'])\n        self.assertTrue(\n            str(df) ==\n            '     a    b                                           c  d\\n'\n            '0  foo  bar  uncomfortably long line with lots of stuff  1\\n'\n            '1  foo  bar                                       stuff  1')\n        with option_context('max_colwidth', 20):\n            self.assertTrue(str(df) == '     a    b                    c  d\\n'\n                            '0  foo  bar  uncomfortably lo...  1\\n'\n                            '1  foo  bar                stuff  1')\n\n    def test_auto_detect(self):\n        term_width, term_height = get_terminal_size()\n        fac = 1.05  # Arbitrary large factor to exceed term widht\n        cols = range(int(term_width * fac))\n        index = range(10)\n        df = DataFrame(index=index, columns=cols)\n        with option_context('mode.sim_interactive', True):\n            with option_context('max_rows', None):\n                with option_context('max_columns', None):\n                    # Wrap around with None\n                    self.assertTrue(has_expanded_repr(df))\n            with option_context('max_rows', 0):\n                with option_context('max_columns', 0):\n                    # Truncate with auto detection.\n                    self.assertTrue(has_horizontally_truncated_repr(df))\n\n            index = range(int(term_height * fac))\n            df = DataFrame(index=index, columns=cols)\n            with option_context('max_rows', 0):\n                with option_context('max_columns', None):\n                    # Wrap around with None\n                    self.assertTrue(has_expanded_repr(df))\n                    # Truncate vertically\n                    self.assertTrue(has_vertically_truncated_repr(df))\n\n            with option_context('max_rows', None):\n                with option_context('max_columns', 0):\n                    self.assertTrue(has_horizontally_truncated_repr(df))\n\n    def test_to_string_repr_unicode(self):\n        buf = StringIO()\n\n        unicode_values = [u('\\u03c3')] * 10\n        unicode_values = np.array(unicode_values, dtype=object)\n        df = DataFrame({'unicode': unicode_values})\n        df.to_string(col_space=10, buf=buf)\n\n        # it works!\n        repr(df)\n\n        idx = Index(['abc', u('\\u03c3a'), 'aegdvg'])\n        ser = Series(np.random.randn(len(idx)), idx)\n        rs = repr(ser).split('\\n')\n        line_len = len(rs[0])\n        for line in rs[1:]:\n            try:\n                line = line.decode(get_option(\"display.encoding\"))\n            except:\n                pass\n            if not line.startswith('dtype:'):\n                self.assertEqual(len(line), line_len)\n\n        # it works even if sys.stdin in None\n        _stdin = sys.stdin\n        try:\n            sys.stdin = None\n            repr(df)\n        finally:\n            sys.stdin = _stdin\n\n    def test_to_string_unicode_columns(self):\n        df = DataFrame({u('\\u03c3'): np.arange(10.)})\n\n        buf = StringIO()\n        df.to_string(buf=buf)\n        buf.getvalue()\n\n        buf = StringIO()\n        df.info(buf=buf)\n        buf.getvalue()\n\n        result = self.frame.to_string()\n        tm.assertIsInstance(result, compat.text_type)\n\n    def test_to_string_utf8_columns(self):\n        n = u(\"\\u05d0\").encode('utf-8')\n\n        with option_context('display.max_rows', 1):\n            df = DataFrame([1, 2], columns=[n])\n            repr(df)\n\n    def test_to_string_unicode_two(self):\n        dm = DataFrame({u('c/\\u03c3'): []})\n        buf = StringIO()\n        dm.to_string(buf)\n\n    def test_to_string_unicode_three(self):\n        dm = DataFrame(['\\xc2'])\n        buf = StringIO()\n        dm.to_string(buf)\n\n    def test_to_string_with_formatters(self):\n        df = DataFrame({'int': [1, 2, 3],\n                        'float': [1.0, 2.0, 3.0],\n                        'object': [(1, 2), True, False]},\n                       columns=['int', 'float', 'object'])\n\n        formatters = [('int', lambda x: '0x%x' % x),\n                      ('float', lambda x: '[% 4.1f]' % x),\n                      ('object', lambda x: '-%s-' % str(x))]\n        result = df.to_string(formatters=dict(formatters))\n        result2 = df.to_string(formatters=lzip(*formatters)[1])\n        self.assertEqual(result, ('  int  float    object\\n'\n                                  '0 0x1 [ 1.0]  -(1, 2)-\\n'\n                                  '1 0x2 [ 2.0]    -True-\\n'\n                                  '2 0x3 [ 3.0]   -False-'))\n        self.assertEqual(result, result2)\n\n    def test_to_string_with_datetime64_monthformatter(self):\n        months = [datetime(2016, 1, 1), datetime(2016, 2, 2)]\n        x = DataFrame({'months': months})\n\n        def format_func(x):\n            return x.strftime('%Y-%m')\n        result = x.to_string(formatters={'months': format_func})\n        expected = 'months\\n0 2016-01\\n1 2016-02'\n        self.assertEqual(result.strip(), expected)\n\n    def test_to_string_with_datetime64_hourformatter(self):\n\n        x = DataFrame({'hod': pd.to_datetime(['10:10:10.100', '12:12:12.120'],\n                                             format='%H:%M:%S.%f')})\n\n        def format_func(x):\n            return x.strftime('%H:%M')\n\n        result = x.to_string(formatters={'hod': format_func})\n        expected = 'hod\\n0 10:10\\n1 12:12'\n        self.assertEqual(result.strip(), expected)\n\n    def test_to_string_with_formatters_unicode(self):\n        df = DataFrame({u('c/\\u03c3'): [1, 2, 3]})\n        result = df.to_string(formatters={u('c/\\u03c3'): lambda x: '%s' % x})\n        self.assertEqual(result, u('  c/\\u03c3\\n') + '0   1\\n1   2\\n2   3')\n\n    def test_east_asian_unicode_frame(self):\n        if PY3:\n            _rep = repr\n        else:\n            _rep = unicode\n\n        # not alighned properly because of east asian width\n\n        # mid col\n        df = DataFrame({'a': [u'あ', u'いいい', u'う', u'ええええええ'],\n                        'b': [1, 222, 33333, 4]},\n                       index=['a', 'bb', 'c', 'ddd'])\n        expected = (u\"          a      b\\na         あ      1\\n\"\n                    u\"bb      いいい    222\\nc         う  33333\\n\"\n                    u\"ddd  ええええええ      4\")\n        self.assertEqual(_rep(df), expected)\n\n        # last col\n        df = DataFrame({'a': [1, 222, 33333, 4],\n                        'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                       index=['a', 'bb', 'c', 'ddd'])\n        expected = (u\"         a       b\\na        1       あ\\n\"\n                    u\"bb     222     いいい\\nc    33333       う\\n\"\n                    u\"ddd      4  ええええええ\")\n        self.assertEqual(_rep(df), expected)\n\n        # all col\n        df = DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                        'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                       index=['a', 'bb', 'c', 'ddd'])\n        expected = (u\"         a       b\\na    あああああ       あ\\n\"\n                    u\"bb       い     いいい\\nc        う       う\\n\"\n                    u\"ddd    えええ  ええええええ\")\n        self.assertEqual(_rep(df), expected)\n\n        # column name\n        df = DataFrame({u'あああああ': [1, 222, 33333, 4],\n                        'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                       index=['a', 'bb', 'c', 'ddd'])\n        expected = (u\"          b  あああああ\\na         あ      1\\n\"\n                    u\"bb      いいい    222\\nc         う  33333\\n\"\n                    u\"ddd  ええええええ      4\")\n        self.assertEqual(_rep(df), expected)\n\n        # index\n        df = DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                        'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                       index=[u'あああ', u'いいいいいい', u'うう', u'え'])\n        expected = (u\"            a       b\\nあああ     あああああ       あ\\n\"\n                    u\"いいいいいい      い     いいい\\nうう          う       う\\n\"\n                    u\"え         えええ  ええええええ\")\n        self.assertEqual(_rep(df), expected)\n\n        # index name\n        df = DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                        'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                       index=pd.Index([u'あ', u'い', u'うう', u'え'], name=u'おおおお'))\n        expected = (u\"          a       b\\nおおおお               \\nあ     あああああ       あ\\n\"\n                    u\"い         い     いいい\\nうう        う       う\\nえ       えええ  ええええええ\"\n                    )\n        self.assertEqual(_rep(df), expected)\n\n        # all\n        df = DataFrame({u'あああ': [u'あああ', u'い', u'う', u'えええええ'],\n                        u'いいいいい': [u'あ', u'いいい', u'う', u'ええ']},\n                       index=pd.Index([u'あ', u'いいい', u'うう', u'え'], name=u'お'))\n        expected = (u\"       あああ いいいいい\\nお               \\nあ      あああ     あ\\n\"\n                    u\"いいい      い   いいい\\nうう       う     う\\nえ    えええええ    ええ\")\n        self.assertEqual(_rep(df), expected)\n\n        # MultiIndex\n        idx = pd.MultiIndex.from_tuples([(u'あ', u'いい'), (u'う', u'え'), (\n            u'おおお', u'かかかか'), (u'き', u'くく')])\n        df = DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                        'b': [u'あ', u'いいい', u'う', u'ええええええ']}, index=idx)\n        expected = (u\"              a       b\\nあ   いい    あああああ       あ\\n\"\n                    u\"う   え         い     いいい\\nおおお かかかか      う       う\\n\"\n                    u\"き   くく      えええ  ええええええ\")\n        self.assertEqual(_rep(df), expected)\n\n        # truncate\n        with option_context('display.max_rows', 3, 'display.max_columns', 3):\n            df = pd.DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                               'b': [u'あ', u'いいい', u'う', u'ええええええ'],\n                               'c': [u'お', u'か', u'ききき', u'くくくくくく'],\n                               u'ああああ': [u'さ', u'し', u'す', u'せ']},\n                              columns=['a', 'b', 'c', u'ああああ'])\n\n            expected = (u\"        a ...  ああああ\\n0   あああああ ...     さ\\n\"\n                        u\"..    ... ...   ...\\n3     えええ ...     せ\\n\"\n                        u\"\\n[4 rows x 4 columns]\")\n            self.assertEqual(_rep(df), expected)\n\n            df.index = [u'あああ', u'いいいい', u'う', 'aaa']\n            expected = (u\"         a ...  ああああ\\nあああ  あああああ ...     さ\\n\"\n                        u\"..     ... ...   ...\\naaa    えええ ...     せ\\n\"\n                        u\"\\n[4 rows x 4 columns]\")\n            self.assertEqual(_rep(df), expected)\n\n        # Emable Unicode option -----------------------------------------\n        with option_context('display.unicode.east_asian_width', True):\n\n            # mid col\n            df = DataFrame({'a': [u'あ', u'いいい', u'う', u'ええええええ'],\n                            'b': [1, 222, 33333, 4]},\n                           index=['a', 'bb', 'c', 'ddd'])\n            expected = (u\"                a      b\\na              あ      1\\n\"\n                        u\"bb         いいい    222\\nc              う  33333\\n\"\n                        u\"ddd  ええええええ      4\")\n            self.assertEqual(_rep(df), expected)\n\n            # last col\n            df = DataFrame({'a': [1, 222, 33333, 4],\n                            'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                           index=['a', 'bb', 'c', 'ddd'])\n            expected = (u\"         a             b\\na        1            あ\\n\"\n                        u\"bb     222        いいい\\nc    33333            う\\n\"\n                        u\"ddd      4  ええええええ\")\n            self.assertEqual(_rep(df), expected)\n\n            # all col\n            df = DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                            'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                           index=['a', 'bb', 'c', 'ddd'])\n            expected = (u\"              a             b\\na    あああああ            あ\\n\"\n                        u\"bb           い        いいい\\nc            う            う\\n\"\n                        u\"ddd      えええ  ええええええ\"\n                        \"\")\n            self.assertEqual(_rep(df), expected)\n\n            # column name\n            df = DataFrame({u'あああああ': [1, 222, 33333, 4],\n                            'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                           index=['a', 'bb', 'c', 'ddd'])\n            expected = (u\"                b  あああああ\\na              あ           1\\n\"\n                        u\"bb         いいい         222\\nc              う       33333\\n\"\n                        u\"ddd  ええええええ           4\")\n            self.assertEqual(_rep(df), expected)\n\n            # index\n            df = DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                            'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                           index=[u'あああ', u'いいいいいい', u'うう', u'え'])\n            expected = (u\"                       a             b\\nあああ        あああああ            あ\\n\"\n                        u\"いいいいいい          い        いいい\\nうう                  う            う\\n\"\n                        u\"え                えええ  ええええええ\")\n            self.assertEqual(_rep(df), expected)\n\n            # index name\n            df = DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                            'b': [u'あ', u'いいい', u'う', u'ええええええ']},\n                           index=pd.Index([u'あ', u'い', u'うう', u'え'], name=u'おおおお'))\n            expected = (u\"                   a             b\\nおおおお                          \\n\"\n                        u\"あ        あああああ            あ\\nい                い        いいい\\n\"\n                        u\"うう              う            う\\nえ            えええ  ええええええ\"\n                        )\n            self.assertEqual(_rep(df), expected)\n\n            # all\n            df = DataFrame({u'あああ': [u'あああ', u'い', u'う', u'えええええ'],\n                            u'いいいいい': [u'あ', u'いいい', u'う', u'ええ']},\n                           index=pd.Index([u'あ', u'いいい', u'うう', u'え'], name=u'お'))\n            expected = (u\"            あああ いいいいい\\nお                           \\n\"\n                        u\"あ          あああ         あ\\nいいい          い     いいい\\n\"\n                        u\"うう            う         う\\nえ      えええええ       ええ\")\n            self.assertEqual(_rep(df), expected)\n\n            # MultiIndex\n            idx = pd.MultiIndex.from_tuples([(u'あ', u'いい'), (u'う', u'え'), (\n                u'おおお', u'かかかか'), (u'き', u'くく')])\n            df = DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                            'b': [u'あ', u'いいい', u'う', u'ええええええ']}, index=idx)\n            expected = (u\"                          a             b\\nあ     いい      あああああ            あ\\n\"\n                        u\"う     え                い        いいい\\nおおお かかかか          う            う\\n\"\n                        u\"き     くく          えええ  ええええええ\")\n            self.assertEqual(_rep(df), expected)\n\n            # truncate\n            with option_context('display.max_rows', 3, 'display.max_columns',\n                                3):\n\n                df = pd.DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'],\n                                   'b': [u'あ', u'いいい', u'う', u'ええええええ'],\n                                   'c': [u'お', u'か', u'ききき', u'くくくくくく'],\n                                   u'ああああ': [u'さ', u'し', u'す', u'せ']},\n                                  columns=['a', 'b', 'c', u'ああああ'])\n\n                expected = (u\"             a   ...    ああああ\\n0   あああああ   ...          さ\\n\"\n                            u\"..         ...   ...         ...\\n3       えええ   ...          せ\\n\"\n                            u\"\\n[4 rows x 4 columns]\")\n                self.assertEqual(_rep(df), expected)\n\n                df.index = [u'あああ', u'いいいい', u'う', 'aaa']\n                expected = (u\"                 a   ...    ああああ\\nあああ  あああああ   ...          さ\\n\"\n                            u\"...            ...   ...         ...\\naaa         えええ   ...          せ\\n\"\n                            u\"\\n[4 rows x 4 columns]\")\n                self.assertEqual(_rep(df), expected)\n\n            # ambiguous unicode\n            df = DataFrame({u'あああああ': [1, 222, 33333, 4],\n                            'b': [u'あ', u'いいい', u'¡¡', u'ええええええ']},\n                           index=['a', 'bb', 'c', '¡¡¡'])\n            expected = (u\"                b  あああああ\\na              あ           1\\n\"\n                        u\"bb         いいい         222\\nc              ¡¡       33333\\n\"\n                        u\"¡¡¡  ええええええ           4\")\n            self.assertEqual(_rep(df), expected)\n\n    def test_to_string_buffer_all_unicode(self):\n        buf = StringIO()\n\n        empty = DataFrame({u('c/\\u03c3'): Series()})\n        nonempty = DataFrame({u('c/\\u03c3'): Series([1, 2, 3])})\n\n        print(empty, file=buf)\n        print(nonempty, file=buf)\n\n        # this should work\n        buf.getvalue()\n\n    def test_to_string_with_col_space(self):\n        df = DataFrame(np.random.random(size=(1, 3)))\n        c10 = len(df.to_string(col_space=10).split(\"\\n\")[1])\n        c20 = len(df.to_string(col_space=20).split(\"\\n\")[1])\n        c30 = len(df.to_string(col_space=30).split(\"\\n\")[1])\n        self.assertTrue(c10 < c20 < c30)\n\n        # GH 8230\n        # col_space wasn't being applied with header=False\n        with_header = df.to_string(col_space=20)\n        with_header_row1 = with_header.splitlines()[1]\n        no_header = df.to_string(col_space=20, header=False)\n        self.assertEqual(len(with_header_row1), len(no_header))\n\n    def test_to_string_truncate_indices(self):\n        for index in [tm.makeStringIndex, tm.makeUnicodeIndex, tm.makeIntIndex,\n                      tm.makeDateIndex, tm.makePeriodIndex]:\n            for column in [tm.makeStringIndex]:\n                for h in [10, 20]:\n                    for w in [10, 20]:\n                        with option_context(\"display.expand_frame_repr\",\n                                            False):\n                            df = DataFrame(index=index(h), columns=column(w))\n                            with option_context(\"display.max_rows\", 15):\n                                if h == 20:\n                                    self.assertTrue(\n                                        has_vertically_truncated_repr(df))\n                                else:\n                                    self.assertFalse(\n                                        has_vertically_truncated_repr(df))\n                            with option_context(\"display.max_columns\", 15):\n                                if w == 20:\n                                    self.assertTrue(\n                                        has_horizontally_truncated_repr(df))\n                                else:\n                                    self.assertFalse(\n                                        has_horizontally_truncated_repr(df))\n                            with option_context(\"display.max_rows\", 15,\n                                                \"display.max_columns\", 15):\n                                if h == 20 and w == 20:\n                                    self.assertTrue(has_doubly_truncated_repr(\n                                        df))\n                                else:\n                                    self.assertFalse(has_doubly_truncated_repr(\n                                        df))\n\n    def test_to_string_truncate_multilevel(self):\n        arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n                  ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\n        df = DataFrame(index=arrays, columns=arrays)\n        with option_context(\"display.max_rows\", 7, \"display.max_columns\", 7):\n            self.assertTrue(has_doubly_truncated_repr(df))\n\n    def test_truncate_with_different_dtypes(self):\n\n        # 11594, 12045\n        # when truncated the dtypes of the splits can differ\n\n        # 11594\n        import datetime\n        s = Series([datetime.datetime(2012, 1, 1)] * 10 +\n                   [datetime.datetime(1012, 1, 2)] + [datetime.datetime(2012, 1, 3)] * 10)\n\n        with pd.option_context('display.max_rows', 8):\n            result = str(s)\n            self.assertTrue('object' in result)\n\n        # 12045\n        df = DataFrame({'text': ['some words'] + [None] * 9})\n\n        with pd.option_context('display.max_rows', 8, 'display.max_columns', 3):\n            result = str(df)\n            self.assertTrue('None' in result)\n            self.assertFalse('NaN' in result)\n\n    def test_datetimelike_frame(self):\n\n        # GH 12211\n        df = DataFrame(\n            {'date': [pd.Timestamp('20130101').tz_localize('UTC')] + [pd.NaT] * 5})\n\n        with option_context(\"display.max_rows\", 5):\n            result = str(df)\n            self.assertTrue('2013-01-01 00:00:00+00:00' in result)\n            self.assertTrue('NaT' in result)\n            self.assertTrue('...' in result)\n            self.assertTrue('[6 rows x 1 columns]' in result)\n\n        dts = [pd.Timestamp('2011-01-01', tz='US/Eastern')] * 5 + [pd.NaT] * 5\n        df = pd.DataFrame({\"dt\": dts,\n                           \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n        with option_context('display.max_rows', 5):\n            expected = ('                          dt   x\\n'\n                        '0  2011-01-01 00:00:00-05:00   1\\n'\n                        '1  2011-01-01 00:00:00-05:00   2\\n'\n                        '..                       ...  ..\\n'\n                        '8                        NaT   9\\n'\n                        '9                        NaT  10\\n\\n'\n                        '[10 rows x 2 columns]')\n            self.assertEqual(repr(df), expected)\n\n        dts = [pd.NaT] * 5 + [pd.Timestamp('2011-01-01', tz='US/Eastern')] * 5\n        df = pd.DataFrame({\"dt\": dts,\n                           \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n        with option_context('display.max_rows', 5):\n            expected = ('                          dt   x\\n'\n                        '0                        NaT   1\\n'\n                        '1                        NaT   2\\n'\n                        '..                       ...  ..\\n'\n                        '8  2011-01-01 00:00:00-05:00   9\\n'\n                        '9  2011-01-01 00:00:00-05:00  10\\n\\n'\n                        '[10 rows x 2 columns]')\n            self.assertEqual(repr(df), expected)\n\n        dts = ([pd.Timestamp('2011-01-01', tz='Asia/Tokyo')] * 5 +\n               [pd.Timestamp('2011-01-01', tz='US/Eastern')] * 5)\n        df = pd.DataFrame({\"dt\": dts,\n                           \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n        with option_context('display.max_rows', 5):\n            expected = ('                           dt   x\\n'\n                        '0   2011-01-01 00:00:00+09:00   1\\n'\n                        '1   2011-01-01 00:00:00+09:00   2\\n'\n                        '..                        ...  ..\\n'\n                        '8   2011-01-01 00:00:00-05:00   9\\n'\n                        '9   2011-01-01 00:00:00-05:00  10\\n\\n'\n                        '[10 rows x 2 columns]')\n            self.assertEqual(repr(df), expected)\n\n    def test_to_html_with_col_space(self):\n        def check_with_width(df, col_space):\n            import re\n            # check that col_space affects HTML generation\n            # and be very brittle about it.\n            html = df.to_html(col_space=col_space)\n            hdrs = [x for x in html.split(r\"\\n\") if re.search(r\"<th[>\\s]\", x)]\n            self.assertTrue(len(hdrs) > 0)\n            for h in hdrs:\n                self.assertTrue(\"min-width\" in h)\n                self.assertTrue(str(col_space) in h)\n\n        df = DataFrame(np.random.random(size=(1, 3)))\n\n        check_with_width(df, 30)\n        check_with_width(df, 50)\n\n    def test_to_html_with_empty_string_label(self):\n        # GH3547, to_html regards empty string labels as repeated labels\n        data = {'c1': ['a', 'b'], 'c2': ['a', ''], 'data': [1, 2]}\n        df = DataFrame(data).set_index(['c1', 'c2'])\n        res = df.to_html()\n        self.assertTrue(\"rowspan\" not in res)\n\n    def test_to_html_unicode(self):\n        df = DataFrame({u('\\u03c3'): np.arange(10.)})\n        expected = u'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>\\u03c3</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>0.0</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>1.0</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2.0</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>3.0</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>4.0</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>5.0</td>\\n    </tr>\\n    <tr>\\n      <th>6</th>\\n      <td>6.0</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>7.0</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>8.0</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>9.0</td>\\n    </tr>\\n  </tbody>\\n</table>'\n        self.assertEqual(df.to_html(), expected)\n        df = DataFrame({'A': [u('\\u03c3')]})\n        expected = u'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>A</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>\\u03c3</td>\\n    </tr>\\n  </tbody>\\n</table>'\n        self.assertEqual(df.to_html(), expected)\n\n    def test_to_html_decimal(self):\n        # GH 12031\n        df = DataFrame({'A': [6.0, 3.1, 2.2]})\n        result = df.to_html(decimal=',')\n        expected = ('<table border=\"1\" class=\"dataframe\">\\n'\n                    '  <thead>\\n'\n                    '    <tr style=\"text-align: right;\">\\n'\n                    '      <th></th>\\n'\n                    '      <th>A</th>\\n'\n                    '    </tr>\\n'\n                    '  </thead>\\n'\n                    '  <tbody>\\n'\n                    '    <tr>\\n'\n                    '      <th>0</th>\\n'\n                    '      <td>6,0</td>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th>1</th>\\n'\n                    '      <td>3,1</td>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th>2</th>\\n'\n                    '      <td>2,2</td>\\n'\n                    '    </tr>\\n'\n                    '  </tbody>\\n'\n                    '</table>')\n        self.assertEqual(result, expected)\n\n    def test_to_html_escaped(self):\n        a = 'str<ing1 &amp;'\n        b = 'stri>ng2 &amp;'\n\n        test_dict = {'co<l1': {a: \"<type 'str'>\",\n                               b: \"<type 'str'>\"},\n                     'co>l2': {a: \"<type 'str'>\",\n                               b: \"<type 'str'>\"}}\n        rs = DataFrame(test_dict).to_html()\n        xp = \"\"\"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>co&lt;l1</th>\n      <th>co&gt;l2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>str&lt;ing1 &amp;amp;</th>\n      <td>&lt;type 'str'&gt;</td>\n      <td>&lt;type 'str'&gt;</td>\n    </tr>\n    <tr>\n      <th>stri&gt;ng2 &amp;amp;</th>\n      <td>&lt;type 'str'&gt;</td>\n      <td>&lt;type 'str'&gt;</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n\n        self.assertEqual(xp, rs)\n\n    def test_to_html_escape_disabled(self):\n        a = 'str<ing1 &amp;'\n        b = 'stri>ng2 &amp;'\n\n        test_dict = {'co<l1': {a: \"<b>bold</b>\",\n                               b: \"<b>bold</b>\"},\n                     'co>l2': {a: \"<b>bold</b>\",\n                               b: \"<b>bold</b>\"}}\n        rs = DataFrame(test_dict).to_html(escape=False)\n        xp = \"\"\"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>co<l1</th>\n      <th>co>l2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>str<ing1 &amp;</th>\n      <td><b>bold</b></td>\n      <td><b>bold</b></td>\n    </tr>\n    <tr>\n      <th>stri>ng2 &amp;</th>\n      <td><b>bold</b></td>\n      <td><b>bold</b></td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n\n        self.assertEqual(xp, rs)\n\n    def test_to_html_multiindex_index_false(self):\n        # issue 8452\n        df = DataFrame({\n            'a': range(2),\n            'b': range(3, 5),\n            'c': range(5, 7),\n            'd': range(3, 5)\n        })\n        df.columns = MultiIndex.from_product([['a', 'b'], ['c', 'd']])\n        result = df.to_html(index=False)\n        expected = \"\"\"\\\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th colspan=\"2\" halign=\"left\">a</th>\n      <th colspan=\"2\" halign=\"left\">b</th>\n    </tr>\n    <tr>\n      <th>c</th>\n      <th>d</th>\n      <th>c</th>\n      <th>d</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n\n        self.assertEqual(result, expected)\n\n        df.index = Index(df.index.values, name='idx')\n        result = df.to_html(index=False)\n        self.assertEqual(result, expected)\n\n    def test_to_html_multiindex_sparsify_false_multi_sparse(self):\n        with option_context('display.multi_sparse', False):\n            index = MultiIndex.from_arrays([[0, 0, 1, 1], [0, 1, 0, 1]],\n                                           names=['foo', None])\n\n            df = DataFrame([[0, 1], [2, 3], [4, 5], [6, 7]], index=index)\n\n            result = df.to_html()\n            expected = \"\"\"\\\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>foo</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>1</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>1</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n\n            self.assertEqual(result, expected)\n\n            df = DataFrame([[0, 1], [2, 3], [4, 5], [6, 7]],\n                           columns=index[::2], index=index)\n\n            result = df.to_html()\n            expected = \"\"\"\\\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>foo</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th>0</th>\n      <th>0</th>\n    </tr>\n    <tr>\n      <th>foo</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>1</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>1</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n\n            self.assertEqual(result, expected)\n\n    def test_to_html_multiindex_sparsify(self):\n        index = MultiIndex.from_arrays([[0, 0, 1, 1], [0, 1, 0, 1]],\n                                       names=['foo', None])\n\n        df = DataFrame([[0, 1], [2, 3], [4, 5], [6, 7]], index=index)\n\n        result = df.to_html()\n        expected = \"\"\"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>foo</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n\n        self.assertEqual(result, expected)\n\n        df = DataFrame([[0, 1], [2, 3], [4, 5], [6, 7]], columns=index[::2],\n                       index=index)\n\n        result = df.to_html()\n        expected = \"\"\"\\\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>foo</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th>0</th>\n      <th>0</th>\n    </tr>\n    <tr>\n      <th>foo</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n\n        self.assertEqual(result, expected)\n\n    def test_to_html_multiindex_odd_even_truncate(self):\n        # GH 14882 - Issue on truncation with odd length DataFrame\n        mi = MultiIndex.from_product([[100, 200, 300],\n                                      [10, 20, 30],\n                                      [1, 2, 3, 4, 5, 6, 7]],\n                                     names=['a', 'b', 'c'])\n        df = DataFrame({'n': range(len(mi))}, index=mi)\n        result = df.to_html(max_rows=60)\n        expected = \"\"\"\\\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>n</th>\n    </tr>\n    <tr>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"21\" valign=\"top\">100</th>\n      <th rowspan=\"7\" valign=\"top\">10</th>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">20</th>\n      <th>1</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">30</th>\n      <th>1</th>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th rowspan=\"19\" valign=\"top\">200</th>\n      <th rowspan=\"7\" valign=\"top\">10</th>\n      <th>1</th>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">20</th>\n      <th>1</th>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">30</th>\n      <th>1</th>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th rowspan=\"21\" valign=\"top\">300</th>\n      <th rowspan=\"7\" valign=\"top\">10</th>\n      <th>1</th>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">20</th>\n      <th>1</th>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">30</th>\n      <th>1</th>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>62</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n        self.assertEqual(result, expected)\n\n        # Test that ... appears in a middle level\n        result = df.to_html(max_rows=56)\n        expected = \"\"\"\\\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>n</th>\n    </tr>\n    <tr>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"21\" valign=\"top\">100</th>\n      <th rowspan=\"7\" valign=\"top\">10</th>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">20</th>\n      <th>1</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">30</th>\n      <th>1</th>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th rowspan=\"15\" valign=\"top\">200</th>\n      <th rowspan=\"7\" valign=\"top\">10</th>\n      <th>1</th>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">30</th>\n      <th>1</th>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th rowspan=\"21\" valign=\"top\">300</th>\n      <th rowspan=\"7\" valign=\"top\">10</th>\n      <th>1</th>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">20</th>\n      <th>1</th>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">30</th>\n      <th>1</th>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>62</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n        self.assertEqual(result, expected)\n\n    def test_to_html_index_formatter(self):\n        df = DataFrame([[0, 1], [2, 3], [4, 5], [6, 7]], columns=['foo', None],\n                       index=lrange(4))\n\n        f = lambda x: 'abcd' [x]\n        result = df.to_html(formatters={'__index__': f})\n        expected = \"\"\"\\\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>foo</th>\n      <th>None</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>b</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>c</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>d</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n\n        self.assertEqual(result, expected)\n\n    def test_to_html_datetime64_monthformatter(self):\n        months = [datetime(2016, 1, 1), datetime(2016, 2, 2)]\n        x = DataFrame({'months': months})\n\n        def format_func(x):\n            return x.strftime('%Y-%m')\n        result = x.to_html(formatters={'months': format_func})\n        expected = \"\"\"\\\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>months</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016-02</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n        self.assertEqual(result, expected)\n\n    def test_to_html_datetime64_hourformatter(self):\n\n        x = DataFrame({'hod': pd.to_datetime(['10:10:10.100', '12:12:12.120'],\n                                             format='%H:%M:%S.%f')})\n\n        def format_func(x):\n            return x.strftime('%H:%M')\n        result = x.to_html(formatters={'hod': format_func})\n        expected = \"\"\"\\\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hod</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10:10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12:12</td>\n    </tr>\n  </tbody>\n</table>\"\"\"\n        self.assertEqual(result, expected)\n\n    def test_to_html_regression_GH6098(self):\n        df = DataFrame({u('clé1'): [u('a'), u('a'), u('b'), u('b'), u('a')],\n                        u('clé2'): [u('1er'), u('2ème'), u('1er'), u('2ème'),\n                                    u('1er')],\n                        'données1': np.random.randn(5),\n                        'données2': np.random.randn(5)})\n        # it works\n        df.pivot_table(index=[u('clé1')], columns=[u('clé2')])._repr_html_()\n\n    def test_to_html_truncate(self):\n        raise nose.SkipTest(\"unreliable on travis\")\n        index = pd.DatetimeIndex(start='20010101', freq='D', periods=20)\n        df = DataFrame(index=index, columns=range(20))\n        fmt.set_option('display.max_rows', 8)\n        fmt.set_option('display.max_columns', 4)\n        result = df._repr_html_()\n        expected = '''\\\n<div{0}>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>...</th>\n      <th>18</th>\n      <th>19</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2001-01-01</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2001-01-02</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2001-01-03</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2001-01-04</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2001-01-17</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2001-01-18</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2001-01-19</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2001-01-20</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows × 20 columns</p>\n</div>'''.format(div_style)\n        if compat.PY2:\n            expected = expected.decode('utf-8')\n        self.assertEqual(result, expected)\n\n    def test_to_html_truncate_multi_index(self):\n        raise nose.SkipTest(\"unreliable on travis\")\n        arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n                  ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\n        df = DataFrame(index=arrays, columns=arrays)\n        fmt.set_option('display.max_rows', 7)\n        fmt.set_option('display.max_columns', 7)\n        result = df._repr_html_()\n        expected = '''\\\n<div{0}>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">bar</th>\n      <th>baz</th>\n      <th>...</th>\n      <th>foo</th>\n      <th colspan=\"2\" halign=\"left\">qux</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th>one</th>\n      <th>two</th>\n      <th>one</th>\n      <th>...</th>\n      <th>two</th>\n      <th>one</th>\n      <th>two</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">bar</th>\n      <th>one</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>two</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>baz</th>\n      <th>one</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>foo</th>\n      <th>two</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">qux</th>\n      <th>one</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>two</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 8 columns</p>\n</div>'''.format(div_style)\n        if compat.PY2:\n            expected = expected.decode('utf-8')\n        self.assertEqual(result, expected)\n\n    def test_to_html_truncate_multi_index_sparse_off(self):\n        raise nose.SkipTest(\"unreliable on travis\")\n        arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n                  ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\n        df = DataFrame(index=arrays, columns=arrays)\n        fmt.set_option('display.max_rows', 7)\n        fmt.set_option('display.max_columns', 7)\n        fmt.set_option('display.multi_sparse', False)\n        result = df._repr_html_()\n        expected = '''\\\n<div{0}>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th></th>\n      <th>bar</th>\n      <th>bar</th>\n      <th>baz</th>\n      <th>...</th>\n      <th>foo</th>\n      <th>qux</th>\n      <th>qux</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th>one</th>\n      <th>two</th>\n      <th>one</th>\n      <th>...</th>\n      <th>two</th>\n      <th>one</th>\n      <th>two</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bar</th>\n      <th>one</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>bar</th>\n      <th>two</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>baz</th>\n      <th>one</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>foo</th>\n      <th>two</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>qux</th>\n      <th>one</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>qux</th>\n      <th>two</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 8 columns</p>\n</div>'''.format(div_style)\n        if compat.PY2:\n            expected = expected.decode('utf-8')\n        self.assertEqual(result, expected)\n\n    def test_to_html_border(self):\n        df = DataFrame({'A': [1, 2]})\n        result = df.to_html()\n        assert 'border=\"1\"' in result\n\n    def test_to_html_border_option(self):\n        df = DataFrame({'A': [1, 2]})\n        with pd.option_context('html.border', 0):\n            result = df.to_html()\n            self.assertTrue('border=\"0\"' in result)\n            self.assertTrue('border=\"0\"' in df._repr_html_())\n\n    def test_to_html_border_zero(self):\n        df = DataFrame({'A': [1, 2]})\n        result = df.to_html(border=0)\n        self.assertTrue('border=\"0\"' in result)\n\n    def test_nonunicode_nonascii_alignment(self):\n        df = DataFrame([[\"aa\\xc3\\xa4\\xc3\\xa4\", 1], [\"bbbb\", 2]])\n        rep_str = df.to_string()\n        lines = rep_str.split('\\n')\n        self.assertEqual(len(lines[1]), len(lines[2]))\n\n    def test_unicode_problem_decoding_as_ascii(self):\n        dm = DataFrame({u('c/\\u03c3'): Series({'test': np.NaN})})\n        compat.text_type(dm.to_string())\n\n    def test_string_repr_encoding(self):\n        filepath = tm.get_data_path('unicode_series.csv')\n        df = pd.read_csv(filepath, header=None, encoding='latin1')\n        repr(df)\n        repr(df[1])\n\n    def test_repr_corner(self):\n        # representing infs poses no problems\n        df = DataFrame({'foo': [-np.inf, np.inf]})\n        repr(df)\n\n    def test_frame_info_encoding(self):\n        index = ['\\'Til There Was You (1997)',\n                 'ldum klaka (Cold Fever) (1994)']\n        fmt.set_option('display.max_rows', 1)\n        df = DataFrame(columns=['a', 'b', 'c'], index=index)\n        repr(df)\n        repr(df.T)\n        fmt.set_option('display.max_rows', 200)\n\n    def test_pprint_thing(self):\n        from pandas.formats.printing import pprint_thing as pp_t\n\n        if PY3:\n            raise nose.SkipTest(\"doesn't work on Python 3\")\n\n        self.assertEqual(pp_t('a'), u('a'))\n        self.assertEqual(pp_t(u('a')), u('a'))\n        self.assertEqual(pp_t(None), 'None')\n        self.assertEqual(pp_t(u('\\u05d0'), quote_strings=True), u(\"u'\\u05d0'\"))\n        self.assertEqual(pp_t(u('\\u05d0'), quote_strings=False), u('\\u05d0'))\n        self.assertEqual(pp_t((u('\\u05d0'),\n                               u('\\u05d1')), quote_strings=True),\n                         u(\"(u'\\u05d0', u'\\u05d1')\"))\n        self.assertEqual(pp_t((u('\\u05d0'), (u('\\u05d1'),\n                                             u('\\u05d2'))),\n                              quote_strings=True),\n                         u(\"(u'\\u05d0', (u'\\u05d1', u'\\u05d2'))\"))\n        self.assertEqual(pp_t(('foo', u('\\u05d0'), (u('\\u05d0'),\n                                                    u('\\u05d0'))),\n                              quote_strings=True),\n                         u(\"(u'foo', u'\\u05d0', (u'\\u05d0', u'\\u05d0'))\"))\n\n        # escape embedded tabs in string\n        # GH #2038\n        self.assertTrue(not \"\\t\" in pp_t(\"a\\tb\", escape_chars=(\"\\t\", )))\n\n    def test_wide_repr(self):\n        with option_context('mode.sim_interactive', True,\n                            'display.show_dimensions', True):\n            max_cols = get_option('display.max_columns')\n            df = DataFrame(tm.rands_array(25, size=(10, max_cols - 1)))\n            set_option('display.expand_frame_repr', False)\n            rep_str = repr(df)\n\n            assert \"10 rows x %d columns\" % (max_cols - 1) in rep_str\n            set_option('display.expand_frame_repr', True)\n            wide_repr = repr(df)\n            self.assertNotEqual(rep_str, wide_repr)\n\n            with option_context('display.width', 120):\n                wider_repr = repr(df)\n                self.assertTrue(len(wider_repr) < len(wide_repr))\n\n        reset_option('display.expand_frame_repr')\n\n    def test_wide_repr_wide_columns(self):\n        with option_context('mode.sim_interactive', True):\n            df = DataFrame(randn(5, 3), columns=['a' * 90, 'b' * 90, 'c' * 90])\n            rep_str = repr(df)\n\n            self.assertEqual(len(rep_str.splitlines()), 20)\n\n    def test_wide_repr_named(self):\n        with option_context('mode.sim_interactive', True):\n            max_cols = get_option('display.max_columns')\n            df = DataFrame(tm.rands_array(25, size=(10, max_cols - 1)))\n            df.index.name = 'DataFrame Index'\n            set_option('display.expand_frame_repr', False)\n\n            rep_str = repr(df)\n            set_option('display.expand_frame_repr', True)\n            wide_repr = repr(df)\n            self.assertNotEqual(rep_str, wide_repr)\n\n            with option_context('display.width', 150):\n                wider_repr = repr(df)\n                self.assertTrue(len(wider_repr) < len(wide_repr))\n\n            for line in wide_repr.splitlines()[1::13]:\n                self.assertIn('DataFrame Index', line)\n\n        reset_option('display.expand_frame_repr')\n\n    def test_wide_repr_multiindex(self):\n        with option_context('mode.sim_interactive', True):\n            midx = MultiIndex.from_arrays(tm.rands_array(5, size=(2, 10)))\n            max_cols = get_option('display.max_columns')\n            df = DataFrame(tm.rands_array(25, size=(10, max_cols - 1)),\n                           index=midx)\n            df.index.names = ['Level 0', 'Level 1']\n            set_option('display.expand_frame_repr', False)\n            rep_str = repr(df)\n            set_option('display.expand_frame_repr', True)\n            wide_repr = repr(df)\n            self.assertNotEqual(rep_str, wide_repr)\n\n            with option_context('display.width', 150):\n                wider_repr = repr(df)\n                self.assertTrue(len(wider_repr) < len(wide_repr))\n\n            for line in wide_repr.splitlines()[1::13]:\n                self.assertIn('Level 0 Level 1', line)\n\n        reset_option('display.expand_frame_repr')\n\n    def test_wide_repr_multiindex_cols(self):\n        with option_context('mode.sim_interactive', True):\n            max_cols = get_option('display.max_columns')\n            midx = MultiIndex.from_arrays(tm.rands_array(5, size=(2, 10)))\n            mcols = MultiIndex.from_arrays(tm.rands_array(3, size=(2, max_cols\n                                                                   - 1)))\n            df = DataFrame(tm.rands_array(25, (10, max_cols - 1)),\n                           index=midx, columns=mcols)\n            df.index.names = ['Level 0', 'Level 1']\n            set_option('display.expand_frame_repr', False)\n            rep_str = repr(df)\n            set_option('display.expand_frame_repr', True)\n            wide_repr = repr(df)\n            self.assertNotEqual(rep_str, wide_repr)\n\n        with option_context('display.width', 150):\n            wider_repr = repr(df)\n            self.assertTrue(len(wider_repr) < len(wide_repr))\n\n        reset_option('display.expand_frame_repr')\n\n    def test_wide_repr_unicode(self):\n        with option_context('mode.sim_interactive', True):\n            max_cols = get_option('display.max_columns')\n            df = DataFrame(tm.rands_array(25, size=(10, max_cols - 1)))\n            set_option('display.expand_frame_repr', False)\n            rep_str = repr(df)\n            set_option('display.expand_frame_repr', True)\n            wide_repr = repr(df)\n            self.assertNotEqual(rep_str, wide_repr)\n\n            with option_context('display.width', 150):\n                wider_repr = repr(df)\n                self.assertTrue(len(wider_repr) < len(wide_repr))\n\n        reset_option('display.expand_frame_repr')\n\n    def test_wide_repr_wide_long_columns(self):\n        with option_context('mode.sim_interactive', True):\n            df = DataFrame({'a': ['a' * 30, 'b' * 30],\n                            'b': ['c' * 70, 'd' * 80]})\n\n            result = repr(df)\n            self.assertTrue('ccccc' in result)\n            self.assertTrue('ddddd' in result)\n\n    def test_long_series(self):\n        n = 1000\n        s = Series(\n            np.random.randint(-50, 50, n),\n            index=['s%04d' % x for x in range(n)], dtype='int64')\n\n        import re\n        str_rep = str(s)\n        nmatches = len(re.findall('dtype', str_rep))\n        self.assertEqual(nmatches, 1)\n\n    def test_index_with_nan(self):\n        #  GH 2850\n        df = DataFrame({'id1': {0: '1a3',\n                                1: '9h4'},\n                        'id2': {0: np.nan,\n                                1: 'd67'},\n                        'id3': {0: '78d',\n                                1: '79d'},\n                        'value': {0: 123,\n                                  1: 64}})\n\n        # multi-index\n        y = df.set_index(['id1', 'id2', 'id3'])\n        result = y.to_string()\n        expected = u(\n            '             value\\nid1 id2 id3       \\n1a3 NaN 78d    123\\n9h4 d67 79d     64')\n        self.assertEqual(result, expected)\n\n        # index\n        y = df.set_index('id2')\n        result = y.to_string()\n        expected = u(\n            '     id1  id3  value\\nid2                 \\nNaN  1a3  78d    123\\nd67  9h4  79d     64')\n        self.assertEqual(result, expected)\n\n        # with append (this failed in 0.12)\n        y = df.set_index(['id1', 'id2']).set_index('id3', append=True)\n        result = y.to_string()\n        expected = u(\n            '             value\\nid1 id2 id3       \\n1a3 NaN 78d    123\\n9h4 d67 79d     64')\n        self.assertEqual(result, expected)\n\n        # all-nan in mi\n        df2 = df.copy()\n        df2.loc[:, 'id2'] = np.nan\n        y = df2.set_index('id2')\n        result = y.to_string()\n        expected = u(\n            '     id1  id3  value\\nid2                 \\nNaN  1a3  78d    123\\nNaN  9h4  79d     64')\n        self.assertEqual(result, expected)\n\n        # partial nan in mi\n        df2 = df.copy()\n        df2.loc[:, 'id2'] = np.nan\n        y = df2.set_index(['id2', 'id3'])\n        result = y.to_string()\n        expected = u(\n            '         id1  value\\nid2 id3            \\nNaN 78d  1a3    123\\n    79d  9h4     64')\n        self.assertEqual(result, expected)\n\n        df = DataFrame({'id1': {0: np.nan,\n                                1: '9h4'},\n                        'id2': {0: np.nan,\n                                1: 'd67'},\n                        'id3': {0: np.nan,\n                                1: '79d'},\n                        'value': {0: 123,\n                                  1: 64}})\n\n        y = df.set_index(['id1', 'id2', 'id3'])\n        result = y.to_string()\n        expected = u(\n            '             value\\nid1 id2 id3       \\nNaN NaN NaN    123\\n9h4 d67 79d     64')\n        self.assertEqual(result, expected)\n\n    def test_to_string(self):\n        from pandas import read_table\n        import re\n\n        # big mixed\n        biggie = DataFrame({'A': randn(200),\n                            'B': tm.makeStringIndex(200)},\n                           index=lrange(200))\n\n        biggie.loc[:20, 'A'] = nan\n        biggie.loc[:20, 'B'] = nan\n        s = biggie.to_string()\n\n        buf = StringIO()\n        retval = biggie.to_string(buf=buf)\n        self.assertIsNone(retval)\n        self.assertEqual(buf.getvalue(), s)\n\n        tm.assertIsInstance(s, compat.string_types)\n\n        # print in right order\n        result = biggie.to_string(columns=['B', 'A'], col_space=17,\n                                  float_format='%.5f'.__mod__)\n        lines = result.split('\\n')\n        header = lines[0].strip().split()\n        joined = '\\n'.join([re.sub(r'\\s+', ' ', x).strip() for x in lines[1:]])\n        recons = read_table(StringIO(joined), names=header,\n                            header=None, sep=' ')\n        tm.assert_series_equal(recons['B'], biggie['B'])\n        self.assertEqual(recons['A'].count(), biggie['A'].count())\n        self.assertTrue((np.abs(recons['A'].dropna() - biggie['A'].dropna()) <\n                         0.1).all())\n\n        # expected = ['B', 'A']\n        # self.assertEqual(header, expected)\n\n        result = biggie.to_string(columns=['A'], col_space=17)\n        header = result.split('\\n')[0].strip().split()\n        expected = ['A']\n        self.assertEqual(header, expected)\n\n        biggie.to_string(columns=['B', 'A'],\n                         formatters={'A': lambda x: '%.1f' % x})\n\n        biggie.to_string(columns=['B', 'A'], float_format=str)\n        biggie.to_string(columns=['B', 'A'], col_space=12, float_format=str)\n\n        frame = DataFrame(index=np.arange(200))\n        frame.to_string()\n\n    def test_to_string_no_header(self):\n        df = DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n\n        df_s = df.to_string(header=False)\n        expected = \"0  1  4\\n1  2  5\\n2  3  6\"\n\n        self.assertEqual(df_s, expected)\n\n    def test_to_string_no_index(self):\n        df = DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n\n        df_s = df.to_string(index=False)\n        expected = \"x  y\\n1  4\\n2  5\\n3  6\"\n\n        self.assertEqual(df_s, expected)\n\n    def test_to_string_line_width_no_index(self):\n        df = DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n\n        df_s = df.to_string(line_width=1, index=False)\n        expected = \"x  \\\\\\n1   \\n2   \\n3   \\n\\ny  \\n4  \\n5  \\n6\"\n\n        self.assertEqual(df_s, expected)\n\n    def test_to_string_float_formatting(self):\n        self.reset_display_options()\n        fmt.set_option('display.precision', 5, 'display.column_space', 12,\n                       'display.notebook_repr_html', False)\n\n        df = DataFrame({'x': [0, 0.25, 3456.000, 12e+45, 1.64e+6, 1.7e+8,\n                              1.253456, np.pi, -1e6]})\n\n        df_s = df.to_string()\n\n        # Python 2.5 just wants me to be sad. And debian 32-bit\n        # sys.version_info[0] == 2 and sys.version_info[1] < 6:\n        if _three_digit_exp():\n            expected = ('              x\\n0  0.00000e+000\\n1  2.50000e-001\\n'\n                        '2  3.45600e+003\\n3  1.20000e+046\\n4  1.64000e+006\\n'\n                        '5  1.70000e+008\\n6  1.25346e+000\\n7  3.14159e+000\\n'\n                        '8 -1.00000e+006')\n        else:\n            expected = ('             x\\n0  0.00000e+00\\n1  2.50000e-01\\n'\n                        '2  3.45600e+03\\n3  1.20000e+46\\n4  1.64000e+06\\n'\n                        '5  1.70000e+08\\n6  1.25346e+00\\n7  3.14159e+00\\n'\n                        '8 -1.00000e+06')\n        self.assertEqual(df_s, expected)\n\n        df = DataFrame({'x': [3234, 0.253]})\n        df_s = df.to_string()\n\n        expected = ('          x\\n' '0  3234.000\\n' '1     0.253')\n        self.assertEqual(df_s, expected)\n\n        self.reset_display_options()\n        self.assertEqual(get_option(\"display.precision\"), 6)\n\n        df = DataFrame({'x': [1e9, 0.2512]})\n        df_s = df.to_string()\n        # Python 2.5 just wants me to be sad. And debian 32-bit\n        # sys.version_info[0] == 2 and sys.version_info[1] < 6:\n        if _three_digit_exp():\n            expected = ('               x\\n'\n                        '0  1.000000e+009\\n'\n                        '1  2.512000e-001')\n        else:\n            expected = ('              x\\n'\n                        '0  1.000000e+09\\n'\n                        '1  2.512000e-01')\n        self.assertEqual(df_s, expected)\n\n    def test_to_string_small_float_values(self):\n        df = DataFrame({'a': [1.5, 1e-17, -5.5e-7]})\n\n        result = df.to_string()\n        # sadness per above\n        if '%.4g' % 1.7e8 == '1.7e+008':\n            expected = ('               a\\n'\n                        '0  1.500000e+000\\n'\n                        '1  1.000000e-017\\n'\n                        '2 -5.500000e-007')\n        else:\n            expected = ('              a\\n'\n                        '0  1.500000e+00\\n'\n                        '1  1.000000e-17\\n'\n                        '2 -5.500000e-07')\n        self.assertEqual(result, expected)\n\n        # but not all exactly zero\n        df = df * 0\n        result = df.to_string()\n        expected = ('   0\\n' '0  0\\n' '1  0\\n' '2 -0')\n\n    def test_to_string_float_index(self):\n        index = Index([1.5, 2, 3, 4, 5])\n        df = DataFrame(lrange(5), index=index)\n\n        result = df.to_string()\n        expected = ('     0\\n'\n                    '1.5  0\\n'\n                    '2.0  1\\n'\n                    '3.0  2\\n'\n                    '4.0  3\\n'\n                    '5.0  4')\n        self.assertEqual(result, expected)\n\n    def test_to_string_ascii_error(self):\n        data = [('0  ', u('                        .gitignore '), u('     5 '),\n                 ' \\xe2\\x80\\xa2\\xe2\\x80\\xa2\\xe2\\x80'\n                 '\\xa2\\xe2\\x80\\xa2\\xe2\\x80\\xa2')]\n        df = DataFrame(data)\n\n        # it works!\n        repr(df)\n\n    def test_to_string_int_formatting(self):\n        df = DataFrame({'x': [-15, 20, 25, -35]})\n        self.assertTrue(issubclass(df['x'].dtype.type, np.integer))\n\n        output = df.to_string()\n        expected = ('    x\\n' '0 -15\\n' '1  20\\n' '2  25\\n' '3 -35')\n        self.assertEqual(output, expected)\n\n    def test_to_string_index_formatter(self):\n        df = DataFrame([lrange(5), lrange(5, 10), lrange(10, 15)])\n\n        rs = df.to_string(formatters={'__index__': lambda x: 'abc' [x]})\n\n        xp = \"\"\"\\\n    0   1   2   3   4\na   0   1   2   3   4\nb   5   6   7   8   9\nc  10  11  12  13  14\\\n\"\"\"\n\n        self.assertEqual(rs, xp)\n\n    def test_to_string_left_justify_cols(self):\n        self.reset_display_options()\n        df = DataFrame({'x': [3234, 0.253]})\n        df_s = df.to_string(justify='left')\n        expected = ('   x       \\n' '0  3234.000\\n' '1     0.253')\n        self.assertEqual(df_s, expected)\n\n    def test_to_string_format_na(self):\n        self.reset_display_options()\n        df = DataFrame({'A': [np.nan, -1, -2.1234, 3, 4],\n                        'B': [np.nan, 'foo', 'foooo', 'fooooo', 'bar']})\n        result = df.to_string()\n\n        expected = ('        A       B\\n'\n                    '0     NaN     NaN\\n'\n                    '1 -1.0000     foo\\n'\n                    '2 -2.1234   foooo\\n'\n                    '3  3.0000  fooooo\\n'\n                    '4  4.0000     bar')\n        self.assertEqual(result, expected)\n\n        df = DataFrame({'A': [np.nan, -1., -2., 3., 4.],\n                        'B': [np.nan, 'foo', 'foooo', 'fooooo', 'bar']})\n        result = df.to_string()\n\n        expected = ('     A       B\\n'\n                    '0  NaN     NaN\\n'\n                    '1 -1.0     foo\\n'\n                    '2 -2.0   foooo\\n'\n                    '3  3.0  fooooo\\n'\n                    '4  4.0     bar')\n        self.assertEqual(result, expected)\n\n    def test_to_string_line_width(self):\n        df = DataFrame(123, lrange(10, 15), lrange(30))\n        s = df.to_string(line_width=80)\n        self.assertEqual(max(len(l) for l in s.split('\\n')), 80)\n\n    def test_show_dimensions(self):\n        df = DataFrame(123, lrange(10, 15), lrange(30))\n\n        with option_context('display.max_rows', 10, 'display.max_columns', 40,\n                            'display.width', 500, 'display.expand_frame_repr',\n                            'info', 'display.show_dimensions', True):\n            self.assertTrue('5 rows' in str(df))\n            self.assertTrue('5 rows' in df._repr_html_())\n        with option_context('display.max_rows', 10, 'display.max_columns', 40,\n                            'display.width', 500, 'display.expand_frame_repr',\n                            'info', 'display.show_dimensions', False):\n            self.assertFalse('5 rows' in str(df))\n            self.assertFalse('5 rows' in df._repr_html_())\n        with option_context('display.max_rows', 2, 'display.max_columns', 2,\n                            'display.width', 500, 'display.expand_frame_repr',\n                            'info', 'display.show_dimensions', 'truncate'):\n            self.assertTrue('5 rows' in str(df))\n            self.assertTrue('5 rows' in df._repr_html_())\n        with option_context('display.max_rows', 10, 'display.max_columns', 40,\n                            'display.width', 500, 'display.expand_frame_repr',\n                            'info', 'display.show_dimensions', 'truncate'):\n            self.assertFalse('5 rows' in str(df))\n            self.assertFalse('5 rows' in df._repr_html_())\n\n    def test_to_html(self):\n        # big mixed\n        biggie = DataFrame({'A': randn(200),\n                            'B': tm.makeStringIndex(200)},\n                           index=lrange(200))\n\n        biggie.loc[:20, 'A'] = nan\n        biggie.loc[:20, 'B'] = nan\n        s = biggie.to_html()\n\n        buf = StringIO()\n        retval = biggie.to_html(buf=buf)\n        self.assertIsNone(retval)\n        self.assertEqual(buf.getvalue(), s)\n\n        tm.assertIsInstance(s, compat.string_types)\n\n        biggie.to_html(columns=['B', 'A'], col_space=17)\n        biggie.to_html(columns=['B', 'A'],\n                       formatters={'A': lambda x: '%.1f' % x})\n\n        biggie.to_html(columns=['B', 'A'], float_format=str)\n        biggie.to_html(columns=['B', 'A'], col_space=12, float_format=str)\n\n        frame = DataFrame(index=np.arange(200))\n        frame.to_html()\n\n    def test_to_html_filename(self):\n        biggie = DataFrame({'A': randn(200),\n                            'B': tm.makeStringIndex(200)},\n                           index=lrange(200))\n\n        biggie.loc[:20, 'A'] = nan\n        biggie.loc[:20, 'B'] = nan\n        with tm.ensure_clean('test.html') as path:\n            biggie.to_html(path)\n            with open(path, 'r') as f:\n                s = biggie.to_html()\n                s2 = f.read()\n                self.assertEqual(s, s2)\n\n        frame = DataFrame(index=np.arange(200))\n        with tm.ensure_clean('test.html') as path:\n            frame.to_html(path)\n            with open(path, 'r') as f:\n                self.assertEqual(frame.to_html(), f.read())\n\n    def test_to_html_with_no_bold(self):\n        x = DataFrame({'x': randn(5)})\n        ashtml = x.to_html(bold_rows=False)\n        self.assertFalse('<strong' in ashtml[ashtml.find(\"</thead>\")])\n\n    def test_to_html_columns_arg(self):\n        result = self.frame.to_html(columns=['A'])\n        self.assertNotIn('<th>B</th>', result)\n\n    def test_to_html_multiindex(self):\n        columns = MultiIndex.from_tuples(list(zip(np.arange(2).repeat(2),\n                                                  np.mod(lrange(4), 2))),\n                                         names=['CL0', 'CL1'])\n        df = DataFrame([list('abcd'), list('efgh')], columns=columns)\n        result = df.to_html(justify='left')\n        expected = ('<table border=\"1\" class=\"dataframe\">\\n'\n                    '  <thead>\\n'\n                    '    <tr>\\n'\n                    '      <th>CL0</th>\\n'\n                    '      <th colspan=\"2\" halign=\"left\">0</th>\\n'\n                    '      <th colspan=\"2\" halign=\"left\">1</th>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th>CL1</th>\\n'\n                    '      <th>0</th>\\n'\n                    '      <th>1</th>\\n'\n                    '      <th>0</th>\\n'\n                    '      <th>1</th>\\n'\n                    '    </tr>\\n'\n                    '  </thead>\\n'\n                    '  <tbody>\\n'\n                    '    <tr>\\n'\n                    '      <th>0</th>\\n'\n                    '      <td>a</td>\\n'\n                    '      <td>b</td>\\n'\n                    '      <td>c</td>\\n'\n                    '      <td>d</td>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th>1</th>\\n'\n                    '      <td>e</td>\\n'\n                    '      <td>f</td>\\n'\n                    '      <td>g</td>\\n'\n                    '      <td>h</td>\\n'\n                    '    </tr>\\n'\n                    '  </tbody>\\n'\n                    '</table>')\n\n        self.assertEqual(result, expected)\n\n        columns = MultiIndex.from_tuples(list(zip(\n            range(4), np.mod(\n                lrange(4), 2))))\n        df = DataFrame([list('abcd'), list('efgh')], columns=columns)\n\n        result = df.to_html(justify='right')\n        expected = ('<table border=\"1\" class=\"dataframe\">\\n'\n                    '  <thead>\\n'\n                    '    <tr>\\n'\n                    '      <th></th>\\n'\n                    '      <th>0</th>\\n'\n                    '      <th>1</th>\\n'\n                    '      <th>2</th>\\n'\n                    '      <th>3</th>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th></th>\\n'\n                    '      <th>0</th>\\n'\n                    '      <th>1</th>\\n'\n                    '      <th>0</th>\\n'\n                    '      <th>1</th>\\n'\n                    '    </tr>\\n'\n                    '  </thead>\\n'\n                    '  <tbody>\\n'\n                    '    <tr>\\n'\n                    '      <th>0</th>\\n'\n                    '      <td>a</td>\\n'\n                    '      <td>b</td>\\n'\n                    '      <td>c</td>\\n'\n                    '      <td>d</td>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th>1</th>\\n'\n                    '      <td>e</td>\\n'\n                    '      <td>f</td>\\n'\n                    '      <td>g</td>\\n'\n                    '      <td>h</td>\\n'\n                    '    </tr>\\n'\n                    '  </tbody>\\n'\n                    '</table>')\n\n        self.assertEqual(result, expected)\n\n    def test_to_html_justify(self):\n        df = DataFrame({'A': [6, 30000, 2],\n                        'B': [1, 2, 70000],\n                        'C': [223442, 0, 1]},\n                       columns=['A', 'B', 'C'])\n        result = df.to_html(justify='left')\n        expected = ('<table border=\"1\" class=\"dataframe\">\\n'\n                    '  <thead>\\n'\n                    '    <tr style=\"text-align: left;\">\\n'\n                    '      <th></th>\\n'\n                    '      <th>A</th>\\n'\n                    '      <th>B</th>\\n'\n                    '      <th>C</th>\\n'\n                    '    </tr>\\n'\n                    '  </thead>\\n'\n                    '  <tbody>\\n'\n                    '    <tr>\\n'\n                    '      <th>0</th>\\n'\n                    '      <td>6</td>\\n'\n                    '      <td>1</td>\\n'\n                    '      <td>223442</td>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th>1</th>\\n'\n                    '      <td>30000</td>\\n'\n                    '      <td>2</td>\\n'\n                    '      <td>0</td>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th>2</th>\\n'\n                    '      <td>2</td>\\n'\n                    '      <td>70000</td>\\n'\n                    '      <td>1</td>\\n'\n                    '    </tr>\\n'\n                    '  </tbody>\\n'\n                    '</table>')\n        self.assertEqual(result, expected)\n\n        result = df.to_html(justify='right')\n        expected = ('<table border=\"1\" class=\"dataframe\">\\n'\n                    '  <thead>\\n'\n                    '    <tr style=\"text-align: right;\">\\n'\n                    '      <th></th>\\n'\n                    '      <th>A</th>\\n'\n                    '      <th>B</th>\\n'\n                    '      <th>C</th>\\n'\n                    '    </tr>\\n'\n                    '  </thead>\\n'\n                    '  <tbody>\\n'\n                    '    <tr>\\n'\n                    '      <th>0</th>\\n'\n                    '      <td>6</td>\\n'\n                    '      <td>1</td>\\n'\n                    '      <td>223442</td>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th>1</th>\\n'\n                    '      <td>30000</td>\\n'\n                    '      <td>2</td>\\n'\n                    '      <td>0</td>\\n'\n                    '    </tr>\\n'\n                    '    <tr>\\n'\n                    '      <th>2</th>\\n'\n                    '      <td>2</td>\\n'\n                    '      <td>70000</td>\\n'\n                    '      <td>1</td>\\n'\n                    '    </tr>\\n'\n                    '  </tbody>\\n'\n                    '</table>')\n        self.assertEqual(result, expected)\n\n    def test_to_html_index(self):\n        index = ['foo', 'bar', 'baz']\n        df = DataFrame({'A': [1, 2, 3],\n                        'B': [1.2, 3.4, 5.6],\n                        'C': ['one', 'two', np.NaN]},\n                       columns=['A', 'B', 'C'],\n                       index=index)\n        expected_with_index = ('<table border=\"1\" class=\"dataframe\">\\n'\n                               '  <thead>\\n'\n                               '    <tr style=\"text-align: right;\">\\n'\n                               '      <th></th>\\n'\n                               '      <th>A</th>\\n'\n                               '      <th>B</th>\\n'\n                               '      <th>C</th>\\n'\n                               '    </tr>\\n'\n                               '  </thead>\\n'\n                               '  <tbody>\\n'\n                               '    <tr>\\n'\n                               '      <th>foo</th>\\n'\n                               '      <td>1</td>\\n'\n                               '      <td>1.2</td>\\n'\n                               '      <td>one</td>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>bar</th>\\n'\n                               '      <td>2</td>\\n'\n                               '      <td>3.4</td>\\n'\n                               '      <td>two</td>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>baz</th>\\n'\n                               '      <td>3</td>\\n'\n                               '      <td>5.6</td>\\n'\n                               '      <td>NaN</td>\\n'\n                               '    </tr>\\n'\n                               '  </tbody>\\n'\n                               '</table>')\n        self.assertEqual(df.to_html(), expected_with_index)\n\n        expected_without_index = ('<table border=\"1\" class=\"dataframe\">\\n'\n                                  '  <thead>\\n'\n                                  '    <tr style=\"text-align: right;\">\\n'\n                                  '      <th>A</th>\\n'\n                                  '      <th>B</th>\\n'\n                                  '      <th>C</th>\\n'\n                                  '    </tr>\\n'\n                                  '  </thead>\\n'\n                                  '  <tbody>\\n'\n                                  '    <tr>\\n'\n                                  '      <td>1</td>\\n'\n                                  '      <td>1.2</td>\\n'\n                                  '      <td>one</td>\\n'\n                                  '    </tr>\\n'\n                                  '    <tr>\\n'\n                                  '      <td>2</td>\\n'\n                                  '      <td>3.4</td>\\n'\n                                  '      <td>two</td>\\n'\n                                  '    </tr>\\n'\n                                  '    <tr>\\n'\n                                  '      <td>3</td>\\n'\n                                  '      <td>5.6</td>\\n'\n                                  '      <td>NaN</td>\\n'\n                                  '    </tr>\\n'\n                                  '  </tbody>\\n'\n                                  '</table>')\n        result = df.to_html(index=False)\n        for i in index:\n            self.assertNotIn(i, result)\n        self.assertEqual(result, expected_without_index)\n        df.index = Index(['foo', 'bar', 'baz'], name='idx')\n        expected_with_index = ('<table border=\"1\" class=\"dataframe\">\\n'\n                               '  <thead>\\n'\n                               '    <tr style=\"text-align: right;\">\\n'\n                               '      <th></th>\\n'\n                               '      <th>A</th>\\n'\n                               '      <th>B</th>\\n'\n                               '      <th>C</th>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>idx</th>\\n'\n                               '      <th></th>\\n'\n                               '      <th></th>\\n'\n                               '      <th></th>\\n'\n                               '    </tr>\\n'\n                               '  </thead>\\n'\n                               '  <tbody>\\n'\n                               '    <tr>\\n'\n                               '      <th>foo</th>\\n'\n                               '      <td>1</td>\\n'\n                               '      <td>1.2</td>\\n'\n                               '      <td>one</td>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>bar</th>\\n'\n                               '      <td>2</td>\\n'\n                               '      <td>3.4</td>\\n'\n                               '      <td>two</td>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>baz</th>\\n'\n                               '      <td>3</td>\\n'\n                               '      <td>5.6</td>\\n'\n                               '      <td>NaN</td>\\n'\n                               '    </tr>\\n'\n                               '  </tbody>\\n'\n                               '</table>')\n        self.assertEqual(df.to_html(), expected_with_index)\n        self.assertEqual(df.to_html(index=False), expected_without_index)\n\n        tuples = [('foo', 'car'), ('foo', 'bike'), ('bar', 'car')]\n        df.index = MultiIndex.from_tuples(tuples)\n\n        expected_with_index = ('<table border=\"1\" class=\"dataframe\">\\n'\n                               '  <thead>\\n'\n                               '    <tr style=\"text-align: right;\">\\n'\n                               '      <th></th>\\n'\n                               '      <th></th>\\n'\n                               '      <th>A</th>\\n'\n                               '      <th>B</th>\\n'\n                               '      <th>C</th>\\n'\n                               '    </tr>\\n'\n                               '  </thead>\\n'\n                               '  <tbody>\\n'\n                               '    <tr>\\n'\n                               '      <th rowspan=\"2\" valign=\"top\">foo</th>\\n'\n                               '      <th>car</th>\\n'\n                               '      <td>1</td>\\n'\n                               '      <td>1.2</td>\\n'\n                               '      <td>one</td>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>bike</th>\\n'\n                               '      <td>2</td>\\n'\n                               '      <td>3.4</td>\\n'\n                               '      <td>two</td>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>bar</th>\\n'\n                               '      <th>car</th>\\n'\n                               '      <td>3</td>\\n'\n                               '      <td>5.6</td>\\n'\n                               '      <td>NaN</td>\\n'\n                               '    </tr>\\n'\n                               '  </tbody>\\n'\n                               '</table>')\n        self.assertEqual(df.to_html(), expected_with_index)\n\n        result = df.to_html(index=False)\n        for i in ['foo', 'bar', 'car', 'bike']:\n            self.assertNotIn(i, result)\n        # must be the same result as normal index\n        self.assertEqual(result, expected_without_index)\n\n        df.index = MultiIndex.from_tuples(tuples, names=['idx1', 'idx2'])\n        expected_with_index = ('<table border=\"1\" class=\"dataframe\">\\n'\n                               '  <thead>\\n'\n                               '    <tr style=\"text-align: right;\">\\n'\n                               '      <th></th>\\n'\n                               '      <th></th>\\n'\n                               '      <th>A</th>\\n'\n                               '      <th>B</th>\\n'\n                               '      <th>C</th>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>idx1</th>\\n'\n                               '      <th>idx2</th>\\n'\n                               '      <th></th>\\n'\n                               '      <th></th>\\n'\n                               '      <th></th>\\n'\n                               '    </tr>\\n'\n                               '  </thead>\\n'\n                               '  <tbody>\\n'\n                               '    <tr>\\n'\n                               '      <th rowspan=\"2\" valign=\"top\">foo</th>\\n'\n                               '      <th>car</th>\\n'\n                               '      <td>1</td>\\n'\n                               '      <td>1.2</td>\\n'\n                               '      <td>one</td>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>bike</th>\\n'\n                               '      <td>2</td>\\n'\n                               '      <td>3.4</td>\\n'\n                               '      <td>two</td>\\n'\n                               '    </tr>\\n'\n                               '    <tr>\\n'\n                               '      <th>bar</th>\\n'\n                               '      <th>car</th>\\n'\n                               '      <td>3</td>\\n'\n                               '      <td>5.6</td>\\n'\n                               '      <td>NaN</td>\\n'\n                               '    </tr>\\n'\n                               '  </tbody>\\n'\n                               '</table>')\n        self.assertEqual(df.to_html(), expected_with_index)\n        self.assertEqual(df.to_html(index=False), expected_without_index)\n\n    def test_repr_html(self):\n        self.frame._repr_html_()\n\n        fmt.set_option('display.max_rows', 1, 'display.max_columns', 1)\n        self.frame._repr_html_()\n\n        fmt.set_option('display.notebook_repr_html', False)\n        self.frame._repr_html_()\n\n        self.reset_display_options()\n\n        df = DataFrame([[1, 2], [3, 4]])\n        fmt.set_option('display.show_dimensions', True)\n        self.assertTrue('2 rows' in df._repr_html_())\n        fmt.set_option('display.show_dimensions', False)\n        self.assertFalse('2 rows' in df._repr_html_())\n\n        self.reset_display_options()\n\n    def test_repr_html_wide(self):\n        max_cols = get_option('display.max_columns')\n        df = DataFrame(tm.rands_array(25, size=(10, max_cols - 1)))\n        reg_repr = df._repr_html_()\n        assert \"...\" not in reg_repr\n\n        wide_df = DataFrame(tm.rands_array(25, size=(10, max_cols + 1)))\n        wide_repr = wide_df._repr_html_()\n        assert \"...\" in wide_repr\n\n    def test_repr_html_wide_multiindex_cols(self):\n        max_cols = get_option('display.max_columns')\n\n        mcols = MultiIndex.from_product([np.arange(max_cols // 2),\n                                         ['foo', 'bar']],\n                                        names=['first', 'second'])\n        df = DataFrame(tm.rands_array(25, size=(10, len(mcols))),\n                       columns=mcols)\n        reg_repr = df._repr_html_()\n        assert '...' not in reg_repr\n\n        mcols = MultiIndex.from_product((np.arange(1 + (max_cols // 2)),\n                                         ['foo', 'bar']),\n                                        names=['first', 'second'])\n        df = DataFrame(tm.rands_array(25, size=(10, len(mcols))),\n                       columns=mcols)\n        wide_repr = df._repr_html_()\n        assert '...' in wide_repr\n\n    def test_repr_html_long(self):\n        max_rows = get_option('display.max_rows')\n        h = max_rows - 1\n        df = DataFrame({'A': np.arange(1, 1 + h), 'B': np.arange(41, 41 + h)})\n        reg_repr = df._repr_html_()\n        assert '..' not in reg_repr\n        assert str(41 + max_rows // 2) in reg_repr\n\n        h = max_rows + 1\n        df = DataFrame({'A': np.arange(1, 1 + h), 'B': np.arange(41, 41 + h)})\n        long_repr = df._repr_html_()\n        assert '..' in long_repr\n        assert str(41 + max_rows // 2) not in long_repr\n        assert u('%d rows ') % h in long_repr\n        assert u('2 columns') in long_repr\n\n    def test_repr_html_float(self):\n        max_rows = get_option('display.max_rows')\n        h = max_rows - 1\n        df = DataFrame({'idx': np.linspace(-10, 10, h),\n                        'A': np.arange(1, 1 + h),\n                        'B': np.arange(41, 41 + h)}).set_index('idx')\n        reg_repr = df._repr_html_()\n        assert '..' not in reg_repr\n        assert str(40 + h) in reg_repr\n\n        h = max_rows + 1\n        df = DataFrame({'idx': np.linspace(-10, 10, h),\n                        'A': np.arange(1, 1 + h),\n                        'B': np.arange(41, 41 + h)}).set_index('idx')\n        long_repr = df._repr_html_()\n        assert '..' in long_repr\n        assert '31' not in long_repr\n        assert u('%d rows ') % h in long_repr\n        assert u('2 columns') in long_repr\n\n    def test_repr_html_long_multiindex(self):\n        max_rows = get_option('display.max_rows')\n        max_L1 = max_rows // 2\n\n        tuples = list(itertools.product(np.arange(max_L1), ['foo', 'bar']))\n        idx = MultiIndex.from_tuples(tuples, names=['first', 'second'])\n        df = DataFrame(np.random.randn(max_L1 * 2, 2), index=idx,\n                       columns=['A', 'B'])\n        reg_repr = df._repr_html_()\n        assert '...' not in reg_repr\n\n        tuples = list(itertools.product(np.arange(max_L1 + 1), ['foo', 'bar']))\n        idx = MultiIndex.from_tuples(tuples, names=['first', 'second'])\n        df = DataFrame(np.random.randn((max_L1 + 1) * 2, 2), index=idx,\n                       columns=['A', 'B'])\n        long_repr = df._repr_html_()\n        assert '...' in long_repr\n\n    def test_repr_html_long_and_wide(self):\n        max_cols = get_option('display.max_columns')\n        max_rows = get_option('display.max_rows')\n\n        h, w = max_rows - 1, max_cols - 1\n        df = DataFrame(dict((k, np.arange(1, 1 + h)) for k in np.arange(w)))\n        assert '...' not in df._repr_html_()\n\n        h, w = max_rows + 1, max_cols + 1\n        df = DataFrame(dict((k, np.arange(1, 1 + h)) for k in np.arange(w)))\n        assert '...' in df._repr_html_()\n\n    def test_info_repr(self):\n        max_rows = get_option('display.max_rows')\n        max_cols = get_option('display.max_columns')\n        # Long\n        h, w = max_rows + 1, max_cols - 1\n        df = DataFrame(dict((k, np.arange(1, 1 + h)) for k in np.arange(w)))\n        assert has_vertically_truncated_repr(df)\n        with option_context('display.large_repr', 'info'):\n            assert has_info_repr(df)\n\n        # Wide\n        h, w = max_rows - 1, max_cols + 1\n        df = DataFrame(dict((k, np.arange(1, 1 + h)) for k in np.arange(w)))\n        assert has_horizontally_truncated_repr(df)\n        with option_context('display.large_repr', 'info'):\n            assert has_info_repr(df)\n\n    def test_info_repr_max_cols(self):\n        # GH #6939\n        df = DataFrame(randn(10, 5))\n        with option_context('display.large_repr', 'info',\n                            'display.max_columns', 1,\n                            'display.max_info_columns', 4):\n            self.assertTrue(has_non_verbose_info_repr(df))\n\n        with option_context('display.large_repr', 'info',\n                            'display.max_columns', 1,\n                            'display.max_info_columns', 5):\n            self.assertFalse(has_non_verbose_info_repr(df))\n\n        # test verbose overrides\n        # fmt.set_option('display.max_info_columns', 4)  # exceeded\n\n    def test_info_repr_html(self):\n        max_rows = get_option('display.max_rows')\n        max_cols = get_option('display.max_columns')\n        # Long\n        h, w = max_rows + 1, max_cols - 1\n        df = DataFrame(dict((k, np.arange(1, 1 + h)) for k in np.arange(w)))\n        assert r'&lt;class' not in df._repr_html_()\n        with option_context('display.large_repr', 'info'):\n            assert r'&lt;class' in df._repr_html_()\n\n        # Wide\n        h, w = max_rows - 1, max_cols + 1\n        df = DataFrame(dict((k, np.arange(1, 1 + h)) for k in np.arange(w)))\n        assert '<class' not in df._repr_html_()\n        with option_context('display.large_repr', 'info'):\n            assert '&lt;class' in df._repr_html_()\n\n    def test_fake_qtconsole_repr_html(self):\n        def get_ipython():\n            return {'config': {'KernelApp':\n                               {'parent_appname': 'ipython-qtconsole'}}}\n\n        repstr = self.frame._repr_html_()\n        self.assertIsNotNone(repstr)\n\n        fmt.set_option('display.max_rows', 5, 'display.max_columns', 2)\n        repstr = self.frame._repr_html_()\n        self.assertIn('class', repstr)  # info fallback\n\n        self.reset_display_options()\n\n    def test_to_html_with_classes(self):\n        df = DataFrame()\n        result = df.to_html(classes=\"sortable draggable\")\n        expected = dedent(\"\"\"\n\n            <table border=\"1\" class=\"dataframe sortable draggable\">\n              <thead>\n                <tr style=\"text-align: right;\">\n                  <th></th>\n                </tr>\n              </thead>\n              <tbody>\n              </tbody>\n            </table>\n\n        \"\"\").strip()\n        self.assertEqual(result, expected)\n\n        result = df.to_html(classes=[\"sortable\", \"draggable\"])\n        self.assertEqual(result, expected)\n\n    def test_to_html_no_index_max_rows(self):\n        # GH https://github.com/pandas-dev/pandas/issues/14998\n        df = DataFrame({\"A\": [1, 2, 3, 4]})\n        result = df.to_html(index=False, max_rows=1)\n        expected = dedent(\"\"\"\\\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr style=\"text-align: right;\">\n              <th>A</th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <td>1</td>\n            </tr>\n          </tbody>\n        </table>\"\"\")\n        self.assertEqual(result, expected)\n\n    def test_pprint_pathological_object(self):\n        \"\"\"\n        if the test fails, the stack will overflow and nose crash,\n        but it won't hang.\n        \"\"\"\n\n        class A:\n\n            def __getitem__(self, key):\n                return 3  # obviously simplified\n\n        df = DataFrame([A()])\n        repr(df)  # just don't dine\n\n    def test_float_trim_zeros(self):\n        vals = [2.08430917305e+10, 3.52205017305e+10, 2.30674817305e+10,\n                2.03954217305e+10, 5.59897817305e+10]\n        skip = True\n        for line in repr(DataFrame({'A': vals})).split('\\n')[:-2]:\n            if line.startswith('dtype:'):\n                continue\n            if _three_digit_exp():\n                self.assertTrue(('+010' in line) or skip)\n            else:\n                self.assertTrue(('+10' in line) or skip)\n            skip = False\n\n    def test_dict_entries(self):\n        df = DataFrame({'A': [{'a': 1, 'b': 2}]})\n\n        val = df.to_string()\n        self.assertTrue(\"'a': 1\" in val)\n        self.assertTrue(\"'b': 2\" in val)\n\n    def test_to_latex_filename(self):\n        with tm.ensure_clean('test.tex') as path:\n            self.frame.to_latex(path)\n\n            with open(path, 'r') as f:\n                self.assertEqual(self.frame.to_latex(), f.read())\n\n        # test with utf-8 and encoding option (GH 7061)\n        df = DataFrame([[u'au\\xdfgangen']])\n        with tm.ensure_clean('test.tex') as path:\n            df.to_latex(path, encoding='utf-8')\n            with codecs.open(path, 'r', encoding='utf-8') as f:\n                self.assertEqual(df.to_latex(), f.read())\n\n        # test with utf-8 without encoding option\n        if compat.PY3:  # python3: pandas default encoding is utf-8\n            with tm.ensure_clean('test.tex') as path:\n                df.to_latex(path)\n                with codecs.open(path, 'r', encoding='utf-8') as f:\n                    self.assertEqual(df.to_latex(), f.read())\n        else:\n            # python2 default encoding is ascii, so an error should be raised\n            with tm.ensure_clean('test.tex') as path:\n                self.assertRaises(UnicodeEncodeError, df.to_latex, path)\n\n    def test_to_latex(self):\n        # it works!\n        self.frame.to_latex()\n\n        df = DataFrame({'a': [1, 2], 'b': ['b1', 'b2']})\n        withindex_result = df.to_latex()\n        withindex_expected = r\"\"\"\\begin{tabular}{lrl}\n\\toprule\n{} &  a &   b \\\\\n\\midrule\n0 &  1 &  b1 \\\\\n1 &  2 &  b2 \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(withindex_result, withindex_expected)\n\n        withoutindex_result = df.to_latex(index=False)\n        withoutindex_expected = r\"\"\"\\begin{tabular}{rl}\n\\toprule\n a &   b \\\\\n\\midrule\n 1 &  b1 \\\\\n 2 &  b2 \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(withoutindex_result, withoutindex_expected)\n\n    def test_to_latex_format(self):\n        # GH Bug #9402\n        self.frame.to_latex(column_format='ccc')\n\n        df = DataFrame({'a': [1, 2], 'b': ['b1', 'b2']})\n        withindex_result = df.to_latex(column_format='ccc')\n        withindex_expected = r\"\"\"\\begin{tabular}{ccc}\n\\toprule\n{} &  a &   b \\\\\n\\midrule\n0 &  1 &  b1 \\\\\n1 &  2 &  b2 \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(withindex_result, withindex_expected)\n\n    def test_to_latex_with_formatters(self):\n        df = DataFrame({'int': [1, 2, 3],\n                        'float': [1.0, 2.0, 3.0],\n                        'object': [(1, 2), True, False],\n                        'datetime64': [datetime(2016, 1, 1),\n                                       datetime(2016, 2, 5),\n                                       datetime(2016, 3, 3)]})\n\n        formatters = {'int': lambda x: '0x%x' % x,\n                      'float': lambda x: '[% 4.1f]' % x,\n                      'object': lambda x: '-%s-' % str(x),\n                      'datetime64': lambda x: x.strftime('%Y-%m'),\n                      '__index__': lambda x: 'index: %s' % x}\n        result = df.to_latex(formatters=dict(formatters))\n\n        expected = r\"\"\"\\begin{tabular}{llrrl}\n\\toprule\n{} & datetime64 &  float & int &    object \\\\\n\\midrule\nindex: 0 &    2016-01 & [ 1.0] & 0x1 &  -(1, 2)- \\\\\nindex: 1 &    2016-02 & [ 2.0] & 0x2 &    -True- \\\\\nindex: 2 &    2016-03 & [ 3.0] & 0x3 &   -False- \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n        self.assertEqual(result, expected)\n\n    def test_to_latex_multiindex(self):\n        df = DataFrame({('x', 'y'): ['a']})\n        result = df.to_latex()\n        expected = r\"\"\"\\begin{tabular}{ll}\n\\toprule\n{} &  x \\\\\n{} &  y \\\\\n\\midrule\n0 &  a \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(result, expected)\n\n        result = df.T.to_latex()\n        expected = r\"\"\"\\begin{tabular}{lll}\n\\toprule\n  &   &  0 \\\\\n\\midrule\nx & y &  a \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(result, expected)\n\n        df = DataFrame.from_dict({\n            ('c1', 0): pd.Series(dict((x, x) for x in range(4))),\n            ('c1', 1): pd.Series(dict((x, x + 4) for x in range(4))),\n            ('c2', 0): pd.Series(dict((x, x) for x in range(4))),\n            ('c2', 1): pd.Series(dict((x, x + 4) for x in range(4))),\n            ('c3', 0): pd.Series(dict((x, x) for x in range(4))),\n        }).T\n        result = df.to_latex()\n        expected = r\"\"\"\\begin{tabular}{llrrrr}\n\\toprule\n   &   &  0 &  1 &  2 &  3 \\\\\n\\midrule\nc1 & 0 &  0 &  1 &  2 &  3 \\\\\n   & 1 &  4 &  5 &  6 &  7 \\\\\nc2 & 0 &  0 &  1 &  2 &  3 \\\\\n   & 1 &  4 &  5 &  6 &  7 \\\\\nc3 & 0 &  0 &  1 &  2 &  3 \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(result, expected)\n\n        # GH 10660\n        df = pd.DataFrame({'a': [0, 0, 1, 1],\n                           'b': list('abab'),\n                           'c': [1, 2, 3, 4]})\n        result = df.set_index(['a', 'b']).to_latex()\n        expected = r\"\"\"\\begin{tabular}{llr}\n\\toprule\n  &   &  c \\\\\na & b &    \\\\\n\\midrule\n0 & a &  1 \\\\\n  & b &  2 \\\\\n1 & a &  3 \\\\\n  & b &  4 \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(result, expected)\n\n        result = df.groupby('a').describe().to_latex()\n        expected = ('\\\\begin{tabular}{lrrrrrrrr}\\n\\\\toprule\\n{} &     c &     '\n                    ' &           &      &       &      &       &      '\n                    '\\\\\\\\\\n{} & count & mean &       std &  min &   25\\\\% &  '\n                    '50\\\\% &   75\\\\% &  max \\\\\\\\\\na &       &      &          '\n                    ' &      &       &      &       &      \\\\\\\\\\n\\\\midrule\\n0 '\n                    '&   2.0 &  1.5 &  0.707107 &  1.0 &  1.25 &  1.5 &  1.75 '\n                    '&  2.0 \\\\\\\\\\n1 &   2.0 &  3.5 &  0.707107 &  3.0 &  3.25 '\n                    '&  3.5 &  3.75 &  4.0 '\n                    '\\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n')\n\n        self.assertEqual(result, expected)\n\n    def test_to_latex_escape(self):\n        a = 'a'\n        b = 'b'\n\n        test_dict = {u('co^l1'): {a: \"a\",\n                                  b: \"b\"},\n                     u('co$e^x$'): {a: \"a\",\n                                    b: \"b\"}}\n\n        unescaped_result = DataFrame(test_dict).to_latex(escape=False)\n        escaped_result = DataFrame(test_dict).to_latex(\n        )  # default: escape=True\n\n        unescaped_expected = r'''\\begin{tabular}{lll}\n\\toprule\n{} & co$e^x$ & co^l1 \\\\\n\\midrule\na &       a &     a \\\\\nb &       b &     b \\\\\n\\bottomrule\n\\end{tabular}\n'''\n\n        escaped_expected = r'''\\begin{tabular}{lll}\n\\toprule\n{} & co\\$e\\textasciicircumx\\$ & co\\textasciicircuml1 \\\\\n\\midrule\na &       a &     a \\\\\nb &       b &     b \\\\\n\\bottomrule\n\\end{tabular}\n'''\n\n        self.assertEqual(unescaped_result, unescaped_expected)\n        self.assertEqual(escaped_result, escaped_expected)\n\n    def test_to_latex_longtable(self):\n        self.frame.to_latex(longtable=True)\n\n        df = DataFrame({'a': [1, 2], 'b': ['b1', 'b2']})\n        withindex_result = df.to_latex(longtable=True)\n        withindex_expected = r\"\"\"\\begin{longtable}{lrl}\n\\toprule\n{} &  a &   b \\\\\n\\midrule\n\\endhead\n\\midrule\n\\multicolumn{3}{r}{{Continued on next page}} \\\\\n\\midrule\n\\endfoot\n\n\\bottomrule\n\\endlastfoot\n0 &  1 &  b1 \\\\\n1 &  2 &  b2 \\\\\n\\end{longtable}\n\"\"\"\n\n        self.assertEqual(withindex_result, withindex_expected)\n\n        withoutindex_result = df.to_latex(index=False, longtable=True)\n        withoutindex_expected = r\"\"\"\\begin{longtable}{rl}\n\\toprule\n a &   b \\\\\n\\midrule\n\\endhead\n\\midrule\n\\multicolumn{3}{r}{{Continued on next page}} \\\\\n\\midrule\n\\endfoot\n\n\\bottomrule\n\\endlastfoot\n 1 &  b1 \\\\\n 2 &  b2 \\\\\n\\end{longtable}\n\"\"\"\n\n        self.assertEqual(withoutindex_result, withoutindex_expected)\n\n    def test_to_latex_escape_special_chars(self):\n        special_characters = ['&', '%', '$', '#', '_', '{', '}', '~', '^',\n                              '\\\\']\n        df = DataFrame(data=special_characters)\n        observed = df.to_latex()\n        expected = r\"\"\"\\begin{tabular}{ll}\n\\toprule\n{} &  0 \\\\\n\\midrule\n0 &  \\& \\\\\n1 &  \\% \\\\\n2 &  \\$ \\\\\n3 &  \\# \\\\\n4 &  \\_ \\\\\n5 &  \\{ \\\\\n6 &  \\} \\\\\n7 &  \\textasciitilde \\\\\n8 &  \\textasciicircum \\\\\n9 &  \\textbackslash \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(observed, expected)\n\n    def test_to_latex_no_header(self):\n        # GH 7124\n        df = DataFrame({'a': [1, 2], 'b': ['b1', 'b2']})\n        withindex_result = df.to_latex(header=False)\n        withindex_expected = r\"\"\"\\begin{tabular}{lrl}\n\\toprule\n0 &  1 &  b1 \\\\\n1 &  2 &  b2 \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(withindex_result, withindex_expected)\n\n        withoutindex_result = df.to_latex(index=False, header=False)\n        withoutindex_expected = r\"\"\"\\begin{tabular}{rl}\n\\toprule\n 1 &  b1 \\\\\n 2 &  b2 \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(withoutindex_result, withoutindex_expected)\n\n    def test_to_latex_decimal(self):\n        # GH 12031\n        self.frame.to_latex()\n        df = DataFrame({'a': [1.0, 2.1], 'b': ['b1', 'b2']})\n        withindex_result = df.to_latex(decimal=',')\n        print(\"WHAT THE\")\n        withindex_expected = r\"\"\"\\begin{tabular}{lrl}\n\\toprule\n{} &    a &   b \\\\\n\\midrule\n0 &  1,0 &  b1 \\\\\n1 &  2,1 &  b2 \\\\\n\\bottomrule\n\\end{tabular}\n\"\"\"\n\n        self.assertEqual(withindex_result, withindex_expected)\n\n    def test_to_csv_quotechar(self):\n        df = DataFrame({'col': [1, 2]})\n        expected = \"\"\"\\\n\"\",\"col\"\n\"0\",\"1\"\n\"1\",\"2\"\n\"\"\"\n\n        with tm.ensure_clean('test.csv') as path:\n            df.to_csv(path, quoting=1)  # 1=QUOTE_ALL\n            with open(path, 'r') as f:\n                self.assertEqual(f.read(), expected)\n\n        expected = \"\"\"\\\n$$,$col$\n$0$,$1$\n$1$,$2$\n\"\"\"\n\n        with tm.ensure_clean('test.csv') as path:\n            df.to_csv(path, quoting=1, quotechar=\"$\")\n            with open(path, 'r') as f:\n                self.assertEqual(f.read(), expected)\n\n        with tm.ensure_clean('test.csv') as path:\n            with tm.assertRaisesRegexp(TypeError, 'quotechar'):\n                df.to_csv(path, quoting=1, quotechar=None)\n\n    def test_to_csv_doublequote(self):\n        df = DataFrame({'col': ['a\"a', '\"bb\"']})\n        expected = '''\\\n\"\",\"col\"\n\"0\",\"a\"\"a\"\n\"1\",\"\"\"bb\"\"\"\n'''\n\n        with tm.ensure_clean('test.csv') as path:\n            df.to_csv(path, quoting=1, doublequote=True)  # QUOTE_ALL\n            with open(path, 'r') as f:\n                self.assertEqual(f.read(), expected)\n\n        from _csv import Error\n        with tm.ensure_clean('test.csv') as path:\n            with tm.assertRaisesRegexp(Error, 'escapechar'):\n                df.to_csv(path, doublequote=False)  # no escapechar set\n\n    def test_to_csv_escapechar(self):\n        df = DataFrame({'col': ['a\"a', '\"bb\"']})\n        expected = '''\\\n\"\",\"col\"\n\"0\",\"a\\\\\"a\"\n\"1\",\"\\\\\"bb\\\\\"\"\n'''\n\n        with tm.ensure_clean('test.csv') as path:  # QUOTE_ALL\n            df.to_csv(path, quoting=1, doublequote=False, escapechar='\\\\')\n            with open(path, 'r') as f:\n                self.assertEqual(f.read(), expected)\n\n        df = DataFrame({'col': ['a,a', ',bb,']})\n        expected = \"\"\"\\\n,col\n0,a\\\\,a\n1,\\\\,bb\\\\,\n\"\"\"\n\n        with tm.ensure_clean('test.csv') as path:\n            df.to_csv(path, quoting=3, escapechar='\\\\')  # QUOTE_NONE\n            with open(path, 'r') as f:\n                self.assertEqual(f.read(), expected)\n\n    def test_csv_to_string(self):\n        df = DataFrame({'col': [1, 2]})\n        expected = ',col\\n0,1\\n1,2\\n'\n        self.assertEqual(df.to_csv(), expected)\n\n    def test_to_csv_decimal(self):\n        # GH 781\n        df = DataFrame({'col1': [1], 'col2': ['a'], 'col3': [10.1]})\n\n        expected_default = ',col1,col2,col3\\n0,1,a,10.1\\n'\n        self.assertEqual(df.to_csv(), expected_default)\n\n        expected_european_excel = ';col1;col2;col3\\n0;1;a;10,1\\n'\n        self.assertEqual(\n            df.to_csv(decimal=',', sep=';'), expected_european_excel)\n\n        expected_float_format_default = ',col1,col2,col3\\n0,1,a,10.10\\n'\n        self.assertEqual(\n            df.to_csv(float_format='%.2f'), expected_float_format_default)\n\n        expected_float_format = ';col1;col2;col3\\n0;1;a;10,10\\n'\n        self.assertEqual(\n            df.to_csv(decimal=',', sep=';',\n                      float_format='%.2f'), expected_float_format)\n\n        # GH 11553: testing if decimal is taken into account for '0.0'\n        df = pd.DataFrame({'a': [0, 1.1], 'b': [2.2, 3.3], 'c': 1})\n        expected = 'a,b,c\\n0^0,2^2,1\\n1^1,3^3,1\\n'\n        self.assertEqual(df.to_csv(index=False, decimal='^'), expected)\n\n        # same but for an index\n        self.assertEqual(df.set_index('a').to_csv(decimal='^'), expected)\n\n        # same for a multi-index\n        self.assertEqual(\n            df.set_index(['a', 'b']).to_csv(decimal=\"^\"), expected)\n\n    def test_to_csv_float_format(self):\n        # testing if float_format is taken into account for the index\n        # GH 11553\n        df = pd.DataFrame({'a': [0, 1], 'b': [2.2, 3.3], 'c': 1})\n        expected = 'a,b,c\\n0,2.20,1\\n1,3.30,1\\n'\n        self.assertEqual(\n            df.set_index('a').to_csv(float_format='%.2f'), expected)\n\n        # same for a multi-index\n        self.assertEqual(\n            df.set_index(['a', 'b']).to_csv(float_format='%.2f'), expected)\n\n    def test_to_csv_na_rep(self):\n        # testing if NaN values are correctly represented in the index\n        # GH 11553\n        df = DataFrame({'a': [0, np.NaN], 'b': [0, 1], 'c': [2, 3]})\n        expected = \"a,b,c\\n0.0,0,2\\n_,1,3\\n\"\n        self.assertEqual(df.set_index('a').to_csv(na_rep='_'), expected)\n        self.assertEqual(df.set_index(['a', 'b']).to_csv(na_rep='_'), expected)\n\n        # now with an index containing only NaNs\n        df = DataFrame({'a': np.NaN, 'b': [0, 1], 'c': [2, 3]})\n        expected = \"a,b,c\\n_,0,2\\n_,1,3\\n\"\n        self.assertEqual(df.set_index('a').to_csv(na_rep='_'), expected)\n        self.assertEqual(df.set_index(['a', 'b']).to_csv(na_rep='_'), expected)\n\n        # check if na_rep parameter does not break anything when no NaN\n        df = DataFrame({'a': 0, 'b': [0, 1], 'c': [2, 3]})\n        expected = \"a,b,c\\n0,0,2\\n0,1,3\\n\"\n        self.assertEqual(df.set_index('a').to_csv(na_rep='_'), expected)\n        self.assertEqual(df.set_index(['a', 'b']).to_csv(na_rep='_'), expected)\n\n    def test_to_csv_date_format(self):\n        # GH 10209\n        df_sec = DataFrame({'A': pd.date_range('20130101', periods=5, freq='s')\n                            })\n        df_day = DataFrame({'A': pd.date_range('20130101', periods=5, freq='d')\n                            })\n\n        expected_default_sec = ',A\\n0,2013-01-01 00:00:00\\n1,2013-01-01 00:00:01\\n2,2013-01-01 00:00:02' + \\\n                               '\\n3,2013-01-01 00:00:03\\n4,2013-01-01 00:00:04\\n'\n        self.assertEqual(df_sec.to_csv(), expected_default_sec)\n\n        expected_ymdhms_day = ',A\\n0,2013-01-01 00:00:00\\n1,2013-01-02 00:00:00\\n2,2013-01-03 00:00:00' + \\\n                              '\\n3,2013-01-04 00:00:00\\n4,2013-01-05 00:00:00\\n'\n        self.assertEqual(\n            df_day.to_csv(\n                date_format='%Y-%m-%d %H:%M:%S'), expected_ymdhms_day)\n\n        expected_ymd_sec = ',A\\n0,2013-01-01\\n1,2013-01-01\\n2,2013-01-01\\n3,2013-01-01\\n4,2013-01-01\\n'\n        self.assertEqual(\n            df_sec.to_csv(date_format='%Y-%m-%d'), expected_ymd_sec)\n\n        expected_default_day = ',A\\n0,2013-01-01\\n1,2013-01-02\\n2,2013-01-03\\n3,2013-01-04\\n4,2013-01-05\\n'\n        self.assertEqual(df_day.to_csv(), expected_default_day)\n        self.assertEqual(\n            df_day.to_csv(date_format='%Y-%m-%d'), expected_default_day)\n\n        # testing if date_format parameter is taken into account for\n        # multi-indexed dataframes (GH 7791)\n        df_sec['B'] = 0\n        df_sec['C'] = 1\n        expected_ymd_sec = 'A,B,C\\n2013-01-01,0,1\\n'\n        df_sec_grouped = df_sec.groupby([pd.Grouper(key='A', freq='1h'), 'B'])\n        self.assertEqual(df_sec_grouped.mean().to_csv(date_format='%Y-%m-%d'),\n                         expected_ymd_sec)\n\n    def test_to_csv_multi_index(self):\n        # see gh-6618\n        df = DataFrame([1], columns=pd.MultiIndex.from_arrays([[1], [2]]))\n\n        exp = \",1\\n,2\\n0,1\\n\"\n        self.assertEqual(df.to_csv(), exp)\n\n        exp = \"1\\n2\\n1\\n\"\n        self.assertEqual(df.to_csv(index=False), exp)\n\n        df = DataFrame([1], columns=pd.MultiIndex.from_arrays([[1], [2]]),\n                       index=pd.MultiIndex.from_arrays([[1], [2]]))\n\n        exp = \",,1\\n,,2\\n1,2,1\\n\"\n        self.assertEqual(df.to_csv(), exp)\n\n        exp = \"1\\n2\\n1\\n\"\n        self.assertEqual(df.to_csv(index=False), exp)\n\n        df = DataFrame(\n            [1], columns=pd.MultiIndex.from_arrays([['foo'], ['bar']]))\n\n        exp = \",foo\\n,bar\\n0,1\\n\"\n        self.assertEqual(df.to_csv(), exp)\n\n        exp = \"foo\\nbar\\n1\\n\"\n        self.assertEqual(df.to_csv(index=False), exp)\n\n    def test_period(self):\n        # GH 12615\n        df = pd.DataFrame({'A': pd.period_range('2013-01',\n                                                periods=4, freq='M'),\n                           'B': [pd.Period('2011-01', freq='M'),\n                                 pd.Period('2011-02-01', freq='D'),\n                                 pd.Period('2011-03-01 09:00', freq='H'),\n                                 pd.Period('2011-04', freq='M')],\n                           'C': list('abcd')})\n        exp = (\"        A                B  C\\n0 2013-01          2011-01  a\\n\"\n               \"1 2013-02       2011-02-01  b\\n2 2013-03 2011-03-01 09:00  c\\n\"\n               \"3 2013-04          2011-04  d\")\n        self.assertEqual(str(df), exp)\n\n\nclass TestSeriesFormatting(tm.TestCase):\n\n    def setUp(self):\n        self.ts = tm.makeTimeSeries()\n\n    def test_repr_unicode(self):\n        s = Series([u('\\u03c3')] * 10)\n        repr(s)\n\n        a = Series([u(\"\\u05d0\")] * 1000)\n        a.name = 'title1'\n        repr(a)\n\n    def test_to_string(self):\n        buf = StringIO()\n\n        s = self.ts.to_string()\n\n        retval = self.ts.to_string(buf=buf)\n        self.assertIsNone(retval)\n        self.assertEqual(buf.getvalue().strip(), s)\n\n        # pass float_format\n        format = '%.4f'.__mod__\n        result = self.ts.to_string(float_format=format)\n        result = [x.split()[1] for x in result.split('\\n')[:-1]]\n        expected = [format(x) for x in self.ts]\n        self.assertEqual(result, expected)\n\n        # empty string\n        result = self.ts[:0].to_string()\n        self.assertEqual(result, 'Series([], Freq: B)')\n\n        result = self.ts[:0].to_string(length=0)\n        self.assertEqual(result, 'Series([], Freq: B)')\n\n        # name and length\n        cp = self.ts.copy()\n        cp.name = 'foo'\n        result = cp.to_string(length=True, name=True, dtype=True)\n        last_line = result.split('\\n')[-1].strip()\n        self.assertEqual(last_line,\n                         \"Freq: B, Name: foo, Length: %d, dtype: float64\" %\n                         len(cp))\n\n    def test_freq_name_separation(self):\n        s = Series(np.random.randn(10),\n                   index=date_range('1/1/2000', periods=10), name=0)\n\n        result = repr(s)\n        self.assertTrue('Freq: D, Name: 0' in result)\n\n    def test_to_string_mixed(self):\n        s = Series(['foo', np.nan, -1.23, 4.56])\n        result = s.to_string()\n        expected = (u('0     foo\\n') + u('1     NaN\\n') + u('2   -1.23\\n') +\n                    u('3    4.56'))\n        self.assertEqual(result, expected)\n\n        # but don't count NAs as floats\n        s = Series(['foo', np.nan, 'bar', 'baz'])\n        result = s.to_string()\n        expected = (u('0    foo\\n') + '1    NaN\\n' + '2    bar\\n' + '3    baz')\n        self.assertEqual(result, expected)\n\n        s = Series(['foo', 5, 'bar', 'baz'])\n        result = s.to_string()\n        expected = (u('0    foo\\n') + '1      5\\n' + '2    bar\\n' + '3    baz')\n        self.assertEqual(result, expected)\n\n    def test_to_string_float_na_spacing(self):\n        s = Series([0., 1.5678, 2., -3., 4.])\n        s[::2] = np.nan\n\n        result = s.to_string()\n        expected = (u('0       NaN\\n') + '1    1.5678\\n' + '2       NaN\\n' +\n                    '3   -3.0000\\n' + '4       NaN')\n        self.assertEqual(result, expected)\n\n    def test_to_string_without_index(self):\n        # GH 11729 Test index=False option\n        s = Series([1, 2, 3, 4])\n        result = s.to_string(index=False)\n        expected = (u('1\\n') + '2\\n' + '3\\n' + '4')\n        self.assertEqual(result, expected)\n\n    def test_unicode_name_in_footer(self):\n        s = Series([1, 2], name=u('\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea'))\n        sf = fmt.SeriesFormatter(s, name=u('\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea'))\n        sf._get_footer()  # should not raise exception\n\n    def test_east_asian_unicode_series(self):\n        if PY3:\n            _rep = repr\n        else:\n            _rep = unicode\n        # not alighned properly because of east asian width\n\n        # unicode index\n        s = Series(['a', 'bb', 'CCC', 'D'],\n                   index=[u'あ', u'いい', u'ううう', u'ええええ'])\n        expected = (u\"あ         a\\nいい       bb\\nううう     CCC\\n\"\n                    u\"ええええ      D\\ndtype: object\")\n        self.assertEqual(_rep(s), expected)\n\n        # unicode values\n        s = Series([u'あ', u'いい', u'ううう', u'ええええ'],\n                   index=['a', 'bb', 'c', 'ddd'])\n        expected = (u\"a         あ\\nbb       いい\\nc       ううう\\n\"\n                    u\"ddd    ええええ\\ndtype: object\")\n        self.assertEqual(_rep(s), expected)\n\n        # both\n        s = Series([u'あ', u'いい', u'ううう', u'ええええ'],\n                   index=[u'ああ', u'いいいい', u'う', u'えええ'])\n        expected = (u\"ああ         あ\\nいいいい      いい\\nう        ううう\\n\"\n                    u\"えええ     ええええ\\ndtype: object\")\n        self.assertEqual(_rep(s), expected)\n\n        # unicode footer\n        s = Series([u'あ', u'いい', u'ううう', u'ええええ'],\n                   index=[u'ああ', u'いいいい', u'う', u'えええ'], name=u'おおおおおおお')\n        expected = (u\"ああ         あ\\nいいいい      いい\\nう        ううう\\n\"\n                    u\"えええ     ええええ\\nName: おおおおおおお, dtype: object\")\n        self.assertEqual(_rep(s), expected)\n\n        # MultiIndex\n        idx = pd.MultiIndex.from_tuples([(u'あ', u'いい'), (u'う', u'え'), (\n            u'おおお', u'かかかか'), (u'き', u'くく')])\n        s = Series([1, 22, 3333, 44444], index=idx)\n        expected = (u\"あ    いい          1\\nう    え          22\\nおおお  かかかか     3333\\n\"\n                    u\"き    くく      44444\\ndtype: int64\")\n        self.assertEqual(_rep(s), expected)\n\n        # object dtype, shorter than unicode repr\n        s = Series([1, 22, 3333, 44444], index=[1, 'AB', np.nan, u'あああ'])\n        expected = (u\"1          1\\nAB        22\\nNaN     3333\\n\"\n                    u\"あああ    44444\\ndtype: int64\")\n        self.assertEqual(_rep(s), expected)\n\n        # object dtype, longer than unicode repr\n        s = Series([1, 22, 3333, 44444],\n                   index=[1, 'AB', pd.Timestamp('2011-01-01'), u'あああ'])\n        expected = (u\"1                          1\\nAB                        22\\n\"\n                    u\"2011-01-01 00:00:00     3333\\nあああ                    44444\\ndtype: int64\"\n                    )\n        self.assertEqual(_rep(s), expected)\n\n        # truncate\n        with option_context('display.max_rows', 3):\n            s = Series([u'あ', u'いい', u'ううう', u'ええええ'], name=u'おおおおおおお')\n\n            expected = (u\"0       あ\\n     ... \\n\"\n                        u\"3    ええええ\\nName: おおおおおおお, dtype: object\")\n            self.assertEqual(_rep(s), expected)\n\n            s.index = [u'ああ', u'いいいい', u'う', u'えええ']\n            expected = (u\"ああ        あ\\n       ... \\n\"\n                        u\"えええ    ええええ\\nName: おおおおおおお, dtype: object\")\n            self.assertEqual(_rep(s), expected)\n\n        # Emable Unicode option -----------------------------------------\n        with option_context('display.unicode.east_asian_width', True):\n\n            # unicode index\n            s = Series(['a', 'bb', 'CCC', 'D'],\n                       index=[u'あ', u'いい', u'ううう', u'ええええ'])\n            expected = (u\"あ            a\\nいい         bb\\nううう      CCC\\n\"\n                        u\"ええええ      D\\ndtype: object\")\n            self.assertEqual(_rep(s), expected)\n\n            # unicode values\n            s = Series([u'あ', u'いい', u'ううう', u'ええええ'],\n                       index=['a', 'bb', 'c', 'ddd'])\n            expected = (u\"a            あ\\nbb         いい\\nc        ううう\\n\"\n                        u\"ddd    ええええ\\ndtype: object\")\n            self.assertEqual(_rep(s), expected)\n\n            # both\n            s = Series([u'あ', u'いい', u'ううう', u'ええええ'],\n                       index=[u'ああ', u'いいいい', u'う', u'えええ'])\n            expected = (u\"ああ              あ\\nいいいい        いい\\nう            ううう\\n\"\n                        u\"えええ      ええええ\\ndtype: object\")\n            self.assertEqual(_rep(s), expected)\n\n            # unicode footer\n            s = Series([u'あ', u'いい', u'ううう', u'ええええ'],\n                       index=[u'ああ', u'いいいい', u'う', u'えええ'], name=u'おおおおおおお')\n            expected = (u\"ああ              あ\\nいいいい        いい\\nう            ううう\\n\"\n                        u\"えええ      ええええ\\nName: おおおおおおお, dtype: object\")\n            self.assertEqual(_rep(s), expected)\n\n            # MultiIndex\n            idx = pd.MultiIndex.from_tuples([(u'あ', u'いい'), (u'う', u'え'), (\n                u'おおお', u'かかかか'), (u'き', u'くく')])\n            s = Series([1, 22, 3333, 44444], index=idx)\n            expected = (u\"あ      いい            1\\nう      え             22\\nおおお  かかかか     3333\\n\"\n                        u\"き      くく        44444\\ndtype: int64\")\n            self.assertEqual(_rep(s), expected)\n\n            # object dtype, shorter than unicode repr\n            s = Series([1, 22, 3333, 44444], index=[1, 'AB', np.nan, u'あああ'])\n            expected = (u\"1             1\\nAB           22\\nNaN        3333\\n\"\n                        u\"あああ    44444\\ndtype: int64\")\n            self.assertEqual(_rep(s), expected)\n\n            # object dtype, longer than unicode repr\n            s = Series([1, 22, 3333, 44444],\n                       index=[1, 'AB', pd.Timestamp('2011-01-01'), u'あああ'])\n            expected = (u\"1                          1\\nAB                        22\\n\"\n                        u\"2011-01-01 00:00:00     3333\\nあああ                 44444\\ndtype: int64\"\n                        )\n            self.assertEqual(_rep(s), expected)\n\n            # truncate\n            with option_context('display.max_rows', 3):\n                s = Series([u'あ', u'いい', u'ううう', u'ええええ'], name=u'おおおおおおお')\n                expected = (u\"0          あ\\n       ...   \\n\"\n                            u\"3    ええええ\\nName: おおおおおおお, dtype: object\")\n                self.assertEqual(_rep(s), expected)\n\n                s.index = [u'ああ', u'いいいい', u'う', u'えええ']\n                expected = (u\"ああ            あ\\n            ...   \\n\"\n                            u\"えええ    ええええ\\nName: おおおおおおお, dtype: object\")\n                self.assertEqual(_rep(s), expected)\n\n            # ambiguous unicode\n            s = Series([u'¡¡', u'い¡¡', u'ううう', u'ええええ'],\n                       index=[u'ああ', u'¡¡¡¡いい', u'¡¡', u'えええ'])\n            expected = (u\"ああ              ¡¡\\n¡¡¡¡いい        い¡¡\\n¡¡            ううう\\n\"\n                        u\"えええ      ええええ\\ndtype: object\")\n            self.assertEqual(_rep(s), expected)\n\n    def test_float_trim_zeros(self):\n        vals = [2.08430917305e+10, 3.52205017305e+10, 2.30674817305e+10,\n                2.03954217305e+10, 5.59897817305e+10]\n        for line in repr(Series(vals)).split('\\n'):\n            if line.startswith('dtype:'):\n                continue\n            if _three_digit_exp():\n                self.assertIn('+010', line)\n            else:\n                self.assertIn('+10', line)\n\n    def test_datetimeindex(self):\n\n        index = date_range('20130102', periods=6)\n        s = Series(1, index=index)\n        result = s.to_string()\n        self.assertTrue('2013-01-02' in result)\n\n        # nat in index\n        s2 = Series(2, index=[Timestamp('20130111'), NaT])\n        s = s2.append(s)\n        result = s.to_string()\n        self.assertTrue('NaT' in result)\n\n        # nat in summary\n        result = str(s2.index)\n        self.assertTrue('NaT' in result)\n\n    def test_timedelta64(self):\n\n        from datetime import datetime, timedelta\n\n        Series(np.array([1100, 20], dtype='timedelta64[ns]')).to_string()\n\n        s = Series(date_range('2012-1-1', periods=3, freq='D'))\n\n        # GH2146\n\n        # adding NaTs\n        y = s - s.shift(1)\n        result = y.to_string()\n        self.assertTrue('1 days' in result)\n        self.assertTrue('00:00:00' not in result)\n        self.assertTrue('NaT' in result)\n\n        # with frac seconds\n        o = Series([datetime(2012, 1, 1, microsecond=150)] * 3)\n        y = s - o\n        result = y.to_string()\n        self.assertTrue('-1 days +23:59:59.999850' in result)\n\n        # rounding?\n        o = Series([datetime(2012, 1, 1, 1)] * 3)\n        y = s - o\n        result = y.to_string()\n        self.assertTrue('-1 days +23:00:00' in result)\n        self.assertTrue('1 days 23:00:00' in result)\n\n        o = Series([datetime(2012, 1, 1, 1, 1)] * 3)\n        y = s - o\n        result = y.to_string()\n        self.assertTrue('-1 days +22:59:00' in result)\n        self.assertTrue('1 days 22:59:00' in result)\n\n        o = Series([datetime(2012, 1, 1, 1, 1, microsecond=150)] * 3)\n        y = s - o\n        result = y.to_string()\n        self.assertTrue('-1 days +22:58:59.999850' in result)\n        self.assertTrue('0 days 22:58:59.999850' in result)\n\n        # neg time\n        td = timedelta(minutes=5, seconds=3)\n        s2 = Series(date_range('2012-1-1', periods=3, freq='D')) + td\n        y = s - s2\n        result = y.to_string()\n        self.assertTrue('-1 days +23:54:57' in result)\n\n        td = timedelta(microseconds=550)\n        s2 = Series(date_range('2012-1-1', periods=3, freq='D')) + td\n        y = s - td\n        result = y.to_string()\n        self.assertTrue('2012-01-01 23:59:59.999450' in result)\n\n        # no boxing of the actual elements\n        td = Series(pd.timedelta_range('1 days', periods=3))\n        result = td.to_string()\n        self.assertEqual(result, u(\"0   1 days\\n1   2 days\\n2   3 days\"))\n\n    def test_mixed_datetime64(self):\n        df = DataFrame({'A': [1, 2], 'B': ['2012-01-01', '2012-01-02']})\n        df['B'] = pd.to_datetime(df.B)\n\n        result = repr(df.loc[0])\n        self.assertTrue('2012-01-01' in result)\n\n    def test_period(self):\n        # GH 12615\n        index = pd.period_range('2013-01', periods=6, freq='M')\n        s = Series(np.arange(6, dtype='int64'), index=index)\n        exp = (\"2013-01    0\\n2013-02    1\\n2013-03    2\\n2013-04    3\\n\"\n               \"2013-05    4\\n2013-06    5\\nFreq: M, dtype: int64\")\n        self.assertEqual(str(s), exp)\n\n        s = Series(index)\n        exp = (\"0   2013-01\\n1   2013-02\\n2   2013-03\\n3   2013-04\\n\"\n               \"4   2013-05\\n5   2013-06\\ndtype: object\")\n        self.assertEqual(str(s), exp)\n\n        # periods with mixed freq\n        s = Series([pd.Period('2011-01', freq='M'),\n                    pd.Period('2011-02-01', freq='D'),\n                    pd.Period('2011-03-01 09:00', freq='H')])\n        exp = (\"0            2011-01\\n1         2011-02-01\\n\"\n               \"2   2011-03-01 09:00\\ndtype: object\")\n        self.assertEqual(str(s), exp)\n\n    def test_max_multi_index_display(self):\n        # GH 7101\n\n        # doc example (indexing.rst)\n\n        # multi-index\n        arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n                  ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\n        tuples = list(zip(*arrays))\n        index = MultiIndex.from_tuples(tuples, names=['first', 'second'])\n        s = Series(randn(8), index=index)\n\n        with option_context(\"display.max_rows\", 10):\n            self.assertEqual(len(str(s).split('\\n')), 10)\n        with option_context(\"display.max_rows\", 3):\n            self.assertEqual(len(str(s).split('\\n')), 5)\n        with option_context(\"display.max_rows\", 2):\n            self.assertEqual(len(str(s).split('\\n')), 5)\n        with option_context(\"display.max_rows\", 1):\n            self.assertEqual(len(str(s).split('\\n')), 4)\n        with option_context(\"display.max_rows\", 0):\n            self.assertEqual(len(str(s).split('\\n')), 10)\n\n        # index\n        s = Series(randn(8), None)\n\n        with option_context(\"display.max_rows\", 10):\n            self.assertEqual(len(str(s).split('\\n')), 9)\n        with option_context(\"display.max_rows\", 3):\n            self.assertEqual(len(str(s).split('\\n')), 4)\n        with option_context(\"display.max_rows\", 2):\n            self.assertEqual(len(str(s).split('\\n')), 4)\n        with option_context(\"display.max_rows\", 1):\n            self.assertEqual(len(str(s).split('\\n')), 3)\n        with option_context(\"display.max_rows\", 0):\n            self.assertEqual(len(str(s).split('\\n')), 9)\n\n    # Make sure #8532 is fixed\n    def test_consistent_format(self):\n        s = pd.Series([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.9999, 1, 1] * 10)\n        with option_context(\"display.max_rows\", 10):\n            res = repr(s)\n        exp = ('0      1.0000\\n1      1.0000\\n2      1.0000\\n3      '\n               '1.0000\\n4      1.0000\\n        ...  \\n125    '\n               '1.0000\\n126    1.0000\\n127    0.9999\\n128    '\n               '1.0000\\n129    1.0000\\ndtype: float64')\n        self.assertEqual(res, exp)\n\n    @staticmethod\n    def gen_test_series():\n        s1 = pd.Series(['a'] * 100)\n        s2 = pd.Series(['ab'] * 100)\n        s3 = pd.Series(['a', 'ab', 'abc', 'abcd', 'abcde', 'abcdef'])\n        s4 = s3[::-1]\n        test_sers = {'onel': s1, 'twol': s2, 'asc': s3, 'desc': s4}\n        return test_sers\n\n    def chck_ncols(self, s):\n        with option_context(\"display.max_rows\", 10):\n            res = repr(s)\n        lines = res.split('\\n')\n        lines = [line for line in repr(s).split('\\n')\n                 if not re.match(r'[^\\.]*\\.+', line)][:-1]\n        ncolsizes = len(set(len(line.strip()) for line in lines))\n        self.assertEqual(ncolsizes, 1)\n\n    def test_format_explicit(self):\n        test_sers = self.gen_test_series()\n        with option_context(\"display.max_rows\", 4):\n            res = repr(test_sers['onel'])\n            exp = '0     a\\n1     a\\n     ..\\n98    a\\n99    a\\ndtype: object'\n            self.assertEqual(exp, res)\n            res = repr(test_sers['twol'])\n            exp = ('0     ab\\n1     ab\\n      ..\\n98    ab\\n99    ab\\ndtype:'\n                   ' object')\n            self.assertEqual(exp, res)\n            res = repr(test_sers['asc'])\n            exp = ('0         a\\n1        ab\\n      ...  \\n4     abcde\\n5'\n                   '    abcdef\\ndtype: object')\n            self.assertEqual(exp, res)\n            res = repr(test_sers['desc'])\n            exp = ('5    abcdef\\n4     abcde\\n      ...  \\n1        ab\\n0'\n                   '         a\\ndtype: object')\n            self.assertEqual(exp, res)\n\n    def test_ncols(self):\n        test_sers = self.gen_test_series()\n        for s in test_sers.values():\n            self.chck_ncols(s)\n\n    def test_max_rows_eq_one(self):\n        s = Series(range(10), dtype='int64')\n        with option_context(\"display.max_rows\", 1):\n            strrepr = repr(s).split('\\n')\n        exp1 = ['0', '0']\n        res1 = strrepr[0].split()\n        self.assertEqual(exp1, res1)\n        exp2 = ['..']\n        res2 = strrepr[1].split()\n        self.assertEqual(exp2, res2)\n\n    def test_truncate_ndots(self):\n        def getndots(s):\n            return len(re.match(r'[^\\.]*(\\.*)', s).groups()[0])\n\n        s = Series([0, 2, 3, 6])\n        with option_context(\"display.max_rows\", 2):\n            strrepr = repr(s).replace('\\n', '')\n        self.assertEqual(getndots(strrepr), 2)\n\n        s = Series([0, 100, 200, 400])\n        with option_context(\"display.max_rows\", 2):\n            strrepr = repr(s).replace('\\n', '')\n        self.assertEqual(getndots(strrepr), 3)\n\n    def test_to_string_name(self):\n        s = Series(range(100), dtype='int64')\n        s.name = 'myser'\n        res = s.to_string(max_rows=2, name=True)\n        exp = '0      0\\n      ..\\n99    99\\nName: myser'\n        self.assertEqual(res, exp)\n        res = s.to_string(max_rows=2, name=False)\n        exp = '0      0\\n      ..\\n99    99'\n        self.assertEqual(res, exp)\n\n    def test_to_string_dtype(self):\n        s = Series(range(100), dtype='int64')\n        res = s.to_string(max_rows=2, dtype=True)\n        exp = '0      0\\n      ..\\n99    99\\ndtype: int64'\n        self.assertEqual(res, exp)\n        res = s.to_string(max_rows=2, dtype=False)\n        exp = '0      0\\n      ..\\n99    99'\n        self.assertEqual(res, exp)\n\n    def test_to_string_length(self):\n        s = Series(range(100), dtype='int64')\n        res = s.to_string(max_rows=2, length=True)\n        exp = '0      0\\n      ..\\n99    99\\nLength: 100'\n        self.assertEqual(res, exp)\n\n    def test_to_string_na_rep(self):\n        s = pd.Series(index=range(100))\n        res = s.to_string(na_rep='foo', max_rows=2)\n        exp = '0    foo\\n      ..\\n99   foo'\n        self.assertEqual(res, exp)\n\n    def test_to_string_float_format(self):\n        s = pd.Series(range(10), dtype='float64')\n        res = s.to_string(float_format=lambda x: '{0:2.1f}'.format(x),\n                          max_rows=2)\n        exp = '0   0.0\\n     ..\\n9   9.0'\n        self.assertEqual(res, exp)\n\n    def test_to_string_header(self):\n        s = pd.Series(range(10), dtype='int64')\n        s.index.name = 'foo'\n        res = s.to_string(header=True, max_rows=2)\n        exp = 'foo\\n0    0\\n    ..\\n9    9'\n        self.assertEqual(res, exp)\n        res = s.to_string(header=False, max_rows=2)\n        exp = '0    0\\n    ..\\n9    9'\n        self.assertEqual(res, exp)\n\n\nclass TestEngFormatter(tm.TestCase):\n\n    def test_eng_float_formatter(self):\n        df = DataFrame({'A': [1.41, 141., 14100, 1410000.]})\n\n        fmt.set_eng_float_format()\n        result = df.to_string()\n        expected = ('             A\\n'\n                    '0    1.410E+00\\n'\n                    '1  141.000E+00\\n'\n                    '2   14.100E+03\\n'\n                    '3    1.410E+06')\n        self.assertEqual(result, expected)\n\n        fmt.set_eng_float_format(use_eng_prefix=True)\n        result = df.to_string()\n        expected = ('         A\\n'\n                    '0    1.410\\n'\n                    '1  141.000\\n'\n                    '2  14.100k\\n'\n                    '3   1.410M')\n        self.assertEqual(result, expected)\n\n        fmt.set_eng_float_format(accuracy=0)\n        result = df.to_string()\n        expected = ('         A\\n'\n                    '0    1E+00\\n'\n                    '1  141E+00\\n'\n                    '2   14E+03\\n'\n                    '3    1E+06')\n        self.assertEqual(result, expected)\n\n        self.reset_display_options()\n\n    def compare(self, formatter, input, output):\n        formatted_input = formatter(input)\n        msg = (\"formatting of %s results in '%s', expected '%s'\" %\n               (str(input), formatted_input, output))\n        self.assertEqual(formatted_input, output, msg)\n\n    def compare_all(self, formatter, in_out):\n        \"\"\"\n        Parameters:\n        -----------\n        formatter: EngFormatter under test\n        in_out: list of tuples. Each tuple = (number, expected_formatting)\n\n        It is tested if 'formatter(number) == expected_formatting'.\n        *number* should be >= 0 because formatter(-number) == fmt is also\n        tested. *fmt* is derived from *expected_formatting*\n        \"\"\"\n        for input, output in in_out:\n            self.compare(formatter, input, output)\n            self.compare(formatter, -input, \"-\" + output[1:])\n\n    def test_exponents_with_eng_prefix(self):\n        formatter = fmt.EngFormatter(accuracy=3, use_eng_prefix=True)\n        f = np.sqrt(2)\n        in_out = [(f * 10 ** -24, \" 1.414y\"), (f * 10 ** -23, \" 14.142y\"),\n                  (f * 10 ** -22, \" 141.421y\"), (f * 10 ** -21, \" 1.414z\"),\n                  (f * 10 ** -20, \" 14.142z\"), (f * 10 ** -19, \" 141.421z\"),\n                  (f * 10 ** -18, \" 1.414a\"), (f * 10 ** -17, \" 14.142a\"),\n                  (f * 10 ** -16, \" 141.421a\"), (f * 10 ** -15, \" 1.414f\"),\n                  (f * 10 ** -14, \" 14.142f\"), (f * 10 ** -13, \" 141.421f\"),\n                  (f * 10 ** -12, \" 1.414p\"), (f * 10 ** -11, \" 14.142p\"),\n                  (f * 10 ** -10, \" 141.421p\"), (f * 10 ** -9, \" 1.414n\"),\n                  (f * 10 ** -8, \" 14.142n\"), (f * 10 ** -7, \" 141.421n\"),\n                  (f * 10 ** -6, \" 1.414u\"), (f * 10 ** -5, \" 14.142u\"),\n                  (f * 10 ** -4, \" 141.421u\"), (f * 10 ** -3, \" 1.414m\"),\n                  (f * 10 ** -2, \" 14.142m\"), (f * 10 ** -1, \" 141.421m\"),\n                  (f * 10 ** 0, \" 1.414\"), (f * 10 ** 1, \" 14.142\"),\n                  (f * 10 ** 2, \" 141.421\"), (f * 10 ** 3, \" 1.414k\"),\n                  (f * 10 ** 4, \" 14.142k\"), (f * 10 ** 5, \" 141.421k\"),\n                  (f * 10 ** 6, \" 1.414M\"), (f * 10 ** 7, \" 14.142M\"),\n                  (f * 10 ** 8, \" 141.421M\"), (f * 10 ** 9, \" 1.414G\"), (\n                      f * 10 ** 10, \" 14.142G\"), (f * 10 ** 11, \" 141.421G\"),\n                  (f * 10 ** 12, \" 1.414T\"), (f * 10 ** 13, \" 14.142T\"), (\n                      f * 10 ** 14, \" 141.421T\"), (f * 10 ** 15, \" 1.414P\"), (\n                          f * 10 ** 16, \" 14.142P\"), (f * 10 ** 17, \" 141.421P\"), (\n                              f * 10 ** 18, \" 1.414E\"), (f * 10 ** 19, \" 14.142E\"),\n                  (f * 10 ** 20, \" 141.421E\"), (f * 10 ** 21, \" 1.414Z\"), (\n                      f * 10 ** 22, \" 14.142Z\"), (f * 10 ** 23, \" 141.421Z\"), (\n                          f * 10 ** 24, \" 1.414Y\"), (f * 10 ** 25, \" 14.142Y\"), (\n                              f * 10 ** 26, \" 141.421Y\")]\n        self.compare_all(formatter, in_out)\n\n    def test_exponents_without_eng_prefix(self):\n        formatter = fmt.EngFormatter(accuracy=4, use_eng_prefix=False)\n        f = np.pi\n        in_out = [(f * 10 ** -24, \" 3.1416E-24\"),\n                  (f * 10 ** -23, \" 31.4159E-24\"),\n                  (f * 10 ** -22, \" 314.1593E-24\"),\n                  (f * 10 ** -21, \" 3.1416E-21\"),\n                  (f * 10 ** -20, \" 31.4159E-21\"),\n                  (f * 10 ** -19, \" 314.1593E-21\"),\n                  (f * 10 ** -18, \" 3.1416E-18\"),\n                  (f * 10 ** -17, \" 31.4159E-18\"),\n                  (f * 10 ** -16, \" 314.1593E-18\"),\n                  (f * 10 ** -15, \" 3.1416E-15\"),\n                  (f * 10 ** -14, \" 31.4159E-15\"),\n                  (f * 10 ** -13, \" 314.1593E-15\"),\n                  (f * 10 ** -12, \" 3.1416E-12\"),\n                  (f * 10 ** -11, \" 31.4159E-12\"),\n                  (f * 10 ** -10, \" 314.1593E-12\"),\n                  (f * 10 ** -9, \" 3.1416E-09\"), (f * 10 ** -8, \" 31.4159E-09\"),\n                  (f * 10 ** -7, \" 314.1593E-09\"), (f * 10 ** -6, \" 3.1416E-06\"),\n                  (f * 10 ** -5, \" 31.4159E-06\"), (f * 10 ** -4,\n                                                   \" 314.1593E-06\"),\n                  (f * 10 ** -3, \" 3.1416E-03\"), (f * 10 ** -2, \" 31.4159E-03\"),\n                  (f * 10 ** -1, \" 314.1593E-03\"), (f * 10 ** 0, \" 3.1416E+00\"), (\n                      f * 10 ** 1, \" 31.4159E+00\"), (f * 10 ** 2, \" 314.1593E+00\"),\n                  (f * 10 ** 3, \" 3.1416E+03\"), (f * 10 ** 4, \" 31.4159E+03\"), (\n                      f * 10 ** 5, \" 314.1593E+03\"), (f * 10 ** 6, \" 3.1416E+06\"),\n                  (f * 10 ** 7, \" 31.4159E+06\"), (f * 10 ** 8, \" 314.1593E+06\"), (\n                      f * 10 ** 9, \" 3.1416E+09\"), (f * 10 ** 10, \" 31.4159E+09\"),\n                  (f * 10 ** 11, \" 314.1593E+09\"), (f * 10 ** 12, \" 3.1416E+12\"),\n                  (f * 10 ** 13, \" 31.4159E+12\"), (f * 10 ** 14, \" 314.1593E+12\"),\n                  (f * 10 ** 15, \" 3.1416E+15\"), (f * 10 ** 16, \" 31.4159E+15\"),\n                  (f * 10 ** 17, \" 314.1593E+15\"), (f * 10 ** 18, \" 3.1416E+18\"),\n                  (f * 10 ** 19, \" 31.4159E+18\"), (f * 10 ** 20, \" 314.1593E+18\"),\n                  (f * 10 ** 21, \" 3.1416E+21\"), (f * 10 ** 22, \" 31.4159E+21\"),\n                  (f * 10 ** 23, \" 314.1593E+21\"), (f * 10 ** 24, \" 3.1416E+24\"),\n                  (f * 10 ** 25, \" 31.4159E+24\"), (f * 10 ** 26, \" 314.1593E+24\")]\n        self.compare_all(formatter, in_out)\n\n    def test_rounding(self):\n        formatter = fmt.EngFormatter(accuracy=3, use_eng_prefix=True)\n        in_out = [(5.55555, ' 5.556'), (55.5555, ' 55.556'),\n                  (555.555, ' 555.555'), (5555.55, ' 5.556k'),\n                  (55555.5, ' 55.556k'), (555555, ' 555.555k')]\n        self.compare_all(formatter, in_out)\n\n        formatter = fmt.EngFormatter(accuracy=1, use_eng_prefix=True)\n        in_out = [(5.55555, ' 5.6'), (55.5555, ' 55.6'), (555.555, ' 555.6'),\n                  (5555.55, ' 5.6k'), (55555.5, ' 55.6k'), (555555, ' 555.6k')]\n        self.compare_all(formatter, in_out)\n\n        formatter = fmt.EngFormatter(accuracy=0, use_eng_prefix=True)\n        in_out = [(5.55555, ' 6'), (55.5555, ' 56'), (555.555, ' 556'),\n                  (5555.55, ' 6k'), (55555.5, ' 56k'), (555555, ' 556k')]\n        self.compare_all(formatter, in_out)\n\n        formatter = fmt.EngFormatter(accuracy=3, use_eng_prefix=True)\n        result = formatter(0)\n        self.assertEqual(result, u(' 0.000'))\n\n    def test_nan(self):\n        # Issue #11981\n\n        formatter = fmt.EngFormatter(accuracy=1, use_eng_prefix=True)\n        result = formatter(np.nan)\n        self.assertEqual(result, u('NaN'))\n\n        df = pd.DataFrame({'a': [1.5, 10.3, 20.5],\n                           'b': [50.3, 60.67, 70.12],\n                           'c': [100.2, 101.33, 120.33]})\n        pt = df.pivot_table(values='a', index='b', columns='c')\n        fmt.set_eng_float_format(accuracy=1)\n        result = pt.to_string()\n        self.assertTrue('NaN' in result)\n        self.reset_display_options()\n\n    def test_inf(self):\n        # Issue #11981\n\n        formatter = fmt.EngFormatter(accuracy=1, use_eng_prefix=True)\n        result = formatter(np.inf)\n        self.assertEqual(result, u('inf'))\n\n\ndef _three_digit_exp():\n    return '%.4g' % 1.7e8 == '1.7e+008'\n\n\nclass TestFloatArrayFormatter(tm.TestCase):\n\n    def test_misc(self):\n        obj = fmt.FloatArrayFormatter(np.array([], dtype=np.float64))\n        result = obj.get_result()\n        self.assertTrue(len(result) == 0)\n\n    def test_format(self):\n        obj = fmt.FloatArrayFormatter(np.array([12, 0], dtype=np.float64))\n        result = obj.get_result()\n        self.assertEqual(result[0], \" 12.0\")\n        self.assertEqual(result[1], \"  0.0\")\n\n    def test_output_significant_digits(self):\n        # Issue #9764\n\n        # In case default display precision changes:\n        with pd.option_context('display.precision', 6):\n            # DataFrame example from issue #9764\n            d = pd.DataFrame(\n                {'col1': [9.999e-8, 1e-7, 1.0001e-7, 2e-7, 4.999e-7, 5e-7,\n                          5.0001e-7, 6e-7, 9.999e-7, 1e-6, 1.0001e-6, 2e-6,\n                          4.999e-6, 5e-6, 5.0001e-6, 6e-6]})\n\n            expected_output = {\n                (0, 6):\n                '           col1\\n0  9.999000e-08\\n1  1.000000e-07\\n2  1.000100e-07\\n3  2.000000e-07\\n4  4.999000e-07\\n5  5.000000e-07',\n                (1, 6):\n                '           col1\\n1  1.000000e-07\\n2  1.000100e-07\\n3  2.000000e-07\\n4  4.999000e-07\\n5  5.000000e-07',\n                (1, 8):\n                '           col1\\n1  1.000000e-07\\n2  1.000100e-07\\n3  2.000000e-07\\n4  4.999000e-07\\n5  5.000000e-07\\n6  5.000100e-07\\n7  6.000000e-07',\n                (8, 16):\n                '            col1\\n8   9.999000e-07\\n9   1.000000e-06\\n10  1.000100e-06\\n11  2.000000e-06\\n12  4.999000e-06\\n13  5.000000e-06\\n14  5.000100e-06\\n15  6.000000e-06',\n                (9, 16):\n                '        col1\\n9   0.000001\\n10  0.000001\\n11  0.000002\\n12  0.000005\\n13  0.000005\\n14  0.000005\\n15  0.000006'\n            }\n\n            for (start, stop), v in expected_output.items():\n                self.assertEqual(str(d[start:stop]), v)\n\n    def test_too_long(self):\n        # GH 10451\n        with pd.option_context('display.precision', 4):\n            # need both a number > 1e6 and something that normally formats to\n            # having length > display.precision + 6\n            df = pd.DataFrame(dict(x=[12345.6789]))\n            self.assertEqual(str(df), '            x\\n0  12345.6789')\n            df = pd.DataFrame(dict(x=[2e6]))\n            self.assertEqual(str(df), '           x\\n0  2000000.0')\n            df = pd.DataFrame(dict(x=[12345.6789, 2e6]))\n            self.assertEqual(\n                str(df), '            x\\n0  1.2346e+04\\n1  2.0000e+06')\n\n\nclass TestRepr_timedelta64(tm.TestCase):\n\n    def test_none(self):\n        delta_1d = pd.to_timedelta(1, unit='D')\n        delta_0d = pd.to_timedelta(0, unit='D')\n        delta_1s = pd.to_timedelta(1, unit='s')\n        delta_500ms = pd.to_timedelta(500, unit='ms')\n\n        drepr = lambda x: x._repr_base()\n        self.assertEqual(drepr(delta_1d), \"1 days\")\n        self.assertEqual(drepr(-delta_1d), \"-1 days\")\n        self.assertEqual(drepr(delta_0d), \"0 days\")\n        self.assertEqual(drepr(delta_1s), \"0 days 00:00:01\")\n        self.assertEqual(drepr(delta_500ms), \"0 days 00:00:00.500000\")\n        self.assertEqual(drepr(delta_1d + delta_1s), \"1 days 00:00:01\")\n        self.assertEqual(\n            drepr(delta_1d + delta_500ms), \"1 days 00:00:00.500000\")\n\n    def test_even_day(self):\n        delta_1d = pd.to_timedelta(1, unit='D')\n        delta_0d = pd.to_timedelta(0, unit='D')\n        delta_1s = pd.to_timedelta(1, unit='s')\n        delta_500ms = pd.to_timedelta(500, unit='ms')\n\n        drepr = lambda x: x._repr_base(format='even_day')\n        self.assertEqual(drepr(delta_1d), \"1 days\")\n        self.assertEqual(drepr(-delta_1d), \"-1 days\")\n        self.assertEqual(drepr(delta_0d), \"0 days\")\n        self.assertEqual(drepr(delta_1s), \"0 days 00:00:01\")\n        self.assertEqual(drepr(delta_500ms), \"0 days 00:00:00.500000\")\n        self.assertEqual(drepr(delta_1d + delta_1s), \"1 days 00:00:01\")\n        self.assertEqual(\n            drepr(delta_1d + delta_500ms), \"1 days 00:00:00.500000\")\n\n    def test_sub_day(self):\n        delta_1d = pd.to_timedelta(1, unit='D')\n        delta_0d = pd.to_timedelta(0, unit='D')\n        delta_1s = pd.to_timedelta(1, unit='s')\n        delta_500ms = pd.to_timedelta(500, unit='ms')\n\n        drepr = lambda x: x._repr_base(format='sub_day')\n        self.assertEqual(drepr(delta_1d), \"1 days\")\n        self.assertEqual(drepr(-delta_1d), \"-1 days\")\n        self.assertEqual(drepr(delta_0d), \"00:00:00\")\n        self.assertEqual(drepr(delta_1s), \"00:00:01\")\n        self.assertEqual(drepr(delta_500ms), \"00:00:00.500000\")\n        self.assertEqual(drepr(delta_1d + delta_1s), \"1 days 00:00:01\")\n        self.assertEqual(\n            drepr(delta_1d + delta_500ms), \"1 days 00:00:00.500000\")\n\n    def test_long(self):\n        delta_1d = pd.to_timedelta(1, unit='D')\n        delta_0d = pd.to_timedelta(0, unit='D')\n        delta_1s = pd.to_timedelta(1, unit='s')\n        delta_500ms = pd.to_timedelta(500, unit='ms')\n\n        drepr = lambda x: x._repr_base(format='long')\n        self.assertEqual(drepr(delta_1d), \"1 days 00:00:00\")\n        self.assertEqual(drepr(-delta_1d), \"-1 days +00:00:00\")\n        self.assertEqual(drepr(delta_0d), \"0 days 00:00:00\")\n        self.assertEqual(drepr(delta_1s), \"0 days 00:00:01\")\n        self.assertEqual(drepr(delta_500ms), \"0 days 00:00:00.500000\")\n        self.assertEqual(drepr(delta_1d + delta_1s), \"1 days 00:00:01\")\n        self.assertEqual(\n            drepr(delta_1d + delta_500ms), \"1 days 00:00:00.500000\")\n\n    def test_all(self):\n        delta_1d = pd.to_timedelta(1, unit='D')\n        delta_0d = pd.to_timedelta(0, unit='D')\n        delta_1ns = pd.to_timedelta(1, unit='ns')\n\n        drepr = lambda x: x._repr_base(format='all')\n        self.assertEqual(drepr(delta_1d), \"1 days 00:00:00.000000000\")\n        self.assertEqual(drepr(delta_0d), \"0 days 00:00:00.000000000\")\n        self.assertEqual(drepr(delta_1ns), \"0 days 00:00:00.000000001\")\n\n\nclass TestTimedelta64Formatter(tm.TestCase):\n\n    def test_days(self):\n        x = pd.to_timedelta(list(range(5)) + [pd.NaT], unit='D')\n        result = fmt.Timedelta64Formatter(x, box=True).get_result()\n        self.assertEqual(result[0].strip(), \"'0 days'\")\n        self.assertEqual(result[1].strip(), \"'1 days'\")\n\n        result = fmt.Timedelta64Formatter(x[1:2], box=True).get_result()\n        self.assertEqual(result[0].strip(), \"'1 days'\")\n\n        result = fmt.Timedelta64Formatter(x, box=False).get_result()\n        self.assertEqual(result[0].strip(), \"0 days\")\n        self.assertEqual(result[1].strip(), \"1 days\")\n\n        result = fmt.Timedelta64Formatter(x[1:2], box=False).get_result()\n        self.assertEqual(result[0].strip(), \"1 days\")\n\n    def test_days_neg(self):\n        x = pd.to_timedelta(list(range(5)) + [pd.NaT], unit='D')\n        result = fmt.Timedelta64Formatter(-x, box=True).get_result()\n        self.assertEqual(result[0].strip(), \"'0 days'\")\n        self.assertEqual(result[1].strip(), \"'-1 days'\")\n\n    def test_subdays(self):\n        y = pd.to_timedelta(list(range(5)) + [pd.NaT], unit='s')\n        result = fmt.Timedelta64Formatter(y, box=True).get_result()\n        self.assertEqual(result[0].strip(), \"'00:00:00'\")\n        self.assertEqual(result[1].strip(), \"'00:00:01'\")\n\n    def test_subdays_neg(self):\n        y = pd.to_timedelta(list(range(5)) + [pd.NaT], unit='s')\n        result = fmt.Timedelta64Formatter(-y, box=True).get_result()\n        self.assertEqual(result[0].strip(), \"'00:00:00'\")\n        self.assertEqual(result[1].strip(), \"'-1 days +23:59:59'\")\n\n    def test_zero(self):\n        x = pd.to_timedelta(list(range(1)) + [pd.NaT], unit='D')\n        result = fmt.Timedelta64Formatter(x, box=True).get_result()\n        self.assertEqual(result[0].strip(), \"'0 days'\")\n\n        x = pd.to_timedelta(list(range(1)), unit='D')\n        result = fmt.Timedelta64Formatter(x, box=True).get_result()\n        self.assertEqual(result[0].strip(), \"'0 days'\")\n\n\nclass TestDatetime64Formatter(tm.TestCase):\n\n    def test_mixed(self):\n        x = Series([datetime(2013, 1, 1), datetime(2013, 1, 1, 12), pd.NaT])\n        result = fmt.Datetime64Formatter(x).get_result()\n        self.assertEqual(result[0].strip(), \"2013-01-01 00:00:00\")\n        self.assertEqual(result[1].strip(), \"2013-01-01 12:00:00\")\n\n    def test_dates(self):\n        x = Series([datetime(2013, 1, 1), datetime(2013, 1, 2), pd.NaT])\n        result = fmt.Datetime64Formatter(x).get_result()\n        self.assertEqual(result[0].strip(), \"2013-01-01\")\n        self.assertEqual(result[1].strip(), \"2013-01-02\")\n\n    def test_date_nanos(self):\n        x = Series([Timestamp(200)])\n        result = fmt.Datetime64Formatter(x).get_result()\n        self.assertEqual(result[0].strip(), \"1970-01-01 00:00:00.000000200\")\n\n    def test_dates_display(self):\n\n        # 10170\n        # make sure that we are consistently display date formatting\n        x = Series(date_range('20130101 09:00:00', periods=5, freq='D'))\n        x.iloc[1] = np.nan\n        result = fmt.Datetime64Formatter(x).get_result()\n        self.assertEqual(result[0].strip(), \"2013-01-01 09:00:00\")\n        self.assertEqual(result[1].strip(), \"NaT\")\n        self.assertEqual(result[4].strip(), \"2013-01-05 09:00:00\")\n\n        x = Series(date_range('20130101 09:00:00', periods=5, freq='s'))\n        x.iloc[1] = np.nan\n        result = fmt.Datetime64Formatter(x).get_result()\n        self.assertEqual(result[0].strip(), \"2013-01-01 09:00:00\")\n        self.assertEqual(result[1].strip(), \"NaT\")\n        self.assertEqual(result[4].strip(), \"2013-01-01 09:00:04\")\n\n        x = Series(date_range('20130101 09:00:00', periods=5, freq='ms'))\n        x.iloc[1] = np.nan\n        result = fmt.Datetime64Formatter(x).get_result()\n        self.assertEqual(result[0].strip(), \"2013-01-01 09:00:00.000\")\n        self.assertEqual(result[1].strip(), \"NaT\")\n        self.assertEqual(result[4].strip(), \"2013-01-01 09:00:00.004\")\n\n        x = Series(date_range('20130101 09:00:00', periods=5, freq='us'))\n        x.iloc[1] = np.nan\n        result = fmt.Datetime64Formatter(x).get_result()\n        self.assertEqual(result[0].strip(), \"2013-01-01 09:00:00.000000\")\n        self.assertEqual(result[1].strip(), \"NaT\")\n        self.assertEqual(result[4].strip(), \"2013-01-01 09:00:00.000004\")\n\n        x = Series(date_range('20130101 09:00:00', periods=5, freq='N'))\n        x.iloc[1] = np.nan\n        result = fmt.Datetime64Formatter(x).get_result()\n        self.assertEqual(result[0].strip(), \"2013-01-01 09:00:00.000000000\")\n        self.assertEqual(result[1].strip(), \"NaT\")\n        self.assertEqual(result[4].strip(), \"2013-01-01 09:00:00.000000004\")\n\n    def test_datetime64formatter_yearmonth(self):\n        x = Series([datetime(2016, 1, 1), datetime(2016, 2, 2)])\n\n        def format_func(x):\n            return x.strftime('%Y-%m')\n\n        formatter = fmt.Datetime64Formatter(x, formatter=format_func)\n        result = formatter.get_result()\n        self.assertEqual(result, ['2016-01', '2016-02'])\n\n    def test_datetime64formatter_hoursecond(self):\n\n        x = Series(pd.to_datetime(['10:10:10.100', '12:12:12.120'],\n                                  format='%H:%M:%S.%f'))\n\n        def format_func(x):\n            return x.strftime('%H:%M')\n\n        formatter = fmt.Datetime64Formatter(x, formatter=format_func)\n        result = formatter.get_result()\n        self.assertEqual(result, ['10:10', '12:12'])\n\n\nclass TestNaTFormatting(tm.TestCase):\n\n    def test_repr(self):\n        self.assertEqual(repr(pd.NaT), \"NaT\")\n\n    def test_str(self):\n        self.assertEqual(str(pd.NaT), \"NaT\")\n\n\nclass TestDatetimeIndexFormat(tm.TestCase):\n\n    def test_datetime(self):\n        formatted = pd.to_datetime([datetime(2003, 1, 1, 12), pd.NaT]).format()\n        self.assertEqual(formatted[0], \"2003-01-01 12:00:00\")\n        self.assertEqual(formatted[1], \"NaT\")\n\n    def test_date(self):\n        formatted = pd.to_datetime([datetime(2003, 1, 1), pd.NaT]).format()\n        self.assertEqual(formatted[0], \"2003-01-01\")\n        self.assertEqual(formatted[1], \"NaT\")\n\n    def test_date_tz(self):\n        formatted = pd.to_datetime([datetime(2013, 1, 1)], utc=True).format()\n        self.assertEqual(formatted[0], \"2013-01-01 00:00:00+00:00\")\n\n        formatted = pd.to_datetime(\n            [datetime(2013, 1, 1), pd.NaT], utc=True).format()\n        self.assertEqual(formatted[0], \"2013-01-01 00:00:00+00:00\")\n\n    def test_date_explict_date_format(self):\n        formatted = pd.to_datetime([datetime(2003, 2, 1), pd.NaT]).format(\n            date_format=\"%m-%d-%Y\", na_rep=\"UT\")\n        self.assertEqual(formatted[0], \"02-01-2003\")\n        self.assertEqual(formatted[1], \"UT\")\n\n\nclass TestDatetimeIndexUnicode(tm.TestCase):\n\n    def test_dates(self):\n        text = str(pd.to_datetime([datetime(2013, 1, 1), datetime(2014, 1, 1)\n                                   ]))\n        self.assertTrue(\"['2013-01-01',\" in text)\n        self.assertTrue(\", '2014-01-01']\" in text)\n\n    def test_mixed(self):\n        text = str(pd.to_datetime([datetime(2013, 1, 1), datetime(\n            2014, 1, 1, 12), datetime(2014, 1, 1)]))\n        self.assertTrue(\"'2013-01-01 00:00:00',\" in text)\n        self.assertTrue(\"'2014-01-01 00:00:00']\" in text)\n\n\nclass TestStringRepTimestamp(tm.TestCase):\n\n    def test_no_tz(self):\n        dt_date = datetime(2013, 1, 2)\n        self.assertEqual(str(dt_date), str(Timestamp(dt_date)))\n\n        dt_datetime = datetime(2013, 1, 2, 12, 1, 3)\n        self.assertEqual(str(dt_datetime), str(Timestamp(dt_datetime)))\n\n        dt_datetime_us = datetime(2013, 1, 2, 12, 1, 3, 45)\n        self.assertEqual(str(dt_datetime_us), str(Timestamp(dt_datetime_us)))\n\n        ts_nanos_only = Timestamp(200)\n        self.assertEqual(str(ts_nanos_only), \"1970-01-01 00:00:00.000000200\")\n\n        ts_nanos_micros = Timestamp(1200)\n        self.assertEqual(str(ts_nanos_micros), \"1970-01-01 00:00:00.000001200\")\n\n    def test_tz_pytz(self):\n        tm._skip_if_no_pytz()\n\n        import pytz\n\n        dt_date = datetime(2013, 1, 2, tzinfo=pytz.utc)\n        self.assertEqual(str(dt_date), str(Timestamp(dt_date)))\n\n        dt_datetime = datetime(2013, 1, 2, 12, 1, 3, tzinfo=pytz.utc)\n        self.assertEqual(str(dt_datetime), str(Timestamp(dt_datetime)))\n\n        dt_datetime_us = datetime(2013, 1, 2, 12, 1, 3, 45, tzinfo=pytz.utc)\n        self.assertEqual(str(dt_datetime_us), str(Timestamp(dt_datetime_us)))\n\n    def test_tz_dateutil(self):\n        tm._skip_if_no_dateutil()\n        import dateutil\n        utc = dateutil.tz.tzutc()\n\n        dt_date = datetime(2013, 1, 2, tzinfo=utc)\n        self.assertEqual(str(dt_date), str(Timestamp(dt_date)))\n\n        dt_datetime = datetime(2013, 1, 2, 12, 1, 3, tzinfo=utc)\n        self.assertEqual(str(dt_datetime), str(Timestamp(dt_datetime)))\n\n        dt_datetime_us = datetime(2013, 1, 2, 12, 1, 3, 45, tzinfo=utc)\n        self.assertEqual(str(dt_datetime_us), str(Timestamp(dt_datetime_us)))\n\n    def test_nat_representations(self):\n        for f in (str, repr, methodcaller('isoformat')):\n            self.assertEqual(f(pd.NaT), 'NaT')\n\n\ndef test_format_percentiles():\n    result = fmt.format_percentiles([0.01999, 0.02001, 0.5, 0.666666, 0.9999])\n    expected = ['1.999%', '2.001%', '50%', '66.667%', '99.99%']\n    tm.assert_equal(result, expected)\n\n    result = fmt.format_percentiles([0, 0.5, 0.02001, 0.5, 0.666666, 0.9999])\n    expected = ['0%', '50%', '2.0%', '50%', '66.67%', '99.99%']\n    tm.assert_equal(result, expected)\n\n    tm.assertRaises(ValueError, fmt.format_percentiles, [0.1, np.nan, 0.5])\n    tm.assertRaises(ValueError, fmt.format_percentiles, [-0.001, 0.1, 0.5])\n    tm.assertRaises(ValueError, fmt.format_percentiles, [2, 0.1, 0.5])\n    tm.assertRaises(ValueError, fmt.format_percentiles, [0.1, 0.5, 'a'])\n"
    },
    {
      "filename": "pandas/tests/groupby/test_categorical.py",
      "content": "# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom datetime import datetime\n\nimport numpy as np\nfrom numpy import nan\n\nimport pandas as pd\nfrom pandas import (Index, MultiIndex, CategoricalIndex,\n                    DataFrame, Categorical, Series)\nfrom pandas.util.testing import assert_frame_equal, assert_series_equal\nimport pandas.util.testing as tm\nfrom .common import MixIn\n\n\nclass TestGroupByCategorical(MixIn, tm.TestCase):\n\n    def test_level_groupby_get_group(self):\n        # GH15155\n        df = DataFrame(data=np.arange(2, 22, 2),\n                       index=MultiIndex(\n                           levels=[pd.CategoricalIndex([\"a\", \"b\"]), range(10)],\n                           labels=[[0] * 5 + [1] * 5, range(10)],\n                           names=[\"Index1\", \"Index2\"]))\n        g = df.groupby(level=[\"Index1\"])\n\n        # expected should equal test.loc[[\"a\"]]\n        # GH15166\n        expected = DataFrame(data=np.arange(2, 12, 2),\n                             index=pd.MultiIndex(levels=[pd.CategoricalIndex(\n                                 [\"a\", \"b\"]), range(5)],\n            labels=[[0] * 5, range(5)],\n            names=[\"Index1\", \"Index2\"]))\n        result = g.get_group('a')\n\n        assert_frame_equal(result, expected)\n\n    def test_apply_use_categorical_name(self):\n        from pandas import qcut\n        cats = qcut(self.df.C, 4)\n\n        def get_stats(group):\n            return {'min': group.min(),\n                    'max': group.max(),\n                    'count': group.count(),\n                    'mean': group.mean()}\n\n        result = self.df.groupby(cats).D.apply(get_stats)\n        self.assertEqual(result.index.names[0], 'C')\n\n    def test_apply_categorical_data(self):\n        # GH 10138\n        for ordered in [True, False]:\n            dense = Categorical(list('abc'), ordered=ordered)\n            # 'b' is in the categories but not in the list\n            missing = Categorical(\n                list('aaa'), categories=['a', 'b'], ordered=ordered)\n            values = np.arange(len(dense))\n            df = DataFrame({'missing': missing,\n                            'dense': dense,\n                            'values': values})\n            grouped = df.groupby(['missing', 'dense'])\n\n            # missing category 'b' should still exist in the output index\n            idx = MultiIndex.from_product(\n                [Categorical(['a', 'b'], ordered=ordered),\n                 Categorical(['a', 'b', 'c'], ordered=ordered)],\n                names=['missing', 'dense'])\n            expected = DataFrame([0, 1, 2, np.nan, np.nan, np.nan],\n                                 index=idx,\n                                 columns=['values'])\n\n            assert_frame_equal(grouped.apply(lambda x: np.mean(x)), expected)\n            assert_frame_equal(grouped.mean(), expected)\n            assert_frame_equal(grouped.agg(np.mean), expected)\n\n            # but for transform we should still get back the original index\n            idx = MultiIndex.from_product([['a'], ['a', 'b', 'c']],\n                                          names=['missing', 'dense'])\n            expected = Series(1, index=idx)\n            assert_series_equal(grouped.apply(lambda x: 1), expected)\n\n    def test_groupby_categorical(self):\n        levels = ['foo', 'bar', 'baz', 'qux']\n        codes = np.random.randint(0, 4, size=100)\n\n        cats = Categorical.from_codes(codes, levels, ordered=True)\n\n        data = DataFrame(np.random.randn(100, 4))\n\n        result = data.groupby(cats).mean()\n\n        expected = data.groupby(np.asarray(cats)).mean()\n        exp_idx = CategoricalIndex(levels, categories=cats.categories,\n                                   ordered=True)\n        expected = expected.reindex(exp_idx)\n\n        assert_frame_equal(result, expected)\n\n        grouped = data.groupby(cats)\n        desc_result = grouped.describe()\n\n        idx = cats.codes.argsort()\n        ord_labels = np.asarray(cats).take(idx)\n        ord_data = data.take(idx)\n\n        exp_cats = Categorical(ord_labels, ordered=True,\n                               categories=['foo', 'bar', 'baz', 'qux'])\n        expected = ord_data.groupby(exp_cats, sort=False).describe()\n        assert_frame_equal(desc_result, expected)\n\n        # GH 10460\n        expc = Categorical.from_codes(np.arange(4).repeat(8),\n                                      levels, ordered=True)\n        exp = CategoricalIndex(expc)\n        self.assert_index_equal((desc_result.stack()\n                                            .index\n                                            .get_level_values(0)), exp)\n        exp = Index(['count', 'mean', 'std', 'min', '25%', '50%',\n                     '75%', 'max'] * 4)\n        self.assert_index_equal((desc_result.stack()\n                                            .index\n                                            .get_level_values(1)), exp)\n\n    def test_groupby_datetime_categorical(self):\n        # GH9049: ensure backward compatibility\n        levels = pd.date_range('2014-01-01', periods=4)\n        codes = np.random.randint(0, 4, size=100)\n\n        cats = Categorical.from_codes(codes, levels, ordered=True)\n\n        data = DataFrame(np.random.randn(100, 4))\n        result = data.groupby(cats).mean()\n\n        expected = data.groupby(np.asarray(cats)).mean()\n        expected = expected.reindex(levels)\n        expected.index = CategoricalIndex(expected.index,\n                                          categories=expected.index,\n                                          ordered=True)\n\n        assert_frame_equal(result, expected)\n\n        grouped = data.groupby(cats)\n        desc_result = grouped.describe()\n\n        idx = cats.codes.argsort()\n        ord_labels = cats.take_nd(idx)\n        ord_data = data.take(idx)\n        expected = ord_data.groupby(ord_labels).describe()\n        assert_frame_equal(desc_result, expected)\n        tm.assert_index_equal(desc_result.index, expected.index)\n        tm.assert_index_equal(\n            desc_result.index.get_level_values(0),\n            expected.index.get_level_values(0))\n\n        # GH 10460\n        expc = Categorical.from_codes(\n            np.arange(4).repeat(8), levels, ordered=True)\n        exp = CategoricalIndex(expc)\n        self.assert_index_equal((desc_result.stack()\n                                            .index\n                                            .get_level_values(0)), exp)\n        exp = Index(['count', 'mean', 'std', 'min', '25%', '50%',\n                     '75%', 'max'] * 4)\n        self.assert_index_equal((desc_result.stack()\n                                            .index\n                                            .get_level_values(1)), exp)\n\n    def test_groupby_categorical_index(self):\n\n        s = np.random.RandomState(12345)\n        levels = ['foo', 'bar', 'baz', 'qux']\n        codes = s.randint(0, 4, size=20)\n        cats = Categorical.from_codes(codes, levels, ordered=True)\n        df = DataFrame(\n            np.repeat(\n                np.arange(20), 4).reshape(-1, 4), columns=list('abcd'))\n        df['cats'] = cats\n\n        # with a cat index\n        result = df.set_index('cats').groupby(level=0).sum()\n        expected = df[list('abcd')].groupby(cats.codes).sum()\n        expected.index = CategoricalIndex(\n            Categorical.from_codes(\n                [0, 1, 2, 3], levels, ordered=True), name='cats')\n        assert_frame_equal(result, expected)\n\n        # with a cat column, should produce a cat index\n        result = df.groupby('cats').sum()\n        expected = df[list('abcd')].groupby(cats.codes).sum()\n        expected.index = CategoricalIndex(\n            Categorical.from_codes(\n                [0, 1, 2, 3], levels, ordered=True), name='cats')\n        assert_frame_equal(result, expected)\n\n    def test_groupby_describe_categorical_columns(self):\n        # GH 11558\n        cats = pd.CategoricalIndex(['qux', 'foo', 'baz', 'bar'],\n                                   categories=['foo', 'bar', 'baz', 'qux'],\n                                   ordered=True)\n        df = DataFrame(np.random.randn(20, 4), columns=cats)\n        result = df.groupby([1, 2, 3, 4] * 5).describe()\n\n        tm.assert_index_equal(result.stack().columns, cats)\n        tm.assert_categorical_equal(result.stack().columns.values, cats.values)\n\n    def test_groupby_unstack_categorical(self):\n        # GH11558 (example is taken from the original issue)\n        df = pd.DataFrame({'a': range(10),\n                           'medium': ['A', 'B'] * 5,\n                           'artist': list('XYXXY') * 2})\n        df['medium'] = df['medium'].astype('category')\n\n        gcat = df.groupby(['artist', 'medium'])['a'].count().unstack()\n        result = gcat.describe()\n\n        exp_columns = pd.CategoricalIndex(['A', 'B'], ordered=False,\n                                          name='medium')\n        tm.assert_index_equal(result.columns, exp_columns)\n        tm.assert_categorical_equal(result.columns.values, exp_columns.values)\n\n        result = gcat['A'] + gcat['B']\n        expected = pd.Series([6, 4], index=pd.Index(['X', 'Y'], name='artist'))\n        tm.assert_series_equal(result, expected)\n\n    def test_groupby_bins_unequal_len(self):\n        # GH3011\n        series = Series([np.nan, np.nan, 1, 1, 2, 2, 3, 3, 4, 4])\n        bins = pd.cut(series.dropna().values, 4)\n\n        # len(bins) != len(series) here\n        def f():\n            series.groupby(bins).mean()\n        self.assertRaises(ValueError, f)\n\n    def test_groupby_multi_categorical_as_index(self):\n        # GH13204\n        df = DataFrame({'cat': Categorical([1, 2, 2], [1, 2, 3]),\n                        'A': [10, 11, 11],\n                        'B': [101, 102, 103]})\n        result = df.groupby(['cat', 'A'], as_index=False).sum()\n        expected = DataFrame({'cat': Categorical([1, 1, 2, 2, 3, 3]),\n                              'A': [10, 11, 10, 11, 10, 11],\n                              'B': [101.0, nan, nan, 205.0, nan, nan]},\n                             columns=['cat', 'A', 'B'])\n        tm.assert_frame_equal(result, expected)\n\n        # function grouper\n        f = lambda r: df.loc[r, 'A']\n        result = df.groupby(['cat', f], as_index=False).sum()\n        expected = DataFrame({'cat': Categorical([1, 1, 2, 2, 3, 3]),\n                              'A': [10.0, nan, nan, 22.0, nan, nan],\n                              'B': [101.0, nan, nan, 205.0, nan, nan]},\n                             columns=['cat', 'A', 'B'])\n        tm.assert_frame_equal(result, expected)\n\n        # another not in-axis grouper (conflicting names in index)\n        s = Series(['a', 'b', 'b'], name='cat')\n        result = df.groupby(['cat', s], as_index=False).sum()\n        expected = DataFrame({'cat': Categorical([1, 1, 2, 2, 3, 3]),\n                              'A': [10.0, nan, nan, 22.0, nan, nan],\n                              'B': [101.0, nan, nan, 205.0, nan, nan]},\n                             columns=['cat', 'A', 'B'])\n        tm.assert_frame_equal(result, expected)\n\n        # is original index dropped?\n        expected = DataFrame({'cat': Categorical([1, 1, 2, 2, 3, 3]),\n                              'A': [10, 11, 10, 11, 10, 11],\n                              'B': [101.0, nan, nan, 205.0, nan, nan]},\n                             columns=['cat', 'A', 'B'])\n\n        group_columns = ['cat', 'A']\n\n        for name in [None, 'X', 'B', 'cat']:\n            df.index = Index(list(\"abc\"), name=name)\n\n            if name in group_columns and name in df.index.names:\n                with tm.assert_produces_warning(FutureWarning,\n                                                check_stacklevel=False):\n                    result = df.groupby(group_columns, as_index=False).sum()\n\n            else:\n                result = df.groupby(group_columns, as_index=False).sum()\n\n            tm.assert_frame_equal(result, expected, check_index_type=True)\n\n    def test_groupby_preserve_categorical_dtype(self):\n        # GH13743, GH13854\n        df = DataFrame({'A': [1, 2, 1, 1, 2],\n                        'B': [10, 16, 22, 28, 34],\n                        'C1': Categorical(list(\"abaab\"),\n                                          categories=list(\"bac\"),\n                                          ordered=False),\n                        'C2': Categorical(list(\"abaab\"),\n                                          categories=list(\"bac\"),\n                                          ordered=True)})\n        # single grouper\n        exp_full = DataFrame({'A': [2.0, 1.0, np.nan],\n                              'B': [25.0, 20.0, np.nan],\n                              'C1': Categorical(list(\"bac\"),\n                                                categories=list(\"bac\"),\n                                                ordered=False),\n                              'C2': Categorical(list(\"bac\"),\n                                                categories=list(\"bac\"),\n                                                ordered=True)})\n        for col in ['C1', 'C2']:\n            result1 = df.groupby(by=col, as_index=False).mean()\n            result2 = df.groupby(by=col, as_index=True).mean().reset_index()\n            expected = exp_full.reindex(columns=result1.columns)\n            tm.assert_frame_equal(result1, expected)\n            tm.assert_frame_equal(result2, expected)\n\n        # multiple grouper\n        exp_full = DataFrame({'A': [1, 1, 1, 2, 2, 2],\n                              'B': [np.nan, 20.0, np.nan, 25.0, np.nan,\n                                    np.nan],\n                              'C1': Categorical(list(\"bacbac\"),\n                                                categories=list(\"bac\"),\n                                                ordered=False),\n                              'C2': Categorical(list(\"bacbac\"),\n                                                categories=list(\"bac\"),\n                                                ordered=True)})\n        for cols in [['A', 'C1'], ['A', 'C2']]:\n            result1 = df.groupby(by=cols, as_index=False).mean()\n            result2 = df.groupby(by=cols, as_index=True).mean().reset_index()\n            expected = exp_full.reindex(columns=result1.columns)\n            tm.assert_frame_equal(result1, expected)\n            tm.assert_frame_equal(result2, expected)\n\n    def test_groupby_categorical_no_compress(self):\n        data = Series(np.random.randn(9))\n\n        codes = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n        cats = Categorical.from_codes(codes, [0, 1, 2], ordered=True)\n\n        result = data.groupby(cats).mean()\n        exp = data.groupby(codes).mean()\n\n        exp.index = CategoricalIndex(exp.index, categories=cats.categories,\n                                     ordered=cats.ordered)\n        assert_series_equal(result, exp)\n\n        codes = np.array([0, 0, 0, 1, 1, 1, 3, 3, 3])\n        cats = Categorical.from_codes(codes, [0, 1, 2, 3], ordered=True)\n\n        result = data.groupby(cats).mean()\n        exp = data.groupby(codes).mean().reindex(cats.categories)\n        exp.index = CategoricalIndex(exp.index, categories=cats.categories,\n                                     ordered=cats.ordered)\n        assert_series_equal(result, exp)\n\n        cats = Categorical([\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\"],\n                           categories=[\"a\", \"b\", \"c\", \"d\"], ordered=True)\n        data = DataFrame({\"a\": [1, 1, 1, 2, 2, 2, 3, 4, 5], \"b\": cats})\n\n        result = data.groupby(\"b\").mean()\n        result = result[\"a\"].values\n        exp = np.array([1, 2, 4, np.nan])\n        self.assert_numpy_array_equal(result, exp)\n\n    def test_groupby_sort_categorical(self):\n        # dataframe groupby sort was being ignored # GH 8868\n        df = DataFrame([['(7.5, 10]', 10, 10],\n                        ['(7.5, 10]', 8, 20],\n                        ['(2.5, 5]', 5, 30],\n                        ['(5, 7.5]', 6, 40],\n                        ['(2.5, 5]', 4, 50],\n                        ['(0, 2.5]', 1, 60],\n                        ['(5, 7.5]', 7, 70]], columns=['range', 'foo', 'bar'])\n        df['range'] = Categorical(df['range'], ordered=True)\n        index = CategoricalIndex(['(0, 2.5]', '(2.5, 5]', '(5, 7.5]',\n                                  '(7.5, 10]'], name='range', ordered=True)\n        result_sort = DataFrame([[1, 60], [5, 30], [6, 40], [10, 10]],\n                                columns=['foo', 'bar'], index=index)\n\n        col = 'range'\n        assert_frame_equal(result_sort, df.groupby(col, sort=True).first())\n        # when categories is ordered, group is ordered by category's order\n        assert_frame_equal(result_sort, df.groupby(col, sort=False).first())\n\n        df['range'] = Categorical(df['range'], ordered=False)\n        index = CategoricalIndex(['(0, 2.5]', '(2.5, 5]', '(5, 7.5]',\n                                  '(7.5, 10]'], name='range')\n        result_sort = DataFrame([[1, 60], [5, 30], [6, 40], [10, 10]],\n                                columns=['foo', 'bar'], index=index)\n\n        index = CategoricalIndex(['(7.5, 10]', '(2.5, 5]', '(5, 7.5]',\n                                  '(0, 2.5]'],\n                                 categories=['(7.5, 10]', '(2.5, 5]',\n                                             '(5, 7.5]', '(0, 2.5]'],\n                                 name='range')\n        result_nosort = DataFrame([[10, 10], [5, 30], [6, 40], [1, 60]],\n                                  index=index, columns=['foo', 'bar'])\n\n        col = 'range'\n        # this is an unordered categorical, but we allow this ####\n        assert_frame_equal(result_sort, df.groupby(col, sort=True).first())\n        assert_frame_equal(result_nosort, df.groupby(col, sort=False).first())\n\n    def test_groupby_sort_categorical_datetimelike(self):\n        # GH10505\n\n        # use same data as test_groupby_sort_categorical, which category is\n        # corresponding to datetime.month\n        df = DataFrame({'dt': [datetime(2011, 7, 1), datetime(2011, 7, 1),\n                               datetime(2011, 2, 1), datetime(2011, 5, 1),\n                               datetime(2011, 2, 1), datetime(2011, 1, 1),\n                               datetime(2011, 5, 1)],\n                        'foo': [10, 8, 5, 6, 4, 1, 7],\n                        'bar': [10, 20, 30, 40, 50, 60, 70]},\n                       columns=['dt', 'foo', 'bar'])\n\n        # ordered=True\n        df['dt'] = Categorical(df['dt'], ordered=True)\n        index = [datetime(2011, 1, 1), datetime(2011, 2, 1),\n                 datetime(2011, 5, 1), datetime(2011, 7, 1)]\n        result_sort = DataFrame(\n            [[1, 60], [5, 30], [6, 40], [10, 10]], columns=['foo', 'bar'])\n        result_sort.index = CategoricalIndex(index, name='dt', ordered=True)\n\n        index = [datetime(2011, 7, 1), datetime(2011, 2, 1),\n                 datetime(2011, 5, 1), datetime(2011, 1, 1)]\n        result_nosort = DataFrame([[10, 10], [5, 30], [6, 40], [1, 60]],\n                                  columns=['foo', 'bar'])\n        result_nosort.index = CategoricalIndex(index, categories=index,\n                                               name='dt', ordered=True)\n\n        col = 'dt'\n        assert_frame_equal(result_sort, df.groupby(col, sort=True).first())\n        # when categories is ordered, group is ordered by category's order\n        assert_frame_equal(result_sort, df.groupby(col, sort=False).first())\n\n        # ordered = False\n        df['dt'] = Categorical(df['dt'], ordered=False)\n        index = [datetime(2011, 1, 1), datetime(2011, 2, 1),\n                 datetime(2011, 5, 1), datetime(2011, 7, 1)]\n        result_sort = DataFrame(\n            [[1, 60], [5, 30], [6, 40], [10, 10]], columns=['foo', 'bar'])\n        result_sort.index = CategoricalIndex(index, name='dt')\n\n        index = [datetime(2011, 7, 1), datetime(2011, 2, 1),\n                 datetime(2011, 5, 1), datetime(2011, 1, 1)]\n        result_nosort = DataFrame([[10, 10], [5, 30], [6, 40], [1, 60]],\n                                  columns=['foo', 'bar'])\n        result_nosort.index = CategoricalIndex(index, categories=index,\n                                               name='dt')\n\n        col = 'dt'\n        assert_frame_equal(result_sort, df.groupby(col, sort=True).first())\n        assert_frame_equal(result_nosort, df.groupby(col, sort=False).first())\n\n    def test_groupby_categorical_two_columns(self):\n\n        # https://github.com/pandas-dev/pandas/issues/8138\n        d = {'cat':\n             pd.Categorical([\"a\", \"b\", \"a\", \"b\"], categories=[\"a\", \"b\", \"c\"],\n                            ordered=True),\n             'ints': [1, 1, 2, 2],\n             'val': [10, 20, 30, 40]}\n        test = pd.DataFrame(d)\n\n        # Grouping on a single column\n        groups_single_key = test.groupby(\"cat\")\n        res = groups_single_key.agg('mean')\n\n        exp_index = pd.CategoricalIndex([\"a\", \"b\", \"c\"], name=\"cat\",\n                                        ordered=True)\n        exp = DataFrame({\"ints\": [1.5, 1.5, np.nan], \"val\": [20, 30, np.nan]},\n                        index=exp_index)\n        tm.assert_frame_equal(res, exp)\n\n        # Grouping on two columns\n        groups_double_key = test.groupby([\"cat\", \"ints\"])\n        res = groups_double_key.agg('mean')\n        exp = DataFrame({\"val\": [10, 30, 20, 40, np.nan, np.nan],\n                         \"cat\": pd.Categorical([\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"],\n                                               ordered=True),\n                         \"ints\": [1, 2, 1, 2, 1, 2]}).set_index([\"cat\", \"ints\"\n                                                                 ])\n        tm.assert_frame_equal(res, exp)\n\n        # GH 10132\n        for key in [('a', 1), ('b', 2), ('b', 1), ('a', 2)]:\n            c, i = key\n            result = groups_double_key.get_group(key)\n            expected = test[(test.cat == c) & (test.ints == i)]\n            assert_frame_equal(result, expected)\n\n        d = {'C1': [3, 3, 4, 5], 'C2': [1, 2, 3, 4], 'C3': [10, 100, 200, 34]}\n        test = pd.DataFrame(d)\n        values = pd.cut(test['C1'], [1, 2, 3, 6])\n        values.name = \"cat\"\n        groups_double_key = test.groupby([values, 'C2'])\n\n        res = groups_double_key.agg('mean')\n        nan = np.nan\n        idx = MultiIndex.from_product(\n            [Categorical([\"(1, 2]\", \"(2, 3]\", \"(3, 6]\"], ordered=True),\n             [1, 2, 3, 4]],\n            names=[\"cat\", \"C2\"])\n        exp = DataFrame({\"C1\": [nan, nan, nan, nan, 3, 3,\n                                nan, nan, nan, nan, 4, 5],\n                         \"C3\": [nan, nan, nan, nan, 10, 100,\n                                nan, nan, nan, nan, 200, 34]}, index=idx)\n        tm.assert_frame_equal(res, exp)\n"
    },
    {
      "filename": "pandas/tests/groupby/test_groupby.py",
      "content": "# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\nfrom string import ascii_lowercase\nfrom datetime import datetime\nfrom numpy import nan\n\nfrom pandas import (date_range, bdate_range, Timestamp,\n                    isnull, Index, MultiIndex, DataFrame, Series)\nfrom pandas.core.common import UnsupportedFunctionCall\nfrom pandas.util.testing import (assert_panel_equal, assert_frame_equal,\n                                 assert_series_equal, assert_almost_equal,\n                                 assert_index_equal, assertRaisesRegexp)\nfrom pandas.compat import (range, long, lrange, StringIO, lmap, lzip, map, zip,\n                           builtins, OrderedDict, product as cart_product)\nfrom pandas import compat\nfrom pandas.core.panel import Panel\nfrom pandas.tools.merge import concat\nfrom collections import defaultdict\nimport pandas.core.common as com\nimport numpy as np\n\nimport pandas.core.nanops as nanops\nimport pandas.util.testing as tm\nimport pandas as pd\nfrom .common import MixIn\n\n\nclass TestGroupBy(MixIn, tm.TestCase):\n\n    def test_basic(self):\n        def checkit(dtype):\n            data = Series(np.arange(9) // 3, index=np.arange(9), dtype=dtype)\n\n            index = np.arange(9)\n            np.random.shuffle(index)\n            data = data.reindex(index)\n\n            grouped = data.groupby(lambda x: x // 3)\n\n            for k, v in grouped:\n                self.assertEqual(len(v), 3)\n\n            agged = grouped.aggregate(np.mean)\n            self.assertEqual(agged[1], 1)\n\n            assert_series_equal(agged, grouped.agg(np.mean))  # shorthand\n            assert_series_equal(agged, grouped.mean())\n            assert_series_equal(grouped.agg(np.sum), grouped.sum())\n\n            expected = grouped.apply(lambda x: x * x.sum())\n            transformed = grouped.transform(lambda x: x * x.sum())\n            self.assertEqual(transformed[7], 12)\n            assert_series_equal(transformed, expected)\n\n            value_grouped = data.groupby(data)\n            assert_series_equal(value_grouped.aggregate(np.mean), agged,\n                                check_index_type=False)\n\n            # complex agg\n            agged = grouped.aggregate([np.mean, np.std])\n            agged = grouped.aggregate({'one': np.mean, 'two': np.std})\n\n            group_constants = {0: 10, 1: 20, 2: 30}\n            agged = grouped.agg(lambda x: group_constants[x.name] + x.mean())\n            self.assertEqual(agged[1], 21)\n\n            # corner cases\n            self.assertRaises(Exception, grouped.aggregate, lambda x: x * 2)\n\n        for dtype in ['int64', 'int32', 'float64', 'float32']:\n            checkit(dtype)\n\n    def test_select_bad_cols(self):\n        df = DataFrame([[1, 2]], columns=['A', 'B'])\n        g = df.groupby('A')\n        self.assertRaises(KeyError, g.__getitem__, ['C'])  # g[['C']]\n\n        self.assertRaises(KeyError, g.__getitem__, ['A', 'C'])  # g[['A', 'C']]\n        with assertRaisesRegexp(KeyError, '^[^A]+$'):\n            # A should not be referenced as a bad column...\n            # will have to rethink regex if you change message!\n            g[['A', 'C']]\n\n    def test_first_last_nth(self):\n        # tests for first / last / nth\n        grouped = self.df.groupby('A')\n        first = grouped.first()\n        expected = self.df.loc[[1, 0], ['B', 'C', 'D']]\n        expected.index = Index(['bar', 'foo'], name='A')\n        expected = expected.sort_index()\n        assert_frame_equal(first, expected)\n\n        nth = grouped.nth(0)\n        assert_frame_equal(nth, expected)\n\n        last = grouped.last()\n        expected = self.df.loc[[5, 7], ['B', 'C', 'D']]\n        expected.index = Index(['bar', 'foo'], name='A')\n        assert_frame_equal(last, expected)\n\n        nth = grouped.nth(-1)\n        assert_frame_equal(nth, expected)\n\n        nth = grouped.nth(1)\n        expected = self.df.loc[[2, 3], ['B', 'C', 'D']].copy()\n        expected.index = Index(['foo', 'bar'], name='A')\n        expected = expected.sort_index()\n        assert_frame_equal(nth, expected)\n\n        # it works!\n        grouped['B'].first()\n        grouped['B'].last()\n        grouped['B'].nth(0)\n\n        self.df.loc[self.df['A'] == 'foo', 'B'] = np.nan\n        self.assertTrue(isnull(grouped['B'].first()['foo']))\n        self.assertTrue(isnull(grouped['B'].last()['foo']))\n        self.assertTrue(isnull(grouped['B'].nth(0)['foo']))\n\n        # v0.14.0 whatsnew\n        df = DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\n        g = df.groupby('A')\n        result = g.first()\n        expected = df.iloc[[1, 2]].set_index('A')\n        assert_frame_equal(result, expected)\n\n        expected = df.iloc[[1, 2]].set_index('A')\n        result = g.nth(0, dropna='any')\n        assert_frame_equal(result, expected)\n\n    def test_first_last_nth_dtypes(self):\n\n        df = self.df_mixed_floats.copy()\n        df['E'] = True\n        df['F'] = 1\n\n        # tests for first / last / nth\n        grouped = df.groupby('A')\n        first = grouped.first()\n        expected = df.loc[[1, 0], ['B', 'C', 'D', 'E', 'F']]\n        expected.index = Index(['bar', 'foo'], name='A')\n        expected = expected.sort_index()\n        assert_frame_equal(first, expected)\n\n        last = grouped.last()\n        expected = df.loc[[5, 7], ['B', 'C', 'D', 'E', 'F']]\n        expected.index = Index(['bar', 'foo'], name='A')\n        expected = expected.sort_index()\n        assert_frame_equal(last, expected)\n\n        nth = grouped.nth(1)\n        expected = df.loc[[3, 2], ['B', 'C', 'D', 'E', 'F']]\n        expected.index = Index(['bar', 'foo'], name='A')\n        expected = expected.sort_index()\n        assert_frame_equal(nth, expected)\n\n        # GH 2763, first/last shifting dtypes\n        idx = lrange(10)\n        idx.append(9)\n        s = Series(data=lrange(11), index=idx, name='IntCol')\n        self.assertEqual(s.dtype, 'int64')\n        f = s.groupby(level=0).first()\n        self.assertEqual(f.dtype, 'int64')\n\n    def test_nth(self):\n        df = DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\n        g = df.groupby('A')\n\n        assert_frame_equal(g.nth(0), df.iloc[[0, 2]].set_index('A'))\n        assert_frame_equal(g.nth(1), df.iloc[[1]].set_index('A'))\n        assert_frame_equal(g.nth(2), df.loc[[]].set_index('A'))\n        assert_frame_equal(g.nth(-1), df.iloc[[1, 2]].set_index('A'))\n        assert_frame_equal(g.nth(-2), df.iloc[[0]].set_index('A'))\n        assert_frame_equal(g.nth(-3), df.loc[[]].set_index('A'))\n        assert_series_equal(g.B.nth(0), df.set_index('A').B.iloc[[0, 2]])\n        assert_series_equal(g.B.nth(1), df.set_index('A').B.iloc[[1]])\n        assert_frame_equal(g[['B']].nth(0),\n                           df.loc[[0, 2], ['A', 'B']].set_index('A'))\n\n        exp = df.set_index('A')\n        assert_frame_equal(g.nth(0, dropna='any'), exp.iloc[[1, 2]])\n        assert_frame_equal(g.nth(-1, dropna='any'), exp.iloc[[1, 2]])\n\n        exp['B'] = np.nan\n        assert_frame_equal(g.nth(7, dropna='any'), exp.iloc[[1, 2]])\n        assert_frame_equal(g.nth(2, dropna='any'), exp.iloc[[1, 2]])\n\n        # out of bounds, regression from 0.13.1\n        # GH 6621\n        df = DataFrame({'color': {0: 'green',\n                                  1: 'green',\n                                  2: 'red',\n                                  3: 'red',\n                                  4: 'red'},\n                        'food': {0: 'ham',\n                                 1: 'eggs',\n                                 2: 'eggs',\n                                 3: 'ham',\n                                 4: 'pork'},\n                        'two': {0: 1.5456590000000001,\n                                1: -0.070345000000000005,\n                                2: -2.4004539999999999,\n                                3: 0.46206000000000003,\n                                4: 0.52350799999999997},\n                        'one': {0: 0.56573799999999996,\n                                1: -0.9742360000000001,\n                                2: 1.033801,\n                                3: -0.78543499999999999,\n                                4: 0.70422799999999997}}).set_index(['color',\n                                                                     'food'])\n\n        result = df.groupby(level=0, as_index=False).nth(2)\n        expected = df.iloc[[-1]]\n        assert_frame_equal(result, expected)\n\n        result = df.groupby(level=0, as_index=False).nth(3)\n        expected = df.loc[[]]\n        assert_frame_equal(result, expected)\n\n        # GH 7559\n        # from the vbench\n        df = DataFrame(np.random.randint(1, 10, (100, 2)), dtype='int64')\n        s = df[1]\n        g = df[0]\n        expected = s.groupby(g).first()\n        expected2 = s.groupby(g).apply(lambda x: x.iloc[0])\n        assert_series_equal(expected2, expected, check_names=False)\n        self.assertTrue(expected.name, 0)\n        self.assertEqual(expected.name, 1)\n\n        # validate first\n        v = s[g == 1].iloc[0]\n        self.assertEqual(expected.iloc[0], v)\n        self.assertEqual(expected2.iloc[0], v)\n\n        # this is NOT the same as .first (as sorted is default!)\n        # as it keeps the order in the series (and not the group order)\n        # related GH 7287\n        expected = s.groupby(g, sort=False).first()\n        result = s.groupby(g, sort=False).nth(0, dropna='all')\n        assert_series_equal(result, expected)\n\n        # doc example\n        df = DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\n        g = df.groupby('A')\n        result = g.B.nth(0, dropna=True)\n        expected = g.B.first()\n        assert_series_equal(result, expected)\n\n        # test multiple nth values\n        df = DataFrame([[1, np.nan], [1, 3], [1, 4], [5, 6], [5, 7]],\n                       columns=['A', 'B'])\n        g = df.groupby('A')\n\n        assert_frame_equal(g.nth(0), df.iloc[[0, 3]].set_index('A'))\n        assert_frame_equal(g.nth([0]), df.iloc[[0, 3]].set_index('A'))\n        assert_frame_equal(g.nth([0, 1]), df.iloc[[0, 1, 3, 4]].set_index('A'))\n        assert_frame_equal(\n            g.nth([0, -1]), df.iloc[[0, 2, 3, 4]].set_index('A'))\n        assert_frame_equal(\n            g.nth([0, 1, 2]), df.iloc[[0, 1, 2, 3, 4]].set_index('A'))\n        assert_frame_equal(\n            g.nth([0, 1, -1]), df.iloc[[0, 1, 2, 3, 4]].set_index('A'))\n        assert_frame_equal(g.nth([2]), df.iloc[[2]].set_index('A'))\n        assert_frame_equal(g.nth([3, 4]), df.loc[[]].set_index('A'))\n\n        business_dates = pd.date_range(start='4/1/2014', end='6/30/2014',\n                                       freq='B')\n        df = DataFrame(1, index=business_dates, columns=['a', 'b'])\n        # get the first, fourth and last two business days for each month\n        key = (df.index.year, df.index.month)\n        result = df.groupby(key, as_index=False).nth([0, 3, -2, -1])\n        expected_dates = pd.to_datetime(\n            ['2014/4/1', '2014/4/4', '2014/4/29', '2014/4/30', '2014/5/1',\n             '2014/5/6', '2014/5/29', '2014/5/30', '2014/6/2', '2014/6/5',\n             '2014/6/27', '2014/6/30'])\n        expected = DataFrame(1, columns=['a', 'b'], index=expected_dates)\n        assert_frame_equal(result, expected)\n\n    def test_nth_multi_index(self):\n        # PR 9090, related to issue 8979\n        # test nth on MultiIndex, should match .first()\n        grouped = self.three_group.groupby(['A', 'B'])\n        result = grouped.nth(0)\n        expected = grouped.first()\n        assert_frame_equal(result, expected)\n\n    def test_nth_multi_index_as_expected(self):\n        # PR 9090, related to issue 8979\n        # test nth on MultiIndex\n        three_group = DataFrame(\n            {'A': ['foo', 'foo', 'foo', 'foo', 'bar', 'bar', 'bar', 'bar',\n                   'foo', 'foo', 'foo'],\n             'B': ['one', 'one', 'one', 'two', 'one', 'one', 'one', 'two',\n                   'two', 'two', 'one'],\n             'C': ['dull', 'dull', 'shiny', 'dull', 'dull', 'shiny', 'shiny',\n                   'dull', 'shiny', 'shiny', 'shiny']})\n        grouped = three_group.groupby(['A', 'B'])\n        result = grouped.nth(0)\n        expected = DataFrame(\n            {'C': ['dull', 'dull', 'dull', 'dull']},\n            index=MultiIndex.from_arrays([['bar', 'bar', 'foo', 'foo'],\n                                          ['one', 'two', 'one', 'two']],\n                                         names=['A', 'B']))\n        assert_frame_equal(result, expected)\n\n    def test_group_selection_cache(self):\n        # GH 12839 nth, head, and tail should return same result consistently\n        df = DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])\n        expected = df.iloc[[0, 2]].set_index('A')\n\n        g = df.groupby('A')\n        result1 = g.head(n=2)\n        result2 = g.nth(0)\n        assert_frame_equal(result1, df)\n        assert_frame_equal(result2, expected)\n\n        g = df.groupby('A')\n        result1 = g.tail(n=2)\n        result2 = g.nth(0)\n        assert_frame_equal(result1, df)\n        assert_frame_equal(result2, expected)\n\n        g = df.groupby('A')\n        result1 = g.nth(0)\n        result2 = g.head(n=2)\n        assert_frame_equal(result1, expected)\n        assert_frame_equal(result2, df)\n\n        g = df.groupby('A')\n        result1 = g.nth(0)\n        result2 = g.tail(n=2)\n        assert_frame_equal(result1, expected)\n        assert_frame_equal(result2, df)\n\n    def test_grouper_index_types(self):\n        # related GH5375\n        # groupby misbehaving when using a Floatlike index\n        df = DataFrame(np.arange(10).reshape(5, 2), columns=list('AB'))\n        for index in [tm.makeFloatIndex, tm.makeStringIndex,\n                      tm.makeUnicodeIndex, tm.makeIntIndex, tm.makeDateIndex,\n                      tm.makePeriodIndex]:\n\n            df.index = index(len(df))\n            df.groupby(list('abcde')).apply(lambda x: x)\n\n            df.index = list(reversed(df.index.tolist()))\n            df.groupby(list('abcde')).apply(lambda x: x)\n\n    def test_grouper_multilevel_freq(self):\n\n        # GH 7885\n        # with level and freq specified in a pd.Grouper\n        from datetime import date, timedelta\n        d0 = date.today() - timedelta(days=14)\n        dates = date_range(d0, date.today())\n        date_index = pd.MultiIndex.from_product(\n            [dates, dates], names=['foo', 'bar'])\n        df = pd.DataFrame(np.random.randint(0, 100, 225), index=date_index)\n\n        # Check string level\n        expected = df.reset_index().groupby([pd.Grouper(\n            key='foo', freq='W'), pd.Grouper(key='bar', freq='W')]).sum()\n        # reset index changes columns dtype to object\n        expected.columns = pd.Index([0], dtype='int64')\n\n        result = df.groupby([pd.Grouper(level='foo', freq='W'), pd.Grouper(\n            level='bar', freq='W')]).sum()\n        assert_frame_equal(result, expected)\n\n        # Check integer level\n        result = df.groupby([pd.Grouper(level=0, freq='W'), pd.Grouper(\n            level=1, freq='W')]).sum()\n        assert_frame_equal(result, expected)\n\n    def test_grouper_creation_bug(self):\n\n        # GH 8795\n        df = DataFrame({'A': [0, 0, 1, 1, 2, 2], 'B': [1, 2, 3, 4, 5, 6]})\n        g = df.groupby('A')\n        expected = g.sum()\n\n        g = df.groupby(pd.Grouper(key='A'))\n        result = g.sum()\n        assert_frame_equal(result, expected)\n\n        result = g.apply(lambda x: x.sum())\n        assert_frame_equal(result, expected)\n\n        g = df.groupby(pd.Grouper(key='A', axis=0))\n        result = g.sum()\n        assert_frame_equal(result, expected)\n\n        # GH14334\n        # pd.Grouper(key=...) may be passed in a list\n        df = DataFrame({'A': [0, 0, 0, 1, 1, 1],\n                        'B': [1, 1, 2, 2, 3, 3],\n                        'C': [1, 2, 3, 4, 5, 6]})\n        # Group by single column\n        expected = df.groupby('A').sum()\n        g = df.groupby([pd.Grouper(key='A')])\n        result = g.sum()\n        assert_frame_equal(result, expected)\n\n        # Group by two columns\n        # using a combination of strings and Grouper objects\n        expected = df.groupby(['A', 'B']).sum()\n\n        # Group with two Grouper objects\n        g = df.groupby([pd.Grouper(key='A'), pd.Grouper(key='B')])\n        result = g.sum()\n        assert_frame_equal(result, expected)\n\n        # Group with a string and a Grouper object\n        g = df.groupby(['A', pd.Grouper(key='B')])\n        result = g.sum()\n        assert_frame_equal(result, expected)\n\n        # Group with a Grouper object and a string\n        g = df.groupby([pd.Grouper(key='A'), 'B'])\n        result = g.sum()\n        assert_frame_equal(result, expected)\n\n        # GH8866\n        s = Series(np.arange(8, dtype='int64'),\n                   index=pd.MultiIndex.from_product(\n                       [list('ab'), range(2),\n                        date_range('20130101', periods=2)],\n                       names=['one', 'two', 'three']))\n        result = s.groupby(pd.Grouper(level='three', freq='M')).sum()\n        expected = Series([28], index=Index(\n            [Timestamp('2013-01-31')], freq='M', name='three'))\n        assert_series_equal(result, expected)\n\n        # just specifying a level breaks\n        result = s.groupby(pd.Grouper(level='one')).sum()\n        expected = s.groupby(level='one').sum()\n        assert_series_equal(result, expected)\n\n    def test_grouper_column_and_index(self):\n        # GH 14327\n\n        # Grouping a multi-index frame by a column and an index level should\n        # be equivalent to resetting the index and grouping by two columns\n        idx = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('a', 3),\n                                         ('b', 1), ('b', 2), ('b', 3)])\n        idx.names = ['outer', 'inner']\n        df_multi = pd.DataFrame({\"A\": np.arange(6),\n                                 'B': ['one', 'one', 'two',\n                                       'two', 'one', 'one']},\n                                index=idx)\n        result = df_multi.groupby(['B', pd.Grouper(level='inner')]).mean()\n        expected = df_multi.reset_index().groupby(['B', 'inner']).mean()\n        assert_frame_equal(result, expected)\n\n        # Test the reverse grouping order\n        result = df_multi.groupby([pd.Grouper(level='inner'), 'B']).mean()\n        expected = df_multi.reset_index().groupby(['inner', 'B']).mean()\n        assert_frame_equal(result, expected)\n\n        # Grouping a single-index frame by a column and the index should\n        # be equivalent to resetting the index and grouping by two columns\n        df_single = df_multi.reset_index('outer')\n        result = df_single.groupby(['B', pd.Grouper(level='inner')]).mean()\n        expected = df_single.reset_index().groupby(['B', 'inner']).mean()\n        assert_frame_equal(result, expected)\n\n        # Test the reverse grouping order\n        result = df_single.groupby([pd.Grouper(level='inner'), 'B']).mean()\n        expected = df_single.reset_index().groupby(['inner', 'B']).mean()\n        assert_frame_equal(result, expected)\n\n    def test_grouper_index_level_as_string(self):\n        # GH 5677, allow strings passed as the `by` parameter to reference\n        # columns or index levels\n\n        idx = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('a', 3),\n                                         ('b', 1), ('b', 2), ('b', 3)])\n        idx.names = ['outer', 'inner']\n        df_multi = pd.DataFrame({\"A\": np.arange(6),\n                                 'B': ['one', 'one', 'two',\n                                       'two', 'one', 'one']},\n                                index=idx)\n\n        df_single = df_multi.reset_index('outer')\n\n        # Column and Index on MultiIndex\n        result = df_multi.groupby(['B', 'inner']).mean()\n        expected = df_multi.groupby(['B', pd.Grouper(level='inner')]).mean()\n        assert_frame_equal(result, expected)\n\n        # Index and Column on MultiIndex\n        result = df_multi.groupby(['inner', 'B']).mean()\n        expected = df_multi.groupby([pd.Grouper(level='inner'), 'B']).mean()\n        assert_frame_equal(result, expected)\n\n        # Column and Index on single Index\n        result = df_single.groupby(['B', 'inner']).mean()\n        expected = df_single.groupby(['B', pd.Grouper(level='inner')]).mean()\n        assert_frame_equal(result, expected)\n\n        # Index and Column on single Index\n        result = df_single.groupby(['inner', 'B']).mean()\n        expected = df_single.groupby([pd.Grouper(level='inner'), 'B']).mean()\n        assert_frame_equal(result, expected)\n\n        # Single element list of Index on MultiIndex\n        result = df_multi.groupby(['inner']).mean()\n        expected = df_multi.groupby(pd.Grouper(level='inner')).mean()\n        assert_frame_equal(result, expected)\n\n        # Single element list of Index on single Index\n        result = df_single.groupby(['inner']).mean()\n        expected = df_single.groupby(pd.Grouper(level='inner')).mean()\n        assert_frame_equal(result, expected)\n\n        # Index on MultiIndex\n        result = df_multi.groupby('inner').mean()\n        expected = df_multi.groupby(pd.Grouper(level='inner')).mean()\n        assert_frame_equal(result, expected)\n\n        # Index on single Index\n        result = df_single.groupby('inner').mean()\n        expected = df_single.groupby(pd.Grouper(level='inner')).mean()\n        assert_frame_equal(result, expected)\n\n    def test_grouper_column_index_level_precedence(self):\n        # GH 5677, when a string passed as the `by` parameter\n        # matches a column and an index level the column takes\n        # precedence\n\n        idx = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('a', 3),\n                                         ('b', 1), ('b', 2), ('b', 3)])\n        idx.names = ['outer', 'inner']\n        df_multi_both = pd.DataFrame({\"A\": np.arange(6),\n                                      'B': ['one', 'one', 'two',\n                                            'two', 'one', 'one'],\n                                      'inner': [1, 1, 1, 1, 1, 1]},\n                                     index=idx)\n\n        df_single_both = df_multi_both.reset_index('outer')\n\n        # Group MultiIndex by single key\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = df_multi_both.groupby('inner').mean()\n\n        expected = df_multi_both.groupby([pd.Grouper(key='inner')]).mean()\n        assert_frame_equal(result, expected)\n        not_expected = df_multi_both.groupby(pd.Grouper(level='inner')).mean()\n        self.assertFalse(result.index.equals(not_expected.index))\n\n        # Group single Index by single key\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = df_single_both.groupby('inner').mean()\n\n        expected = df_single_both.groupby([pd.Grouper(key='inner')]).mean()\n        assert_frame_equal(result, expected)\n        not_expected = df_single_both.groupby(pd.Grouper(level='inner')).mean()\n        self.assertFalse(result.index.equals(not_expected.index))\n\n        # Group MultiIndex by single key list\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = df_multi_both.groupby(['inner']).mean()\n\n        expected = df_multi_both.groupby([pd.Grouper(key='inner')]).mean()\n        assert_frame_equal(result, expected)\n        not_expected = df_multi_both.groupby(pd.Grouper(level='inner')).mean()\n        self.assertFalse(result.index.equals(not_expected.index))\n\n        # Group single Index by single key list\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = df_single_both.groupby(['inner']).mean()\n\n        expected = df_single_both.groupby([pd.Grouper(key='inner')]).mean()\n        assert_frame_equal(result, expected)\n        not_expected = df_single_both.groupby(pd.Grouper(level='inner')).mean()\n        self.assertFalse(result.index.equals(not_expected.index))\n\n        # Group MultiIndex by two keys (1)\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = df_multi_both.groupby(['B', 'inner']).mean()\n\n        expected = df_multi_both.groupby(['B',\n                                          pd.Grouper(key='inner')]).mean()\n        assert_frame_equal(result, expected)\n        not_expected = df_multi_both.groupby(['B',\n                                              pd.Grouper(level='inner')\n                                              ]).mean()\n        self.assertFalse(result.index.equals(not_expected.index))\n\n        # Group MultiIndex by two keys (2)\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = df_multi_both.groupby(['inner', 'B']).mean()\n\n        expected = df_multi_both.groupby([pd.Grouper(key='inner'),\n                                          'B']).mean()\n        assert_frame_equal(result, expected)\n        not_expected = df_multi_both.groupby([pd.Grouper(level='inner'),\n                                              'B']).mean()\n        self.assertFalse(result.index.equals(not_expected.index))\n\n        # Group single Index by two keys (1)\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = df_single_both.groupby(['B', 'inner']).mean()\n\n        expected = df_single_both.groupby(['B',\n                                           pd.Grouper(key='inner')]).mean()\n        assert_frame_equal(result, expected)\n        not_expected = df_single_both.groupby(['B',\n                                               pd.Grouper(level='inner')\n                                               ]).mean()\n        self.assertFalse(result.index.equals(not_expected.index))\n\n        # Group single Index by two keys (2)\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = df_single_both.groupby(['inner', 'B']).mean()\n\n        expected = df_single_both.groupby([pd.Grouper(key='inner'),\n                                           'B']).mean()\n        assert_frame_equal(result, expected)\n        not_expected = df_single_both.groupby([pd.Grouper(level='inner'),\n                                               'B']).mean()\n        self.assertFalse(result.index.equals(not_expected.index))\n\n    def test_grouper_getting_correct_binner(self):\n\n        # GH 10063\n        # using a non-time-based grouper and a time-based grouper\n        # and specifying levels\n        df = DataFrame({'A': 1}, index=pd.MultiIndex.from_product(\n            [list('ab'), date_range('20130101', periods=80)], names=['one',\n                                                                     'two']))\n        result = df.groupby([pd.Grouper(level='one'), pd.Grouper(\n            level='two', freq='M')]).sum()\n        expected = DataFrame({'A': [31, 28, 21, 31, 28, 21]},\n                             index=MultiIndex.from_product(\n                                 [list('ab'),\n                                  date_range('20130101', freq='M', periods=3)],\n                                 names=['one', 'two']))\n        assert_frame_equal(result, expected)\n\n    def test_grouper_iter(self):\n        self.assertEqual(sorted(self.df.groupby('A').grouper), ['bar', 'foo'])\n\n    def test_empty_groups(self):\n        # GH # 1048\n        self.assertRaises(ValueError, self.df.groupby, [])\n\n    def test_groupby_grouper(self):\n        grouped = self.df.groupby('A')\n\n        result = self.df.groupby(grouped.grouper).mean()\n        expected = grouped.mean()\n        assert_frame_equal(result, expected)\n\n    def test_groupby_duplicated_column_errormsg(self):\n        # GH7511\n        df = DataFrame(columns=['A', 'B', 'A', 'C'],\n                       data=[range(4), range(2, 6), range(0, 8, 2)])\n\n        self.assertRaises(ValueError, df.groupby, 'A')\n        self.assertRaises(ValueError, df.groupby, ['A', 'B'])\n\n        grouped = df.groupby('B')\n        c = grouped.count()\n        self.assertTrue(c.columns.nlevels == 1)\n        self.assertTrue(c.columns.size == 3)\n\n    def test_groupby_dict_mapping(self):\n        # GH #679\n        from pandas import Series\n        s = Series({'T1': 5})\n        result = s.groupby({'T1': 'T2'}).agg(sum)\n        expected = s.groupby(['T2']).agg(sum)\n        assert_series_equal(result, expected)\n\n        s = Series([1., 2., 3., 4.], index=list('abcd'))\n        mapping = {'a': 0, 'b': 0, 'c': 1, 'd': 1}\n\n        result = s.groupby(mapping).mean()\n        result2 = s.groupby(mapping).agg(np.mean)\n        expected = s.groupby([0, 0, 1, 1]).mean()\n        expected2 = s.groupby([0, 0, 1, 1]).mean()\n        assert_series_equal(result, expected)\n        assert_series_equal(result, result2)\n        assert_series_equal(result, expected2)\n\n    def test_groupby_grouper_f_sanity_checked(self):\n        dates = date_range('01-Jan-2013', periods=12, freq='MS')\n        ts = Series(np.random.randn(12), index=dates)\n\n        # GH3035\n        # index.map is used to apply grouper to the index\n        # if it fails on the elements, map tries it on the entire index as\n        # a sequence. That can yield invalid results that cause trouble\n        # down the line.\n        # the surprise comes from using key[0:6] rather then str(key)[0:6]\n        # when the elements are Timestamp.\n        # the result is Index[0:6], very confusing.\n\n        self.assertRaises(AssertionError, ts.groupby, lambda key: key[0:6])\n\n    def test_groupby_nonobject_dtype(self):\n        key = self.mframe.index.labels[0]\n        grouped = self.mframe.groupby(key)\n        result = grouped.sum()\n\n        expected = self.mframe.groupby(key.astype('O')).sum()\n        assert_frame_equal(result, expected)\n\n        # GH 3911, mixed frame non-conversion\n        df = self.df_mixed_floats.copy()\n        df['value'] = lrange(len(df))\n\n        def max_value(group):\n            return group.loc[group['value'].idxmax()]\n\n        applied = df.groupby('A').apply(max_value)\n        result = applied.get_dtype_counts().sort_values()\n        expected = Series({'object': 2,\n                           'float64': 2,\n                           'int64': 1}).sort_values()\n        assert_series_equal(result, expected)\n\n    def test_groupby_return_type(self):\n\n        # GH2893, return a reduced type\n        df1 = DataFrame(\n            [{\"val1\": 1, \"val2\": 20},\n             {\"val1\": 1, \"val2\": 19},\n             {\"val1\": 2, \"val2\": 27},\n             {\"val1\": 2, \"val2\": 12}\n             ])\n\n        def func(dataf):\n            return dataf[\"val2\"] - dataf[\"val2\"].mean()\n\n        result = df1.groupby(\"val1\", squeeze=True).apply(func)\n        tm.assertIsInstance(result, Series)\n\n        df2 = DataFrame(\n            [{\"val1\": 1, \"val2\": 20},\n             {\"val1\": 1, \"val2\": 19},\n             {\"val1\": 1, \"val2\": 27},\n             {\"val1\": 1, \"val2\": 12}\n             ])\n\n        def func(dataf):\n            return dataf[\"val2\"] - dataf[\"val2\"].mean()\n\n        result = df2.groupby(\"val1\", squeeze=True).apply(func)\n        tm.assertIsInstance(result, Series)\n\n        # GH3596, return a consistent type (regression in 0.11 from 0.10.1)\n        df = DataFrame([[1, 1], [1, 1]], columns=['X', 'Y'])\n        result = df.groupby('X', squeeze=False).count()\n        tm.assertIsInstance(result, DataFrame)\n\n        # GH5592\n        # inconcistent return type\n        df = DataFrame(dict(A=['Tiger', 'Tiger', 'Tiger', 'Lamb', 'Lamb',\n                               'Pony', 'Pony'], B=Series(\n                                   np.arange(7), dtype='int64'), C=date_range(\n                                       '20130101', periods=7)))\n\n        def f(grp):\n            return grp.iloc[0]\n\n        expected = df.groupby('A').first()[['B']]\n        result = df.groupby('A').apply(f)[['B']]\n        assert_frame_equal(result, expected)\n\n        def f(grp):\n            if grp.name == 'Tiger':\n                return None\n            return grp.iloc[0]\n\n        result = df.groupby('A').apply(f)[['B']]\n        e = expected.copy()\n        e.loc['Tiger'] = np.nan\n        assert_frame_equal(result, e)\n\n        def f(grp):\n            if grp.name == 'Pony':\n                return None\n            return grp.iloc[0]\n\n        result = df.groupby('A').apply(f)[['B']]\n        e = expected.copy()\n        e.loc['Pony'] = np.nan\n        assert_frame_equal(result, e)\n\n        # 5592 revisited, with datetimes\n        def f(grp):\n            if grp.name == 'Pony':\n                return None\n            return grp.iloc[0]\n\n        result = df.groupby('A').apply(f)[['C']]\n        e = df.groupby('A').first()[['C']]\n        e.loc['Pony'] = pd.NaT\n        assert_frame_equal(result, e)\n\n        # scalar outputs\n        def f(grp):\n            if grp.name == 'Pony':\n                return None\n            return grp.iloc[0].loc['C']\n\n        result = df.groupby('A').apply(f)\n        e = df.groupby('A').first()['C'].copy()\n        e.loc['Pony'] = np.nan\n        e.name = None\n        assert_series_equal(result, e)\n\n    def test_get_group(self):\n        wp = tm.makePanel()\n        grouped = wp.groupby(lambda x: x.month, axis='major')\n\n        gp = grouped.get_group(1)\n        expected = wp.reindex(major=[x for x in wp.major_axis if x.month == 1])\n        assert_panel_equal(gp, expected)\n\n        # GH 5267\n        # be datelike friendly\n        df = DataFrame({'DATE': pd.to_datetime(\n            ['10-Oct-2013', '10-Oct-2013', '10-Oct-2013', '11-Oct-2013',\n             '11-Oct-2013', '11-Oct-2013']),\n            'label': ['foo', 'foo', 'bar', 'foo', 'foo', 'bar'],\n            'VAL': [1, 2, 3, 4, 5, 6]})\n\n        g = df.groupby('DATE')\n        key = list(g.groups)[0]\n        result1 = g.get_group(key)\n        result2 = g.get_group(Timestamp(key).to_pydatetime())\n        result3 = g.get_group(str(Timestamp(key)))\n        assert_frame_equal(result1, result2)\n        assert_frame_equal(result1, result3)\n\n        g = df.groupby(['DATE', 'label'])\n\n        key = list(g.groups)[0]\n        result1 = g.get_group(key)\n        result2 = g.get_group((Timestamp(key[0]).to_pydatetime(), key[1]))\n        result3 = g.get_group((str(Timestamp(key[0])), key[1]))\n        assert_frame_equal(result1, result2)\n        assert_frame_equal(result1, result3)\n\n        # must pass a same-length tuple with multiple keys\n        self.assertRaises(ValueError, lambda: g.get_group('foo'))\n        self.assertRaises(ValueError, lambda: g.get_group(('foo')))\n        self.assertRaises(ValueError,\n                          lambda: g.get_group(('foo', 'bar', 'baz')))\n\n    def test_get_group_empty_bins(self):\n\n        d = pd.DataFrame([3, 1, 7, 6])\n        bins = [0, 5, 10, 15]\n        g = d.groupby(pd.cut(d[0], bins))\n\n        result = g.get_group('(0, 5]')\n        expected = DataFrame([3, 1], index=[0, 1])\n        assert_frame_equal(result, expected)\n\n        self.assertRaises(KeyError, lambda: g.get_group('(10, 15]'))\n\n    def test_get_group_grouped_by_tuple(self):\n        # GH 8121\n        df = DataFrame([[(1, ), (1, 2), (1, ), (1, 2)]], index=['ids']).T\n        gr = df.groupby('ids')\n        expected = DataFrame({'ids': [(1, ), (1, )]}, index=[0, 2])\n        result = gr.get_group((1, ))\n        assert_frame_equal(result, expected)\n\n        dt = pd.to_datetime(['2010-01-01', '2010-01-02', '2010-01-01',\n                             '2010-01-02'])\n        df = DataFrame({'ids': [(x, ) for x in dt]})\n        gr = df.groupby('ids')\n        result = gr.get_group(('2010-01-01', ))\n        expected = DataFrame({'ids': [(dt[0], ), (dt[0], )]}, index=[0, 2])\n        assert_frame_equal(result, expected)\n\n    def test_grouping_error_on_multidim_input(self):\n        from pandas.core.groupby import Grouping\n        self.assertRaises(ValueError,\n                          Grouping, self.df.index, self.df[['A', 'A']])\n\n    def test_apply_describe_bug(self):\n        grouped = self.mframe.groupby(level='first')\n        grouped.describe()  # it works!\n\n    def test_apply_issues(self):\n        # GH 5788\n\n        s = \"\"\"2011.05.16,00:00,1.40893\n2011.05.16,01:00,1.40760\n2011.05.16,02:00,1.40750\n2011.05.16,03:00,1.40649\n2011.05.17,02:00,1.40893\n2011.05.17,03:00,1.40760\n2011.05.17,04:00,1.40750\n2011.05.17,05:00,1.40649\n2011.05.18,02:00,1.40893\n2011.05.18,03:00,1.40760\n2011.05.18,04:00,1.40750\n2011.05.18,05:00,1.40649\"\"\"\n\n        df = pd.read_csv(\n            StringIO(s), header=None, names=['date', 'time', 'value'],\n            parse_dates=[['date', 'time']])\n        df = df.set_index('date_time')\n\n        expected = df.groupby(df.index.date).idxmax()\n        result = df.groupby(df.index.date).apply(lambda x: x.idxmax())\n        assert_frame_equal(result, expected)\n\n        # GH 5789\n        # don't auto coerce dates\n        df = pd.read_csv(\n            StringIO(s), header=None, names=['date', 'time', 'value'])\n        exp_idx = pd.Index(\n            ['2011.05.16', '2011.05.17', '2011.05.18'\n             ], dtype=object, name='date')\n        expected = Series(['00:00', '02:00', '02:00'], index=exp_idx)\n        result = df.groupby('date').apply(\n            lambda x: x['time'][x['value'].idxmax()])\n        assert_series_equal(result, expected)\n\n    def test_time_field_bug(self):\n        # Test a fix for the following error related to GH issue 11324 When\n        # non-key fields in a group-by dataframe contained time-based fields\n        # that were not returned by the apply function, an exception would be\n        # raised.\n\n        df = pd.DataFrame({'a': 1, 'b': [datetime.now() for nn in range(10)]})\n\n        def func_with_no_date(batch):\n            return pd.Series({'c': 2})\n\n        def func_with_date(batch):\n            return pd.Series({'c': 2, 'b': datetime(2015, 1, 1)})\n\n        dfg_no_conversion = df.groupby(by=['a']).apply(func_with_no_date)\n        dfg_no_conversion_expected = pd.DataFrame({'c': 2}, index=[1])\n        dfg_no_conversion_expected.index.name = 'a'\n\n        dfg_conversion = df.groupby(by=['a']).apply(func_with_date)\n        dfg_conversion_expected = pd.DataFrame(\n            {'b': datetime(2015, 1, 1),\n             'c': 2}, index=[1])\n        dfg_conversion_expected.index.name = 'a'\n\n        self.assert_frame_equal(dfg_no_conversion, dfg_no_conversion_expected)\n        self.assert_frame_equal(dfg_conversion, dfg_conversion_expected)\n\n    def test_len(self):\n        df = tm.makeTimeDataFrame()\n        grouped = df.groupby([lambda x: x.year, lambda x: x.month,\n                              lambda x: x.day])\n        self.assertEqual(len(grouped), len(df))\n\n        grouped = df.groupby([lambda x: x.year, lambda x: x.month])\n        expected = len(set([(x.year, x.month) for x in df.index]))\n        self.assertEqual(len(grouped), expected)\n\n        # issue 11016\n        df = pd.DataFrame(dict(a=[np.nan] * 3, b=[1, 2, 3]))\n        self.assertEqual(len(df.groupby(('a'))), 0)\n        self.assertEqual(len(df.groupby(('b'))), 3)\n        self.assertEqual(len(df.groupby(('a', 'b'))), 3)\n\n    def test_groups(self):\n        grouped = self.df.groupby(['A'])\n        groups = grouped.groups\n        self.assertIs(groups, grouped.groups)  # caching works\n\n        for k, v in compat.iteritems(grouped.groups):\n            self.assertTrue((self.df.loc[v]['A'] == k).all())\n\n        grouped = self.df.groupby(['A', 'B'])\n        groups = grouped.groups\n        self.assertIs(groups, grouped.groups)  # caching works\n        for k, v in compat.iteritems(grouped.groups):\n            self.assertTrue((self.df.loc[v]['A'] == k[0]).all())\n            self.assertTrue((self.df.loc[v]['B'] == k[1]).all())\n\n    def test_basic_regression(self):\n        # regression\n        T = [1.0 * x for x in lrange(1, 10) * 10][:1095]\n        result = Series(T, lrange(0, len(T)))\n\n        groupings = np.random.random((1100, ))\n        groupings = Series(groupings, lrange(0, len(groupings))) * 10.\n\n        grouped = result.groupby(groupings)\n        grouped.mean()\n\n    def test_with_na(self):\n        index = Index(np.arange(10))\n\n        for dtype in ['float64', 'float32', 'int64', 'int32', 'int16', 'int8']:\n            values = Series(np.ones(10), index, dtype=dtype)\n            labels = Series([nan, 'foo', 'bar', 'bar', nan, nan, 'bar',\n                             'bar', nan, 'foo'], index=index)\n\n            # this SHOULD be an int\n            grouped = values.groupby(labels)\n            agged = grouped.agg(len)\n            expected = Series([4, 2], index=['bar', 'foo'])\n\n            assert_series_equal(agged, expected, check_dtype=False)\n\n            # self.assertTrue(issubclass(agged.dtype.type, np.integer))\n\n            # explicity return a float from my function\n            def f(x):\n                return float(len(x))\n\n            agged = grouped.agg(f)\n            expected = Series([4, 2], index=['bar', 'foo'])\n\n            assert_series_equal(agged, expected, check_dtype=False)\n            self.assertTrue(issubclass(agged.dtype.type, np.dtype(dtype).type))\n\n    def test_indices_concatenation_order(self):\n\n        # GH 2808\n\n        def f1(x):\n            y = x[(x.b % 2) == 1] ** 2\n            if y.empty:\n                multiindex = MultiIndex(levels=[[]] * 2, labels=[[]] * 2,\n                                        names=['b', 'c'])\n                res = DataFrame(None, columns=['a'], index=multiindex)\n                return res\n            else:\n                y = y.set_index(['b', 'c'])\n                return y\n\n        def f2(x):\n            y = x[(x.b % 2) == 1] ** 2\n            if y.empty:\n                return DataFrame()\n            else:\n                y = y.set_index(['b', 'c'])\n                return y\n\n        def f3(x):\n            y = x[(x.b % 2) == 1] ** 2\n            if y.empty:\n                multiindex = MultiIndex(levels=[[]] * 2, labels=[[]] * 2,\n                                        names=['foo', 'bar'])\n                res = DataFrame(None, columns=['a', 'b'], index=multiindex)\n                return res\n            else:\n                return y\n\n        df = DataFrame({'a': [1, 2, 2, 2], 'b': lrange(4), 'c': lrange(5, 9)})\n\n        df2 = DataFrame({'a': [3, 2, 2, 2], 'b': lrange(4), 'c': lrange(5, 9)})\n\n        # correct result\n        result1 = df.groupby('a').apply(f1)\n        result2 = df2.groupby('a').apply(f1)\n        assert_frame_equal(result1, result2)\n\n        # should fail (not the same number of levels)\n        self.assertRaises(AssertionError, df.groupby('a').apply, f2)\n        self.assertRaises(AssertionError, df2.groupby('a').apply, f2)\n\n        # should fail (incorrect shape)\n        self.assertRaises(AssertionError, df.groupby('a').apply, f3)\n        self.assertRaises(AssertionError, df2.groupby('a').apply, f3)\n\n    def test_attr_wrapper(self):\n        grouped = self.ts.groupby(lambda x: x.weekday())\n\n        result = grouped.std()\n        expected = grouped.agg(lambda x: np.std(x, ddof=1))\n        assert_series_equal(result, expected)\n\n        # this is pretty cool\n        result = grouped.describe()\n        expected = {}\n        for name, gp in grouped:\n            expected[name] = gp.describe()\n        expected = DataFrame(expected).T\n        assert_frame_equal(result, expected)\n\n        # get attribute\n        result = grouped.dtype\n        expected = grouped.agg(lambda x: x.dtype)\n\n        # make sure raises error\n        self.assertRaises(AttributeError, getattr, grouped, 'foo')\n\n    def test_series_describe_multikey(self):\n        ts = tm.makeTimeSeries()\n        grouped = ts.groupby([lambda x: x.year, lambda x: x.month])\n        result = grouped.describe()\n        assert_series_equal(result['mean'], grouped.mean(), check_names=False)\n        assert_series_equal(result['std'], grouped.std(), check_names=False)\n        assert_series_equal(result['min'], grouped.min(), check_names=False)\n\n    def test_series_describe_single(self):\n        ts = tm.makeTimeSeries()\n        grouped = ts.groupby(lambda x: x.month)\n        result = grouped.apply(lambda x: x.describe())\n        expected = grouped.describe().stack()\n        assert_series_equal(result, expected)\n\n    def test_series_index_name(self):\n        grouped = self.df.loc[:, ['C']].groupby(self.df['A'])\n        result = grouped.agg(lambda x: x.mean())\n        self.assertEqual(result.index.name, 'A')\n\n    def test_frame_describe_multikey(self):\n        grouped = self.tsframe.groupby([lambda x: x.year, lambda x: x.month])\n        result = grouped.describe()\n        desc_groups = []\n        for col in self.tsframe:\n            group = grouped[col].describe()\n            group_col = pd.MultiIndex([[col] * len(group.columns),\n                                       group.columns],\n                                      [[0] * len(group.columns),\n                                       range(len(group.columns))])\n            group = pd.DataFrame(group.values,\n                                 columns=group_col,\n                                 index=group.index)\n            desc_groups.append(group)\n        expected = pd.concat(desc_groups, axis=1)\n        tm.assert_frame_equal(result, expected)\n\n        groupedT = self.tsframe.groupby({'A': 0, 'B': 0,\n                                         'C': 1, 'D': 1}, axis=1)\n        result = groupedT.describe()\n        expected = self.tsframe.describe().T\n        expected.index = pd.MultiIndex([[0, 0, 1, 1], expected.index],\n                                       [range(4), range(len(expected.index))])\n        tm.assert_frame_equal(result, expected)\n\n    def test_frame_describe_tupleindex(self):\n\n        # GH 14848 - regression from 0.19.0 to 0.19.1\n        df1 = DataFrame({'x': [1, 2, 3, 4, 5] * 3,\n                         'y': [10, 20, 30, 40, 50] * 3,\n                         'z': [100, 200, 300, 400, 500] * 3})\n        df1['k'] = [(0, 0, 1), (0, 1, 0), (1, 0, 0)] * 5\n        df2 = df1.rename(columns={'k': 'key'})\n        tm.assertRaises(ValueError, lambda: df1.groupby('k').describe())\n        tm.assertRaises(ValueError, lambda: df2.groupby('key').describe())\n\n    def test_frame_describe_unstacked_format(self):\n        # GH 4792\n        prices = {pd.Timestamp('2011-01-06 10:59:05', tz=None): 24990,\n                  pd.Timestamp('2011-01-06 12:43:33', tz=None): 25499,\n                  pd.Timestamp('2011-01-06 12:54:09', tz=None): 25499}\n        volumes = {pd.Timestamp('2011-01-06 10:59:05', tz=None): 1500000000,\n                   pd.Timestamp('2011-01-06 12:43:33', tz=None): 5000000000,\n                   pd.Timestamp('2011-01-06 12:54:09', tz=None): 100000000}\n        df = pd.DataFrame({'PRICE': prices,\n                           'VOLUME': volumes})\n        result = df.groupby('PRICE').VOLUME.describe()\n        data = [df[df.PRICE == 24990].VOLUME.describe().values.tolist(),\n                df[df.PRICE == 25499].VOLUME.describe().values.tolist()]\n        expected = pd.DataFrame(data,\n                                index=pd.Index([24990, 25499], name='PRICE'),\n                                columns=['count', 'mean', 'std', 'min',\n                                         '25%', '50%', '75%', 'max'])\n        tm.assert_frame_equal(result, expected)\n\n    def test_frame_groupby(self):\n        grouped = self.tsframe.groupby(lambda x: x.weekday())\n\n        # aggregate\n        aggregated = grouped.aggregate(np.mean)\n        self.assertEqual(len(aggregated), 5)\n        self.assertEqual(len(aggregated.columns), 4)\n\n        # by string\n        tscopy = self.tsframe.copy()\n        tscopy['weekday'] = [x.weekday() for x in tscopy.index]\n        stragged = tscopy.groupby('weekday').aggregate(np.mean)\n        assert_frame_equal(stragged, aggregated, check_names=False)\n\n        # transform\n        grouped = self.tsframe.head(30).groupby(lambda x: x.weekday())\n        transformed = grouped.transform(lambda x: x - x.mean())\n        self.assertEqual(len(transformed), 30)\n        self.assertEqual(len(transformed.columns), 4)\n\n        # transform propagate\n        transformed = grouped.transform(lambda x: x.mean())\n        for name, group in grouped:\n            mean = group.mean()\n            for idx in group.index:\n                tm.assert_series_equal(transformed.xs(idx), mean,\n                                       check_names=False)\n\n        # iterate\n        for weekday, group in grouped:\n            self.assertEqual(group.index[0].weekday(), weekday)\n\n        # groups / group_indices\n        groups = grouped.groups\n        indices = grouped.indices\n\n        for k, v in compat.iteritems(groups):\n            samething = self.tsframe.index.take(indices[k])\n            self.assertTrue((samething == v).all())\n\n    def test_grouping_is_iterable(self):\n        # this code path isn't used anywhere else\n        # not sure it's useful\n        grouped = self.tsframe.groupby([lambda x: x.weekday(), lambda x: x.year\n                                        ])\n\n        # test it works\n        for g in grouped.grouper.groupings[0]:\n            pass\n\n    def test_frame_groupby_columns(self):\n        mapping = {'A': 0, 'B': 0, 'C': 1, 'D': 1}\n        grouped = self.tsframe.groupby(mapping, axis=1)\n\n        # aggregate\n        aggregated = grouped.aggregate(np.mean)\n        self.assertEqual(len(aggregated), len(self.tsframe))\n        self.assertEqual(len(aggregated.columns), 2)\n\n        # transform\n        tf = lambda x: x - x.mean()\n        groupedT = self.tsframe.T.groupby(mapping, axis=0)\n        assert_frame_equal(groupedT.transform(tf).T, grouped.transform(tf))\n\n        # iterate\n        for k, v in grouped:\n            self.assertEqual(len(v.columns), 2)\n\n    def test_frame_set_name_single(self):\n        grouped = self.df.groupby('A')\n\n        result = grouped.mean()\n        self.assertEqual(result.index.name, 'A')\n\n        result = self.df.groupby('A', as_index=False).mean()\n        self.assertNotEqual(result.index.name, 'A')\n\n        result = grouped.agg(np.mean)\n        self.assertEqual(result.index.name, 'A')\n\n        result = grouped.agg({'C': np.mean, 'D': np.std})\n        self.assertEqual(result.index.name, 'A')\n\n        result = grouped['C'].mean()\n        self.assertEqual(result.index.name, 'A')\n        result = grouped['C'].agg(np.mean)\n        self.assertEqual(result.index.name, 'A')\n        result = grouped['C'].agg([np.mean, np.std])\n        self.assertEqual(result.index.name, 'A')\n\n        result = grouped['C'].agg({'foo': np.mean, 'bar': np.std})\n        self.assertEqual(result.index.name, 'A')\n\n    def test_multi_iter(self):\n        s = Series(np.arange(6))\n        k1 = np.array(['a', 'a', 'a', 'b', 'b', 'b'])\n        k2 = np.array(['1', '2', '1', '2', '1', '2'])\n\n        grouped = s.groupby([k1, k2])\n\n        iterated = list(grouped)\n        expected = [('a', '1', s[[0, 2]]), ('a', '2', s[[1]]),\n                    ('b', '1', s[[4]]), ('b', '2', s[[3, 5]])]\n        for i, ((one, two), three) in enumerate(iterated):\n            e1, e2, e3 = expected[i]\n            self.assertEqual(e1, one)\n            self.assertEqual(e2, two)\n            assert_series_equal(three, e3)\n\n    def test_multi_iter_frame(self):\n        k1 = np.array(['b', 'b', 'b', 'a', 'a', 'a'])\n        k2 = np.array(['1', '2', '1', '2', '1', '2'])\n        df = DataFrame({'v1': np.random.randn(6),\n                        'v2': np.random.randn(6),\n                        'k1': k1, 'k2': k2},\n                       index=['one', 'two', 'three', 'four', 'five', 'six'])\n\n        grouped = df.groupby(['k1', 'k2'])\n\n        # things get sorted!\n        iterated = list(grouped)\n        idx = df.index\n        expected = [('a', '1', df.loc[idx[[4]]]),\n                    ('a', '2', df.loc[idx[[3, 5]]]),\n                    ('b', '1', df.loc[idx[[0, 2]]]),\n                    ('b', '2', df.loc[idx[[1]]])]\n        for i, ((one, two), three) in enumerate(iterated):\n            e1, e2, e3 = expected[i]\n            self.assertEqual(e1, one)\n            self.assertEqual(e2, two)\n            assert_frame_equal(three, e3)\n\n        # don't iterate through groups with no data\n        df['k1'] = np.array(['b', 'b', 'b', 'a', 'a', 'a'])\n        df['k2'] = np.array(['1', '1', '1', '2', '2', '2'])\n        grouped = df.groupby(['k1', 'k2'])\n        groups = {}\n        for key, gp in grouped:\n            groups[key] = gp\n        self.assertEqual(len(groups), 2)\n\n        # axis = 1\n        three_levels = self.three_group.groupby(['A', 'B', 'C']).mean()\n        grouped = three_levels.T.groupby(axis=1, level=(1, 2))\n        for key, group in grouped:\n            pass\n\n    def test_multi_iter_panel(self):\n        wp = tm.makePanel()\n        grouped = wp.groupby([lambda x: x.month, lambda x: x.weekday()],\n                             axis=1)\n\n        for (month, wd), group in grouped:\n            exp_axis = [x\n                        for x in wp.major_axis\n                        if x.month == month and x.weekday() == wd]\n            expected = wp.reindex(major=exp_axis)\n            assert_panel_equal(group, expected)\n\n    def test_multi_func(self):\n        col1 = self.df['A']\n        col2 = self.df['B']\n\n        grouped = self.df.groupby([col1.get, col2.get])\n        agged = grouped.mean()\n        expected = self.df.groupby(['A', 'B']).mean()\n\n        # TODO groupby get drops names\n        assert_frame_equal(agged.loc[:, ['C', 'D']],\n                           expected.loc[:, ['C', 'D']],\n                           check_names=False)\n\n        # some \"groups\" with no data\n        df = DataFrame({'v1': np.random.randn(6),\n                        'v2': np.random.randn(6),\n                        'k1': np.array(['b', 'b', 'b', 'a', 'a', 'a']),\n                        'k2': np.array(['1', '1', '1', '2', '2', '2'])},\n                       index=['one', 'two', 'three', 'four', 'five', 'six'])\n        # only verify that it works for now\n        grouped = df.groupby(['k1', 'k2'])\n        grouped.agg(np.sum)\n\n    def test_multi_key_multiple_functions(self):\n        grouped = self.df.groupby(['A', 'B'])['C']\n\n        agged = grouped.agg([np.mean, np.std])\n        expected = DataFrame({'mean': grouped.agg(np.mean),\n                              'std': grouped.agg(np.std)})\n        assert_frame_equal(agged, expected)\n\n    def test_frame_multi_key_function_list(self):\n        data = DataFrame(\n            {'A': ['foo', 'foo', 'foo', 'foo', 'bar', 'bar', 'bar', 'bar',\n                   'foo', 'foo', 'foo'],\n             'B': ['one', 'one', 'one', 'two', 'one', 'one', 'one', 'two',\n                   'two', 'two', 'one'],\n             'C': ['dull', 'dull', 'shiny', 'dull', 'dull', 'shiny', 'shiny',\n                   'dull', 'shiny', 'shiny', 'shiny'],\n             'D': np.random.randn(11),\n             'E': np.random.randn(11),\n             'F': np.random.randn(11)})\n\n        grouped = data.groupby(['A', 'B'])\n        funcs = [np.mean, np.std]\n        agged = grouped.agg(funcs)\n        expected = concat([grouped['D'].agg(funcs), grouped['E'].agg(funcs),\n                           grouped['F'].agg(funcs)],\n                          keys=['D', 'E', 'F'], axis=1)\n        assert (isinstance(agged.index, MultiIndex))\n        assert (isinstance(expected.index, MultiIndex))\n        assert_frame_equal(agged, expected)\n\n    def test_groupby_multiple_columns(self):\n        data = self.df\n        grouped = data.groupby(['A', 'B'])\n\n        def _check_op(op):\n\n            result1 = op(grouped)\n\n            expected = defaultdict(dict)\n            for n1, gp1 in data.groupby('A'):\n                for n2, gp2 in gp1.groupby('B'):\n                    expected[n1][n2] = op(gp2.loc[:, ['C', 'D']])\n            expected = dict((k, DataFrame(v))\n                            for k, v in compat.iteritems(expected))\n            expected = Panel.fromDict(expected).swapaxes(0, 1)\n            expected.major_axis.name, expected.minor_axis.name = 'A', 'B'\n\n            # a little bit crude\n            for col in ['C', 'D']:\n                result_col = op(grouped[col])\n                exp = expected[col]\n                pivoted = result1[col].unstack()\n                pivoted2 = result_col.unstack()\n                assert_frame_equal(pivoted.reindex_like(exp), exp)\n                assert_frame_equal(pivoted2.reindex_like(exp), exp)\n\n        _check_op(lambda x: x.sum())\n        _check_op(lambda x: x.mean())\n\n        # test single series works the same\n        result = data['C'].groupby([data['A'], data['B']]).mean()\n        expected = data.groupby(['A', 'B']).mean()['C']\n\n        assert_series_equal(result, expected)\n\n    def test_groupby_as_index_agg(self):\n        grouped = self.df.groupby('A', as_index=False)\n\n        # single-key\n\n        result = grouped.agg(np.mean)\n        expected = grouped.mean()\n        assert_frame_equal(result, expected)\n\n        result2 = grouped.agg(OrderedDict([['C', np.mean], ['D', np.sum]]))\n        expected2 = grouped.mean()\n        expected2['D'] = grouped.sum()['D']\n        assert_frame_equal(result2, expected2)\n\n        grouped = self.df.groupby('A', as_index=True)\n        expected3 = grouped['C'].sum()\n        expected3 = DataFrame(expected3).rename(columns={'C': 'Q'})\n        result3 = grouped['C'].agg({'Q': np.sum})\n        assert_frame_equal(result3, expected3)\n\n        # multi-key\n\n        grouped = self.df.groupby(['A', 'B'], as_index=False)\n\n        result = grouped.agg(np.mean)\n        expected = grouped.mean()\n        assert_frame_equal(result, expected)\n\n        result2 = grouped.agg(OrderedDict([['C', np.mean], ['D', np.sum]]))\n        expected2 = grouped.mean()\n        expected2['D'] = grouped.sum()['D']\n        assert_frame_equal(result2, expected2)\n\n        expected3 = grouped['C'].sum()\n        expected3 = DataFrame(expected3).rename(columns={'C': 'Q'})\n        result3 = grouped['C'].agg({'Q': np.sum})\n        assert_frame_equal(result3, expected3)\n\n        # GH7115 & GH8112 & GH8582\n        df = DataFrame(np.random.randint(0, 100, (50, 3)),\n                       columns=['jim', 'joe', 'jolie'])\n        ts = Series(np.random.randint(5, 10, 50), name='jim')\n\n        gr = df.groupby(ts)\n        gr.nth(0)  # invokes set_selection_from_grouper internally\n        assert_frame_equal(gr.apply(sum), df.groupby(ts).apply(sum))\n\n        for attr in ['mean', 'max', 'count', 'idxmax', 'cumsum', 'all']:\n            gr = df.groupby(ts, as_index=False)\n            left = getattr(gr, attr)()\n\n            gr = df.groupby(ts.values, as_index=True)\n            right = getattr(gr, attr)().reset_index(drop=True)\n\n            assert_frame_equal(left, right)\n\n    def test_series_groupby_nunique(self):\n\n        def check_nunique(df, keys, as_index=True):\n            for sort, dropna in cart_product((False, True), repeat=2):\n                gr = df.groupby(keys, as_index=as_index, sort=sort)\n                left = gr['julie'].nunique(dropna=dropna)\n\n                gr = df.groupby(keys, as_index=as_index, sort=sort)\n                right = gr['julie'].apply(Series.nunique, dropna=dropna)\n                if not as_index:\n                    right = right.reset_index(drop=True)\n\n                assert_series_equal(left, right, check_names=False)\n\n        days = date_range('2015-08-23', periods=10)\n\n        for n, m in cart_product(10 ** np.arange(2, 6), (10, 100, 1000)):\n            frame = DataFrame({\n                'jim': np.random.choice(\n                    list(ascii_lowercase), n),\n                'joe': np.random.choice(days, n),\n                'julie': np.random.randint(0, m, n)\n            })\n\n            check_nunique(frame, ['jim'])\n            check_nunique(frame, ['jim', 'joe'])\n\n            frame.loc[1::17, 'jim'] = None\n            frame.loc[3::37, 'joe'] = None\n            frame.loc[7::19, 'julie'] = None\n            frame.loc[8::19, 'julie'] = None\n            frame.loc[9::19, 'julie'] = None\n\n            check_nunique(frame, ['jim'])\n            check_nunique(frame, ['jim', 'joe'])\n            check_nunique(frame, ['jim'], as_index=False)\n            check_nunique(frame, ['jim', 'joe'], as_index=False)\n\n    def test_series_groupby_value_counts(self):\n        from itertools import product\n        np.random.seed(1234)\n\n        def rebuild_index(df):\n            arr = list(map(df.index.get_level_values, range(df.index.nlevels)))\n            df.index = MultiIndex.from_arrays(arr, names=df.index.names)\n            return df\n\n        def check_value_counts(df, keys, bins):\n            for isort, normalize, sort, ascending, dropna \\\n                    in product((False, True), repeat=5):\n\n                kwargs = dict(normalize=normalize, sort=sort,\n                              ascending=ascending, dropna=dropna, bins=bins)\n\n                gr = df.groupby(keys, sort=isort)\n                left = gr['3rd'].value_counts(**kwargs)\n\n                gr = df.groupby(keys, sort=isort)\n                right = gr['3rd'].apply(Series.value_counts, **kwargs)\n                right.index.names = right.index.names[:-1] + ['3rd']\n\n                # have to sort on index because of unstable sort on values\n                left, right = map(rebuild_index, (left, right))  # xref GH9212\n                assert_series_equal(left.sort_index(), right.sort_index())\n\n        def loop(df):\n            bins = None, np.arange(0, max(5, df['3rd'].max()) + 1, 2)\n            keys = '1st', '2nd', ('1st', '2nd')\n            for k, b in product(keys, bins):\n                check_value_counts(df, k, b)\n\n        days = date_range('2015-08-24', periods=10)\n\n        for n, m in product((100, 1000), (5, 20)):\n            frame = DataFrame({\n                '1st': np.random.choice(\n                    list('abcd'), n),\n                '2nd': np.random.choice(days, n),\n                '3rd': np.random.randint(1, m + 1, n)\n            })\n\n            loop(frame)\n\n            frame.loc[1::11, '1st'] = nan\n            frame.loc[3::17, '2nd'] = nan\n            frame.loc[7::19, '3rd'] = nan\n            frame.loc[8::19, '3rd'] = nan\n            frame.loc[9::19, '3rd'] = nan\n\n            loop(frame)\n\n    def test_multiindex_passthru(self):\n\n        # GH 7997\n        # regression from 0.14.1\n        df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        df.columns = pd.MultiIndex.from_tuples([(0, 1), (1, 1), (2, 1)])\n\n        result = df.groupby(axis=1, level=[0, 1]).first()\n        assert_frame_equal(result, df)\n\n    def test_multiindex_negative_level(self):\n        # GH 13901\n        result = self.mframe.groupby(level=-1).sum()\n        expected = self.mframe.groupby(level='second').sum()\n        assert_frame_equal(result, expected)\n\n        result = self.mframe.groupby(level=-2).sum()\n        expected = self.mframe.groupby(level='first').sum()\n        assert_frame_equal(result, expected)\n\n        result = self.mframe.groupby(level=[-2, -1]).sum()\n        expected = self.mframe\n        assert_frame_equal(result, expected)\n\n        result = self.mframe.groupby(level=[-1, 'first']).sum()\n        expected = self.mframe.groupby(level=['second', 'first']).sum()\n        assert_frame_equal(result, expected)\n\n    def test_multifunc_select_col_integer_cols(self):\n        df = self.df\n        df.columns = np.arange(len(df.columns))\n\n        # it works!\n        df.groupby(1, as_index=False)[2].agg({'Q': np.mean})\n\n    def test_as_index_series_return_frame(self):\n        grouped = self.df.groupby('A', as_index=False)\n        grouped2 = self.df.groupby(['A', 'B'], as_index=False)\n\n        result = grouped['C'].agg(np.sum)\n        expected = grouped.agg(np.sum).loc[:, ['A', 'C']]\n        tm.assertIsInstance(result, DataFrame)\n        assert_frame_equal(result, expected)\n\n        result2 = grouped2['C'].agg(np.sum)\n        expected2 = grouped2.agg(np.sum).loc[:, ['A', 'B', 'C']]\n        tm.assertIsInstance(result2, DataFrame)\n        assert_frame_equal(result2, expected2)\n\n        result = grouped['C'].sum()\n        expected = grouped.sum().loc[:, ['A', 'C']]\n        tm.assertIsInstance(result, DataFrame)\n        assert_frame_equal(result, expected)\n\n        result2 = grouped2['C'].sum()\n        expected2 = grouped2.sum().loc[:, ['A', 'B', 'C']]\n        tm.assertIsInstance(result2, DataFrame)\n        assert_frame_equal(result2, expected2)\n\n        # corner case\n        self.assertRaises(Exception, grouped['C'].__getitem__, 'D')\n\n    def test_groupby_as_index_cython(self):\n        data = self.df\n\n        # single-key\n        grouped = data.groupby('A', as_index=False)\n        result = grouped.mean()\n        expected = data.groupby(['A']).mean()\n        expected.insert(0, 'A', expected.index)\n        expected.index = np.arange(len(expected))\n        assert_frame_equal(result, expected)\n\n        # multi-key\n        grouped = data.groupby(['A', 'B'], as_index=False)\n        result = grouped.mean()\n        expected = data.groupby(['A', 'B']).mean()\n\n        arrays = lzip(*expected.index._tuple_index)\n        expected.insert(0, 'A', arrays[0])\n        expected.insert(1, 'B', arrays[1])\n        expected.index = np.arange(len(expected))\n        assert_frame_equal(result, expected)\n\n    def test_groupby_as_index_series_scalar(self):\n        grouped = self.df.groupby(['A', 'B'], as_index=False)\n\n        # GH #421\n\n        result = grouped['C'].agg(len)\n        expected = grouped.agg(len).loc[:, ['A', 'B', 'C']]\n        assert_frame_equal(result, expected)\n\n    def test_groupby_as_index_corner(self):\n        self.assertRaises(TypeError, self.ts.groupby, lambda x: x.weekday(),\n                          as_index=False)\n\n        self.assertRaises(ValueError, self.df.groupby, lambda x: x.lower(),\n                          as_index=False, axis=1)\n\n    def test_groupby_as_index_apply(self):\n        # GH #4648 and #3417\n        df = DataFrame({'item_id': ['b', 'b', 'a', 'c', 'a', 'b'],\n                        'user_id': [1, 2, 1, 1, 3, 1],\n                        'time': range(6)})\n\n        g_as = df.groupby('user_id', as_index=True)\n        g_not_as = df.groupby('user_id', as_index=False)\n\n        res_as = g_as.head(2).index\n        res_not_as = g_not_as.head(2).index\n        exp = Index([0, 1, 2, 4])\n        assert_index_equal(res_as, exp)\n        assert_index_equal(res_not_as, exp)\n\n        res_as_apply = g_as.apply(lambda x: x.head(2)).index\n        res_not_as_apply = g_not_as.apply(lambda x: x.head(2)).index\n\n        # apply doesn't maintain the original ordering\n        # changed in GH5610 as the as_index=False returns a MI here\n        exp_not_as_apply = MultiIndex.from_tuples([(0, 0), (0, 2), (1, 1), (\n            2, 4)])\n        tp = [(1, 0), (1, 2), (2, 1), (3, 4)]\n        exp_as_apply = MultiIndex.from_tuples(tp, names=['user_id', None])\n\n        assert_index_equal(res_as_apply, exp_as_apply)\n        assert_index_equal(res_not_as_apply, exp_not_as_apply)\n\n        ind = Index(list('abcde'))\n        df = DataFrame([[1, 2], [2, 3], [1, 4], [1, 5], [2, 6]], index=ind)\n        res = df.groupby(0, as_index=False).apply(lambda x: x).index\n        assert_index_equal(res, ind)\n\n    def test_groupby_head_tail(self):\n        df = DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])\n        g_as = df.groupby('A', as_index=True)\n        g_not_as = df.groupby('A', as_index=False)\n\n        # as_index= False, much easier\n        assert_frame_equal(df.loc[[0, 2]], g_not_as.head(1))\n        assert_frame_equal(df.loc[[1, 2]], g_not_as.tail(1))\n\n        empty_not_as = DataFrame(columns=df.columns,\n                                 index=pd.Index([], dtype=df.index.dtype))\n        empty_not_as['A'] = empty_not_as['A'].astype(df.A.dtype)\n        empty_not_as['B'] = empty_not_as['B'].astype(df.B.dtype)\n        assert_frame_equal(empty_not_as, g_not_as.head(0))\n        assert_frame_equal(empty_not_as, g_not_as.tail(0))\n        assert_frame_equal(empty_not_as, g_not_as.head(-1))\n        assert_frame_equal(empty_not_as, g_not_as.tail(-1))\n\n        assert_frame_equal(df, g_not_as.head(7))  # contains all\n        assert_frame_equal(df, g_not_as.tail(7))\n\n        # as_index=True, (used to be different)\n        df_as = df\n\n        assert_frame_equal(df_as.loc[[0, 2]], g_as.head(1))\n        assert_frame_equal(df_as.loc[[1, 2]], g_as.tail(1))\n\n        empty_as = DataFrame(index=df_as.index[:0], columns=df.columns)\n        empty_as['A'] = empty_not_as['A'].astype(df.A.dtype)\n        empty_as['B'] = empty_not_as['B'].astype(df.B.dtype)\n        assert_frame_equal(empty_as, g_as.head(0))\n        assert_frame_equal(empty_as, g_as.tail(0))\n        assert_frame_equal(empty_as, g_as.head(-1))\n        assert_frame_equal(empty_as, g_as.tail(-1))\n\n        assert_frame_equal(df_as, g_as.head(7))  # contains all\n        assert_frame_equal(df_as, g_as.tail(7))\n\n        # test with selection\n        assert_frame_equal(g_as[[]].head(1), df_as.loc[[0, 2], []])\n        assert_frame_equal(g_as[['A']].head(1), df_as.loc[[0, 2], ['A']])\n        assert_frame_equal(g_as[['B']].head(1), df_as.loc[[0, 2], ['B']])\n        assert_frame_equal(g_as[['A', 'B']].head(1), df_as.loc[[0, 2]])\n\n        assert_frame_equal(g_not_as[[]].head(1), df_as.loc[[0, 2], []])\n        assert_frame_equal(g_not_as[['A']].head(1), df_as.loc[[0, 2], ['A']])\n        assert_frame_equal(g_not_as[['B']].head(1), df_as.loc[[0, 2], ['B']])\n        assert_frame_equal(g_not_as[['A', 'B']].head(1), df_as.loc[[0, 2]])\n\n    def test_groupby_multiple_key(self):\n        df = tm.makeTimeDataFrame()\n        grouped = df.groupby([lambda x: x.year, lambda x: x.month,\n                              lambda x: x.day])\n        agged = grouped.sum()\n        assert_almost_equal(df.values, agged.values)\n\n        grouped = df.T.groupby([lambda x: x.year,\n                                lambda x: x.month,\n                                lambda x: x.day], axis=1)\n\n        agged = grouped.agg(lambda x: x.sum())\n        self.assert_index_equal(agged.index, df.columns)\n        assert_almost_equal(df.T.values, agged.values)\n\n        agged = grouped.agg(lambda x: x.sum())\n        assert_almost_equal(df.T.values, agged.values)\n\n    def test_groupby_multi_corner(self):\n        # test that having an all-NA column doesn't mess you up\n        df = self.df.copy()\n        df['bad'] = np.nan\n        agged = df.groupby(['A', 'B']).mean()\n\n        expected = self.df.groupby(['A', 'B']).mean()\n        expected['bad'] = np.nan\n\n        assert_frame_equal(agged, expected)\n\n    def test_omit_nuisance(self):\n        grouped = self.df.groupby('A')\n\n        result = grouped.mean()\n        expected = self.df.loc[:, ['A', 'C', 'D']].groupby('A').mean()\n        assert_frame_equal(result, expected)\n\n        agged = grouped.agg(np.mean)\n        exp = grouped.mean()\n        assert_frame_equal(agged, exp)\n\n        df = self.df.loc[:, ['A', 'C', 'D']]\n        df['E'] = datetime.now()\n        grouped = df.groupby('A')\n        result = grouped.agg(np.sum)\n        expected = grouped.sum()\n        assert_frame_equal(result, expected)\n\n        # won't work with axis = 1\n        grouped = df.groupby({'A': 0, 'C': 0, 'D': 1, 'E': 1}, axis=1)\n        result = self.assertRaises(TypeError, grouped.agg,\n                                   lambda x: x.sum(0, numeric_only=False))\n\n    def test_omit_nuisance_python_multiple(self):\n        grouped = self.three_group.groupby(['A', 'B'])\n\n        agged = grouped.agg(np.mean)\n        exp = grouped.mean()\n        assert_frame_equal(agged, exp)\n\n    def test_empty_groups_corner(self):\n        # handle empty groups\n        df = DataFrame({'k1': np.array(['b', 'b', 'b', 'a', 'a', 'a']),\n                        'k2': np.array(['1', '1', '1', '2', '2', '2']),\n                        'k3': ['foo', 'bar'] * 3,\n                        'v1': np.random.randn(6),\n                        'v2': np.random.randn(6)})\n\n        grouped = df.groupby(['k1', 'k2'])\n        result = grouped.agg(np.mean)\n        expected = grouped.mean()\n        assert_frame_equal(result, expected)\n\n        grouped = self.mframe[3:5].groupby(level=0)\n        agged = grouped.apply(lambda x: x.mean())\n        agged_A = grouped['A'].apply(np.mean)\n        assert_series_equal(agged['A'], agged_A)\n        self.assertEqual(agged.index.name, 'first')\n\n    def test_apply_concat_preserve_names(self):\n        grouped = self.three_group.groupby(['A', 'B'])\n\n        def desc(group):\n            result = group.describe()\n            result.index.name = 'stat'\n            return result\n\n        def desc2(group):\n            result = group.describe()\n            result.index.name = 'stat'\n            result = result[:len(group)]\n            # weirdo\n            return result\n\n        def desc3(group):\n            result = group.describe()\n\n            # names are different\n            result.index.name = 'stat_%d' % len(group)\n\n            result = result[:len(group)]\n            # weirdo\n            return result\n\n        result = grouped.apply(desc)\n        self.assertEqual(result.index.names, ('A', 'B', 'stat'))\n\n        result2 = grouped.apply(desc2)\n        self.assertEqual(result2.index.names, ('A', 'B', 'stat'))\n\n        result3 = grouped.apply(desc3)\n        self.assertEqual(result3.index.names, ('A', 'B', None))\n\n    def test_nonsense_func(self):\n        df = DataFrame([0])\n        self.assertRaises(Exception, df.groupby, lambda x: x + 'foo')\n\n    def test_builtins_apply(self):  # GH8155\n        df = pd.DataFrame(np.random.randint(1, 50, (1000, 2)),\n                          columns=['jim', 'joe'])\n        df['jolie'] = np.random.randn(1000)\n\n        for keys in ['jim', ['jim', 'joe']]:  # single key & multi-key\n            if keys == 'jim':\n                continue\n            for f in [max, min, sum]:\n                fname = f.__name__\n                result = df.groupby(keys).apply(f)\n                result.shape\n                ngroups = len(df.drop_duplicates(subset=keys))\n                assert result.shape == (ngroups, 3), 'invalid frame shape: '\\\n                    '{} (expected ({}, 3))'.format(result.shape, ngroups)\n\n                assert_frame_equal(result,  # numpy's equivalent function\n                                   df.groupby(keys).apply(getattr(np, fname)))\n\n                if f != sum:\n                    expected = df.groupby(keys).agg(fname).reset_index()\n                    expected.set_index(keys, inplace=True, drop=False)\n                    assert_frame_equal(result, expected, check_dtype=False)\n\n                assert_series_equal(getattr(result, fname)(),\n                                    getattr(df, fname)())\n\n    def test_max_min_non_numeric(self):\n        # #2700\n        aa = DataFrame({'nn': [11, 11, 22, 22],\n                        'ii': [1, 2, 3, 4],\n                        'ss': 4 * ['mama']})\n\n        result = aa.groupby('nn').max()\n        self.assertTrue('ss' in result)\n\n        result = aa.groupby('nn').max(numeric_only=False)\n        self.assertTrue('ss' in result)\n\n        result = aa.groupby('nn').min()\n        self.assertTrue('ss' in result)\n\n        result = aa.groupby('nn').min(numeric_only=False)\n        self.assertTrue('ss' in result)\n\n    def test_arg_passthru(self):\n        # make sure that we are passing thru kwargs\n        # to our agg functions\n\n        # GH3668\n        # GH5724\n        df = pd.DataFrame(\n            {'group': [1, 1, 2],\n             'int': [1, 2, 3],\n             'float': [4., 5., 6.],\n             'string': list('abc'),\n             'category_string': pd.Series(list('abc')).astype('category'),\n             'category_int': [7, 8, 9],\n             'datetime': pd.date_range('20130101', periods=3),\n             'datetimetz': pd.date_range('20130101',\n                                         periods=3,\n                                         tz='US/Eastern'),\n             'timedelta': pd.timedelta_range('1 s', periods=3, freq='s')},\n            columns=['group', 'int', 'float', 'string',\n                     'category_string', 'category_int',\n                     'datetime', 'datetimetz',\n                     'timedelta'])\n\n        expected_columns_numeric = Index(['int', 'float', 'category_int'])\n\n        # mean / median\n        expected = pd.DataFrame(\n            {'category_int': [7.5, 9],\n             'float': [4.5, 6.],\n             'timedelta': [pd.Timedelta('1.5s'),\n                           pd.Timedelta('3s')],\n             'int': [1.5, 3],\n             'datetime': [pd.Timestamp('2013-01-01 12:00:00'),\n                          pd.Timestamp('2013-01-03 00:00:00')],\n             'datetimetz': [\n                 pd.Timestamp('2013-01-01 12:00:00', tz='US/Eastern'),\n                 pd.Timestamp('2013-01-03 00:00:00', tz='US/Eastern')]},\n            index=Index([1, 2], name='group'),\n            columns=['int', 'float', 'category_int',\n                     'datetime', 'datetimetz', 'timedelta'])\n        for attr in ['mean', 'median']:\n            f = getattr(df.groupby('group'), attr)\n            result = f()\n            tm.assert_index_equal(result.columns, expected_columns_numeric)\n\n            result = f(numeric_only=False)\n            assert_frame_equal(result.reindex_like(expected), expected)\n\n        # TODO: min, max *should* handle\n        # categorical (ordered) dtype\n        expected_columns = Index(['int', 'float', 'string',\n                                  'category_int',\n                                  'datetime', 'datetimetz',\n                                  'timedelta'])\n        for attr in ['min', 'max']:\n            f = getattr(df.groupby('group'), attr)\n            result = f()\n            tm.assert_index_equal(result.columns, expected_columns)\n\n            result = f(numeric_only=False)\n            tm.assert_index_equal(result.columns, expected_columns)\n\n        expected_columns = Index(['int', 'float', 'string',\n                                  'category_string', 'category_int',\n                                  'datetime', 'datetimetz',\n                                  'timedelta'])\n        for attr in ['first', 'last']:\n            f = getattr(df.groupby('group'), attr)\n            result = f()\n            tm.assert_index_equal(result.columns, expected_columns)\n\n            result = f(numeric_only=False)\n            tm.assert_index_equal(result.columns, expected_columns)\n\n        expected_columns = Index(['int', 'float', 'string',\n                                  'category_int', 'timedelta'])\n        for attr in ['sum']:\n            f = getattr(df.groupby('group'), attr)\n            result = f()\n            tm.assert_index_equal(result.columns, expected_columns_numeric)\n\n            result = f(numeric_only=False)\n            tm.assert_index_equal(result.columns, expected_columns)\n\n        expected_columns = Index(['int', 'float', 'category_int'])\n        for attr in ['prod', 'cumprod']:\n            f = getattr(df.groupby('group'), attr)\n            result = f()\n            tm.assert_index_equal(result.columns, expected_columns_numeric)\n\n            result = f(numeric_only=False)\n            tm.assert_index_equal(result.columns, expected_columns)\n\n        # like min, max, but don't include strings\n        expected_columns = Index(['int', 'float',\n                                  'category_int',\n                                  'datetime', 'datetimetz',\n                                  'timedelta'])\n        for attr in ['cummin', 'cummax']:\n            f = getattr(df.groupby('group'), attr)\n            result = f()\n            tm.assert_index_equal(result.columns, expected_columns_numeric)\n\n            result = f(numeric_only=False)\n            tm.assert_index_equal(result.columns, expected_columns)\n\n        expected_columns = Index(['int', 'float', 'category_int',\n                                  'timedelta'])\n        for attr in ['cumsum']:\n            f = getattr(df.groupby('group'), attr)\n            result = f()\n            tm.assert_index_equal(result.columns, expected_columns_numeric)\n\n            result = f(numeric_only=False)\n            tm.assert_index_equal(result.columns, expected_columns)\n\n    def test_groupby_timedelta_cython_count(self):\n        df = DataFrame({'g': list('ab' * 2),\n                        'delt': np.arange(4).astype('timedelta64[ns]')})\n        expected = Series([\n            2, 2\n        ], index=pd.Index(['a', 'b'], name='g'), name='delt')\n        result = df.groupby('g').delt.count()\n        tm.assert_series_equal(expected, result)\n\n    def test_wrap_aggregated_output_multindex(self):\n        df = self.mframe.T\n        df['baz', 'two'] = 'peekaboo'\n\n        keys = [np.array([0, 0, 1]), np.array([0, 0, 1])]\n        agged = df.groupby(keys).agg(np.mean)\n        tm.assertIsInstance(agged.columns, MultiIndex)\n\n        def aggfun(ser):\n            if ser.name == ('foo', 'one'):\n                raise TypeError\n            else:\n                return ser.sum()\n\n        agged2 = df.groupby(keys).aggregate(aggfun)\n        self.assertEqual(len(agged2.columns) + 1, len(df.columns))\n\n    def test_groupby_level(self):\n        frame = self.mframe\n        deleveled = frame.reset_index()\n\n        result0 = frame.groupby(level=0).sum()\n        result1 = frame.groupby(level=1).sum()\n\n        expected0 = frame.groupby(deleveled['first'].values).sum()\n        expected1 = frame.groupby(deleveled['second'].values).sum()\n\n        expected0 = expected0.reindex(frame.index.levels[0])\n        expected1 = expected1.reindex(frame.index.levels[1])\n\n        self.assertEqual(result0.index.name, 'first')\n        self.assertEqual(result1.index.name, 'second')\n\n        assert_frame_equal(result0, expected0)\n        assert_frame_equal(result1, expected1)\n        self.assertEqual(result0.index.name, frame.index.names[0])\n        self.assertEqual(result1.index.name, frame.index.names[1])\n\n        # groupby level name\n        result0 = frame.groupby(level='first').sum()\n        result1 = frame.groupby(level='second').sum()\n        assert_frame_equal(result0, expected0)\n        assert_frame_equal(result1, expected1)\n\n        # axis=1\n\n        result0 = frame.T.groupby(level=0, axis=1).sum()\n        result1 = frame.T.groupby(level=1, axis=1).sum()\n        assert_frame_equal(result0, expected0.T)\n        assert_frame_equal(result1, expected1.T)\n\n        # raise exception for non-MultiIndex\n        self.assertRaises(ValueError, self.df.groupby, level=1)\n\n    def test_groupby_level_index_names(self):\n        # GH4014 this used to raise ValueError since 'exp'>1 (in py2)\n        df = DataFrame({'exp': ['A'] * 3 + ['B'] * 3,\n                        'var1': lrange(6), }).set_index('exp')\n        df.groupby(level='exp')\n        self.assertRaises(ValueError, df.groupby, level='foo')\n\n    def test_groupby_level_with_nas(self):\n        index = MultiIndex(levels=[[1, 0], [0, 1, 2, 3]],\n                           labels=[[1, 1, 1, 1, 0, 0, 0, 0], [0, 1, 2, 3, 0, 1,\n                                                              2, 3]])\n\n        # factorizing doesn't confuse things\n        s = Series(np.arange(8.), index=index)\n        result = s.groupby(level=0).sum()\n        expected = Series([22., 6.], index=[1, 0])\n        assert_series_equal(result, expected)\n\n        index = MultiIndex(levels=[[1, 0], [0, 1, 2, 3]],\n                           labels=[[1, 1, 1, 1, -1, 0, 0, 0], [0, 1, 2, 3, 0,\n                                                               1, 2, 3]])\n\n        # factorizing doesn't confuse things\n        s = Series(np.arange(8.), index=index)\n        result = s.groupby(level=0).sum()\n        expected = Series([18., 6.], index=[1, 0])\n        assert_series_equal(result, expected)\n\n    def test_groupby_level_apply(self):\n        frame = self.mframe\n\n        result = frame.groupby(level=0).count()\n        self.assertEqual(result.index.name, 'first')\n        result = frame.groupby(level=1).count()\n        self.assertEqual(result.index.name, 'second')\n\n        result = frame['A'].groupby(level=0).count()\n        self.assertEqual(result.index.name, 'first')\n\n    def test_groupby_args(self):\n        # PR8618 and issue 8015\n        frame = self.mframe\n\n        def j():\n            frame.groupby()\n\n        self.assertRaisesRegexp(TypeError,\n                                \"You have to supply one of 'by' and 'level'\",\n                                j)\n\n        def k():\n            frame.groupby(by=None, level=None)\n\n        self.assertRaisesRegexp(TypeError,\n                                \"You have to supply one of 'by' and 'level'\",\n                                k)\n\n    def test_groupby_level_mapper(self):\n        frame = self.mframe\n        deleveled = frame.reset_index()\n\n        mapper0 = {'foo': 0, 'bar': 0, 'baz': 1, 'qux': 1}\n        mapper1 = {'one': 0, 'two': 0, 'three': 1}\n\n        result0 = frame.groupby(mapper0, level=0).sum()\n        result1 = frame.groupby(mapper1, level=1).sum()\n\n        mapped_level0 = np.array([mapper0.get(x) for x in deleveled['first']])\n        mapped_level1 = np.array([mapper1.get(x) for x in deleveled['second']])\n        expected0 = frame.groupby(mapped_level0).sum()\n        expected1 = frame.groupby(mapped_level1).sum()\n        expected0.index.name, expected1.index.name = 'first', 'second'\n\n        assert_frame_equal(result0, expected0)\n        assert_frame_equal(result1, expected1)\n\n    def test_groupby_level_nonmulti(self):\n        # GH 1313, GH 13901\n        s = Series([1, 2, 3, 10, 4, 5, 20, 6],\n                   Index([1, 2, 3, 1, 4, 5, 2, 6], name='foo'))\n        expected = Series([11, 22, 3, 4, 5, 6],\n                          Index(range(1, 7), name='foo'))\n\n        result = s.groupby(level=0).sum()\n        self.assert_series_equal(result, expected)\n        result = s.groupby(level=[0]).sum()\n        self.assert_series_equal(result, expected)\n        result = s.groupby(level=-1).sum()\n        self.assert_series_equal(result, expected)\n        result = s.groupby(level=[-1]).sum()\n        self.assert_series_equal(result, expected)\n\n        tm.assertRaises(ValueError, s.groupby, level=1)\n        tm.assertRaises(ValueError, s.groupby, level=-2)\n        tm.assertRaises(ValueError, s.groupby, level=[])\n        tm.assertRaises(ValueError, s.groupby, level=[0, 0])\n        tm.assertRaises(ValueError, s.groupby, level=[0, 1])\n        tm.assertRaises(ValueError, s.groupby, level=[1])\n\n    def test_groupby_complex(self):\n        # GH 12902\n        a = Series(data=np.arange(4) * (1 + 2j), index=[0, 0, 1, 1])\n        expected = Series((1 + 2j, 5 + 10j))\n\n        result = a.groupby(level=0).sum()\n        assert_series_equal(result, expected)\n\n        result = a.sum(level=0)\n        assert_series_equal(result, expected)\n\n    def test_level_preserve_order(self):\n        grouped = self.mframe.groupby(level=0)\n        exp_labels = np.array([0, 0, 0, 1, 1, 2, 2, 3, 3, 3], np.intp)\n        assert_almost_equal(grouped.grouper.labels[0], exp_labels)\n\n    def test_grouping_labels(self):\n        grouped = self.mframe.groupby(self.mframe.index.get_level_values(0))\n        exp_labels = np.array([2, 2, 2, 0, 0, 1, 1, 3, 3, 3], dtype=np.intp)\n        assert_almost_equal(grouped.grouper.labels[0], exp_labels)\n\n    def test_apply_series_to_frame(self):\n        def f(piece):\n            with np.errstate(invalid='ignore'):\n                logged = np.log(piece)\n            return DataFrame({'value': piece,\n                              'demeaned': piece - piece.mean(),\n                              'logged': logged})\n\n        dr = bdate_range('1/1/2000', periods=100)\n        ts = Series(np.random.randn(100), index=dr)\n\n        grouped = ts.groupby(lambda x: x.month)\n        result = grouped.apply(f)\n\n        tm.assertIsInstance(result, DataFrame)\n        self.assert_index_equal(result.index, ts.index)\n\n    def test_apply_series_yield_constant(self):\n        result = self.df.groupby(['A', 'B'])['C'].apply(len)\n        self.assertEqual(result.index.names[:2], ('A', 'B'))\n\n    def test_apply_frame_yield_constant(self):\n        # GH13568\n        result = self.df.groupby(['A', 'B']).apply(len)\n        self.assertTrue(isinstance(result, Series))\n        self.assertIsNone(result.name)\n\n        result = self.df.groupby(['A', 'B'])[['C', 'D']].apply(len)\n        self.assertTrue(isinstance(result, Series))\n        self.assertIsNone(result.name)\n\n    def test_apply_frame_to_series(self):\n        grouped = self.df.groupby(['A', 'B'])\n        result = grouped.apply(len)\n        expected = grouped.count()['C']\n        self.assert_index_equal(result.index, expected.index)\n        self.assert_numpy_array_equal(result.values, expected.values)\n\n    def test_apply_frame_concat_series(self):\n        def trans(group):\n            return group.groupby('B')['C'].sum().sort_values()[:2]\n\n        def trans2(group):\n            grouped = group.groupby(df.reindex(group.index)['B'])\n            return grouped.sum().sort_values()[:2]\n\n        df = DataFrame({'A': np.random.randint(0, 5, 1000),\n                        'B': np.random.randint(0, 5, 1000),\n                        'C': np.random.randn(1000)})\n\n        result = df.groupby('A').apply(trans)\n        exp = df.groupby('A')['C'].apply(trans2)\n        assert_series_equal(result, exp, check_names=False)\n        self.assertEqual(result.name, 'C')\n\n    def test_apply_transform(self):\n        grouped = self.ts.groupby(lambda x: x.month)\n        result = grouped.apply(lambda x: x * 2)\n        expected = grouped.transform(lambda x: x * 2)\n        assert_series_equal(result, expected)\n\n    def test_apply_multikey_corner(self):\n        grouped = self.tsframe.groupby([lambda x: x.year, lambda x: x.month])\n\n        def f(group):\n            return group.sort_values('A')[-5:]\n\n        result = grouped.apply(f)\n        for key, group in grouped:\n            assert_frame_equal(result.loc[key], f(group))\n\n    def test_mutate_groups(self):\n\n        # GH3380\n\n        mydf = DataFrame({\n            'cat1': ['a'] * 8 + ['b'] * 6,\n            'cat2': ['c'] * 2 + ['d'] * 2 + ['e'] * 2 + ['f'] * 2 + ['c'] * 2 +\n            ['d'] * 2 + ['e'] * 2,\n            'cat3': lmap(lambda x: 'g%s' % x, lrange(1, 15)),\n            'val': np.random.randint(100, size=14),\n        })\n\n        def f_copy(x):\n            x = x.copy()\n            x['rank'] = x.val.rank(method='min')\n            return x.groupby('cat2')['rank'].min()\n\n        def f_no_copy(x):\n            x['rank'] = x.val.rank(method='min')\n            return x.groupby('cat2')['rank'].min()\n\n        grpby_copy = mydf.groupby('cat1').apply(f_copy)\n        grpby_no_copy = mydf.groupby('cat1').apply(f_no_copy)\n        assert_series_equal(grpby_copy, grpby_no_copy)\n\n    def test_no_mutate_but_looks_like(self):\n\n        # GH 8467\n        # first show's mutation indicator\n        # second does not, but should yield the same results\n        df = DataFrame({'key': [1, 1, 1, 2, 2, 2, 3, 3, 3], 'value': range(9)})\n\n        result1 = df.groupby('key', group_keys=True).apply(lambda x: x[:].key)\n        result2 = df.groupby('key', group_keys=True).apply(lambda x: x.key)\n        assert_series_equal(result1, result2)\n\n    def test_apply_chunk_view(self):\n        # Low level tinkering could be unsafe, make sure not\n        df = DataFrame({'key': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n                        'value': lrange(9)})\n\n        # return view\n        f = lambda x: x[:2]\n\n        result = df.groupby('key', group_keys=False).apply(f)\n        expected = df.take([0, 1, 3, 4, 6, 7])\n        assert_frame_equal(result, expected)\n\n    def test_apply_no_name_column_conflict(self):\n        df = DataFrame({'name': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2],\n                        'name2': [0, 0, 0, 1, 1, 1, 0, 0, 1, 1],\n                        'value': lrange(10)[::-1]})\n\n        # it works! #2605\n        grouped = df.groupby(['name', 'name2'])\n        grouped.apply(lambda x: x.sort_values('value', inplace=True))\n\n    def test_groupby_series_indexed_differently(self):\n        s1 = Series([5.0, -9.0, 4.0, 100., -5., 55., 6.7],\n                    index=Index(['a', 'b', 'c', 'd', 'e', 'f', 'g']))\n        s2 = Series([1.0, 1.0, 4.0, 5.0, 5.0, 7.0],\n                    index=Index(['a', 'b', 'd', 'f', 'g', 'h']))\n\n        grouped = s1.groupby(s2)\n        agged = grouped.mean()\n        exp = s1.groupby(s2.reindex(s1.index).get).mean()\n        assert_series_equal(agged, exp)\n\n    def test_groupby_with_hier_columns(self):\n        tuples = list(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux',\n                             'qux'], ['one', 'two', 'one', 'two', 'one', 'two',\n                                      'one', 'two']]))\n        index = MultiIndex.from_tuples(tuples)\n        columns = MultiIndex.from_tuples([('A', 'cat'), ('B', 'dog'), (\n            'B', 'cat'), ('A', 'dog')])\n        df = DataFrame(np.random.randn(8, 4), index=index, columns=columns)\n\n        result = df.groupby(level=0).mean()\n        self.assert_index_equal(result.columns, columns)\n\n        result = df.groupby(level=0, axis=1).mean()\n        self.assert_index_equal(result.index, df.index)\n\n        result = df.groupby(level=0).agg(np.mean)\n        self.assert_index_equal(result.columns, columns)\n\n        result = df.groupby(level=0).apply(lambda x: x.mean())\n        self.assert_index_equal(result.columns, columns)\n\n        result = df.groupby(level=0, axis=1).agg(lambda x: x.mean(1))\n        self.assert_index_equal(result.columns, Index(['A', 'B']))\n        self.assert_index_equal(result.index, df.index)\n\n        # add a nuisance column\n        sorted_columns, _ = columns.sortlevel(0)\n        df['A', 'foo'] = 'bar'\n        result = df.groupby(level=0).mean()\n        self.assert_index_equal(result.columns, df.columns[:-1])\n\n    def test_pass_args_kwargs(self):\n        from numpy import percentile\n\n        def f(x, q=None, axis=0):\n            return percentile(x, q, axis=axis)\n\n        g = lambda x: percentile(x, 80, axis=0)\n\n        # Series\n        ts_grouped = self.ts.groupby(lambda x: x.month)\n        agg_result = ts_grouped.agg(percentile, 80, axis=0)\n        apply_result = ts_grouped.apply(percentile, 80, axis=0)\n        trans_result = ts_grouped.transform(percentile, 80, axis=0)\n\n        agg_expected = ts_grouped.quantile(.8)\n        trans_expected = ts_grouped.transform(g)\n\n        assert_series_equal(apply_result, agg_expected)\n        assert_series_equal(agg_result, agg_expected, check_names=False)\n        assert_series_equal(trans_result, trans_expected)\n\n        agg_result = ts_grouped.agg(f, q=80)\n        apply_result = ts_grouped.apply(f, q=80)\n        trans_result = ts_grouped.transform(f, q=80)\n        assert_series_equal(agg_result, agg_expected)\n        assert_series_equal(apply_result, agg_expected)\n        assert_series_equal(trans_result, trans_expected)\n\n        # DataFrame\n        df_grouped = self.tsframe.groupby(lambda x: x.month)\n        agg_result = df_grouped.agg(percentile, 80, axis=0)\n        apply_result = df_grouped.apply(DataFrame.quantile, .8)\n        expected = df_grouped.quantile(.8)\n        assert_frame_equal(apply_result, expected)\n        assert_frame_equal(agg_result, expected, check_names=False)\n\n        agg_result = df_grouped.agg(f, q=80)\n        apply_result = df_grouped.apply(DataFrame.quantile, q=.8)\n        assert_frame_equal(agg_result, expected, check_names=False)\n        assert_frame_equal(apply_result, expected)\n\n    def test_size(self):\n        grouped = self.df.groupby(['A', 'B'])\n        result = grouped.size()\n        for key, group in grouped:\n            self.assertEqual(result[key], len(group))\n\n        grouped = self.df.groupby('A')\n        result = grouped.size()\n        for key, group in grouped:\n            self.assertEqual(result[key], len(group))\n\n        grouped = self.df.groupby('B')\n        result = grouped.size()\n        for key, group in grouped:\n            self.assertEqual(result[key], len(group))\n\n        df = DataFrame(np.random.choice(20, (1000, 3)), columns=list('abc'))\n        for sort, key in cart_product((False, True), ('a', 'b', ['a', 'b'])):\n            left = df.groupby(key, sort=sort).size()\n            right = df.groupby(key, sort=sort)['c'].apply(lambda a: a.shape[0])\n            assert_series_equal(left, right, check_names=False)\n\n        # GH11699\n        df = DataFrame([], columns=['A', 'B'])\n        out = Series([], dtype='int64', index=Index([], name='A'))\n        assert_series_equal(df.groupby('A').size(), out)\n\n    def test_count(self):\n        from string import ascii_lowercase\n        n = 1 << 15\n        dr = date_range('2015-08-30', periods=n // 10, freq='T')\n\n        df = DataFrame({\n            '1st': np.random.choice(\n                list(ascii_lowercase), n),\n            '2nd': np.random.randint(0, 5, n),\n            '3rd': np.random.randn(n).round(3),\n            '4th': np.random.randint(-10, 10, n),\n            '5th': np.random.choice(dr, n),\n            '6th': np.random.randn(n).round(3),\n            '7th': np.random.randn(n).round(3),\n            '8th': np.random.choice(dr, n) - np.random.choice(dr, 1),\n            '9th': np.random.choice(\n                list(ascii_lowercase), n)\n        })\n\n        for col in df.columns.drop(['1st', '2nd', '4th']):\n            df.loc[np.random.choice(n, n // 10), col] = np.nan\n\n        df['9th'] = df['9th'].astype('category')\n\n        for key in '1st', '2nd', ['1st', '2nd']:\n            left = df.groupby(key).count()\n            right = df.groupby(key).apply(DataFrame.count).drop(key, axis=1)\n            assert_frame_equal(left, right)\n\n        # GH5610\n        # count counts non-nulls\n        df = pd.DataFrame([[1, 2, 'foo'], [1, nan, 'bar'], [3, nan, nan]],\n                          columns=['A', 'B', 'C'])\n\n        count_as = df.groupby('A').count()\n        count_not_as = df.groupby('A', as_index=False).count()\n\n        expected = DataFrame([[1, 2], [0, 0]], columns=['B', 'C'],\n                             index=[1, 3])\n        expected.index.name = 'A'\n        assert_frame_equal(count_not_as, expected.reset_index())\n        assert_frame_equal(count_as, expected)\n\n        count_B = df.groupby('A')['B'].count()\n        assert_series_equal(count_B, expected['B'])\n\n    def test_count_object(self):\n        df = pd.DataFrame({'a': ['a'] * 3 + ['b'] * 3, 'c': [2] * 3 + [3] * 3})\n        result = df.groupby('c').a.count()\n        expected = pd.Series([\n            3, 3\n        ], index=pd.Index([2, 3], name='c'), name='a')\n        tm.assert_series_equal(result, expected)\n\n        df = pd.DataFrame({'a': ['a', np.nan, np.nan] + ['b'] * 3,\n                           'c': [2] * 3 + [3] * 3})\n        result = df.groupby('c').a.count()\n        expected = pd.Series([\n            1, 3\n        ], index=pd.Index([2, 3], name='c'), name='a')\n        tm.assert_series_equal(result, expected)\n\n    def test_count_cross_type(self):  # GH8169\n        vals = np.hstack((np.random.randint(0, 5, (100, 2)), np.random.randint(\n            0, 2, (100, 2))))\n\n        df = pd.DataFrame(vals, columns=['a', 'b', 'c', 'd'])\n        df[df == 2] = np.nan\n        expected = df.groupby(['c', 'd']).count()\n\n        for t in ['float32', 'object']:\n            df['a'] = df['a'].astype(t)\n            df['b'] = df['b'].astype(t)\n            result = df.groupby(['c', 'd']).count()\n            tm.assert_frame_equal(result, expected)\n\n    def test_nunique(self):\n        df = DataFrame({\n            'A': list('abbacc'),\n            'B': list('abxacc'),\n            'C': list('abbacx'),\n        })\n\n        expected = DataFrame({'A': [1] * 3, 'B': [1, 2, 1], 'C': [1, 1, 2]})\n        result = df.groupby('A', as_index=False).nunique()\n        tm.assert_frame_equal(result, expected)\n\n        # as_index\n        expected.index = list('abc')\n        expected.index.name = 'A'\n        result = df.groupby('A').nunique()\n        tm.assert_frame_equal(result, expected)\n\n        # with na\n        result = df.replace({'x': None}).groupby('A').nunique(dropna=False)\n        tm.assert_frame_equal(result, expected)\n\n        # dropna\n        expected = DataFrame({'A': [1] * 3, 'B': [1] * 3, 'C': [1] * 3},\n                             index=list('abc'))\n        expected.index.name = 'A'\n        result = df.replace({'x': None}).groupby('A').nunique()\n        tm.assert_frame_equal(result, expected)\n\n    def test_non_cython_api(self):\n\n        # GH5610\n        # non-cython calls should not include the grouper\n\n        df = DataFrame(\n            [[1, 2, 'foo'], [1,\n                             nan,\n                             'bar', ], [3, nan, 'baz']\n             ], columns=['A', 'B', 'C'])\n        g = df.groupby('A')\n        gni = df.groupby('A', as_index=False)\n\n        # mad\n        expected = DataFrame([[0], [nan]], columns=['B'], index=[1, 3])\n        expected.index.name = 'A'\n        result = g.mad()\n        assert_frame_equal(result, expected)\n\n        expected = DataFrame([[0., 0.], [0, nan]], columns=['A', 'B'],\n                             index=[0, 1])\n        result = gni.mad()\n        assert_frame_equal(result, expected)\n\n        # describe\n        expected_index = pd.Index([1, 3], name='A')\n        expected_col = pd.MultiIndex(levels=[['B'],\n                                             ['count', 'mean', 'std', 'min',\n                                              '25%', '50%', '75%', 'max']],\n                                     labels=[[0] * 8, list(range(8))])\n        expected = pd.DataFrame([[1.0, 2.0, nan, 2.0, 2.0, 2.0, 2.0, 2.0],\n                                 [0.0, nan, nan, nan, nan, nan, nan, nan]],\n                                index=expected_index,\n                                columns=expected_col)\n        result = g.describe()\n        assert_frame_equal(result, expected)\n\n        expected = pd.concat([df[df.A == 1].describe().unstack().to_frame().T,\n                              df[df.A == 3].describe().unstack().to_frame().T])\n        expected.index = pd.Index([0, 1])\n        result = gni.describe()\n        assert_frame_equal(result, expected)\n\n        # any\n        expected = DataFrame([[True, True], [False, True]], columns=['B', 'C'],\n                             index=[1, 3])\n        expected.index.name = 'A'\n        result = g.any()\n        assert_frame_equal(result, expected)\n\n        # idxmax\n        expected = DataFrame([[0], [nan]], columns=['B'], index=[1, 3])\n        expected.index.name = 'A'\n        result = g.idxmax()\n        assert_frame_equal(result, expected)\n\n    def test_cython_api2(self):\n\n        # this takes the fast apply path\n\n        # cumsum (GH5614)\n        df = DataFrame(\n            [[1, 2, np.nan], [1, np.nan, 9], [3, 4, 9]\n             ], columns=['A', 'B', 'C'])\n        expected = DataFrame(\n            [[2, np.nan], [np.nan, 9], [4, 9]], columns=['B', 'C'])\n        result = df.groupby('A').cumsum()\n        assert_frame_equal(result, expected)\n\n        # GH 5755 - cumsum is a transformer and should ignore as_index\n        result = df.groupby('A', as_index=False).cumsum()\n        assert_frame_equal(result, expected)\n\n        # GH 13994\n        result = df.groupby('A').cumsum(axis=1)\n        expected = df.cumsum(axis=1)\n        assert_frame_equal(result, expected)\n        result = df.groupby('A').cumprod(axis=1)\n        expected = df.cumprod(axis=1)\n        assert_frame_equal(result, expected)\n\n    def test_grouping_ndarray(self):\n        grouped = self.df.groupby(self.df['A'].values)\n\n        result = grouped.sum()\n        expected = self.df.groupby('A').sum()\n        assert_frame_equal(result, expected, check_names=False\n                           )  # Note: no names when grouping by value\n\n    def test_apply_typecast_fail(self):\n        df = DataFrame({'d': [1., 1., 1., 2., 2., 2.],\n                        'c': np.tile(\n                            ['a', 'b', 'c'], 2),\n                        'v': np.arange(1., 7.)})\n\n        def f(group):\n            v = group['v']\n            group['v2'] = (v - v.min()) / (v.max() - v.min())\n            return group\n\n        result = df.groupby('d').apply(f)\n\n        expected = df.copy()\n        expected['v2'] = np.tile([0., 0.5, 1], 2)\n\n        assert_frame_equal(result, expected)\n\n    def test_apply_multiindex_fail(self):\n        index = MultiIndex.from_arrays([[0, 0, 0, 1, 1, 1], [1, 2, 3, 1, 2, 3]\n                                        ])\n        df = DataFrame({'d': [1., 1., 1., 2., 2., 2.],\n                        'c': np.tile(['a', 'b', 'c'], 2),\n                        'v': np.arange(1., 7.)}, index=index)\n\n        def f(group):\n            v = group['v']\n            group['v2'] = (v - v.min()) / (v.max() - v.min())\n            return group\n\n        result = df.groupby('d').apply(f)\n\n        expected = df.copy()\n        expected['v2'] = np.tile([0., 0.5, 1], 2)\n\n        assert_frame_equal(result, expected)\n\n    def test_apply_corner(self):\n        result = self.tsframe.groupby(lambda x: x.year).apply(lambda x: x * 2)\n        expected = self.tsframe * 2\n        assert_frame_equal(result, expected)\n\n    def test_apply_without_copy(self):\n        # GH 5545\n        # returning a non-copy in an applied function fails\n\n        data = DataFrame({'id_field': [100, 100, 200, 300],\n                          'category': ['a', 'b', 'c', 'c'],\n                          'value': [1, 2, 3, 4]})\n\n        def filt1(x):\n            if x.shape[0] == 1:\n                return x.copy()\n            else:\n                return x[x.category == 'c']\n\n        def filt2(x):\n            if x.shape[0] == 1:\n                return x\n            else:\n                return x[x.category == 'c']\n\n        expected = data.groupby('id_field').apply(filt1)\n        result = data.groupby('id_field').apply(filt2)\n        assert_frame_equal(result, expected)\n\n    def test_apply_corner_cases(self):\n        # #535, can't use sliding iterator\n\n        N = 1000\n        labels = np.random.randint(0, 100, size=N)\n        df = DataFrame({'key': labels,\n                        'value1': np.random.randn(N),\n                        'value2': ['foo', 'bar', 'baz', 'qux'] * (N // 4)})\n\n        grouped = df.groupby('key')\n\n        def f(g):\n            g['value3'] = g['value1'] * 2\n            return g\n\n        result = grouped.apply(f)\n        self.assertTrue('value3' in result)\n\n    def test_groupby_wrong_multi_labels(self):\n        from pandas import read_csv\n        data = \"\"\"index,foo,bar,baz,spam,data\n0,foo1,bar1,baz1,spam2,20\n1,foo1,bar2,baz1,spam3,30\n2,foo2,bar2,baz1,spam2,40\n3,foo1,bar1,baz2,spam1,50\n4,foo3,bar1,baz2,spam1,60\"\"\"\n\n        data = read_csv(StringIO(data), index_col=0)\n\n        grouped = data.groupby(['foo', 'bar', 'baz', 'spam'])\n\n        result = grouped.agg(np.mean)\n        expected = grouped.mean()\n        assert_frame_equal(result, expected)\n\n    def test_groupby_series_with_name(self):\n        result = self.df.groupby(self.df['A']).mean()\n        result2 = self.df.groupby(self.df['A'], as_index=False).mean()\n        self.assertEqual(result.index.name, 'A')\n        self.assertIn('A', result2)\n\n        result = self.df.groupby([self.df['A'], self.df['B']]).mean()\n        result2 = self.df.groupby([self.df['A'], self.df['B']],\n                                  as_index=False).mean()\n        self.assertEqual(result.index.names, ('A', 'B'))\n        self.assertIn('A', result2)\n        self.assertIn('B', result2)\n\n    def test_seriesgroupby_name_attr(self):\n        # GH 6265\n        result = self.df.groupby('A')['C']\n        self.assertEqual(result.count().name, 'C')\n        self.assertEqual(result.mean().name, 'C')\n\n        testFunc = lambda x: np.sum(x) * 2\n        self.assertEqual(result.agg(testFunc).name, 'C')\n\n    def test_consistency_name(self):\n        # GH 12363\n\n        df = DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n                              'foo', 'bar', 'foo', 'foo'],\n                        'B': ['one', 'one', 'two', 'two',\n                              'two', 'two', 'one', 'two'],\n                        'C': np.random.randn(8) + 1.0,\n                        'D': np.arange(8)})\n\n        expected = df.groupby(['A']).B.count()\n        result = df.B.groupby(df.A).count()\n        assert_series_equal(result, expected)\n\n    def test_groupby_name_propagation(self):\n        # GH 6124\n        def summarize(df, name=None):\n            return Series({'count': 1, 'mean': 2, 'omissions': 3, }, name=name)\n\n        def summarize_random_name(df):\n            # Provide a different name for each Series.  In this case, groupby\n            # should not attempt to propagate the Series name since they are\n            # inconsistent.\n            return Series({\n                'count': 1,\n                'mean': 2,\n                'omissions': 3,\n            }, name=df.iloc[0]['A'])\n\n        metrics = self.df.groupby('A').apply(summarize)\n        self.assertEqual(metrics.columns.name, None)\n        metrics = self.df.groupby('A').apply(summarize, 'metrics')\n        self.assertEqual(metrics.columns.name, 'metrics')\n        metrics = self.df.groupby('A').apply(summarize_random_name)\n        self.assertEqual(metrics.columns.name, None)\n\n    def test_groupby_nonstring_columns(self):\n        df = DataFrame([np.arange(10) for x in range(10)])\n        grouped = df.groupby(0)\n        result = grouped.mean()\n        expected = df.groupby(df[0]).mean()\n        assert_frame_equal(result, expected)\n\n    def test_groupby_mixed_type_columns(self):\n        # GH 13432, unorderable types in py3\n        df = DataFrame([[0, 1, 2]], columns=['A', 'B', 0])\n        expected = DataFrame([[1, 2]], columns=['B', 0],\n                             index=Index([0], name='A'))\n\n        result = df.groupby('A').first()\n        tm.assert_frame_equal(result, expected)\n\n        result = df.groupby('A').sum()\n        tm.assert_frame_equal(result, expected)\n\n    def test_cython_grouper_series_bug_noncontig(self):\n        arr = np.empty((100, 100))\n        arr.fill(np.nan)\n        obj = Series(arr[:, 0], index=lrange(100))\n        inds = np.tile(lrange(10), 10)\n\n        result = obj.groupby(inds).agg(Series.median)\n        self.assertTrue(result.isnull().all())\n\n    def test_series_grouper_noncontig_index(self):\n        index = Index(tm.rands_array(10, 100))\n\n        values = Series(np.random.randn(50), index=index[::2])\n        labels = np.random.randint(0, 5, 50)\n\n        # it works!\n        grouped = values.groupby(labels)\n\n        # accessing the index elements causes segfault\n        f = lambda x: len(set(map(id, x.index)))\n        grouped.agg(f)\n\n    def test_convert_objects_leave_decimal_alone(self):\n\n        from decimal import Decimal\n\n        s = Series(lrange(5))\n        labels = np.array(['a', 'b', 'c', 'd', 'e'], dtype='O')\n\n        def convert_fast(x):\n            return Decimal(str(x.mean()))\n\n        def convert_force_pure(x):\n            # base will be length 0\n            assert (len(x.base) > 0)\n            return Decimal(str(x.mean()))\n\n        grouped = s.groupby(labels)\n\n        result = grouped.agg(convert_fast)\n        self.assertEqual(result.dtype, np.object_)\n        tm.assertIsInstance(result[0], Decimal)\n\n        result = grouped.agg(convert_force_pure)\n        self.assertEqual(result.dtype, np.object_)\n        tm.assertIsInstance(result[0], Decimal)\n\n    def test_fast_apply(self):\n        # make sure that fast apply is correctly called\n        # rather than raising any kind of error\n        # otherwise the python path will be callsed\n        # which slows things down\n        N = 1000\n        labels = np.random.randint(0, 2000, size=N)\n        labels2 = np.random.randint(0, 3, size=N)\n        df = DataFrame({'key': labels,\n                        'key2': labels2,\n                        'value1': np.random.randn(N),\n                        'value2': ['foo', 'bar', 'baz', 'qux'] * (N // 4)})\n\n        def f(g):\n            return 1\n\n        g = df.groupby(['key', 'key2'])\n\n        grouper = g.grouper\n\n        splitter = grouper._get_splitter(g._selected_obj, axis=g.axis)\n        group_keys = grouper._get_group_keys()\n\n        values, mutated = splitter.fast_apply(f, group_keys)\n        self.assertFalse(mutated)\n\n    def test_apply_with_mixed_dtype(self):\n        # GH3480, apply with mixed dtype on axis=1 breaks in 0.11\n        df = DataFrame({'foo1': ['one', 'two', 'two', 'three', 'one', 'two'],\n                        'foo2': np.random.randn(6)})\n        result = df.apply(lambda x: x, axis=1)\n        assert_series_equal(df.get_dtype_counts(), result.get_dtype_counts())\n\n        # GH 3610 incorrect dtype conversion with as_index=False\n        df = DataFrame({\"c1\": [1, 2, 6, 6, 8]})\n        df[\"c2\"] = df.c1 / 2.0\n        result1 = df.groupby(\"c2\").mean().reset_index().c2\n        result2 = df.groupby(\"c2\", as_index=False).mean().c2\n        assert_series_equal(result1, result2)\n\n    def test_groupby_aggregation_mixed_dtype(self):\n\n        # GH 6212\n        expected = DataFrame({\n            'v1': [5, 5, 7, np.nan, 3, 3, 4, 1],\n            'v2': [55, 55, 77, np.nan, 33, 33, 44, 11]},\n            index=MultiIndex.from_tuples([(1, 95), (1, 99), (2, 95), (2, 99),\n                                          ('big', 'damp'),\n                                          ('blue', 'dry'),\n                                          ('red', 'red'), ('red', 'wet')],\n                                         names=['by1', 'by2']))\n\n        df = DataFrame({\n            'v1': [1, 3, 5, 7, 8, 3, 5, np.nan, 4, 5, 7, 9],\n            'v2': [11, 33, 55, 77, 88, 33, 55, np.nan, 44, 55, 77, 99],\n            'by1': [\"red\", \"blue\", 1, 2, np.nan, \"big\", 1, 2, \"red\", 1, np.nan,\n                    12],\n            'by2': [\"wet\", \"dry\", 99, 95, np.nan, \"damp\", 95, 99, \"red\", 99,\n                    np.nan, np.nan]\n        })\n\n        g = df.groupby(['by1', 'by2'])\n        result = g[['v1', 'v2']].mean()\n        assert_frame_equal(result, expected)\n\n    def test_groupby_dtype_inference_empty(self):\n        # GH 6733\n        df = DataFrame({'x': [], 'range': np.arange(0, dtype='int64')})\n        self.assertEqual(df['x'].dtype, np.float64)\n\n        result = df.groupby('x').first()\n        exp_index = Index([], name='x', dtype=np.float64)\n        expected = DataFrame({'range': Series(\n            [], index=exp_index, dtype='int64')})\n        assert_frame_equal(result, expected, by_blocks=True)\n\n    def test_groupby_list_infer_array_like(self):\n        result = self.df.groupby(list(self.df['A'])).mean()\n        expected = self.df.groupby(self.df['A']).mean()\n        assert_frame_equal(result, expected, check_names=False)\n\n        self.assertRaises(Exception, self.df.groupby, list(self.df['A'][:-1]))\n\n        # pathological case of ambiguity\n        df = DataFrame({'foo': [0, 1],\n                        'bar': [3, 4],\n                        'val': np.random.randn(2)})\n\n        result = df.groupby(['foo', 'bar']).mean()\n        expected = df.groupby([df['foo'], df['bar']]).mean()[['val']]\n\n    def test_groupby_keys_same_size_as_index(self):\n        # GH 11185\n        freq = 's'\n        index = pd.date_range(start=pd.Timestamp('2015-09-29T11:34:44-0700'),\n                              periods=2, freq=freq)\n        df = pd.DataFrame([['A', 10], ['B', 15]], columns=[\n            'metric', 'values'\n        ], index=index)\n        result = df.groupby([pd.Grouper(level=0, freq=freq), 'metric']).mean()\n        expected = df.set_index([df.index, 'metric'])\n\n        assert_frame_equal(result, expected)\n\n    def test_groupby_one_row(self):\n        # GH 11741\n        df1 = pd.DataFrame(np.random.randn(1, 4), columns=list('ABCD'))\n        self.assertRaises(KeyError, df1.groupby, 'Z')\n        df2 = pd.DataFrame(np.random.randn(2, 4), columns=list('ABCD'))\n        self.assertRaises(KeyError, df2.groupby, 'Z')\n\n    def test_groupby_nat_exclude(self):\n        # GH 6992\n        df = pd.DataFrame(\n            {'values': np.random.randn(8),\n             'dt': [np.nan, pd.Timestamp('2013-01-01'), np.nan, pd.Timestamp(\n                 '2013-02-01'), np.nan, pd.Timestamp('2013-02-01'), np.nan,\n                pd.Timestamp('2013-01-01')],\n             'str': [np.nan, 'a', np.nan, 'a', np.nan, 'a', np.nan, 'b']})\n        grouped = df.groupby('dt')\n\n        expected = [pd.Index([1, 7]), pd.Index([3, 5])]\n        keys = sorted(grouped.groups.keys())\n        self.assertEqual(len(keys), 2)\n        for k, e in zip(keys, expected):\n            # grouped.groups keys are np.datetime64 with system tz\n            # not to be affected by tz, only compare values\n            tm.assert_index_equal(grouped.groups[k], e)\n\n        # confirm obj is not filtered\n        tm.assert_frame_equal(grouped.grouper.groupings[0].obj, df)\n        self.assertEqual(grouped.ngroups, 2)\n\n        expected = {\n            Timestamp('2013-01-01 00:00:00'): np.array([1, 7], dtype=np.int64),\n            Timestamp('2013-02-01 00:00:00'): np.array([3, 5], dtype=np.int64)\n        }\n\n        for k in grouped.indices:\n            self.assert_numpy_array_equal(grouped.indices[k], expected[k])\n\n        tm.assert_frame_equal(\n            grouped.get_group(Timestamp('2013-01-01')), df.iloc[[1, 7]])\n        tm.assert_frame_equal(\n            grouped.get_group(Timestamp('2013-02-01')), df.iloc[[3, 5]])\n\n        self.assertRaises(KeyError, grouped.get_group, pd.NaT)\n\n        nan_df = DataFrame({'nan': [np.nan, np.nan, np.nan],\n                            'nat': [pd.NaT, pd.NaT, pd.NaT]})\n        self.assertEqual(nan_df['nan'].dtype, 'float64')\n        self.assertEqual(nan_df['nat'].dtype, 'datetime64[ns]')\n\n        for key in ['nan', 'nat']:\n            grouped = nan_df.groupby(key)\n            self.assertEqual(grouped.groups, {})\n            self.assertEqual(grouped.ngroups, 0)\n            self.assertEqual(grouped.indices, {})\n            self.assertRaises(KeyError, grouped.get_group, np.nan)\n            self.assertRaises(KeyError, grouped.get_group, pd.NaT)\n\n    def test_dictify(self):\n        dict(iter(self.df.groupby('A')))\n        dict(iter(self.df.groupby(['A', 'B'])))\n        dict(iter(self.df['C'].groupby(self.df['A'])))\n        dict(iter(self.df['C'].groupby([self.df['A'], self.df['B']])))\n        dict(iter(self.df.groupby('A')['C']))\n        dict(iter(self.df.groupby(['A', 'B'])['C']))\n\n    def test_sparse_friendly(self):\n        sdf = self.df[['C', 'D']].to_sparse()\n        panel = tm.makePanel()\n        tm.add_nans(panel)\n\n        def _check_work(gp):\n            gp.mean()\n            gp.agg(np.mean)\n            dict(iter(gp))\n\n        # it works!\n        _check_work(sdf.groupby(lambda x: x // 2))\n        _check_work(sdf['C'].groupby(lambda x: x // 2))\n        _check_work(sdf.groupby(self.df['A']))\n\n        # do this someday\n        # _check_work(panel.groupby(lambda x: x.month, axis=1))\n\n    def test_panel_groupby(self):\n        self.panel = tm.makePanel()\n        tm.add_nans(self.panel)\n        grouped = self.panel.groupby({'ItemA': 0, 'ItemB': 0, 'ItemC': 1},\n                                     axis='items')\n        agged = grouped.mean()\n        agged2 = grouped.agg(lambda x: x.mean('items'))\n\n        tm.assert_panel_equal(agged, agged2)\n\n        self.assert_index_equal(agged.items, Index([0, 1]))\n\n        grouped = self.panel.groupby(lambda x: x.month, axis='major')\n        agged = grouped.mean()\n\n        exp = Index(sorted(list(set(self.panel.major_axis.month))))\n        self.assert_index_equal(agged.major_axis, exp)\n\n        grouped = self.panel.groupby({'A': 0, 'B': 0, 'C': 1, 'D': 1},\n                                     axis='minor')\n        agged = grouped.mean()\n        self.assert_index_equal(agged.minor_axis, Index([0, 1]))\n\n    def test_numpy_groupby(self):\n        from pandas.core.groupby import numpy_groupby\n\n        data = np.random.randn(100, 100)\n        labels = np.random.randint(0, 10, size=100)\n\n        df = DataFrame(data)\n\n        result = df.groupby(labels).sum().values\n        expected = numpy_groupby(data, labels)\n        assert_almost_equal(result, expected)\n\n        result = df.groupby(labels, axis=1).sum().values\n        expected = numpy_groupby(data, labels, axis=1)\n        assert_almost_equal(result, expected)\n\n    def test_groupby_2d_malformed(self):\n        d = DataFrame(index=lrange(2))\n        d['group'] = ['g1', 'g2']\n        d['zeros'] = [0, 0]\n        d['ones'] = [1, 1]\n        d['label'] = ['l1', 'l2']\n        tmp = d.groupby(['group']).mean()\n        res_values = np.array([[0, 1], [0, 1]], dtype=np.int64)\n        self.assert_index_equal(tmp.columns, Index(['zeros', 'ones']))\n        self.assert_numpy_array_equal(tmp.values, res_values)\n\n    def test_int32_overflow(self):\n        B = np.concatenate((np.arange(10000), np.arange(10000), np.arange(5000)\n                            ))\n        A = np.arange(25000)\n        df = DataFrame({'A': A,\n                        'B': B,\n                        'C': A,\n                        'D': B,\n                        'E': np.random.randn(25000)})\n\n        left = df.groupby(['A', 'B', 'C', 'D']).sum()\n        right = df.groupby(['D', 'C', 'B', 'A']).sum()\n        self.assertEqual(len(left), len(right))\n\n    def test_int64_overflow(self):\n        from pandas.core.groupby import _int64_overflow_possible\n\n        B = np.concatenate((np.arange(1000), np.arange(1000), np.arange(500)))\n        A = np.arange(2500)\n        df = DataFrame({'A': A,\n                        'B': B,\n                        'C': A,\n                        'D': B,\n                        'E': A,\n                        'F': B,\n                        'G': A,\n                        'H': B,\n                        'values': np.random.randn(2500)})\n\n        lg = df.groupby(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'])\n        rg = df.groupby(['H', 'G', 'F', 'E', 'D', 'C', 'B', 'A'])\n\n        left = lg.sum()['values']\n        right = rg.sum()['values']\n\n        exp_index, _ = left.index.sortlevel()\n        self.assert_index_equal(left.index, exp_index)\n\n        exp_index, _ = right.index.sortlevel(0)\n        self.assert_index_equal(right.index, exp_index)\n\n        tups = list(map(tuple, df[['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'\n                                   ]].values))\n        tups = com._asarray_tuplesafe(tups)\n\n        expected = df.groupby(tups).sum()['values']\n\n        for k, v in compat.iteritems(expected):\n            self.assertEqual(left[k], right[k[::-1]])\n            self.assertEqual(left[k], v)\n        self.assertEqual(len(left), len(right))\n\n        # GH9096\n        values = range(55109)\n        data = pd.DataFrame.from_dict({'a': values,\n                                       'b': values,\n                                       'c': values,\n                                       'd': values})\n        grouped = data.groupby(['a', 'b', 'c', 'd'])\n        self.assertEqual(len(grouped), len(values))\n\n        arr = np.random.randint(-1 << 12, 1 << 12, (1 << 15, 5))\n        i = np.random.choice(len(arr), len(arr) * 4)\n        arr = np.vstack((arr, arr[i]))  # add sume duplicate rows\n\n        i = np.random.permutation(len(arr))\n        arr = arr[i]  # shuffle rows\n\n        df = DataFrame(arr, columns=list('abcde'))\n        df['jim'], df['joe'] = np.random.randn(2, len(df)) * 10\n        gr = df.groupby(list('abcde'))\n\n        # verify this is testing what it is supposed to test!\n        self.assertTrue(_int64_overflow_possible(gr.grouper.shape))\n\n        # mannually compute groupings\n        jim, joe = defaultdict(list), defaultdict(list)\n        for key, a, b in zip(map(tuple, arr), df['jim'], df['joe']):\n            jim[key].append(a)\n            joe[key].append(b)\n\n        self.assertEqual(len(gr), len(jim))\n        mi = MultiIndex.from_tuples(jim.keys(), names=list('abcde'))\n\n        def aggr(func):\n            f = lambda a: np.fromiter(map(func, a), dtype='f8')\n            arr = np.vstack((f(jim.values()), f(joe.values()))).T\n            res = DataFrame(arr, columns=['jim', 'joe'], index=mi)\n            return res.sort_index()\n\n        assert_frame_equal(gr.mean(), aggr(np.mean))\n        assert_frame_equal(gr.median(), aggr(np.median))\n\n    def test_groupby_sort_multi(self):\n        df = DataFrame({'a': ['foo', 'bar', 'baz'],\n                        'b': [3, 2, 1],\n                        'c': [0, 1, 2],\n                        'd': np.random.randn(3)})\n\n        tups = lmap(tuple, df[['a', 'b', 'c']].values)\n        tups = com._asarray_tuplesafe(tups)\n        result = df.groupby(['a', 'b', 'c'], sort=True).sum()\n        self.assert_numpy_array_equal(result.index.values, tups[[1, 2, 0]])\n\n        tups = lmap(tuple, df[['c', 'a', 'b']].values)\n        tups = com._asarray_tuplesafe(tups)\n        result = df.groupby(['c', 'a', 'b'], sort=True).sum()\n        self.assert_numpy_array_equal(result.index.values, tups)\n\n        tups = lmap(tuple, df[['b', 'c', 'a']].values)\n        tups = com._asarray_tuplesafe(tups)\n        result = df.groupby(['b', 'c', 'a'], sort=True).sum()\n        self.assert_numpy_array_equal(result.index.values, tups[[2, 1, 0]])\n\n        df = DataFrame({'a': [0, 1, 2, 0, 1, 2],\n                        'b': [0, 0, 0, 1, 1, 1],\n                        'd': np.random.randn(6)})\n        grouped = df.groupby(['a', 'b'])['d']\n        result = grouped.sum()\n        _check_groupby(df, result, ['a', 'b'], 'd')\n\n    def test_intercept_builtin_sum(self):\n        s = Series([1., 2., np.nan, 3.])\n        grouped = s.groupby([0, 1, 2, 2])\n\n        result = grouped.agg(builtins.sum)\n        result2 = grouped.apply(builtins.sum)\n        expected = grouped.sum()\n        assert_series_equal(result, expected)\n        assert_series_equal(result2, expected)\n\n    def test_column_select_via_attr(self):\n        result = self.df.groupby('A').C.sum()\n        expected = self.df.groupby('A')['C'].sum()\n        assert_series_equal(result, expected)\n\n        self.df['mean'] = 1.5\n        result = self.df.groupby('A').mean()\n        expected = self.df.groupby('A').agg(np.mean)\n        assert_frame_equal(result, expected)\n\n    def test_rank_apply(self):\n        lev1 = tm.rands_array(10, 100)\n        lev2 = tm.rands_array(10, 130)\n        lab1 = np.random.randint(0, 100, size=500)\n        lab2 = np.random.randint(0, 130, size=500)\n\n        df = DataFrame({'value': np.random.randn(500),\n                        'key1': lev1.take(lab1),\n                        'key2': lev2.take(lab2)})\n\n        result = df.groupby(['key1', 'key2']).value.rank()\n\n        expected = []\n        for key, piece in df.groupby(['key1', 'key2']):\n            expected.append(piece.value.rank())\n        expected = concat(expected, axis=0)\n        expected = expected.reindex(result.index)\n        assert_series_equal(result, expected)\n\n        result = df.groupby(['key1', 'key2']).value.rank(pct=True)\n\n        expected = []\n        for key, piece in df.groupby(['key1', 'key2']):\n            expected.append(piece.value.rank(pct=True))\n        expected = concat(expected, axis=0)\n        expected = expected.reindex(result.index)\n        assert_series_equal(result, expected)\n\n    def test_dont_clobber_name_column(self):\n        df = DataFrame({'key': ['a', 'a', 'a', 'b', 'b', 'b'],\n                        'name': ['foo', 'bar', 'baz'] * 2})\n\n        result = df.groupby('key').apply(lambda x: x)\n        assert_frame_equal(result, df)\n\n    def test_skip_group_keys(self):\n        from pandas import concat\n\n        tsf = tm.makeTimeDataFrame()\n\n        grouped = tsf.groupby(lambda x: x.month, group_keys=False)\n        result = grouped.apply(lambda x: x.sort_values(by='A')[:3])\n\n        pieces = []\n        for key, group in grouped:\n            pieces.append(group.sort_values(by='A')[:3])\n\n        expected = concat(pieces)\n        assert_frame_equal(result, expected)\n\n        grouped = tsf['A'].groupby(lambda x: x.month, group_keys=False)\n        result = grouped.apply(lambda x: x.sort_values()[:3])\n\n        pieces = []\n        for key, group in grouped:\n            pieces.append(group.sort_values()[:3])\n\n        expected = concat(pieces)\n        assert_series_equal(result, expected)\n\n    def test_no_nonsense_name(self):\n        # GH #995\n        s = self.frame['C'].copy()\n        s.name = None\n\n        result = s.groupby(self.frame['A']).agg(np.sum)\n        self.assertIsNone(result.name)\n\n    def test_multifunc_sum_bug(self):\n        # GH #1065\n        x = DataFrame(np.arange(9).reshape(3, 3))\n        x['test'] = 0\n        x['fl'] = [1.3, 1.5, 1.6]\n\n        grouped = x.groupby('test')\n        result = grouped.agg({'fl': 'sum', 2: 'size'})\n        self.assertEqual(result['fl'].dtype, np.float64)\n\n    def test_handle_dict_return_value(self):\n        def f(group):\n            return {'min': group.min(), 'max': group.max()}\n\n        def g(group):\n            return Series({'min': group.min(), 'max': group.max()})\n\n        result = self.df.groupby('A')['C'].apply(f)\n        expected = self.df.groupby('A')['C'].apply(g)\n\n        tm.assertIsInstance(result, Series)\n        assert_series_equal(result, expected)\n\n    def test_getitem_list_of_columns(self):\n        df = DataFrame(\n            {'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n             'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n             'C': np.random.randn(8),\n             'D': np.random.randn(8),\n             'E': np.random.randn(8)})\n\n        result = df.groupby('A')[['C', 'D']].mean()\n        result2 = df.groupby('A')['C', 'D'].mean()\n        result3 = df.groupby('A')[df.columns[2:4]].mean()\n\n        expected = df.loc[:, ['A', 'C', 'D']].groupby('A').mean()\n\n        assert_frame_equal(result, expected)\n        assert_frame_equal(result2, expected)\n        assert_frame_equal(result3, expected)\n\n    def test_getitem_numeric_column_names(self):\n        # GH #13731\n        df = DataFrame({0: list('abcd') * 2,\n                        2: np.random.randn(8),\n                        4: np.random.randn(8),\n                        6: np.random.randn(8)})\n        result = df.groupby(0)[df.columns[1:3]].mean()\n        result2 = df.groupby(0)[2, 4].mean()\n        result3 = df.groupby(0)[[2, 4]].mean()\n\n        expected = df.loc[:, [0, 2, 4]].groupby(0).mean()\n\n        assert_frame_equal(result, expected)\n        assert_frame_equal(result2, expected)\n        assert_frame_equal(result3, expected)\n\n    def test_set_group_name(self):\n        def f(group):\n            assert group.name is not None\n            return group\n\n        def freduce(group):\n            assert group.name is not None\n            return group.sum()\n\n        def foo(x):\n            return freduce(x)\n\n        def _check_all(grouped):\n            # make sure all these work\n            grouped.apply(f)\n            grouped.aggregate(freduce)\n            grouped.aggregate({'C': freduce, 'D': freduce})\n            grouped.transform(f)\n\n            grouped['C'].apply(f)\n            grouped['C'].aggregate(freduce)\n            grouped['C'].aggregate([freduce, foo])\n            grouped['C'].transform(f)\n\n        _check_all(self.df.groupby('A'))\n        _check_all(self.df.groupby(['A', 'B']))\n\n    def test_no_dummy_key_names(self):\n        # GH #1291\n\n        result = self.df.groupby(self.df['A'].values).sum()\n        self.assertIsNone(result.index.name)\n\n        result = self.df.groupby([self.df['A'].values, self.df['B'].values\n                                  ]).sum()\n        self.assertEqual(result.index.names, (None, None))\n\n    def test_groupby_sort_multiindex_series(self):\n        # series multiindex groupby sort argument was not being passed through\n        # _compress_group_index\n        # GH 9444\n        index = MultiIndex(levels=[[1, 2], [1, 2]],\n                           labels=[[0, 0, 0, 0, 1, 1], [1, 1, 0, 0, 0, 0]],\n                           names=['a', 'b'])\n        mseries = Series([0, 1, 2, 3, 4, 5], index=index)\n        index = MultiIndex(levels=[[1, 2], [1, 2]],\n                           labels=[[0, 0, 1], [1, 0, 0]], names=['a', 'b'])\n        mseries_result = Series([0, 2, 4], index=index)\n\n        result = mseries.groupby(level=['a', 'b'], sort=False).first()\n        assert_series_equal(result, mseries_result)\n        result = mseries.groupby(level=['a', 'b'], sort=True).first()\n        assert_series_equal(result, mseries_result.sort_index())\n\n    def test_groupby_reindex_inside_function(self):\n        from pandas.tseries.api import DatetimeIndex\n\n        periods = 1000\n        ind = DatetimeIndex(start='2012/1/1', freq='5min', periods=periods)\n        df = DataFrame({'high': np.arange(\n            periods), 'low': np.arange(periods)}, index=ind)\n\n        def agg_before(hour, func, fix=False):\n            \"\"\"\n                Run an aggregate func on the subset of data.\n            \"\"\"\n\n            def _func(data):\n                d = data.select(lambda x: x.hour < 11).dropna()\n                if fix:\n                    data[data.index[0]]\n                if len(d) == 0:\n                    return None\n                return func(d)\n\n            return _func\n\n        def afunc(data):\n            d = data.select(lambda x: x.hour < 11).dropna()\n            return np.max(d)\n\n        grouped = df.groupby(lambda x: datetime(x.year, x.month, x.day))\n        closure_bad = grouped.agg({'high': agg_before(11, np.max)})\n        closure_good = grouped.agg({'high': agg_before(11, np.max, True)})\n\n        assert_frame_equal(closure_bad, closure_good)\n\n    def test_multiindex_columns_empty_level(self):\n        l = [['count', 'values'], ['to filter', '']]\n        midx = MultiIndex.from_tuples(l)\n\n        df = DataFrame([[long(1), 'A']], columns=midx)\n\n        grouped = df.groupby('to filter').groups\n        self.assertEqual(grouped['A'], [0])\n\n        grouped = df.groupby([('to filter', '')]).groups\n        self.assertEqual(grouped['A'], [0])\n\n        df = DataFrame([[long(1), 'A'], [long(2), 'B']], columns=midx)\n\n        expected = df.groupby('to filter').groups\n        result = df.groupby([('to filter', '')]).groups\n        self.assertEqual(result, expected)\n\n        df = DataFrame([[long(1), 'A'], [long(2), 'A']], columns=midx)\n\n        expected = df.groupby('to filter').groups\n        result = df.groupby([('to filter', '')]).groups\n        tm.assert_dict_equal(result, expected)\n\n    def test_cython_median(self):\n        df = DataFrame(np.random.randn(1000))\n        df.values[::2] = np.nan\n\n        labels = np.random.randint(0, 50, size=1000).astype(float)\n        labels[::17] = np.nan\n\n        result = df.groupby(labels).median()\n        exp = df.groupby(labels).agg(nanops.nanmedian)\n        assert_frame_equal(result, exp)\n\n        df = DataFrame(np.random.randn(1000, 5))\n        rs = df.groupby(labels).agg(np.median)\n        xp = df.groupby(labels).median()\n        assert_frame_equal(rs, xp)\n\n    def test_median_empty_bins(self):\n        df = pd.DataFrame(np.random.randint(0, 44, 500))\n\n        grps = range(0, 55, 5)\n        bins = pd.cut(df[0], grps)\n\n        result = df.groupby(bins).median()\n        expected = df.groupby(bins).agg(lambda x: x.median())\n        assert_frame_equal(result, expected)\n\n    def test_groupby_non_arithmetic_agg_types(self):\n        # GH9311, GH6620\n        df = pd.DataFrame(\n            [{'a': 1, 'b': 1},\n             {'a': 1, 'b': 2},\n             {'a': 2, 'b': 3},\n             {'a': 2, 'b': 4}])\n\n        dtypes = ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']\n\n        grp_exp = {'first': {'df': [{'a': 1, 'b': 1}, {'a': 2, 'b': 3}]},\n                   'last': {'df': [{'a': 1, 'b': 2}, {'a': 2, 'b': 4}]},\n                   'min': {'df': [{'a': 1, 'b': 1}, {'a': 2, 'b': 3}]},\n                   'max': {'df': [{'a': 1, 'b': 2}, {'a': 2, 'b': 4}]},\n                   'nth': {'df': [{'a': 1, 'b': 2}, {'a': 2, 'b': 4}],\n                           'args': [1]},\n                   'count': {'df': [{'a': 1, 'b': 2}, {'a': 2, 'b': 2}],\n                             'out_type': 'int64'}}\n\n        for dtype in dtypes:\n            df_in = df.copy()\n            df_in['b'] = df_in.b.astype(dtype)\n\n            for method, data in compat.iteritems(grp_exp):\n                if 'args' not in data:\n                    data['args'] = []\n\n                if 'out_type' in data:\n                    out_type = data['out_type']\n                else:\n                    out_type = dtype\n\n                exp = data['df']\n                df_out = pd.DataFrame(exp)\n\n                df_out['b'] = df_out.b.astype(out_type)\n                df_out.set_index('a', inplace=True)\n\n                grpd = df_in.groupby('a')\n                t = getattr(grpd, method)(*data['args'])\n                assert_frame_equal(t, df_out)\n\n    def test_groupby_non_arithmetic_agg_intlike_precision(self):\n        # GH9311, GH6620\n        c = 24650000000000000\n\n        inputs = ((Timestamp('2011-01-15 12:50:28.502376'),\n                   Timestamp('2011-01-20 12:50:28.593448')), (1 + c, 2 + c))\n\n        for i in inputs:\n            df = pd.DataFrame([{'a': 1, 'b': i[0]}, {'a': 1, 'b': i[1]}])\n\n            grp_exp = {'first': {'expected': i[0]},\n                       'last': {'expected': i[1]},\n                       'min': {'expected': i[0]},\n                       'max': {'expected': i[1]},\n                       'nth': {'expected': i[1],\n                               'args': [1]},\n                       'count': {'expected': 2}}\n\n            for method, data in compat.iteritems(grp_exp):\n                if 'args' not in data:\n                    data['args'] = []\n\n                grpd = df.groupby('a')\n                res = getattr(grpd, method)(*data['args'])\n                self.assertEqual(res.iloc[0].b, data['expected'])\n\n    def test_groupby_multiindex_missing_pair(self):\n        # GH9049\n        df = DataFrame({'group1': ['a', 'a', 'a', 'b'],\n                        'group2': ['c', 'c', 'd', 'c'],\n                        'value': [1, 1, 1, 5]})\n        df = df.set_index(['group1', 'group2'])\n        df_grouped = df.groupby(level=['group1', 'group2'], sort=True)\n\n        res = df_grouped.agg('sum')\n        idx = MultiIndex.from_tuples(\n            [('a', 'c'), ('a', 'd'), ('b', 'c')], names=['group1', 'group2'])\n        exp = DataFrame([[2], [1], [5]], index=idx, columns=['value'])\n\n        tm.assert_frame_equal(res, exp)\n\n    def test_groupby_multiindex_not_lexsorted(self):\n        # GH 11640\n\n        # define the lexsorted version\n        lexsorted_mi = MultiIndex.from_tuples(\n            [('a', ''), ('b1', 'c1'), ('b2', 'c2')], names=['b', 'c'])\n        lexsorted_df = DataFrame([[1, 3, 4]], columns=lexsorted_mi)\n        self.assertTrue(lexsorted_df.columns.is_lexsorted())\n\n        # define the non-lexsorted version\n        not_lexsorted_df = DataFrame(columns=['a', 'b', 'c', 'd'],\n                                     data=[[1, 'b1', 'c1', 3],\n                                           [1, 'b2', 'c2', 4]])\n        not_lexsorted_df = not_lexsorted_df.pivot_table(\n            index='a', columns=['b', 'c'], values='d')\n        not_lexsorted_df = not_lexsorted_df.reset_index()\n        self.assertFalse(not_lexsorted_df.columns.is_lexsorted())\n\n        # compare the results\n        tm.assert_frame_equal(lexsorted_df, not_lexsorted_df)\n\n        expected = lexsorted_df.groupby('a').mean()\n        with tm.assert_produces_warning(com.PerformanceWarning):\n            result = not_lexsorted_df.groupby('a').mean()\n        tm.assert_frame_equal(expected, result)\n\n        # a transforming function should work regardless of sort\n        # GH 14776\n        df = DataFrame({'x': ['a', 'a', 'b', 'a'],\n                        'y': [1, 1, 2, 2],\n                        'z': [1, 2, 3, 4]}).set_index(['x', 'y'])\n        self.assertFalse(df.index.is_lexsorted())\n\n        for level in [0, 1, [0, 1]]:\n            for sort in [False, True]:\n                result = df.groupby(level=level, sort=sort).apply(\n                    DataFrame.drop_duplicates)\n                expected = df\n                tm.assert_frame_equal(expected, result)\n\n                result = df.sort_index().groupby(level=level, sort=sort).apply(\n                    DataFrame.drop_duplicates)\n                expected = df.sort_index()\n                tm.assert_frame_equal(expected, result)\n\n    def test_groupby_levels_and_columns(self):\n        # GH9344, GH9049\n        idx_names = ['x', 'y']\n        idx = pd.MultiIndex.from_tuples(\n            [(1, 1), (1, 2), (3, 4), (5, 6)], names=idx_names)\n        df = pd.DataFrame(np.arange(12).reshape(-1, 3), index=idx)\n\n        by_levels = df.groupby(level=idx_names).mean()\n        # reset_index changes columns dtype to object\n        by_columns = df.reset_index().groupby(idx_names).mean()\n\n        tm.assert_frame_equal(by_levels, by_columns, check_column_type=False)\n\n        by_columns.columns = pd.Index(by_columns.columns, dtype=np.int64)\n        tm.assert_frame_equal(by_levels, by_columns)\n\n    def test_gb_apply_list_of_unequal_len_arrays(self):\n\n        # GH1738\n        df = DataFrame({'group1': ['a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'a',\n                                   'b', 'b', 'b'],\n                        'group2': ['c', 'c', 'd', 'd', 'd', 'e', 'c', 'c', 'd',\n                                   'd', 'd', 'e'],\n                        'weight': [1.1, 2, 3, 4, 5, 6, 2, 4, 6, 8, 1, 2],\n                        'value': [7.1, 8, 9, 10, 11, 12, 8, 7, 6, 5, 4, 3]})\n        df = df.set_index(['group1', 'group2'])\n        df_grouped = df.groupby(level=['group1', 'group2'], sort=True)\n\n        def noddy(value, weight):\n            out = np.array(value * weight).repeat(3)\n            return out\n\n        # the kernel function returns arrays of unequal length\n        # pandas sniffs the first one, sees it's an array and not\n        # a list, and assumed the rest are of equal length\n        # and so tries a vstack\n\n        # don't die\n        df_grouped.apply(lambda x: noddy(x.value, x.weight))\n\n    def test_groupby_with_empty(self):\n        index = pd.DatetimeIndex(())\n        data = ()\n        series = pd.Series(data, index)\n        grouper = pd.tseries.resample.TimeGrouper('D')\n        grouped = series.groupby(grouper)\n        assert next(iter(grouped), None) is None\n\n    def test_groupby_with_single_column(self):\n        df = pd.DataFrame({'a': list('abssbab')})\n        tm.assert_frame_equal(df.groupby('a').get_group('a'), df.iloc[[0, 5]])\n        # GH 13530\n        exp = pd.DataFrame([], index=pd.Index(['a', 'b', 's'], name='a'))\n        tm.assert_frame_equal(df.groupby('a').count(), exp)\n        tm.assert_frame_equal(df.groupby('a').sum(), exp)\n        tm.assert_frame_equal(df.groupby('a').nth(1), exp)\n\n    def test_groupby_with_small_elem(self):\n        # GH 8542\n        # length=2\n        df = pd.DataFrame({'event': ['start', 'start'],\n                           'change': [1234, 5678]},\n                          index=pd.DatetimeIndex(['2014-09-10', '2013-10-10']))\n        grouped = df.groupby([pd.TimeGrouper(freq='M'), 'event'])\n        self.assertEqual(len(grouped.groups), 2)\n        self.assertEqual(grouped.ngroups, 2)\n        self.assertIn((pd.Timestamp('2014-09-30'), 'start'), grouped.groups)\n        self.assertIn((pd.Timestamp('2013-10-31'), 'start'), grouped.groups)\n\n        res = grouped.get_group((pd.Timestamp('2014-09-30'), 'start'))\n        tm.assert_frame_equal(res, df.iloc[[0], :])\n        res = grouped.get_group((pd.Timestamp('2013-10-31'), 'start'))\n        tm.assert_frame_equal(res, df.iloc[[1], :])\n\n        df = pd.DataFrame({'event': ['start', 'start', 'start'],\n                           'change': [1234, 5678, 9123]},\n                          index=pd.DatetimeIndex(['2014-09-10', '2013-10-10',\n                                                  '2014-09-15']))\n        grouped = df.groupby([pd.TimeGrouper(freq='M'), 'event'])\n        self.assertEqual(len(grouped.groups), 2)\n        self.assertEqual(grouped.ngroups, 2)\n        self.assertIn((pd.Timestamp('2014-09-30'), 'start'), grouped.groups)\n        self.assertIn((pd.Timestamp('2013-10-31'), 'start'), grouped.groups)\n\n        res = grouped.get_group((pd.Timestamp('2014-09-30'), 'start'))\n        tm.assert_frame_equal(res, df.iloc[[0, 2], :])\n        res = grouped.get_group((pd.Timestamp('2013-10-31'), 'start'))\n        tm.assert_frame_equal(res, df.iloc[[1], :])\n\n        # length=3\n        df = pd.DataFrame({'event': ['start', 'start', 'start'],\n                           'change': [1234, 5678, 9123]},\n                          index=pd.DatetimeIndex(['2014-09-10', '2013-10-10',\n                                                  '2014-08-05']))\n        grouped = df.groupby([pd.TimeGrouper(freq='M'), 'event'])\n        self.assertEqual(len(grouped.groups), 3)\n        self.assertEqual(grouped.ngroups, 3)\n        self.assertIn((pd.Timestamp('2014-09-30'), 'start'), grouped.groups)\n        self.assertIn((pd.Timestamp('2013-10-31'), 'start'), grouped.groups)\n        self.assertIn((pd.Timestamp('2014-08-31'), 'start'), grouped.groups)\n\n        res = grouped.get_group((pd.Timestamp('2014-09-30'), 'start'))\n        tm.assert_frame_equal(res, df.iloc[[0], :])\n        res = grouped.get_group((pd.Timestamp('2013-10-31'), 'start'))\n        tm.assert_frame_equal(res, df.iloc[[1], :])\n        res = grouped.get_group((pd.Timestamp('2014-08-31'), 'start'))\n        tm.assert_frame_equal(res, df.iloc[[2], :])\n\n    def test_cumcount(self):\n        df = DataFrame([['a'], ['a'], ['a'], ['b'], ['a']], columns=['A'])\n        g = df.groupby('A')\n        sg = g.A\n\n        expected = Series([0, 1, 2, 0, 3])\n\n        assert_series_equal(expected, g.cumcount())\n        assert_series_equal(expected, sg.cumcount())\n\n    def test_cumcount_empty(self):\n        ge = DataFrame().groupby(level=0)\n        se = Series().groupby(level=0)\n\n        # edge case, as this is usually considered float\n        e = Series(dtype='int64')\n\n        assert_series_equal(e, ge.cumcount())\n        assert_series_equal(e, se.cumcount())\n\n    def test_cumcount_dupe_index(self):\n        df = DataFrame([['a'], ['a'], ['a'], ['b'], ['a']], columns=['A'],\n                       index=[0] * 5)\n        g = df.groupby('A')\n        sg = g.A\n\n        expected = Series([0, 1, 2, 0, 3], index=[0] * 5)\n\n        assert_series_equal(expected, g.cumcount())\n        assert_series_equal(expected, sg.cumcount())\n\n    def test_cumcount_mi(self):\n        mi = MultiIndex.from_tuples([[0, 1], [1, 2], [2, 2], [2, 2], [1, 0]])\n        df = DataFrame([['a'], ['a'], ['a'], ['b'], ['a']], columns=['A'],\n                       index=mi)\n        g = df.groupby('A')\n        sg = g.A\n\n        expected = Series([0, 1, 2, 0, 3], index=mi)\n\n        assert_series_equal(expected, g.cumcount())\n        assert_series_equal(expected, sg.cumcount())\n\n    def test_cumcount_groupby_not_col(self):\n        df = DataFrame([['a'], ['a'], ['a'], ['b'], ['a']], columns=['A'],\n                       index=[0] * 5)\n        g = df.groupby([0, 0, 0, 1, 0])\n        sg = g.A\n\n        expected = Series([0, 1, 2, 0, 3], index=[0] * 5)\n\n        assert_series_equal(expected, g.cumcount())\n        assert_series_equal(expected, sg.cumcount())\n\n    def test_fill_constistency(self):\n\n        # GH9221\n        # pass thru keyword arguments to the generated wrapper\n        # are set if the passed kw is None (only)\n        df = DataFrame(index=pd.MultiIndex.from_product(\n            [['value1', 'value2'], date_range('2014-01-01', '2014-01-06')]),\n            columns=Index(\n            ['1', '2'], name='id'))\n        df['1'] = [np.nan, 1, np.nan, np.nan, 11, np.nan, np.nan, 2, np.nan,\n                   np.nan, 22, np.nan]\n        df['2'] = [np.nan, 3, np.nan, np.nan, 33, np.nan, np.nan, 4, np.nan,\n                   np.nan, 44, np.nan]\n\n        expected = df.groupby(level=0, axis=0).fillna(method='ffill')\n        result = df.T.groupby(level=0, axis=1).fillna(method='ffill').T\n        assert_frame_equal(result, expected)\n\n    def test_index_label_overlaps_location(self):\n        # checking we don't have any label/location confusion in the\n        # the wake of GH5375\n        df = DataFrame(list('ABCDE'), index=[2, 0, 2, 1, 1])\n        g = df.groupby(list('ababb'))\n        actual = g.filter(lambda x: len(x) > 2)\n        expected = df.iloc[[1, 3, 4]]\n        assert_frame_equal(actual, expected)\n\n        ser = df[0]\n        g = ser.groupby(list('ababb'))\n        actual = g.filter(lambda x: len(x) > 2)\n        expected = ser.take([1, 3, 4])\n        assert_series_equal(actual, expected)\n\n        # ... and again, with a generic Index of floats\n        df.index = df.index.astype(float)\n        g = df.groupby(list('ababb'))\n        actual = g.filter(lambda x: len(x) > 2)\n        expected = df.iloc[[1, 3, 4]]\n        assert_frame_equal(actual, expected)\n\n        ser = df[0]\n        g = ser.groupby(list('ababb'))\n        actual = g.filter(lambda x: len(x) > 2)\n        expected = ser.take([1, 3, 4])\n        assert_series_equal(actual, expected)\n\n    def test_groupby_selection_with_methods(self):\n        # some methods which require DatetimeIndex\n        rng = pd.date_range('2014', periods=len(self.df))\n        self.df.index = rng\n\n        g = self.df.groupby(['A'])[['C']]\n        g_exp = self.df[['C']].groupby(self.df['A'])\n        # TODO check groupby with > 1 col ?\n\n        # methods which are called as .foo()\n        methods = ['count',\n                   'corr',\n                   'cummax',\n                   'cummin',\n                   'cumprod',\n                   'describe',\n                   'rank',\n                   'quantile',\n                   'diff',\n                   'shift',\n                   'all',\n                   'any',\n                   'idxmin',\n                   'idxmax',\n                   'ffill',\n                   'bfill',\n                   'pct_change',\n                   'tshift']\n\n        for m in methods:\n            res = getattr(g, m)()\n            exp = getattr(g_exp, m)()\n            assert_frame_equal(res, exp)  # should always be frames!\n\n        # methods which aren't just .foo()\n        assert_frame_equal(g.fillna(0), g_exp.fillna(0))\n        assert_frame_equal(g.dtypes, g_exp.dtypes)\n        assert_frame_equal(g.apply(lambda x: x.sum()),\n                           g_exp.apply(lambda x: x.sum()))\n\n        assert_frame_equal(g.resample('D').mean(), g_exp.resample('D').mean())\n        assert_frame_equal(g.resample('D').ohlc(),\n                           g_exp.resample('D').ohlc())\n\n        assert_frame_equal(g.filter(lambda x: len(x) == 3),\n                           g_exp.filter(lambda x: len(x) == 3))\n\n    def test_groupby_whitelist(self):\n        from string import ascii_lowercase\n        letters = np.array(list(ascii_lowercase))\n        N = 10\n        random_letters = letters.take(np.random.randint(0, 26, N))\n        df = DataFrame({'floats': N / 10 * Series(np.random.random(N)),\n                        'letters': Series(random_letters)})\n        s = df.floats\n\n        df_whitelist = frozenset([\n            'last',\n            'first',\n            'mean',\n            'sum',\n            'min',\n            'max',\n            'head',\n            'tail',\n            'cumcount',\n            'resample',\n            'rank',\n            'quantile',\n            'fillna',\n            'mad',\n            'any',\n            'all',\n            'take',\n            'idxmax',\n            'idxmin',\n            'shift',\n            'tshift',\n            'ffill',\n            'bfill',\n            'pct_change',\n            'skew',\n            'plot',\n            'boxplot',\n            'hist',\n            'median',\n            'dtypes',\n            'corrwith',\n            'corr',\n            'cov',\n            'diff',\n        ])\n        s_whitelist = frozenset([\n            'last',\n            'first',\n            'mean',\n            'sum',\n            'min',\n            'max',\n            'head',\n            'tail',\n            'cumcount',\n            'resample',\n            'rank',\n            'quantile',\n            'fillna',\n            'mad',\n            'any',\n            'all',\n            'take',\n            'idxmax',\n            'idxmin',\n            'shift',\n            'tshift',\n            'ffill',\n            'bfill',\n            'pct_change',\n            'skew',\n            'plot',\n            'hist',\n            'median',\n            'dtype',\n            'corr',\n            'cov',\n            'diff',\n            'unique',\n            # 'nlargest', 'nsmallest',\n        ])\n\n        for obj, whitelist in zip((df, s), (df_whitelist, s_whitelist)):\n            gb = obj.groupby(df.letters)\n            self.assertEqual(whitelist, gb._apply_whitelist)\n            for m in whitelist:\n                getattr(type(gb), m)\n\n    AGG_FUNCTIONS = ['sum', 'prod', 'min', 'max', 'median', 'mean', 'skew',\n                     'mad', 'std', 'var', 'sem']\n    AGG_FUNCTIONS_WITH_SKIPNA = ['skew', 'mad']\n\n    def test_groupby_whitelist_deprecations(self):\n        from string import ascii_lowercase\n        letters = np.array(list(ascii_lowercase))\n        N = 10\n        random_letters = letters.take(np.random.randint(0, 26, N))\n        df = DataFrame({'floats': N / 10 * Series(np.random.random(N)),\n                        'letters': Series(random_letters)})\n\n        # 10711 deprecated\n        with tm.assert_produces_warning(FutureWarning):\n            df.groupby('letters').irow(0)\n        with tm.assert_produces_warning(FutureWarning):\n            df.groupby('letters').floats.irow(0)\n\n    def test_regression_whitelist_methods(self):\n\n        # GH6944\n        # explicity test the whitelest methods\n        index = MultiIndex(levels=[['foo', 'bar', 'baz', 'qux'], ['one', 'two',\n                                                                  'three']],\n                           labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3],\n                                   [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n                           names=['first', 'second'])\n        raw_frame = DataFrame(np.random.randn(10, 3), index=index,\n                              columns=Index(['A', 'B', 'C'], name='exp'))\n        raw_frame.iloc[1, [1, 2]] = np.nan\n        raw_frame.iloc[7, [0, 1]] = np.nan\n\n        for op, level, axis, skipna in cart_product(self.AGG_FUNCTIONS,\n                                                    lrange(2), lrange(2),\n                                                    [True, False]):\n\n            if axis == 0:\n                frame = raw_frame\n            else:\n                frame = raw_frame.T\n\n            if op in self.AGG_FUNCTIONS_WITH_SKIPNA:\n                grouped = frame.groupby(level=level, axis=axis)\n                result = getattr(grouped, op)(skipna=skipna)\n                expected = getattr(frame, op)(level=level, axis=axis,\n                                              skipna=skipna)\n                assert_frame_equal(result, expected)\n            else:\n                grouped = frame.groupby(level=level, axis=axis)\n                result = getattr(grouped, op)()\n                expected = getattr(frame, op)(level=level, axis=axis)\n                assert_frame_equal(result, expected)\n\n    def test_groupby_blacklist(self):\n        from string import ascii_lowercase\n        letters = np.array(list(ascii_lowercase))\n        N = 10\n        random_letters = letters.take(np.random.randint(0, 26, N))\n        df = DataFrame({'floats': N / 10 * Series(np.random.random(N)),\n                        'letters': Series(random_letters)})\n        s = df.floats\n\n        blacklist = [\n            'eval', 'query', 'abs', 'where',\n            'mask', 'align', 'groupby', 'clip', 'astype',\n            'at', 'combine', 'consolidate', 'convert_objects',\n        ]\n        to_methods = [method for method in dir(df) if method.startswith('to_')]\n\n        blacklist.extend(to_methods)\n\n        # e.g., to_csv\n        defined_but_not_allowed = (\"(?:^Cannot.+{0!r}.+{1!r}.+try using the \"\n                                   \"'apply' method$)\")\n\n        # e.g., query, eval\n        not_defined = \"(?:^{1!r} object has no attribute {0!r}$)\"\n        fmt = defined_but_not_allowed + '|' + not_defined\n        for bl in blacklist:\n            for obj in (df, s):\n                gb = obj.groupby(df.letters)\n                msg = fmt.format(bl, type(gb).__name__)\n                with tm.assertRaisesRegexp(AttributeError, msg):\n                    getattr(gb, bl)\n\n    def test_tab_completion(self):\n        grp = self.mframe.groupby(level='second')\n        results = set([v for v in dir(grp) if not v.startswith('_')])\n        expected = set(\n            ['A', 'B', 'C', 'agg', 'aggregate', 'apply', 'boxplot', 'filter',\n             'first', 'get_group', 'groups', 'hist', 'indices', 'last', 'max',\n             'mean', 'median', 'min', 'name', 'ngroups', 'nth', 'ohlc', 'plot',\n             'prod', 'size', 'std', 'sum', 'transform', 'var', 'sem', 'count',\n             'nunique', 'head', 'irow', 'describe', 'cummax', 'quantile',\n             'rank', 'cumprod', 'tail', 'resample', 'cummin', 'fillna',\n             'cumsum', 'cumcount', 'all', 'shift', 'skew', 'bfill', 'ffill',\n             'take', 'tshift', 'pct_change', 'any', 'mad', 'corr', 'corrwith',\n             'cov', 'dtypes', 'ndim', 'diff', 'idxmax', 'idxmin',\n             'ffill', 'bfill', 'pad', 'backfill', 'rolling', 'expanding'])\n        self.assertEqual(results, expected)\n\n    def test_lower_int_prec_count(self):\n        df = DataFrame({'a': np.array(\n            [0, 1, 2, 100], np.int8),\n            'b': np.array(\n            [1, 2, 3, 6], np.uint32),\n            'c': np.array(\n            [4, 5, 6, 8], np.int16),\n            'grp': list('ab' * 2)})\n        result = df.groupby('grp').count()\n        expected = DataFrame({'a': [2, 2],\n                              'b': [2, 2],\n                              'c': [2, 2]}, index=pd.Index(list('ab'),\n                                                           name='grp'))\n        tm.assert_frame_equal(result, expected)\n\n    def test_count_uses_size_on_exception(self):\n        class RaisingObjectException(Exception):\n            pass\n\n        class RaisingObject(object):\n\n            def __init__(self, msg='I will raise inside Cython'):\n                super(RaisingObject, self).__init__()\n                self.msg = msg\n\n            def __eq__(self, other):\n                # gets called in Cython to check that raising calls the method\n                raise RaisingObjectException(self.msg)\n\n        df = DataFrame({'a': [RaisingObject() for _ in range(4)],\n                        'grp': list('ab' * 2)})\n        result = df.groupby('grp').count()\n        expected = DataFrame({'a': [2, 2]}, index=pd.Index(\n            list('ab'), name='grp'))\n        tm.assert_frame_equal(result, expected)\n\n    def test_groupby_cumprod(self):\n        # GH 4095\n        df = pd.DataFrame({'key': ['b'] * 10, 'value': 2})\n\n        actual = df.groupby('key')['value'].cumprod()\n        expected = df.groupby('key')['value'].apply(lambda x: x.cumprod())\n        expected.name = 'value'\n        tm.assert_series_equal(actual, expected)\n\n        df = pd.DataFrame({'key': ['b'] * 100, 'value': 2})\n        actual = df.groupby('key')['value'].cumprod()\n        # if overflows, groupby product casts to float\n        # while numpy passes back invalid values\n        df['value'] = df['value'].astype(float)\n        expected = df.groupby('key')['value'].apply(lambda x: x.cumprod())\n        expected.name = 'value'\n        tm.assert_series_equal(actual, expected)\n\n    def test_ops_general(self):\n        ops = [('mean', np.mean),\n               ('median', np.median),\n               ('std', np.std),\n               ('var', np.var),\n               ('sum', np.sum),\n               ('prod', np.prod),\n               ('min', np.min),\n               ('max', np.max),\n               ('first', lambda x: x.iloc[0]),\n               ('last', lambda x: x.iloc[-1]),\n               ('count', np.size), ]\n        try:\n            from scipy.stats import sem\n        except ImportError:\n            pass\n        else:\n            ops.append(('sem', sem))\n        df = DataFrame(np.random.randn(1000))\n        labels = np.random.randint(0, 50, size=1000).astype(float)\n\n        for op, targop in ops:\n            result = getattr(df.groupby(labels), op)().astype(float)\n            expected = df.groupby(labels).agg(targop)\n            try:\n                tm.assert_frame_equal(result, expected)\n            except BaseException as exc:\n                exc.args += ('operation: %s' % op, )\n                raise\n\n    def test_max_nan_bug(self):\n        raw = \"\"\",Date,app,File\n2013-04-23,2013-04-23 00:00:00,,log080001.log\n2013-05-06,2013-05-06 00:00:00,,log.log\n2013-05-07,2013-05-07 00:00:00,OE,xlsx\"\"\"\n\n        df = pd.read_csv(StringIO(raw), parse_dates=[0])\n        gb = df.groupby('Date')\n        r = gb[['File']].max()\n        e = gb['File'].max().to_frame()\n        tm.assert_frame_equal(r, e)\n        self.assertFalse(r['File'].isnull().any())\n\n    def test_nlargest(self):\n        a = Series([1, 3, 5, 7, 2, 9, 0, 4, 6, 10])\n        b = Series(list('a' * 5 + 'b' * 5))\n        gb = a.groupby(b)\n        r = gb.nlargest(3)\n        e = Series([\n            7, 5, 3, 10, 9, 6\n        ], index=MultiIndex.from_arrays([list('aaabbb'), [3, 2, 1, 9, 5, 8]]))\n        tm.assert_series_equal(r, e)\n\n        a = Series([1, 1, 3, 2, 0, 3, 3, 2, 1, 0])\n        gb = a.groupby(b)\n        e = Series([\n            3, 2, 1, 3, 3, 2\n        ], index=MultiIndex.from_arrays([list('aaabbb'), [2, 3, 1, 6, 5, 7]]))\n        assert_series_equal(gb.nlargest(3, keep='last'), e)\n        with tm.assert_produces_warning(FutureWarning):\n            assert_series_equal(gb.nlargest(3, take_last=True), e)\n\n    def test_nsmallest(self):\n        a = Series([1, 3, 5, 7, 2, 9, 0, 4, 6, 10])\n        b = Series(list('a' * 5 + 'b' * 5))\n        gb = a.groupby(b)\n        r = gb.nsmallest(3)\n        e = Series([\n            1, 2, 3, 0, 4, 6\n        ], index=MultiIndex.from_arrays([list('aaabbb'), [0, 4, 1, 6, 7, 8]]))\n        tm.assert_series_equal(r, e)\n\n        a = Series([1, 1, 3, 2, 0, 3, 3, 2, 1, 0])\n        gb = a.groupby(b)\n        e = Series([\n            0, 1, 1, 0, 1, 2\n        ], index=MultiIndex.from_arrays([list('aaabbb'), [4, 1, 0, 9, 8, 7]]))\n        assert_series_equal(gb.nsmallest(3, keep='last'), e)\n        with tm.assert_produces_warning(FutureWarning):\n            assert_series_equal(gb.nsmallest(3, take_last=True), e)\n\n    def test_transform_doesnt_clobber_ints(self):\n        # GH 7972\n        n = 6\n        x = np.arange(n)\n        df = DataFrame({'a': x // 2, 'b': 2.0 * x, 'c': 3.0 * x})\n        df2 = DataFrame({'a': x // 2 * 1.0, 'b': 2.0 * x, 'c': 3.0 * x})\n\n        gb = df.groupby('a')\n        result = gb.transform('mean')\n\n        gb2 = df2.groupby('a')\n        expected = gb2.transform('mean')\n        tm.assert_frame_equal(result, expected)\n\n    def test_groupby_apply_all_none(self):\n        # Tests to make sure no errors if apply function returns all None\n        # values. Issue 9684.\n        test_df = DataFrame({'groups': [0, 0, 1, 1],\n                             'random_vars': [8, 7, 4, 5]})\n\n        def test_func(x):\n            pass\n\n        result = test_df.groupby('groups').apply(test_func)\n        expected = DataFrame()\n        tm.assert_frame_equal(result, expected)\n\n    def test_groupby_apply_none_first(self):\n        # GH 12824. Tests if apply returns None first.\n        test_df1 = DataFrame({'groups': [1, 1, 1, 2], 'vars': [0, 1, 2, 3]})\n        test_df2 = DataFrame({'groups': [1, 2, 2, 2], 'vars': [0, 1, 2, 3]})\n\n        def test_func(x):\n            if x.shape[0] < 2:\n                return None\n            return x.iloc[[0, -1]]\n\n        result1 = test_df1.groupby('groups').apply(test_func)\n        result2 = test_df2.groupby('groups').apply(test_func)\n        index1 = MultiIndex.from_arrays([[1, 1], [0, 2]],\n                                        names=['groups', None])\n        index2 = MultiIndex.from_arrays([[2, 2], [1, 3]],\n                                        names=['groups', None])\n        expected1 = DataFrame({'groups': [1, 1], 'vars': [0, 2]},\n                              index=index1)\n        expected2 = DataFrame({'groups': [2, 2], 'vars': [1, 3]},\n                              index=index2)\n        tm.assert_frame_equal(result1, expected1)\n        tm.assert_frame_equal(result2, expected2)\n\n    def test_groupby_preserves_sort(self):\n        # Test to ensure that groupby always preserves sort order of original\n        # object. Issue #8588 and #9651\n\n        df = DataFrame(\n            {'int_groups': [3, 1, 0, 1, 0, 3, 3, 3],\n             'string_groups': ['z', 'a', 'z', 'a', 'a', 'g', 'g', 'g'],\n             'ints': [8, 7, 4, 5, 2, 9, 1, 1],\n             'floats': [2.3, 5.3, 6.2, -2.4, 2.2, 1.1, 1.1, 5],\n             'strings': ['z', 'd', 'a', 'e', 'word', 'word2', '42', '47']})\n\n        # Try sorting on different types and with different group types\n        for sort_column in ['ints', 'floats', 'strings', ['ints', 'floats'],\n                            ['ints', 'strings']]:\n            for group_column in ['int_groups', 'string_groups',\n                                 ['int_groups', 'string_groups']]:\n\n                df = df.sort_values(by=sort_column)\n\n                g = df.groupby(group_column)\n\n                def test_sort(x):\n                    assert_frame_equal(x, x.sort_values(by=sort_column))\n\n                g.apply(test_sort)\n\n    def test_nunique_with_object(self):\n        # GH 11077\n        data = pd.DataFrame(\n            [[100, 1, 'Alice'],\n             [200, 2, 'Bob'],\n             [300, 3, 'Charlie'],\n             [-400, 4, 'Dan'],\n             [500, 5, 'Edith']],\n            columns=['amount', 'id', 'name']\n        )\n\n        result = data.groupby(['id', 'amount'])['name'].nunique()\n        index = MultiIndex.from_arrays([data.id, data.amount])\n        expected = pd.Series([1] * 5, name='name', index=index)\n        tm.assert_series_equal(result, expected)\n\n    def test_nunique_with_empty_series(self):\n        # GH 12553\n        data = pd.Series(name='name')\n        result = data.groupby(level=0).nunique()\n        expected = pd.Series(name='name', dtype='int64')\n        tm.assert_series_equal(result, expected)\n\n    def test_numpy_compat(self):\n        # see gh-12811\n        df = pd.DataFrame({'A': [1, 2, 1], 'B': [1, 2, 3]})\n        g = df.groupby('A')\n\n        msg = \"numpy operations are not valid with groupby\"\n\n        for func in ('mean', 'var', 'std', 'cumprod', 'cumsum'):\n            tm.assertRaisesRegexp(UnsupportedFunctionCall, msg,\n                                  getattr(g, func), 1, 2, 3)\n            tm.assertRaisesRegexp(UnsupportedFunctionCall, msg,\n                                  getattr(g, func), foo=1)\n\n    def test_grouping_string_repr(self):\n        # GH 13394\n        mi = MultiIndex.from_arrays([list(\"AAB\"), list(\"aba\")])\n        df = DataFrame([[1, 2, 3]], columns=mi)\n        gr = df.groupby(df[('A', 'a')])\n\n        result = gr.grouper.groupings[0].__repr__()\n        expected = \"Grouping(('A', 'a'))\"\n        tm.assert_equal(result, expected)\n\n    def test_group_shift_with_null_key(self):\n        # This test is designed to replicate the segfault in issue #13813.\n        n_rows = 1200\n\n        # Generate a moderately large dataframe with occasional missing\n        # values in column `B`, and then group by [`A`, `B`]. This should\n        # force `-1` in `labels` array of `g.grouper.group_info` exactly\n        # at those places, where the group-by key is partilly missing.\n        df = DataFrame([(i % 12, i % 3 if i % 3 else np.nan, i)\n                        for i in range(n_rows)], dtype=float,\n                       columns=[\"A\", \"B\", \"Z\"], index=None)\n        g = df.groupby([\"A\", \"B\"])\n\n        expected = DataFrame([(i + 12 if i % 3 and i < n_rows - 12\n                               else np.nan)\n                              for i in range(n_rows)], dtype=float,\n                             columns=[\"Z\"], index=None)\n        result = g.shift(-1)\n\n        assert_frame_equal(result, expected)\n\n    def test_pivot_table_values_key_error(self):\n        # This test is designed to replicate the error in issue #14938\n        df = pd.DataFrame({'eventDate':\n                           pd.date_range(pd.datetime.today(),\n                                         periods=20, freq='M').tolist(),\n                           'thename': range(0, 20)})\n\n        df['year'] = df.set_index('eventDate').index.year\n        df['month'] = df.set_index('eventDate').index.month\n\n        with self.assertRaises(KeyError):\n            df.reset_index().pivot_table(index='year', columns='month',\n                                         values='badname', aggfunc='count')\n\n    def test_cummin_cummax(self):\n        # GH 15048\n        num_types = [np.int32, np.int64, np.float32, np.float64]\n        num_mins = [np.iinfo(np.int32).min, np.iinfo(np.int64).min,\n                    np.finfo(np.float32).min, np.finfo(np.float64).min]\n        num_max = [np.iinfo(np.int32).max, np.iinfo(np.int64).max,\n                   np.finfo(np.float32).max, np.finfo(np.float64).max]\n        base_df = pd.DataFrame({'A': [1, 1, 1, 1, 2, 2, 2, 2],\n                                'B': [3, 4, 3, 2, 2, 3, 2, 1]})\n        expected_mins = [3, 3, 3, 2, 2, 2, 2, 1]\n        expected_maxs = [3, 4, 4, 4, 2, 3, 3, 3]\n\n        for dtype, min_val, max_val in zip(num_types, num_mins, num_max):\n            df = base_df.astype(dtype)\n\n            # cummin\n            expected = pd.DataFrame({'B': expected_mins}).astype(dtype)\n            result = df.groupby('A').cummin()\n            tm.assert_frame_equal(result, expected)\n            result = df.groupby('A').B.apply(lambda x: x.cummin()).to_frame()\n            tm.assert_frame_equal(result, expected)\n\n            # Test cummin w/ min value for dtype\n            df.loc[[2, 6], 'B'] = min_val\n            expected.loc[[2, 3, 6, 7], 'B'] = min_val\n            result = df.groupby('A').cummin()\n            tm.assert_frame_equal(result, expected)\n            expected = df.groupby('A').B.apply(lambda x: x.cummin()).to_frame()\n            tm.assert_frame_equal(result, expected)\n\n            # cummax\n            expected = pd.DataFrame({'B': expected_maxs}).astype(dtype)\n            result = df.groupby('A').cummax()\n            tm.assert_frame_equal(result, expected)\n            result = df.groupby('A').B.apply(lambda x: x.cummax()).to_frame()\n            tm.assert_frame_equal(result, expected)\n\n            # Test cummax w/ max value for dtype\n            df.loc[[2, 6], 'B'] = max_val\n            expected.loc[[2, 3, 6, 7], 'B'] = max_val\n            result = df.groupby('A').cummax()\n            tm.assert_frame_equal(result, expected)\n            expected = df.groupby('A').B.apply(lambda x: x.cummax()).to_frame()\n            tm.assert_frame_equal(result, expected)\n\n        # Test nan in some values\n        base_df.loc[[0, 2, 4, 6], 'B'] = np.nan\n        expected = pd.DataFrame({'B': [np.nan, 4, np.nan, 2,\n                                       np.nan, 3, np.nan, 1]})\n        result = base_df.groupby('A').cummin()\n        tm.assert_frame_equal(result, expected)\n        expected = (base_df.groupby('A')\n                           .B\n                           .apply(lambda x: x.cummin())\n                           .to_frame())\n        tm.assert_frame_equal(result, expected)\n\n        expected = pd.DataFrame({'B': [np.nan, 4, np.nan, 4,\n                                       np.nan, 3, np.nan, 3]})\n        result = base_df.groupby('A').cummax()\n        tm.assert_frame_equal(result, expected)\n        expected = (base_df.groupby('A')\n                           .B\n                           .apply(lambda x: x.cummax())\n                           .to_frame())\n        tm.assert_frame_equal(result, expected)\n\n        # Test nan in entire column\n        base_df['B'] = np.nan\n        expected = pd.DataFrame({'B': [np.nan] * 8})\n        result = base_df.groupby('A').cummin()\n        tm.assert_frame_equal(expected, result)\n        result = base_df.groupby('A').B.apply(lambda x: x.cummin()).to_frame()\n        tm.assert_frame_equal(expected, result)\n        result = base_df.groupby('A').cummax()\n        tm.assert_frame_equal(expected, result)\n        result = base_df.groupby('A').B.apply(lambda x: x.cummax()).to_frame()\n        tm.assert_frame_equal(expected, result)\n\n\ndef _check_groupby(df, result, keys, field, f=lambda x: x.sum()):\n    tups = lmap(tuple, df[keys].values)\n    tups = com._asarray_tuplesafe(tups)\n    expected = f(df.groupby(tups)[field])\n    for k, v in compat.iteritems(expected):\n        assert (result[k] == v)\n\n\ndef test_decons():\n    from pandas.core.groupby import decons_group_index, get_group_index\n\n    def testit(label_list, shape):\n        group_index = get_group_index(label_list, shape, sort=True, xnull=True)\n        label_list2 = decons_group_index(group_index, shape)\n\n        for a, b in zip(label_list, label_list2):\n            assert (np.array_equal(a, b))\n\n    shape = (4, 5, 6)\n    label_list = [np.tile([0, 1, 2, 3, 0, 1, 2, 3], 100), np.tile(\n        [0, 2, 4, 3, 0, 1, 2, 3], 100), np.tile(\n            [5, 1, 0, 2, 3, 0, 5, 4], 100)]\n    testit(label_list, shape)\n\n    shape = (10000, 10000)\n    label_list = [np.tile(np.arange(10000), 5), np.tile(np.arange(10000), 5)]\n    testit(label_list, shape)\n"
    },
    {
      "filename": "pandas/tests/test_generic.py",
      "content": "# -*- coding: utf-8 -*-\n# pylint: disable-msg=E1101,W0612\n\nfrom operator import methodcaller\nimport nose\nimport numpy as np\nfrom numpy import nan\nimport pandas as pd\n\nfrom distutils.version import LooseVersion\nfrom pandas.types.common import is_scalar\nfrom pandas import (Index, Series, DataFrame, Panel, isnull,\n                    date_range, period_range, Panel4D)\nfrom pandas.core.index import MultiIndex\n\nimport pandas.formats.printing as printing\n\nfrom pandas.compat import range, zip, PY3\nfrom pandas import compat\nfrom pandas.util.testing import (assertRaisesRegexp,\n                                 assert_series_equal,\n                                 assert_frame_equal,\n                                 assert_panel_equal,\n                                 assert_panel4d_equal,\n                                 assert_almost_equal)\n\nimport pandas.util.testing as tm\n\n\n# ----------------------------------------------------------------------\n# Generic types test cases\n\n\nclass Generic(object):\n\n    def setUp(self):\n        pass\n\n    @property\n    def _ndim(self):\n        return self._typ._AXIS_LEN\n\n    def _axes(self):\n        \"\"\" return the axes for my object typ \"\"\"\n        return self._typ._AXIS_ORDERS\n\n    def _construct(self, shape, value=None, dtype=None, **kwargs):\n        \"\"\" construct an object for the given shape\n            if value is specified use that if its a scalar\n            if value is an array, repeat it as needed \"\"\"\n\n        if isinstance(shape, int):\n            shape = tuple([shape] * self._ndim)\n        if value is not None:\n            if is_scalar(value):\n                if value == 'empty':\n                    arr = None\n\n                    # remove the info axis\n                    kwargs.pop(self._typ._info_axis_name, None)\n                else:\n                    arr = np.empty(shape, dtype=dtype)\n                    arr.fill(value)\n            else:\n                fshape = np.prod(shape)\n                arr = value.ravel()\n                new_shape = fshape / arr.shape[0]\n                if fshape % arr.shape[0] != 0:\n                    raise Exception(\"invalid value passed in _construct\")\n\n                arr = np.repeat(arr, new_shape).reshape(shape)\n        else:\n            arr = np.random.randn(*shape)\n        return self._typ(arr, dtype=dtype, **kwargs)\n\n    def _compare(self, result, expected):\n        self._comparator(result, expected)\n\n    def test_rename(self):\n\n        # single axis\n        idx = list('ABCD')\n        # relabeling values passed into self.rename\n        args = [\n            str.lower,\n            {x: x.lower() for x in idx},\n            Series({x: x.lower() for x in idx}),\n        ]\n\n        for axis in self._axes():\n            kwargs = {axis: idx}\n            obj = self._construct(4, **kwargs)\n\n            for arg in args:\n                # rename a single axis\n                result = obj.rename(**{axis: arg})\n                expected = obj.copy()\n                setattr(expected, axis, list('abcd'))\n                self._compare(result, expected)\n\n        # multiple axes at once\n\n    def test_rename_axis(self):\n        idx = list('ABCD')\n        # relabeling values passed into self.rename\n        args = [\n            str.lower,\n            {x: x.lower() for x in idx},\n            Series({x: x.lower() for x in idx}),\n        ]\n\n        for axis in self._axes():\n            kwargs = {axis: idx}\n            obj = self._construct(4, **kwargs)\n\n            for arg in args:\n                # rename a single axis\n                result = obj.rename_axis(arg, axis=axis)\n                expected = obj.copy()\n                setattr(expected, axis, list('abcd'))\n                self._compare(result, expected)\n            # scalar values\n            for arg in ['foo', None]:\n                result = obj.rename_axis(arg, axis=axis)\n                expected = obj.copy()\n                getattr(expected, axis).name = arg\n                self._compare(result, expected)\n\n    def test_get_numeric_data(self):\n\n        n = 4\n        kwargs = {}\n        for i in range(self._ndim):\n            kwargs[self._typ._AXIS_NAMES[i]] = list(range(n))\n\n        # get the numeric data\n        o = self._construct(n, **kwargs)\n        result = o._get_numeric_data()\n        self._compare(result, o)\n\n        # non-inclusion\n        result = o._get_bool_data()\n        expected = self._construct(n, value='empty', **kwargs)\n        self._compare(result, expected)\n\n        # get the bool data\n        arr = np.array([True, True, False, True])\n        o = self._construct(n, value=arr, **kwargs)\n        result = o._get_numeric_data()\n        self._compare(result, o)\n\n        # _get_numeric_data is includes _get_bool_data, so can't test for\n        # non-inclusion\n\n    def test_get_default(self):\n\n        # GH 7725\n        d0 = \"a\", \"b\", \"c\", \"d\"\n        d1 = np.arange(4, dtype='int64')\n        others = \"e\", 10\n\n        for data, index in ((d0, d1), (d1, d0)):\n            s = Series(data, index=index)\n            for i, d in zip(index, data):\n                self.assertEqual(s.get(i), d)\n                self.assertEqual(s.get(i, d), d)\n                self.assertEqual(s.get(i, \"z\"), d)\n                for other in others:\n                    self.assertEqual(s.get(other, \"z\"), \"z\")\n                    self.assertEqual(s.get(other, other), other)\n\n    def test_nonzero(self):\n\n        # GH 4633\n        # look at the boolean/nonzero behavior for objects\n        obj = self._construct(shape=4)\n        self.assertRaises(ValueError, lambda: bool(obj == 0))\n        self.assertRaises(ValueError, lambda: bool(obj == 1))\n        self.assertRaises(ValueError, lambda: bool(obj))\n\n        obj = self._construct(shape=4, value=1)\n        self.assertRaises(ValueError, lambda: bool(obj == 0))\n        self.assertRaises(ValueError, lambda: bool(obj == 1))\n        self.assertRaises(ValueError, lambda: bool(obj))\n\n        obj = self._construct(shape=4, value=np.nan)\n        self.assertRaises(ValueError, lambda: bool(obj == 0))\n        self.assertRaises(ValueError, lambda: bool(obj == 1))\n        self.assertRaises(ValueError, lambda: bool(obj))\n\n        # empty\n        obj = self._construct(shape=0)\n        self.assertRaises(ValueError, lambda: bool(obj))\n\n        # invalid behaviors\n\n        obj1 = self._construct(shape=4, value=1)\n        obj2 = self._construct(shape=4, value=1)\n\n        def f():\n            if obj1:\n                printing.pprint_thing(\"this works and shouldn't\")\n\n        self.assertRaises(ValueError, f)\n        self.assertRaises(ValueError, lambda: obj1 and obj2)\n        self.assertRaises(ValueError, lambda: obj1 or obj2)\n        self.assertRaises(ValueError, lambda: not obj1)\n\n    def test_numpy_1_7_compat_numeric_methods(self):\n        # GH 4435\n        # numpy in 1.7 tries to pass addtional arguments to pandas functions\n\n        o = self._construct(shape=4)\n        for op in ['min', 'max', 'max', 'var', 'std', 'prod', 'sum', 'cumsum',\n                   'cumprod', 'median', 'skew', 'kurt', 'compound', 'cummax',\n                   'cummin', 'all', 'any']:\n            f = getattr(np, op, None)\n            if f is not None:\n                f(o)\n\n    def test_downcast(self):\n        # test close downcasting\n\n        o = self._construct(shape=4, value=9, dtype=np.int64)\n        result = o.copy()\n        result._data = o._data.downcast(dtypes='infer')\n        self._compare(result, o)\n\n        o = self._construct(shape=4, value=9.)\n        expected = o.astype(np.int64)\n        result = o.copy()\n        result._data = o._data.downcast(dtypes='infer')\n        self._compare(result, expected)\n\n        o = self._construct(shape=4, value=9.5)\n        result = o.copy()\n        result._data = o._data.downcast(dtypes='infer')\n        self._compare(result, o)\n\n        # are close\n        o = self._construct(shape=4, value=9.000000000005)\n        result = o.copy()\n        result._data = o._data.downcast(dtypes='infer')\n        expected = o.astype(np.int64)\n        self._compare(result, expected)\n\n    def test_constructor_compound_dtypes(self):\n        # GH 5191\n        # compound dtypes should raise not-implementederror\n\n        def f(dtype):\n            return self._construct(shape=3, dtype=dtype)\n\n        self.assertRaises(NotImplementedError, f, [(\"A\", \"datetime64[h]\"),\n                                                   (\"B\", \"str\"),\n                                                   (\"C\", \"int32\")])\n\n        # these work (though results may be unexpected)\n        f('int64')\n        f('float64')\n        f('M8[ns]')\n\n    def check_metadata(self, x, y=None):\n        for m in x._metadata:\n            v = getattr(x, m, None)\n            if y is None:\n                self.assertIsNone(v)\n            else:\n                self.assertEqual(v, getattr(y, m, None))\n\n    def test_metadata_propagation(self):\n        # check that the metadata matches up on the resulting ops\n\n        o = self._construct(shape=3)\n        o.name = 'foo'\n        o2 = self._construct(shape=3)\n        o2.name = 'bar'\n\n        # TODO\n        # Once panel can do non-trivial combine operations\n        # (currently there is an a raise in the Panel arith_ops to prevent\n        # this, though it actually does work)\n        # can remove all of these try: except: blocks on the actual operations\n\n        # ----------\n        # preserving\n        # ----------\n\n        # simple ops with scalars\n        for op in ['__add__', '__sub__', '__truediv__', '__mul__']:\n            result = getattr(o, op)(1)\n            self.check_metadata(o, result)\n\n        # ops with like\n        for op in ['__add__', '__sub__', '__truediv__', '__mul__']:\n            try:\n                result = getattr(o, op)(o)\n                self.check_metadata(o, result)\n            except (ValueError, AttributeError):\n                pass\n\n        # simple boolean\n        for op in ['__eq__', '__le__', '__ge__']:\n            v1 = getattr(o, op)(o)\n            self.check_metadata(o, v1)\n\n            try:\n                self.check_metadata(o, v1 & v1)\n            except (ValueError):\n                pass\n\n            try:\n                self.check_metadata(o, v1 | v1)\n            except (ValueError):\n                pass\n\n        # combine_first\n        try:\n            result = o.combine_first(o2)\n            self.check_metadata(o, result)\n        except (AttributeError):\n            pass\n\n        # ---------------------------\n        # non-preserving (by default)\n        # ---------------------------\n\n        # add non-like\n        try:\n            result = o + o2\n            self.check_metadata(result)\n        except (ValueError, AttributeError):\n            pass\n\n        # simple boolean\n        for op in ['__eq__', '__le__', '__ge__']:\n\n            # this is a name matching op\n            v1 = getattr(o, op)(o)\n\n            v2 = getattr(o, op)(o2)\n            self.check_metadata(v2)\n\n            try:\n                self.check_metadata(v1 & v2)\n            except (ValueError):\n                pass\n\n            try:\n                self.check_metadata(v1 | v2)\n            except (ValueError):\n                pass\n\n    def test_head_tail(self):\n        # GH5370\n\n        o = self._construct(shape=10)\n\n        # check all index types\n        for index in [tm.makeFloatIndex, tm.makeIntIndex, tm.makeStringIndex,\n                      tm.makeUnicodeIndex, tm.makeDateIndex,\n                      tm.makePeriodIndex]:\n            axis = o._get_axis_name(0)\n            setattr(o, axis, index(len(getattr(o, axis))))\n\n            # Panel + dims\n            try:\n                o.head()\n            except (NotImplementedError):\n                raise nose.SkipTest('not implemented on {0}'.format(\n                    o.__class__.__name__))\n\n            self._compare(o.head(), o.iloc[:5])\n            self._compare(o.tail(), o.iloc[-5:])\n\n            # 0-len\n            self._compare(o.head(0), o.iloc[0:0])\n            self._compare(o.tail(0), o.iloc[0:0])\n\n            # bounded\n            self._compare(o.head(len(o) + 1), o)\n            self._compare(o.tail(len(o) + 1), o)\n\n            # neg index\n            self._compare(o.head(-3), o.head(7))\n            self._compare(o.tail(-3), o.tail(7))\n\n    def test_sample(self):\n        # Fixes issue: 2419\n\n        o = self._construct(shape=10)\n\n        ###\n        # Check behavior of random_state argument\n        ###\n\n        # Check for stability when receives seed or random state -- run 10\n        # times.\n        for test in range(10):\n            seed = np.random.randint(0, 100)\n            self._compare(\n                o.sample(n=4, random_state=seed), o.sample(n=4,\n                                                           random_state=seed))\n            self._compare(\n                o.sample(frac=0.7, random_state=seed), o.sample(\n                    frac=0.7, random_state=seed))\n\n            self._compare(\n                o.sample(n=4, random_state=np.random.RandomState(test)),\n                o.sample(n=4, random_state=np.random.RandomState(test)))\n\n            self._compare(\n                o.sample(frac=0.7, random_state=np.random.RandomState(test)),\n                o.sample(frac=0.7, random_state=np.random.RandomState(test)))\n\n            os1, os2 = [], []\n            for _ in range(2):\n                np.random.seed(test)\n                os1.append(o.sample(n=4))\n                os2.append(o.sample(frac=0.7))\n            self._compare(*os1)\n            self._compare(*os2)\n\n        # Check for error when random_state argument invalid.\n        with tm.assertRaises(ValueError):\n            o.sample(random_state='astring!')\n\n        ###\n        # Check behavior of `frac` and `N`\n        ###\n\n        # Giving both frac and N throws error\n        with tm.assertRaises(ValueError):\n            o.sample(n=3, frac=0.3)\n\n        # Check that raises right error for negative lengths\n        with tm.assertRaises(ValueError):\n            o.sample(n=-3)\n        with tm.assertRaises(ValueError):\n            o.sample(frac=-0.3)\n\n        # Make sure float values of `n` give error\n        with tm.assertRaises(ValueError):\n            o.sample(n=3.2)\n\n        # Check lengths are right\n        self.assertTrue(len(o.sample(n=4) == 4))\n        self.assertTrue(len(o.sample(frac=0.34) == 3))\n        self.assertTrue(len(o.sample(frac=0.36) == 4))\n\n        ###\n        # Check weights\n        ###\n\n        # Weight length must be right\n        with tm.assertRaises(ValueError):\n            o.sample(n=3, weights=[0, 1])\n\n        with tm.assertRaises(ValueError):\n            bad_weights = [0.5] * 11\n            o.sample(n=3, weights=bad_weights)\n\n        with tm.assertRaises(ValueError):\n            bad_weight_series = Series([0, 0, 0.2])\n            o.sample(n=4, weights=bad_weight_series)\n\n        # Check won't accept negative weights\n        with tm.assertRaises(ValueError):\n            bad_weights = [-0.1] * 10\n            o.sample(n=3, weights=bad_weights)\n\n        # Check inf and -inf throw errors:\n        with tm.assertRaises(ValueError):\n            weights_with_inf = [0.1] * 10\n            weights_with_inf[0] = np.inf\n            o.sample(n=3, weights=weights_with_inf)\n\n        with tm.assertRaises(ValueError):\n            weights_with_ninf = [0.1] * 10\n            weights_with_ninf[0] = -np.inf\n            o.sample(n=3, weights=weights_with_ninf)\n\n        # All zeros raises errors\n        zero_weights = [0] * 10\n        with tm.assertRaises(ValueError):\n            o.sample(n=3, weights=zero_weights)\n\n        # All missing weights\n        nan_weights = [np.nan] * 10\n        with tm.assertRaises(ValueError):\n            o.sample(n=3, weights=nan_weights)\n\n        # Check np.nan are replaced by zeros.\n        weights_with_nan = [np.nan] * 10\n        weights_with_nan[5] = 0.5\n        self._compare(\n            o.sample(n=1, axis=0, weights=weights_with_nan), o.iloc[5:6])\n\n        # Check None are also replaced by zeros.\n        weights_with_None = [None] * 10\n        weights_with_None[5] = 0.5\n        self._compare(\n            o.sample(n=1, axis=0, weights=weights_with_None), o.iloc[5:6])\n\n    def test_size_compat(self):\n        # GH8846\n        # size property should be defined\n\n        o = self._construct(shape=10)\n        self.assertTrue(o.size == np.prod(o.shape))\n        self.assertTrue(o.size == 10 ** len(o.axes))\n\n    def test_split_compat(self):\n        # xref GH8846\n        o = self._construct(shape=10)\n        self.assertTrue(len(np.array_split(o, 5)) == 5)\n        self.assertTrue(len(np.array_split(o, 2)) == 2)\n\n    def test_unexpected_keyword(self):  # GH8597\n        df = DataFrame(np.random.randn(5, 2), columns=['jim', 'joe'])\n        ca = pd.Categorical([0, 0, 2, 2, 3, np.nan])\n        ts = df['joe'].copy()\n        ts[2] = np.nan\n\n        with assertRaisesRegexp(TypeError, 'unexpected keyword'):\n            df.drop('joe', axis=1, in_place=True)\n\n        with assertRaisesRegexp(TypeError, 'unexpected keyword'):\n            df.reindex([1, 0], inplace=True)\n\n        with assertRaisesRegexp(TypeError, 'unexpected keyword'):\n            ca.fillna(0, inplace=True)\n\n        with assertRaisesRegexp(TypeError, 'unexpected keyword'):\n            ts.fillna(0, in_place=True)\n\n    # See gh-12301\n    def test_stat_unexpected_keyword(self):\n        obj = self._construct(5)\n        starwars = 'Star Wars'\n        errmsg = 'unexpected keyword'\n\n        with assertRaisesRegexp(TypeError, errmsg):\n            obj.max(epic=starwars)  # stat_function\n        with assertRaisesRegexp(TypeError, errmsg):\n            obj.var(epic=starwars)  # stat_function_ddof\n        with assertRaisesRegexp(TypeError, errmsg):\n            obj.sum(epic=starwars)  # cum_function\n        with assertRaisesRegexp(TypeError, errmsg):\n            obj.any(epic=starwars)  # logical_function\n\n    def test_api_compat(self):\n\n        # GH 12021\n        # compat for __name__, __qualname__\n\n        obj = self._construct(5)\n        for func in ['sum', 'cumsum', 'any', 'var']:\n            f = getattr(obj, func)\n            self.assertEqual(f.__name__, func)\n            if PY3:\n                self.assertTrue(f.__qualname__.endswith(func))\n\n    def test_stat_non_defaults_args(self):\n        obj = self._construct(5)\n        out = np.array([0])\n        errmsg = \"the 'out' parameter is not supported\"\n\n        with assertRaisesRegexp(ValueError, errmsg):\n            obj.max(out=out)  # stat_function\n        with assertRaisesRegexp(ValueError, errmsg):\n            obj.var(out=out)  # stat_function_ddof\n        with assertRaisesRegexp(ValueError, errmsg):\n            obj.sum(out=out)  # cum_function\n        with assertRaisesRegexp(ValueError, errmsg):\n            obj.any(out=out)  # logical_function\n\n    def test_clip(self):\n        lower = 1\n        upper = 3\n        col = np.arange(5)\n\n        obj = self._construct(len(col), value=col)\n\n        if isinstance(obj, Panel):\n            msg = \"clip is not supported yet for panels\"\n            tm.assertRaisesRegexp(NotImplementedError, msg,\n                                  obj.clip, lower=lower,\n                                  upper=upper)\n\n        else:\n            out = obj.clip(lower=lower, upper=upper)\n            expected = self._construct(len(col), value=col\n                                       .clip(lower, upper))\n            self._compare(out, expected)\n\n            bad_axis = 'foo'\n            msg = ('No axis named {axis} '\n                   'for object').format(axis=bad_axis)\n            assertRaisesRegexp(ValueError, msg, obj.clip,\n                               lower=lower, upper=upper,\n                               axis=bad_axis)\n\n    def test_truncate_out_of_bounds(self):\n        # GH11382\n\n        # small\n        shape = [int(2e3)] + ([1] * (self._ndim - 1))\n        small = self._construct(shape, dtype='int8')\n        self._compare(small.truncate(), small)\n        self._compare(small.truncate(before=0, after=3e3), small)\n        self._compare(small.truncate(before=-1, after=2e3), small)\n\n        # big\n        shape = [int(2e6)] + ([1] * (self._ndim - 1))\n        big = self._construct(shape, dtype='int8')\n        self._compare(big.truncate(), big)\n        self._compare(big.truncate(before=0, after=3e6), big)\n        self._compare(big.truncate(before=-1, after=2e6), big)\n\n    def test_numpy_clip(self):\n        lower = 1\n        upper = 3\n        col = np.arange(5)\n\n        obj = self._construct(len(col), value=col)\n\n        if isinstance(obj, Panel):\n            msg = \"clip is not supported yet for panels\"\n            tm.assertRaisesRegexp(NotImplementedError, msg,\n                                  np.clip, obj,\n                                  lower, upper)\n        else:\n            out = np.clip(obj, lower, upper)\n            expected = self._construct(len(col), value=col\n                                       .clip(lower, upper))\n            self._compare(out, expected)\n\n            msg = \"the 'out' parameter is not supported\"\n            tm.assertRaisesRegexp(ValueError, msg,\n                                  np.clip, obj,\n                                  lower, upper, out=col)\n\n    def test_validate_bool_args(self):\n        df = DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n        invalid_values = [1, \"True\", [1, 2, 3], 5.0]\n\n        for value in invalid_values:\n            with self.assertRaises(ValueError):\n                super(DataFrame, df).rename_axis(mapper={'a': 'x', 'b': 'y'},\n                                                 axis=1, inplace=value)\n\n            with self.assertRaises(ValueError):\n                super(DataFrame, df).drop('a', axis=1, inplace=value)\n\n            with self.assertRaises(ValueError):\n                super(DataFrame, df).sort_index(inplace=value)\n\n            with self.assertRaises(ValueError):\n                super(DataFrame, df).consolidate(inplace=value)\n\n            with self.assertRaises(ValueError):\n                super(DataFrame, df).fillna(value=0, inplace=value)\n\n            with self.assertRaises(ValueError):\n                super(DataFrame, df).replace(to_replace=1, value=7,\n                                             inplace=value)\n\n            with self.assertRaises(ValueError):\n                super(DataFrame, df).interpolate(inplace=value)\n\n            with self.assertRaises(ValueError):\n                super(DataFrame, df)._where(cond=df.a > 2, inplace=value)\n\n            with self.assertRaises(ValueError):\n                super(DataFrame, df).mask(cond=df.a > 2, inplace=value)\n\n\nclass TestSeries(tm.TestCase, Generic):\n    _typ = Series\n    _comparator = lambda self, x, y: assert_series_equal(x, y)\n\n    def setUp(self):\n        self.ts = tm.makeTimeSeries()  # Was at top level in test_series\n        self.ts.name = 'ts'\n\n        self.series = tm.makeStringSeries()\n        self.series.name = 'series'\n\n    def test_rename_mi(self):\n        s = Series([11, 21, 31],\n                   index=MultiIndex.from_tuples(\n                       [(\"A\", x) for x in [\"a\", \"B\", \"c\"]]))\n        s.rename(str.lower)\n\n    def test_set_axis_name(self):\n        s = Series([1, 2, 3], index=['a', 'b', 'c'])\n        funcs = ['rename_axis', '_set_axis_name']\n        name = 'foo'\n        for func in funcs:\n            result = methodcaller(func, name)(s)\n            self.assertTrue(s.index.name is None)\n            self.assertEqual(result.index.name, name)\n\n    def test_set_axis_name_mi(self):\n        s = Series([11, 21, 31], index=MultiIndex.from_tuples(\n            [(\"A\", x) for x in [\"a\", \"B\", \"c\"]],\n            names=['l1', 'l2'])\n        )\n        funcs = ['rename_axis', '_set_axis_name']\n        for func in funcs:\n            result = methodcaller(func, ['L1', 'L2'])(s)\n            self.assertTrue(s.index.name is None)\n            self.assertEqual(s.index.names, ['l1', 'l2'])\n            self.assertTrue(result.index.name is None)\n            self.assertTrue(result.index.names, ['L1', 'L2'])\n\n    def test_set_axis_name_raises(self):\n        s = pd.Series([1])\n        with tm.assertRaises(ValueError):\n            s._set_axis_name(name='a', axis=1)\n\n    def test_get_numeric_data_preserve_dtype(self):\n\n        # get the numeric data\n        o = Series([1, 2, 3])\n        result = o._get_numeric_data()\n        self._compare(result, o)\n\n        o = Series([1, '2', 3.])\n        result = o._get_numeric_data()\n        expected = Series([], dtype=object, index=pd.Index([], dtype=object))\n        self._compare(result, expected)\n\n        o = Series([True, False, True])\n        result = o._get_numeric_data()\n        self._compare(result, o)\n\n        o = Series([True, False, True])\n        result = o._get_bool_data()\n        self._compare(result, o)\n\n        o = Series(date_range('20130101', periods=3))\n        result = o._get_numeric_data()\n        expected = Series([], dtype='M8[ns]', index=pd.Index([], dtype=object))\n        self._compare(result, expected)\n\n    def test_nonzero_single_element(self):\n\n        # allow single item via bool method\n        s = Series([True])\n        self.assertTrue(s.bool())\n\n        s = Series([False])\n        self.assertFalse(s.bool())\n\n        # single item nan to raise\n        for s in [Series([np.nan]), Series([pd.NaT]), Series([True]),\n                  Series([False])]:\n            self.assertRaises(ValueError, lambda: bool(s))\n\n        for s in [Series([np.nan]), Series([pd.NaT])]:\n            self.assertRaises(ValueError, lambda: s.bool())\n\n        # multiple bool are still an error\n        for s in [Series([True, True]), Series([False, False])]:\n            self.assertRaises(ValueError, lambda: bool(s))\n            self.assertRaises(ValueError, lambda: s.bool())\n\n        # single non-bool are an error\n        for s in [Series([1]), Series([0]), Series(['a']), Series([0.0])]:\n            self.assertRaises(ValueError, lambda: bool(s))\n            self.assertRaises(ValueError, lambda: s.bool())\n\n    def test_metadata_propagation_indiv(self):\n        # check that the metadata matches up on the resulting ops\n\n        o = Series(range(3), range(3))\n        o.name = 'foo'\n        o2 = Series(range(3), range(3))\n        o2.name = 'bar'\n\n        result = o.T\n        self.check_metadata(o, result)\n\n        # resample\n        ts = Series(np.random.rand(1000),\n                    index=date_range('20130101', periods=1000, freq='s'),\n                    name='foo')\n        result = ts.resample('1T').mean()\n        self.check_metadata(ts, result)\n\n        result = ts.resample('1T').min()\n        self.check_metadata(ts, result)\n\n        result = ts.resample('1T').apply(lambda x: x.sum())\n        self.check_metadata(ts, result)\n\n        _metadata = Series._metadata\n        _finalize = Series.__finalize__\n        Series._metadata = ['name', 'filename']\n        o.filename = 'foo'\n        o2.filename = 'bar'\n\n        def finalize(self, other, method=None, **kwargs):\n            for name in self._metadata:\n                if method == 'concat' and name == 'filename':\n                    value = '+'.join([getattr(\n                        o, name) for o in other.objs if getattr(o, name, None)\n                    ])\n                    object.__setattr__(self, name, value)\n                else:\n                    object.__setattr__(self, name, getattr(other, name, None))\n\n            return self\n\n        Series.__finalize__ = finalize\n\n        result = pd.concat([o, o2])\n        self.assertEqual(result.filename, 'foo+bar')\n        self.assertIsNone(result.name)\n\n        # reset\n        Series._metadata = _metadata\n        Series.__finalize__ = _finalize\n\n    def test_describe(self):\n        self.series.describe()\n        self.ts.describe()\n\n    def test_describe_objects(self):\n        s = Series(['a', 'b', 'b', np.nan, np.nan, np.nan, 'c', 'd', 'a', 'a'])\n        result = s.describe()\n        expected = Series({'count': 7, 'unique': 4,\n                           'top': 'a', 'freq': 3, 'second': 'b',\n                           'second_freq': 2}, index=result.index)\n        assert_series_equal(result, expected)\n\n        dt = list(self.ts.index)\n        dt.append(dt[0])\n        ser = Series(dt)\n        rs = ser.describe()\n        min_date = min(dt)\n        max_date = max(dt)\n        xp = Series({'count': len(dt),\n                     'unique': len(self.ts.index),\n                     'first': min_date, 'last': max_date, 'freq': 2,\n                     'top': min_date}, index=rs.index)\n        assert_series_equal(rs, xp)\n\n    def test_describe_empty(self):\n        result = pd.Series().describe()\n\n        self.assertEqual(result['count'], 0)\n        self.assertTrue(result.drop('count').isnull().all())\n\n        nanSeries = Series([np.nan])\n        nanSeries.name = 'NaN'\n        result = nanSeries.describe()\n        self.assertEqual(result['count'], 0)\n        self.assertTrue(result.drop('count').isnull().all())\n\n    def test_describe_none(self):\n        noneSeries = Series([None])\n        noneSeries.name = 'None'\n        expected = Series([0, 0], index=['count', 'unique'], name='None')\n        assert_series_equal(noneSeries.describe(), expected)\n\n    def test_to_xarray(self):\n\n        tm._skip_if_no_xarray()\n        import xarray\n        from xarray import DataArray\n\n        s = Series([])\n        s.index.name = 'foo'\n        result = s.to_xarray()\n        self.assertEqual(len(result), 0)\n        self.assertEqual(len(result.coords), 1)\n        assert_almost_equal(list(result.coords.keys()), ['foo'])\n        self.assertIsInstance(result, DataArray)\n\n        def testit(index, check_index_type=True, check_categorical=True):\n            s = Series(range(6), index=index(6))\n            s.index.name = 'foo'\n            result = s.to_xarray()\n            repr(result)\n            self.assertEqual(len(result), 6)\n            self.assertEqual(len(result.coords), 1)\n            assert_almost_equal(list(result.coords.keys()), ['foo'])\n            self.assertIsInstance(result, DataArray)\n\n            # idempotency\n            assert_series_equal(result.to_series(), s,\n                                check_index_type=check_index_type,\n                                check_categorical=check_categorical)\n\n        l = [tm.makeFloatIndex, tm.makeIntIndex,\n             tm.makeStringIndex, tm.makeUnicodeIndex,\n             tm.makeDateIndex, tm.makePeriodIndex,\n             tm.makeTimedeltaIndex]\n\n        if LooseVersion(xarray.__version__) >= '0.8.0':\n            l.append(tm.makeCategoricalIndex)\n\n        for index in l:\n            testit(index)\n\n        s = Series(range(6))\n        s.index.name = 'foo'\n        s.index = pd.MultiIndex.from_product([['a', 'b'], range(3)],\n                                             names=['one', 'two'])\n        result = s.to_xarray()\n        self.assertEqual(len(result), 2)\n        assert_almost_equal(list(result.coords.keys()), ['one', 'two'])\n        self.assertIsInstance(result, DataArray)\n        assert_series_equal(result.to_series(), s)\n\n\nclass TestDataFrame(tm.TestCase, Generic):\n    _typ = DataFrame\n    _comparator = lambda self, x, y: assert_frame_equal(x, y)\n\n    def test_rename_mi(self):\n        df = DataFrame([\n            11, 21, 31\n        ], index=MultiIndex.from_tuples([(\"A\", x) for x in [\"a\", \"B\", \"c\"]]))\n        df.rename(str.lower)\n\n    def test_set_axis_name(self):\n        df = pd.DataFrame([[1, 2], [3, 4]])\n        funcs = ['_set_axis_name', 'rename_axis']\n        for func in funcs:\n            result = methodcaller(func, 'foo')(df)\n            self.assertTrue(df.index.name is None)\n            self.assertEqual(result.index.name, 'foo')\n\n            result = methodcaller(func, 'cols', axis=1)(df)\n            self.assertTrue(df.columns.name is None)\n            self.assertEqual(result.columns.name, 'cols')\n\n    def test_set_axis_name_mi(self):\n        df = DataFrame(\n            np.empty((3, 3)),\n            index=MultiIndex.from_tuples([(\"A\", x) for x in list('aBc')]),\n            columns=MultiIndex.from_tuples([('C', x) for x in list('xyz')])\n        )\n\n        level_names = ['L1', 'L2']\n        funcs = ['_set_axis_name', 'rename_axis']\n        for func in funcs:\n            result = methodcaller(func, level_names)(df)\n            self.assertEqual(result.index.names, level_names)\n            self.assertEqual(result.columns.names, [None, None])\n\n            result = methodcaller(func, level_names, axis=1)(df)\n            self.assertEqual(result.columns.names, [\"L1\", \"L2\"])\n            self.assertEqual(result.index.names, [None, None])\n\n    def test_nonzero_single_element(self):\n\n        # allow single item via bool method\n        df = DataFrame([[True]])\n        self.assertTrue(df.bool())\n\n        df = DataFrame([[False]])\n        self.assertFalse(df.bool())\n\n        df = DataFrame([[False, False]])\n        self.assertRaises(ValueError, lambda: df.bool())\n        self.assertRaises(ValueError, lambda: bool(df))\n\n    def test_get_numeric_data_preserve_dtype(self):\n\n        # get the numeric data\n        o = DataFrame({'A': [1, '2', 3.]})\n        result = o._get_numeric_data()\n        expected = DataFrame(index=[0, 1, 2], dtype=object)\n        self._compare(result, expected)\n\n    def test_describe(self):\n        tm.makeDataFrame().describe()\n        tm.makeMixedDataFrame().describe()\n        tm.makeTimeDataFrame().describe()\n\n    def test_describe_percentiles_percent_or_raw(self):\n        msg = 'percentiles should all be in the interval \\\\[0, 1\\\\]'\n\n        df = tm.makeDataFrame()\n        with tm.assertRaisesRegexp(ValueError, msg):\n            df.describe(percentiles=[10, 50, 100])\n\n        with tm.assertRaisesRegexp(ValueError, msg):\n            df.describe(percentiles=[2])\n\n        with tm.assertRaisesRegexp(ValueError, msg):\n            df.describe(percentiles=[-2])\n\n    def test_describe_percentiles_equivalence(self):\n        df = tm.makeDataFrame()\n        d1 = df.describe()\n        d2 = df.describe(percentiles=[.25, .75])\n        assert_frame_equal(d1, d2)\n\n    def test_describe_percentiles_insert_median(self):\n        df = tm.makeDataFrame()\n        d1 = df.describe(percentiles=[.25, .75])\n        d2 = df.describe(percentiles=[.25, .5, .75])\n        assert_frame_equal(d1, d2)\n        self.assertTrue('25%' in d1.index)\n        self.assertTrue('75%' in d2.index)\n\n        # none above\n        d1 = df.describe(percentiles=[.25, .45])\n        d2 = df.describe(percentiles=[.25, .45, .5])\n        assert_frame_equal(d1, d2)\n        self.assertTrue('25%' in d1.index)\n        self.assertTrue('45%' in d2.index)\n\n        # none below\n        d1 = df.describe(percentiles=[.75, 1])\n        d2 = df.describe(percentiles=[.5, .75, 1])\n        assert_frame_equal(d1, d2)\n        self.assertTrue('75%' in d1.index)\n        self.assertTrue('100%' in d2.index)\n\n        # edge\n        d1 = df.describe(percentiles=[0, 1])\n        d2 = df.describe(percentiles=[0, .5, 1])\n        assert_frame_equal(d1, d2)\n        self.assertTrue('0%' in d1.index)\n        self.assertTrue('100%' in d2.index)\n\n    def test_describe_percentiles_insert_median_ndarray(self):\n        # GH14908\n        df = tm.makeDataFrame()\n        result = df.describe(percentiles=np.array([.25, .75]))\n        expected = df.describe(percentiles=[.25, .75])\n        assert_frame_equal(result, expected)\n\n    def test_describe_percentiles_unique(self):\n        # GH13104\n        df = tm.makeDataFrame()\n        with self.assertRaises(ValueError):\n            df.describe(percentiles=[0.1, 0.2, 0.4, 0.5, 0.2, 0.6])\n        with self.assertRaises(ValueError):\n            df.describe(percentiles=[0.1, 0.2, 0.4, 0.2, 0.6])\n\n    def test_describe_percentiles_formatting(self):\n        # GH13104\n        df = tm.makeDataFrame()\n\n        # default\n        result = df.describe().index\n        expected = Index(['count', 'mean', 'std', 'min', '25%', '50%', '75%',\n                          'max'],\n                         dtype='object')\n        tm.assert_index_equal(result, expected)\n\n        result = df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999,\n                                          0.9995, 0.9999]).index\n        expected = Index(['count', 'mean', 'std', 'min', '0.01%', '0.05%',\n                          '0.1%', '50%', '99.9%', '99.95%', '99.99%', 'max'],\n                         dtype='object')\n        tm.assert_index_equal(result, expected)\n\n        result = df.describe(percentiles=[0.00499, 0.005, 0.25, 0.50,\n                                          0.75]).index\n        expected = Index(['count', 'mean', 'std', 'min', '0.499%', '0.5%',\n                          '25%', '50%', '75%', 'max'],\n                         dtype='object')\n        tm.assert_index_equal(result, expected)\n\n        result = df.describe(percentiles=[0.00499, 0.01001, 0.25, 0.50,\n                                          0.75]).index\n        expected = Index(['count', 'mean', 'std', 'min', '0.5%', '1.0%',\n                          '25%', '50%', '75%', 'max'],\n                         dtype='object')\n        tm.assert_index_equal(result, expected)\n\n    def test_describe_column_index_type(self):\n        # GH13288\n        df = pd.DataFrame([1, 2, 3, 4])\n        df.columns = pd.Index([0], dtype=object)\n        result = df.describe().columns\n        expected = Index([0], dtype=object)\n        tm.assert_index_equal(result, expected)\n\n        df = pd.DataFrame({'A': list(\"BCDE\"), 0: [1, 2, 3, 4]})\n        result = df.describe().columns\n        expected = Index([0], dtype=object)\n        tm.assert_index_equal(result, expected)\n\n    def test_describe_no_numeric(self):\n        df = DataFrame({'A': ['foo', 'foo', 'bar'] * 8,\n                        'B': ['a', 'b', 'c', 'd'] * 6})\n        desc = df.describe()\n        expected = DataFrame(dict((k, v.describe())\n                                  for k, v in compat.iteritems(df)),\n                             columns=df.columns)\n        assert_frame_equal(desc, expected)\n\n        ts = tm.makeTimeSeries()\n        df = DataFrame({'time': ts.index})\n        desc = df.describe()\n        self.assertEqual(desc.time['first'], min(ts.index))\n\n    def test_describe_empty(self):\n        df = DataFrame()\n        tm.assertRaisesRegexp(ValueError, 'DataFrame without columns',\n                              df.describe)\n\n        df = DataFrame(columns=['A', 'B'])\n        result = df.describe()\n        expected = DataFrame(0, columns=['A', 'B'], index=['count', 'unique'])\n        tm.assert_frame_equal(result, expected)\n\n    def test_describe_empty_int_columns(self):\n        df = DataFrame([[0, 1], [1, 2]])\n        desc = df[df[0] < 0].describe()  # works\n        assert_series_equal(desc.xs('count'),\n                            Series([0, 0], dtype=float, name='count'))\n        self.assertTrue(isnull(desc.iloc[1:]).all().all())\n\n    def test_describe_objects(self):\n        df = DataFrame({\"C1\": ['a', 'a', 'c'], \"C2\": ['d', 'd', 'f']})\n        result = df.describe()\n        expected = DataFrame({\"C1\": [3, 2, 'a', 2], \"C2\": [3, 2, 'd', 2]},\n                             index=['count', 'unique', 'top', 'freq'])\n        assert_frame_equal(result, expected)\n\n        df = DataFrame({\"C1\": pd.date_range('2010-01-01', periods=4, freq='D')\n                        })\n        df.loc[4] = pd.Timestamp('2010-01-04')\n        result = df.describe()\n        expected = DataFrame({\"C1\": [5, 4, pd.Timestamp('2010-01-04'), 2,\n                                     pd.Timestamp('2010-01-01'),\n                                     pd.Timestamp('2010-01-04')]},\n                             index=['count', 'unique', 'top', 'freq',\n                                    'first', 'last'])\n        assert_frame_equal(result, expected)\n\n        # mix time and str\n        df['C2'] = ['a', 'a', 'b', 'c', 'a']\n        result = df.describe()\n        expected['C2'] = [5, 3, 'a', 3, np.nan, np.nan]\n        assert_frame_equal(result, expected)\n\n        # just str\n        expected = DataFrame({'C2': [5, 3, 'a', 4]},\n                             index=['count', 'unique', 'top', 'freq'])\n        result = df[['C2']].describe()\n\n        # mix of time, str, numeric\n        df['C3'] = [2, 4, 6, 8, 2]\n        result = df.describe()\n        expected = DataFrame({\"C3\": [5., 4.4, 2.607681, 2., 2., 4., 6., 8.]},\n                             index=['count', 'mean', 'std', 'min', '25%',\n                                    '50%', '75%', 'max'])\n        assert_frame_equal(result, expected)\n        assert_frame_equal(df.describe(), df[['C3']].describe())\n\n        assert_frame_equal(df[['C1', 'C3']].describe(), df[['C3']].describe())\n        assert_frame_equal(df[['C2', 'C3']].describe(), df[['C3']].describe())\n\n    def test_describe_typefiltering(self):\n        df = DataFrame({'catA': ['foo', 'foo', 'bar'] * 8,\n                        'catB': ['a', 'b', 'c', 'd'] * 6,\n                        'numC': np.arange(24, dtype='int64'),\n                        'numD': np.arange(24.) + .5,\n                        'ts': tm.makeTimeSeries()[:24].index})\n\n        descN = df.describe()\n        expected_cols = ['numC', 'numD', ]\n        expected = DataFrame(dict((k, df[k].describe())\n                                  for k in expected_cols),\n                             columns=expected_cols)\n        assert_frame_equal(descN, expected)\n\n        desc = df.describe(include=['number'])\n        assert_frame_equal(desc, descN)\n        desc = df.describe(exclude=['object', 'datetime'])\n        assert_frame_equal(desc, descN)\n        desc = df.describe(include=['float'])\n        assert_frame_equal(desc, descN.drop('numC', 1))\n\n        descC = df.describe(include=['O'])\n        expected_cols = ['catA', 'catB']\n        expected = DataFrame(dict((k, df[k].describe())\n                                  for k in expected_cols),\n                             columns=expected_cols)\n        assert_frame_equal(descC, expected)\n\n        descD = df.describe(include=['datetime'])\n        assert_series_equal(descD.ts, df.ts.describe())\n\n        desc = df.describe(include=['object', 'number', 'datetime'])\n        assert_frame_equal(desc.loc[:, [\"numC\", \"numD\"]].dropna(), descN)\n        assert_frame_equal(desc.loc[:, [\"catA\", \"catB\"]].dropna(), descC)\n        descDs = descD.sort_index()  # the index order change for mixed-types\n        assert_frame_equal(desc.loc[:, \"ts\":].dropna().sort_index(), descDs)\n\n        desc = df.loc[:, 'catA':'catB'].describe(include='all')\n        assert_frame_equal(desc, descC)\n        desc = df.loc[:, 'numC':'numD'].describe(include='all')\n        assert_frame_equal(desc, descN)\n\n        desc = df.describe(percentiles=[], include='all')\n        cnt = Series(data=[4, 4, 6, 6, 6],\n                     index=['catA', 'catB', 'numC', 'numD', 'ts'])\n        assert_series_equal(desc.count(), cnt)\n        self.assertTrue('count' in desc.index)\n        self.assertTrue('unique' in desc.index)\n        self.assertTrue('50%' in desc.index)\n        self.assertTrue('first' in desc.index)\n\n        desc = df.drop(\"ts\", 1).describe(percentiles=[], include='all')\n        assert_series_equal(desc.count(), cnt.drop(\"ts\"))\n        self.assertTrue('first' not in desc.index)\n        desc = df.drop([\"numC\", \"numD\"], 1).describe(percentiles=[],\n                                                     include='all')\n        assert_series_equal(desc.count(), cnt.drop([\"numC\", \"numD\"]))\n        self.assertTrue('50%' not in desc.index)\n\n    def test_describe_typefiltering_category_bool(self):\n        df = DataFrame({'A_cat': pd.Categorical(['foo', 'foo', 'bar'] * 8),\n                        'B_str': ['a', 'b', 'c', 'd'] * 6,\n                        'C_bool': [True] * 12 + [False] * 12,\n                        'D_num': np.arange(24.) + .5,\n                        'E_ts': tm.makeTimeSeries()[:24].index})\n\n        desc = df.describe()\n        expected_cols = ['D_num']\n        expected = DataFrame(dict((k, df[k].describe())\n                                  for k in expected_cols),\n                             columns=expected_cols)\n        assert_frame_equal(desc, expected)\n\n        desc = df.describe(include=[\"category\"])\n        self.assertTrue(desc.columns.tolist() == [\"A_cat\"])\n\n        # 'all' includes numpy-dtypes + category\n        desc1 = df.describe(include=\"all\")\n        desc2 = df.describe(include=[np.generic, \"category\"])\n        assert_frame_equal(desc1, desc2)\n\n    def test_describe_timedelta(self):\n        df = DataFrame({\"td\": pd.to_timedelta(np.arange(24) % 20, \"D\")})\n        self.assertTrue(df.describe().loc[\"mean\"][0] == pd.to_timedelta(\n            \"8d4h\"))\n\n    def test_describe_typefiltering_dupcol(self):\n        df = DataFrame({'catA': ['foo', 'foo', 'bar'] * 8,\n                        'catB': ['a', 'b', 'c', 'd'] * 6,\n                        'numC': np.arange(24),\n                        'numD': np.arange(24.) + .5,\n                        'ts': tm.makeTimeSeries()[:24].index})\n        s = df.describe(include='all').shape[1]\n        df = pd.concat([df, df], axis=1)\n        s2 = df.describe(include='all').shape[1]\n        self.assertTrue(s2 == 2 * s)\n\n    def test_describe_typefiltering_groupby(self):\n        df = DataFrame({'catA': ['foo', 'foo', 'bar'] * 8,\n                        'catB': ['a', 'b', 'c', 'd'] * 6,\n                        'numC': np.arange(24),\n                        'numD': np.arange(24.) + .5,\n                        'ts': tm.makeTimeSeries()[:24].index})\n        G = df.groupby('catA')\n        self.assertTrue(G.describe(include=['number']).shape == (2, 16))\n        self.assertTrue(G.describe(include=['number', 'object']).shape == (2,\n                                                                           33))\n        self.assertTrue(G.describe(include='all').shape == (2, 52))\n\n    def test_describe_multi_index_df_column_names(self):\n        \"\"\" Test that column names persist after the describe operation.\"\"\"\n\n        df = pd.DataFrame(\n            {'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n             'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n             'C': np.random.randn(8),\n             'D': np.random.randn(8)})\n\n        # GH 11517\n        # test for hierarchical index\n        hierarchical_index_df = df.groupby(['A', 'B']).mean().T\n        self.assertTrue(hierarchical_index_df.columns.names == ['A', 'B'])\n        self.assertTrue(hierarchical_index_df.describe().columns.names ==\n                        ['A', 'B'])\n\n        # test for non-hierarchical index\n        non_hierarchical_index_df = df.groupby(['A']).mean().T\n        self.assertTrue(non_hierarchical_index_df.columns.names == ['A'])\n        self.assertTrue(non_hierarchical_index_df.describe().columns.names ==\n                        ['A'])\n\n    def test_metadata_propagation_indiv(self):\n\n        # groupby\n        df = DataFrame(\n            {'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n             'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n             'C': np.random.randn(8),\n             'D': np.random.randn(8)})\n        result = df.groupby('A').sum()\n        self.check_metadata(df, result)\n\n        # resample\n        df = DataFrame(np.random.randn(1000, 2),\n                       index=date_range('20130101', periods=1000, freq='s'))\n        result = df.resample('1T')\n        self.check_metadata(df, result)\n\n        # merging with override\n        # GH 6923\n        _metadata = DataFrame._metadata\n        _finalize = DataFrame.__finalize__\n\n        np.random.seed(10)\n        df1 = DataFrame(np.random.randint(0, 4, (3, 2)), columns=['a', 'b'])\n        df2 = DataFrame(np.random.randint(0, 4, (3, 2)), columns=['c', 'd'])\n        DataFrame._metadata = ['filename']\n        df1.filename = 'fname1.csv'\n        df2.filename = 'fname2.csv'\n\n        def finalize(self, other, method=None, **kwargs):\n\n            for name in self._metadata:\n                if method == 'merge':\n                    left, right = other.left, other.right\n                    value = getattr(left, name, '') + '|' + getattr(right,\n                                                                    name, '')\n                    object.__setattr__(self, name, value)\n                else:\n                    object.__setattr__(self, name, getattr(other, name, ''))\n\n            return self\n\n        DataFrame.__finalize__ = finalize\n        result = df1.merge(df2, left_on=['a'], right_on=['c'], how='inner')\n        self.assertEqual(result.filename, 'fname1.csv|fname2.csv')\n\n        # concat\n        # GH 6927\n        DataFrame._metadata = ['filename']\n        df1 = DataFrame(np.random.randint(0, 4, (3, 2)), columns=list('ab'))\n        df1.filename = 'foo'\n\n        def finalize(self, other, method=None, **kwargs):\n            for name in self._metadata:\n                if method == 'concat':\n                    value = '+'.join([getattr(\n                        o, name) for o in other.objs if getattr(o, name, None)\n                    ])\n                    object.__setattr__(self, name, value)\n                else:\n                    object.__setattr__(self, name, getattr(other, name, None))\n\n            return self\n\n        DataFrame.__finalize__ = finalize\n\n        result = pd.concat([df1, df1])\n        self.assertEqual(result.filename, 'foo+foo')\n\n        # reset\n        DataFrame._metadata = _metadata\n        DataFrame.__finalize__ = _finalize\n\n    def test_tz_convert_and_localize(self):\n        l0 = date_range('20140701', periods=5, freq='D')\n\n        # TODO: l1 should be a PeriodIndex for testing\n        #       after GH2106 is addressed\n        with tm.assertRaises(NotImplementedError):\n            period_range('20140701', periods=1).tz_convert('UTC')\n        with tm.assertRaises(NotImplementedError):\n            period_range('20140701', periods=1).tz_localize('UTC')\n        # l1 = period_range('20140701', periods=5, freq='D')\n        l1 = date_range('20140701', periods=5, freq='D')\n\n        int_idx = Index(range(5))\n\n        for fn in ['tz_localize', 'tz_convert']:\n\n            if fn == 'tz_convert':\n                l0 = l0.tz_localize('UTC')\n                l1 = l1.tz_localize('UTC')\n\n            for idx in [l0, l1]:\n\n                l0_expected = getattr(idx, fn)('US/Pacific')\n                l1_expected = getattr(idx, fn)('US/Pacific')\n\n                df1 = DataFrame(np.ones(5), index=l0)\n                df1 = getattr(df1, fn)('US/Pacific')\n                self.assert_index_equal(df1.index, l0_expected)\n\n                # MultiIndex\n                # GH7846\n                df2 = DataFrame(np.ones(5), MultiIndex.from_arrays([l0, l1]))\n\n                df3 = getattr(df2, fn)('US/Pacific', level=0)\n                self.assertFalse(df3.index.levels[0].equals(l0))\n                self.assert_index_equal(df3.index.levels[0], l0_expected)\n                self.assert_index_equal(df3.index.levels[1], l1)\n                self.assertFalse(df3.index.levels[1].equals(l1_expected))\n\n                df3 = getattr(df2, fn)('US/Pacific', level=1)\n                self.assert_index_equal(df3.index.levels[0], l0)\n                self.assertFalse(df3.index.levels[0].equals(l0_expected))\n                self.assert_index_equal(df3.index.levels[1], l1_expected)\n                self.assertFalse(df3.index.levels[1].equals(l1))\n\n                df4 = DataFrame(np.ones(5),\n                                MultiIndex.from_arrays([int_idx, l0]))\n\n                # TODO: untested\n                df5 = getattr(df4, fn)('US/Pacific', level=1)  # noqa\n\n                self.assert_index_equal(df3.index.levels[0], l0)\n                self.assertFalse(df3.index.levels[0].equals(l0_expected))\n                self.assert_index_equal(df3.index.levels[1], l1_expected)\n                self.assertFalse(df3.index.levels[1].equals(l1))\n\n        # Bad Inputs\n        for fn in ['tz_localize', 'tz_convert']:\n            # Not DatetimeIndex / PeriodIndex\n            with tm.assertRaisesRegexp(TypeError, 'DatetimeIndex'):\n                df = DataFrame(index=int_idx)\n                df = getattr(df, fn)('US/Pacific')\n\n            # Not DatetimeIndex / PeriodIndex\n            with tm.assertRaisesRegexp(TypeError, 'DatetimeIndex'):\n                df = DataFrame(np.ones(5),\n                               MultiIndex.from_arrays([int_idx, l0]))\n                df = getattr(df, fn)('US/Pacific', level=0)\n\n            # Invalid level\n            with tm.assertRaisesRegexp(ValueError, 'not valid'):\n                df = DataFrame(index=l0)\n                df = getattr(df, fn)('US/Pacific', level=1)\n\n    def test_set_attribute(self):\n        # Test for consistent setattr behavior when an attribute and a column\n        # have the same name (Issue #8994)\n        df = DataFrame({'x': [1, 2, 3]})\n\n        df.y = 2\n        df['y'] = [2, 4, 6]\n        df.y = 5\n\n        self.assertEqual(df.y, 5)\n        assert_series_equal(df['y'], Series([2, 4, 6], name='y'))\n\n    def test_pct_change(self):\n        # GH 11150\n        pnl = DataFrame([np.arange(0, 40, 10), np.arange(0, 40, 10), np.arange(\n            0, 40, 10)]).astype(np.float64)\n        pnl.iat[1, 0] = np.nan\n        pnl.iat[1, 1] = np.nan\n        pnl.iat[2, 3] = 60\n\n        mask = pnl.isnull()\n\n        for axis in range(2):\n            expected = pnl.ffill(axis=axis) / pnl.ffill(axis=axis).shift(\n                axis=axis) - 1\n            expected[mask] = np.nan\n            result = pnl.pct_change(axis=axis, fill_method='pad')\n\n            self.assert_frame_equal(result, expected)\n\n    def test_to_xarray(self):\n\n        tm._skip_if_no_xarray()\n        from xarray import Dataset\n\n        df = DataFrame({'a': list('abc'),\n                        'b': list(range(1, 4)),\n                        'c': np.arange(3, 6).astype('u1'),\n                        'd': np.arange(4.0, 7.0, dtype='float64'),\n                        'e': [True, False, True],\n                        'f': pd.Categorical(list('abc')),\n                        'g': pd.date_range('20130101', periods=3),\n                        'h': pd.date_range('20130101',\n                                           periods=3,\n                                           tz='US/Eastern')}\n                       )\n\n        df.index.name = 'foo'\n        result = df[0:0].to_xarray()\n        self.assertEqual(result.dims['foo'], 0)\n        self.assertIsInstance(result, Dataset)\n\n        for index in [tm.makeFloatIndex, tm.makeIntIndex,\n                      tm.makeStringIndex, tm.makeUnicodeIndex,\n                      tm.makeDateIndex, tm.makePeriodIndex,\n                      tm.makeCategoricalIndex, tm.makeTimedeltaIndex]:\n            df.index = index(3)\n            df.index.name = 'foo'\n            df.columns.name = 'bar'\n            result = df.to_xarray()\n            self.assertEqual(result.dims['foo'], 3)\n            self.assertEqual(len(result.coords), 1)\n            self.assertEqual(len(result.data_vars), 8)\n            assert_almost_equal(list(result.coords.keys()), ['foo'])\n            self.assertIsInstance(result, Dataset)\n\n            # idempotency\n            # categoricals are not preserved\n            # datetimes w/tz are not preserved\n            # column names are lost\n            expected = df.copy()\n            expected['f'] = expected['f'].astype(object)\n            expected['h'] = expected['h'].astype('datetime64[ns]')\n            expected.columns.name = None\n            assert_frame_equal(result.to_dataframe(), expected,\n                               check_index_type=False, check_categorical=False)\n\n        # available in 0.7.1\n        # MultiIndex\n        df.index = pd.MultiIndex.from_product([['a'], range(3)],\n                                              names=['one', 'two'])\n        result = df.to_xarray()\n        self.assertEqual(result.dims['one'], 1)\n        self.assertEqual(result.dims['two'], 3)\n        self.assertEqual(len(result.coords), 2)\n        self.assertEqual(len(result.data_vars), 8)\n        assert_almost_equal(list(result.coords.keys()), ['one', 'two'])\n        self.assertIsInstance(result, Dataset)\n\n        result = result.to_dataframe()\n        expected = df.copy()\n        expected['f'] = expected['f'].astype(object)\n        expected['h'] = expected['h'].astype('datetime64[ns]')\n        expected.columns.name = None\n        assert_frame_equal(result,\n                           expected,\n                           check_index_type=False)\n\n\nclass TestPanel(tm.TestCase, Generic):\n    _typ = Panel\n    _comparator = lambda self, x, y: assert_panel_equal(x, y, by_blocks=True)\n\n    def test_to_xarray(self):\n\n        tm._skip_if_no_xarray()\n        from xarray import DataArray\n\n        p = tm.makePanel()\n\n        result = p.to_xarray()\n        self.assertIsInstance(result, DataArray)\n        self.assertEqual(len(result.coords), 3)\n        assert_almost_equal(list(result.coords.keys()),\n                            ['items', 'major_axis', 'minor_axis'])\n        self.assertEqual(len(result.dims), 3)\n\n        # idempotency\n        assert_panel_equal(result.to_pandas(), p)\n\n\nclass TestPanel4D(tm.TestCase, Generic):\n    _typ = Panel4D\n    _comparator = lambda self, x, y: assert_panel4d_equal(x, y, by_blocks=True)\n\n    def test_sample(self):\n        raise nose.SkipTest(\"sample on Panel4D\")\n\n    def test_to_xarray(self):\n\n        tm._skip_if_no_xarray()\n        from xarray import DataArray\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            p = tm.makePanel4D()\n\n            result = p.to_xarray()\n            self.assertIsInstance(result, DataArray)\n            self.assertEqual(len(result.coords), 4)\n            assert_almost_equal(list(result.coords.keys()),\n                                ['labels', 'items', 'major_axis',\n                                 'minor_axis'])\n            self.assertEqual(len(result.dims), 4)\n\n            # non-convertible\n            self.assertRaises(ValueError, lambda: result.to_pandas())\n\n# run all the tests, but wrap each in a warning catcher\nfor t in ['test_rename', 'test_rename_axis', 'test_get_numeric_data',\n          'test_get_default', 'test_nonzero',\n          'test_numpy_1_7_compat_numeric_methods',\n          'test_downcast', 'test_constructor_compound_dtypes',\n          'test_head_tail',\n          'test_size_compat', 'test_split_compat',\n          'test_unexpected_keyword',\n          'test_stat_unexpected_keyword', 'test_api_compat',\n          'test_stat_non_defaults_args',\n          'test_clip', 'test_truncate_out_of_bounds', 'test_numpy_clip',\n          'test_metadata_propagation']:\n\n    def f():\n        def tester(self):\n            with tm.assert_produces_warning(FutureWarning,\n                                            check_stacklevel=False):\n                return getattr(super(TestPanel4D, self), t)()\n        return tester\n\n    setattr(TestPanel4D, t, f())\n\n\nclass TestNDFrame(tm.TestCase):\n    # tests that don't fit elsewhere\n\n    def test_sample(sel):\n        # Fixes issue: 2419\n        # additional specific object based tests\n\n        # A few dataframe test with degenerate weights.\n        easy_weight_list = [0] * 10\n        easy_weight_list[5] = 1\n\n        df = pd.DataFrame({'col1': range(10, 20),\n                           'col2': range(20, 30),\n                           'colString': ['a'] * 10,\n                           'easyweights': easy_weight_list})\n        sample1 = df.sample(n=1, weights='easyweights')\n        assert_frame_equal(sample1, df.iloc[5:6])\n\n        # Ensure proper error if string given as weight for Series, panel, or\n        # DataFrame with axis = 1.\n        s = Series(range(10))\n        with tm.assertRaises(ValueError):\n            s.sample(n=3, weights='weight_column')\n\n        panel = pd.Panel(items=[0, 1, 2], major_axis=[2, 3, 4],\n                         minor_axis=[3, 4, 5])\n        with tm.assertRaises(ValueError):\n            panel.sample(n=1, weights='weight_column')\n\n        with tm.assertRaises(ValueError):\n            df.sample(n=1, weights='weight_column', axis=1)\n\n        # Check weighting key error\n        with tm.assertRaises(KeyError):\n            df.sample(n=3, weights='not_a_real_column_name')\n\n        # Check that re-normalizes weights that don't sum to one.\n        weights_less_than_1 = [0] * 10\n        weights_less_than_1[0] = 0.5\n        tm.assert_frame_equal(\n            df.sample(n=1, weights=weights_less_than_1), df.iloc[:1])\n\n        ###\n        # Test axis argument\n        ###\n\n        # Test axis argument\n        df = pd.DataFrame({'col1': range(10), 'col2': ['a'] * 10})\n        second_column_weight = [0, 1]\n        assert_frame_equal(\n            df.sample(n=1, axis=1, weights=second_column_weight), df[['col2']])\n\n        # Different axis arg types\n        assert_frame_equal(df.sample(n=1, axis='columns',\n                                     weights=second_column_weight),\n                           df[['col2']])\n\n        weight = [0] * 10\n        weight[5] = 0.5\n        assert_frame_equal(df.sample(n=1, axis='rows', weights=weight),\n                           df.iloc[5:6])\n        assert_frame_equal(df.sample(n=1, axis='index', weights=weight),\n                           df.iloc[5:6])\n\n        # Check out of range axis values\n        with tm.assertRaises(ValueError):\n            df.sample(n=1, axis=2)\n\n        with tm.assertRaises(ValueError):\n            df.sample(n=1, axis='not_a_name')\n\n        with tm.assertRaises(ValueError):\n            s = pd.Series(range(10))\n            s.sample(n=1, axis=1)\n\n        # Test weight length compared to correct axis\n        with tm.assertRaises(ValueError):\n            df.sample(n=1, axis=1, weights=[0.5] * 10)\n\n        # Check weights with axis = 1\n        easy_weight_list = [0] * 3\n        easy_weight_list[2] = 1\n\n        df = pd.DataFrame({'col1': range(10, 20),\n                           'col2': range(20, 30),\n                           'colString': ['a'] * 10})\n        sample1 = df.sample(n=1, axis=1, weights=easy_weight_list)\n        assert_frame_equal(sample1, df[['colString']])\n\n        # Test default axes\n        p = pd.Panel(items=['a', 'b', 'c'], major_axis=[2, 4, 6],\n                     minor_axis=[1, 3, 5])\n        assert_panel_equal(\n            p.sample(n=3, random_state=42), p.sample(n=3, axis=1,\n                                                     random_state=42))\n        assert_frame_equal(\n            df.sample(n=3, random_state=42), df.sample(n=3, axis=0,\n                                                       random_state=42))\n\n        # Test that function aligns weights with frame\n        df = DataFrame(\n            {'col1': [5, 6, 7],\n             'col2': ['a', 'b', 'c'], }, index=[9, 5, 3])\n        s = Series([1, 0, 0], index=[3, 5, 9])\n        assert_frame_equal(df.loc[[3]], df.sample(1, weights=s))\n\n        # Weights have index values to be dropped because not in\n        # sampled DataFrame\n        s2 = Series([0.001, 0, 10000], index=[3, 5, 10])\n        assert_frame_equal(df.loc[[3]], df.sample(1, weights=s2))\n\n        # Weights have empty values to be filed with zeros\n        s3 = Series([0.01, 0], index=[3, 5])\n        assert_frame_equal(df.loc[[3]], df.sample(1, weights=s3))\n\n        # No overlap in weight and sampled DataFrame indices\n        s4 = Series([1, 0], index=[1, 2])\n        with tm.assertRaises(ValueError):\n            df.sample(1, weights=s4)\n\n    def test_squeeze(self):\n        # noop\n        for s in [tm.makeFloatSeries(), tm.makeStringSeries(),\n                  tm.makeObjectSeries()]:\n            tm.assert_series_equal(s.squeeze(), s)\n        for df in [tm.makeTimeDataFrame()]:\n            tm.assert_frame_equal(df.squeeze(), df)\n        for p in [tm.makePanel()]:\n            tm.assert_panel_equal(p.squeeze(), p)\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            for p4d in [tm.makePanel4D()]:\n                tm.assert_panel4d_equal(p4d.squeeze(), p4d)\n\n        # squeezing\n        df = tm.makeTimeDataFrame().reindex(columns=['A'])\n        tm.assert_series_equal(df.squeeze(), df['A'])\n\n        p = tm.makePanel().reindex(items=['ItemA'])\n        tm.assert_frame_equal(p.squeeze(), p['ItemA'])\n\n        p = tm.makePanel().reindex(items=['ItemA'], minor_axis=['A'])\n        tm.assert_series_equal(p.squeeze(), p.loc['ItemA', :, 'A'])\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            p4d = tm.makePanel4D().reindex(labels=['label1'])\n            tm.assert_panel_equal(p4d.squeeze(), p4d['label1'])\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            p4d = tm.makePanel4D().reindex(labels=['label1'], items=['ItemA'])\n            tm.assert_frame_equal(p4d.squeeze(), p4d.loc['label1', 'ItemA'])\n\n        # don't fail with 0 length dimensions GH11229 & GH8999\n        empty_series = pd.Series([], name='five')\n        empty_frame = pd.DataFrame([empty_series])\n        empty_panel = pd.Panel({'six': empty_frame})\n\n        [tm.assert_series_equal(empty_series, higher_dim.squeeze())\n         for higher_dim in [empty_series, empty_frame, empty_panel]]\n\n        # axis argument\n        df = tm.makeTimeDataFrame(nper=1).iloc[:, :1]\n        tm.assert_equal(df.shape, (1, 1))\n        tm.assert_series_equal(df.squeeze(axis=0), df.iloc[0])\n        tm.assert_series_equal(df.squeeze(axis='index'), df.iloc[0])\n        tm.assert_series_equal(df.squeeze(axis=1), df.iloc[:, 0])\n        tm.assert_series_equal(df.squeeze(axis='columns'), df.iloc[:, 0])\n        tm.assert_equal(df.squeeze(), df.iloc[0, 0])\n        tm.assertRaises(ValueError, df.squeeze, axis=2)\n        tm.assertRaises(ValueError, df.squeeze, axis='x')\n\n        df = tm.makeTimeDataFrame(3)\n        tm.assert_frame_equal(df.squeeze(axis=0), df)\n\n    def test_numpy_squeeze(self):\n        s = tm.makeFloatSeries()\n        tm.assert_series_equal(np.squeeze(s), s)\n\n        df = tm.makeTimeDataFrame().reindex(columns=['A'])\n        tm.assert_series_equal(np.squeeze(df), df['A'])\n\n    def test_transpose(self):\n        msg = (r\"transpose\\(\\) got multiple values for \"\n               r\"keyword argument 'axes'\")\n        for s in [tm.makeFloatSeries(), tm.makeStringSeries(),\n                  tm.makeObjectSeries()]:\n            # calls implementation in pandas/core/base.py\n            tm.assert_series_equal(s.transpose(), s)\n        for df in [tm.makeTimeDataFrame()]:\n            tm.assert_frame_equal(df.transpose().transpose(), df)\n        for p in [tm.makePanel()]:\n            tm.assert_panel_equal(p.transpose(2, 0, 1)\n                                  .transpose(1, 2, 0), p)\n            tm.assertRaisesRegexp(TypeError, msg, p.transpose,\n                                  2, 0, 1, axes=(2, 0, 1))\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            for p4d in [tm.makePanel4D()]:\n                tm.assert_panel4d_equal(p4d.transpose(2, 0, 3, 1)\n                                        .transpose(1, 3, 0, 2), p4d)\n                tm.assertRaisesRegexp(TypeError, msg, p4d.transpose,\n                                      2, 0, 3, 1, axes=(2, 0, 3, 1))\n\n    def test_numpy_transpose(self):\n        msg = \"the 'axes' parameter is not supported\"\n\n        s = tm.makeFloatSeries()\n        tm.assert_series_equal(\n            np.transpose(s), s)\n        tm.assertRaisesRegexp(ValueError, msg,\n                              np.transpose, s, axes=1)\n\n        df = tm.makeTimeDataFrame()\n        tm.assert_frame_equal(np.transpose(\n            np.transpose(df)), df)\n        tm.assertRaisesRegexp(ValueError, msg,\n                              np.transpose, df, axes=1)\n\n        p = tm.makePanel()\n        tm.assert_panel_equal(np.transpose(\n            np.transpose(p, axes=(2, 0, 1)),\n            axes=(1, 2, 0)), p)\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            p4d = tm.makePanel4D()\n            tm.assert_panel4d_equal(np.transpose(\n                np.transpose(p4d, axes=(2, 0, 3, 1)),\n                axes=(1, 3, 0, 2)), p4d)\n\n    def test_take(self):\n        indices = [1, 5, -2, 6, 3, -1]\n        for s in [tm.makeFloatSeries(), tm.makeStringSeries(),\n                  tm.makeObjectSeries()]:\n            out = s.take(indices)\n            expected = Series(data=s.values.take(indices),\n                              index=s.index.take(indices))\n            tm.assert_series_equal(out, expected)\n        for df in [tm.makeTimeDataFrame()]:\n            out = df.take(indices)\n            expected = DataFrame(data=df.values.take(indices, axis=0),\n                                 index=df.index.take(indices),\n                                 columns=df.columns)\n            tm.assert_frame_equal(out, expected)\n\n        indices = [-3, 2, 0, 1]\n        for p in [tm.makePanel()]:\n            out = p.take(indices)\n            expected = Panel(data=p.values.take(indices, axis=0),\n                             items=p.items.take(indices),\n                             major_axis=p.major_axis,\n                             minor_axis=p.minor_axis)\n            tm.assert_panel_equal(out, expected)\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            for p4d in [tm.makePanel4D()]:\n                out = p4d.take(indices)\n                expected = Panel4D(data=p4d.values.take(indices, axis=0),\n                                   labels=p4d.labels.take(indices),\n                                   major_axis=p4d.major_axis,\n                                   minor_axis=p4d.minor_axis,\n                                   items=p4d.items)\n                tm.assert_panel4d_equal(out, expected)\n\n    def test_take_invalid_kwargs(self):\n        indices = [-3, 2, 0, 1]\n        s = tm.makeFloatSeries()\n        df = tm.makeTimeDataFrame()\n        p = tm.makePanel()\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            p4d = tm.makePanel4D()\n\n        for obj in (s, df, p, p4d):\n            msg = r\"take\\(\\) got an unexpected keyword argument 'foo'\"\n            tm.assertRaisesRegexp(TypeError, msg, obj.take,\n                                  indices, foo=2)\n\n            msg = \"the 'out' parameter is not supported\"\n            tm.assertRaisesRegexp(ValueError, msg, obj.take,\n                                  indices, out=indices)\n\n            msg = \"the 'mode' parameter is not supported\"\n            tm.assertRaisesRegexp(ValueError, msg, obj.take,\n                                  indices, mode='clip')\n\n    def test_equals(self):\n        s1 = pd.Series([1, 2, 3], index=[0, 2, 1])\n        s2 = s1.copy()\n        self.assertTrue(s1.equals(s2))\n\n        s1[1] = 99\n        self.assertFalse(s1.equals(s2))\n\n        # NaNs compare as equal\n        s1 = pd.Series([1, np.nan, 3, np.nan], index=[0, 2, 1, 3])\n        s2 = s1.copy()\n        self.assertTrue(s1.equals(s2))\n\n        s2[0] = 9.9\n        self.assertFalse(s1.equals(s2))\n\n        idx = MultiIndex.from_tuples([(0, 'a'), (1, 'b'), (2, 'c')])\n        s1 = Series([1, 2, np.nan], index=idx)\n        s2 = s1.copy()\n        self.assertTrue(s1.equals(s2))\n\n        # Add object dtype column with nans\n        index = np.random.random(10)\n        df1 = DataFrame(\n            np.random.random(10, ), index=index, columns=['floats'])\n        df1['text'] = 'the sky is so blue. we could use more chocolate.'.split(\n        )\n        df1['start'] = date_range('2000-1-1', periods=10, freq='T')\n        df1['end'] = date_range('2000-1-1', periods=10, freq='D')\n        df1['diff'] = df1['end'] - df1['start']\n        df1['bool'] = (np.arange(10) % 3 == 0)\n        df1.loc[::2] = nan\n        df2 = df1.copy()\n        self.assertTrue(df1['text'].equals(df2['text']))\n        self.assertTrue(df1['start'].equals(df2['start']))\n        self.assertTrue(df1['end'].equals(df2['end']))\n        self.assertTrue(df1['diff'].equals(df2['diff']))\n        self.assertTrue(df1['bool'].equals(df2['bool']))\n        self.assertTrue(df1.equals(df2))\n        self.assertFalse(df1.equals(object))\n\n        # different dtype\n        different = df1.copy()\n        different['floats'] = different['floats'].astype('float32')\n        self.assertFalse(df1.equals(different))\n\n        # different index\n        different_index = -index\n        different = df2.set_index(different_index)\n        self.assertFalse(df1.equals(different))\n\n        # different columns\n        different = df2.copy()\n        different.columns = df2.columns[::-1]\n        self.assertFalse(df1.equals(different))\n\n        # DatetimeIndex\n        index = pd.date_range('2000-1-1', periods=10, freq='T')\n        df1 = df1.set_index(index)\n        df2 = df1.copy()\n        self.assertTrue(df1.equals(df2))\n\n        # MultiIndex\n        df3 = df1.set_index(['text'], append=True)\n        df2 = df1.set_index(['text'], append=True)\n        self.assertTrue(df3.equals(df2))\n\n        df2 = df1.set_index(['floats'], append=True)\n        self.assertFalse(df3.equals(df2))\n\n        # NaN in index\n        df3 = df1.set_index(['floats'], append=True)\n        df2 = df1.set_index(['floats'], append=True)\n        self.assertTrue(df3.equals(df2))\n\n        # GH 8437\n        a = pd.Series([False, np.nan])\n        b = pd.Series([False, np.nan])\n        c = pd.Series(index=range(2))\n        d = pd.Series(index=range(2))\n        e = pd.Series(index=range(2))\n        f = pd.Series(index=range(2))\n        c[:-1] = d[:-1] = e[0] = f[0] = False\n        self.assertTrue(a.equals(a))\n        self.assertTrue(a.equals(b))\n        self.assertTrue(a.equals(c))\n        self.assertTrue(a.equals(d))\n        self.assertFalse(a.equals(e))\n        self.assertTrue(e.equals(f))\n\n    def test_describe_raises(self):\n        with tm.assertRaises(NotImplementedError):\n            tm.makePanel().describe()\n\n    def test_pipe(self):\n        df = DataFrame({'A': [1, 2, 3]})\n        f = lambda x, y: x ** y\n        result = df.pipe(f, 2)\n        expected = DataFrame({'A': [1, 4, 9]})\n        self.assert_frame_equal(result, expected)\n\n        result = df.A.pipe(f, 2)\n        self.assert_series_equal(result, expected.A)\n\n    def test_pipe_tuple(self):\n        df = DataFrame({'A': [1, 2, 3]})\n        f = lambda x, y: y\n        result = df.pipe((f, 'y'), 0)\n        self.assert_frame_equal(result, df)\n\n        result = df.A.pipe((f, 'y'), 0)\n        self.assert_series_equal(result, df.A)\n\n    def test_pipe_tuple_error(self):\n        df = DataFrame({\"A\": [1, 2, 3]})\n        f = lambda x, y: y\n        with tm.assertRaises(ValueError):\n            df.pipe((f, 'y'), x=1, y=0)\n\n        with tm.assertRaises(ValueError):\n            df.A.pipe((f, 'y'), x=1, y=0)\n\n    def test_pipe_panel(self):\n        wp = Panel({'r1': DataFrame({\"A\": [1, 2, 3]})})\n        f = lambda x, y: x + y\n        result = wp.pipe(f, 2)\n        expected = wp + 2\n        assert_panel_equal(result, expected)\n\n        result = wp.pipe((f, 'y'), x=1)\n        expected = wp + 1\n        assert_panel_equal(result, expected)\n\n        with tm.assertRaises(ValueError):\n            result = wp.pipe((f, 'y'), x=1, y=1)\n"
    }
  ],
  "questions": [
    "OK, I think I understand what you are saying, but I'm a bit confused still about what the \"expected result\" should be here. If I'm reading you correctly, you're saying that the templates need to be changed such that `DataFrameGroupby.idxmin()` (and, presumably, `idxmax()` and possibly a host of other methods where the same thing happens) explicitly states that it returns a `DataFrame`, right?\r\n\r\nI'm just confused because you say that \"the documentation is actually right\", but then go on to explain why the documentation says the wrong thing. I'm guessing that you meant that it's the right documentation for `x.idxmin()`, but that when it gets inherited by things that stack it, the *final* documentation is wrong.",
    "Hello I would like to help, it seems the issue is here:\r\n```\r\n        Returns\r\n        -------\r\n        Series\r\n            Indexes of minima along the specified axis.\r\n```\r\nthis is in the code in `pandas/pandas/core/frame.py` for idxmin.\r\n\r\nShould I make a PR by updating the docstring to Dataframe in that file ?",
    "#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame([[0, 0],\r\n                  [3, 0],\r\n                  [1, 1]], index=list('ABC'), columns=list('ab'))\r\n\r\ngby = df.groupby(by='b')\r\n\r\nprint(type(gby))            # <class 'pandas.core.groupby.DataFrameGroupBy'>\r\nprint(type(gby.idxmin()))   # <class 'pandas.core.frame.DataFrame'>\r\n```\r\n#### Problem description\r\nAccording to [the documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.idxmin.html), this is supposed to output a `pandas.core.Series`. To me, that seems to be what makes sense, but I'm not sure how or why this ended up returning a `DataFrame`. Is this just an issue with the documentation, or is it an issue with the code?\r\n\r\n**Edit**: Ah, I understand why this returns a `DataFrame` now - if you have multiple columns the `idxmin()` might be different for each column. Seems that the documentation needs to be updated. I can make a PR if that's appropriate.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.6.0.final.0\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.9.6-1-ARCH\r\nmachine: x86_64\r\nprocessor: \r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 34.0.3\r\nCython: 0.25.2\r\nnumpy: 1.12.0\r\nscipy: None\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.1.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.6.0\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: None\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: None\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n</details>"
  ],
  "golden_answers": [
    "OK, yeah, that's what I thought. I agree on the point of documentation, by the way - I've always felt like documentation tools (at least in Python, which is where I'm most familiar with them) were lacking in support for the DRY (don't repeat yourself) principle. You end up having to do some funky stuff like creating metaclasses or dynamically generate docstrings on import (which makes things a bit harder when navigating the code instead of looking at the generated documentation).\r\n\r\nI'll take a look at how the templating looks and see if I can find an easy way to either make the return type a replaceable parameter, infer it from the code or both.",
    "I don't think so - the issue is in `pandas.core.groupby.generic`.",
    "The documentation is actually right. What happens is that ``.groupby`` uses the doc string for certain 'whitelisted' methods, IOW, methods that are effectively called like\r\n\r\n``df.groupby(...).apply(lambda x: getattr(x, method)(...))`` so in this case\r\n``df.groupby(...).apply(lambda x: x.idxmin())``\r\n\r\nthen the results are inferred to be of the appropriate shape. Since this is a groupby, you get a Series for each group which are then stacked.\r\n\r\nSo the result is correct and the method of generating the document is 'correct', but this is a method that generates a Series (and not a scalar return), so ``.apply`` infers the correct output shape.\r\n\r\nSo we would need to update the templates a bit to fix this. A PR is welcome for this. I suspect there are several other methods that have this issue as well, but you would have to scan the doc-strings I think to see.\r\n\r\nwe actually just had some discussion w.r.t. this for ``.describe`` https://github.com/pandas-dev/pandas/pull/15260#issuecomment-276073724 and in #15272 so these are going to be solved differently (but the idea is the same)"
  ],
  "questions_generated": [
    "Why does DataFrameGroupBy.idxmin() return a DataFrame instead of a Series, contrary to the documentation?",
    "How does the current implementation of DataFrameGroupBy.idxmin() determine the return type for its output?",
    "What changes are suggested to resolve the discrepancy between the documentation and the actual behavior of DataFrameGroupBy.idxmin()?",
    "What are the implications of this issue for users of the pandas library?",
    "How does the pandas library determine the shape of the output for groupby operations in general?"
  ],
  "golden_answers_generated": [
    "DataFrameGroupBy.idxmin() returns a DataFrame because, when applied to a grouped DataFrame with multiple columns, the index of the minimum value could differ across columns. This results in a DataFrame where each column represents the idxmin of the corresponding column in the original DataFrame. The documentation incorrectly states that it should return a Series, which would only be applicable if there was a single column or if the operation was applied to a single column of a DataFrame.",
    "The current implementation determines the return type based on how the method is invoked on the grouped object using a lambda function: df.groupby(...).apply(lambda x: x.idxmin()). Since this operation is performed on each group independently, the output is inferred to be a DataFrame, stacking the Series obtained from each group. The resulting DataFrame has indices corresponding to each group and columns representing the minimum index for each column of the original DataFrame.",
    "To resolve the discrepancy, the documentation needs to be updated to accurately reflect that DataFrameGroupBy.idxmin() returns a DataFrame when applied to a grouped DataFrame with multiple columns. Additionally, the templates used for generating such documentation should be reviewed and updated to ensure that the correct output shapes are documented for methods returning multi-dimensional results.",
    "The implications of this issue are primarily related to user expectations and code correctness. Users relying on the documentation might expect a Series output and code their logic accordingly, leading to potential errors or unexpected behavior when they receive a DataFrame instead. This could affect data processing pipelines, result interpretations, and necessitate changes in user code to handle DataFrame outputs correctly.",
    "The pandas library determines the shape of the output for groupby operations by inferring the output shape based on the function applied to each group. For methods that apply a function across multiple columns or return aggregated results, the output is typically a DataFrame. This inference is part of the groupby framework, which uses function applications like .apply(lambda x: method(x)) to perform operations on each group and construct the final output by stacking or concatenating results from individual groups."
  ]
}