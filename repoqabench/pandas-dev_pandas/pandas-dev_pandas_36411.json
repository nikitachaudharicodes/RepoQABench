{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "36411",
  "issue_description": "# DOC: add searchsorted examples\n\nin this PR: https://github.com/pandas-dev/pandas/pull/36346\r\n\r\nwe updated `.searchsorted()` on the datetimelike indexes to accept string and a list-like, we should add some examples to the doc-string.\r\n\r\ncc @jbrockmendel ",
  "issue_comments": [
    {
      "id": 694262521,
      "user": "wprazuch",
      "body": "I would like to take this issue - is anyone working on that?"
    },
    {
      "id": 694616601,
      "user": "hardikpnsp",
      "body": "> I would like to take this issue - is anyone working on that?\r\n\r\nHey, seems like no one has been assigned the issue yet, you can just comment \"take\" and the bot will assign it to you. "
    },
    {
      "id": 694701802,
      "user": "2796gaurav",
      "body": "take"
    },
    {
      "id": 699523201,
      "user": "Ayushihelloworld",
      "body": "take"
    },
    {
      "id": 699647845,
      "user": "MananKGarg",
      "body": "take"
    },
    {
      "id": 702041302,
      "user": "Ajaydeep123",
      "body": "take\r\n"
    },
    {
      "id": 702306108,
      "user": "Ayushihelloworld",
      "body": "Hi I am working on this"
    },
    {
      "id": 702481651,
      "user": "Ajaydeep123",
      "body": "Okay no issue, go ahead.\n\nOn Thu, 1 Oct 2020, 23:36 Ayushihelloworld, <notifications@github.com>\nwrote:\n\n> Hi I am working on this\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/36411#issuecomment-702306108>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AL4AU3YA7BV7BP2UQPPUEQDSITAIZANCNFSM4RPXGGLQ>\n> .\n>\n"
    },
    {
      "id": 715586910,
      "user": "Mikhaylov-yv",
      "body": "take"
    },
    {
      "id": 715595707,
      "user": "Mikhaylov-yv",
      "body": "Please see if I understood the problem correctly."
    }
  ],
  "text_context": "# DOC: add searchsorted examples\n\nin this PR: https://github.com/pandas-dev/pandas/pull/36346\r\n\r\nwe updated `.searchsorted()` on the datetimelike indexes to accept string and a list-like, we should add some examples to the doc-string.\r\n\r\ncc @jbrockmendel \n\nI would like to take this issue - is anyone working on that?\n\n> I would like to take this issue - is anyone working on that?\r\n\r\nHey, seems like no one has been assigned the issue yet, you can just comment \"take\" and the bot will assign it to you. \n\ntake\n\ntake\n\ntake\n\ntake\r\n\n\nHi I am working on this\n\nOkay no issue, go ahead.\n\nOn Thu, 1 Oct 2020, 23:36 Ayushihelloworld, <notifications@github.com>\nwrote:\n\n> Hi I am working on this\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/36411#issuecomment-702306108>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AL4AU3YA7BV7BP2UQPPUEQDSITAIZANCNFSM4RPXGGLQ>\n> .\n>\n\n\ntake\n\nPlease see if I understood the problem correctly.",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/36346",
  "code_context": [
    {
      "filename": "pandas/core/arrays/datetimelike.py",
      "content": "from datetime import datetime, timedelta\nimport operator\nfrom typing import Any, Callable, Optional, Sequence, Tuple, Type, TypeVar, Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import algos, lib\nfrom pandas._libs.tslibs import (\n    BaseOffset,\n    NaT,\n    NaTType,\n    Period,\n    Resolution,\n    Tick,\n    Timestamp,\n    delta_to_nanoseconds,\n    iNaT,\n    to_offset,\n)\nfrom pandas._libs.tslibs.timestamps import (\n    RoundTo,\n    integer_op_not_supported,\n    round_nsint64,\n)\nfrom pandas._typing import DatetimeLikeScalar, DtypeObj\nfrom pandas.compat import set_function_name\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\nfrom pandas.util._validators import validate_fillna_kwargs\n\nfrom pandas.core.dtypes.common import (\n    is_categorical_dtype,\n    is_datetime64_any_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_datetime_or_timedelta_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float_dtype,\n    is_integer_dtype,\n    is_list_like,\n    is_object_dtype,\n    is_period_dtype,\n    is_string_dtype,\n    is_timedelta64_dtype,\n    is_unsigned_integer_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.generic import ABCSeries\nfrom pandas.core.dtypes.inference import is_array_like\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n\nfrom pandas.core import missing, nanops, ops\nfrom pandas.core.algorithms import checked_add_with_arr, unique1d, value_counts\nfrom pandas.core.arrays._mixins import NDArrayBackedExtensionArray\nfrom pandas.core.arrays.base import ExtensionOpsMixin\nimport pandas.core.common as com\nfrom pandas.core.construction import array, extract_array\nfrom pandas.core.indexers import check_array_indexer, check_setitem_lengths\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\nfrom pandas.core.ops.invalid import invalid_comparison, make_invalid_op\n\nfrom pandas.tseries import frequencies\n\nDTScalarOrNaT = Union[DatetimeLikeScalar, NaTType]\n\n\ndef _datetimelike_array_cmp(cls, op):\n    \"\"\"\n    Wrap comparison operations to convert Timestamp/Timedelta/Period-like to\n    boxed scalars/arrays.\n    \"\"\"\n    opname = f\"__{op.__name__}__\"\n    nat_result = opname == \"__ne__\"\n\n    class InvalidComparison(Exception):\n        pass\n\n    def _validate_comparison_value(self, other):\n        if isinstance(other, str):\n            try:\n                # GH#18435 strings get a pass from tzawareness compat\n                other = self._scalar_from_string(other)\n            except ValueError:\n                # failed to parse as Timestamp/Timedelta/Period\n                raise InvalidComparison(other)\n\n        if isinstance(other, self._recognized_scalars) or other is NaT:\n            other = self._scalar_type(other)\n            self._check_compatible_with(other)\n\n        elif not is_list_like(other):\n            raise InvalidComparison(other)\n\n        elif len(other) != len(self):\n            raise ValueError(\"Lengths must match\")\n\n        else:\n            try:\n                other = self._validate_listlike(other, opname, allow_object=True)\n            except TypeError as err:\n                raise InvalidComparison(other) from err\n\n        return other\n\n    @unpack_zerodim_and_defer(opname)\n    def wrapper(self, other):\n        if self.ndim > 1 and getattr(other, \"shape\", None) == self.shape:\n            # TODO: handle 2D-like listlikes\n            return op(self.ravel(), other.ravel()).reshape(self.shape)\n\n        try:\n            other = _validate_comparison_value(self, other)\n        except InvalidComparison:\n            return invalid_comparison(self, other, op)\n\n        dtype = getattr(other, \"dtype\", None)\n        if is_object_dtype(dtype):\n            # We have to use comp_method_OBJECT_ARRAY instead of numpy\n            #  comparison otherwise it would fail to raise when\n            #  comparing tz-aware and tz-naive\n            with np.errstate(all=\"ignore\"):\n                result = ops.comp_method_OBJECT_ARRAY(op, self.astype(object), other)\n            return result\n\n        other_i8 = self._unbox(other)\n        result = op(self.asi8, other_i8)\n\n        o_mask = isna(other)\n        if self._hasnans | np.any(o_mask):\n            result[self._isnan | o_mask] = nat_result\n\n        return result\n\n    return set_function_name(wrapper, opname, cls)\n\n\nclass AttributesMixin:\n    _data: np.ndarray\n\n    @classmethod\n    def _simple_new(cls, values: np.ndarray, **kwargs):\n        raise AbstractMethodError(cls)\n\n    @property\n    def _scalar_type(self) -> Type[DatetimeLikeScalar]:\n        \"\"\"\n        The scalar associated with this datelike\n\n        * PeriodArray : Period\n        * DatetimeArray : Timestamp\n        * TimedeltaArray : Timedelta\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _scalar_from_string(self, value: str) -> DTScalarOrNaT:\n        \"\"\"\n        Construct a scalar type from a string.\n\n        Parameters\n        ----------\n        value : str\n\n        Returns\n        -------\n        Period, Timestamp, or Timedelta, or NaT\n            Whatever the type of ``self._scalar_type`` is.\n\n        Notes\n        -----\n        This should call ``self._check_compatible_with`` before\n        unboxing the result.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @classmethod\n    def _rebox_native(cls, value: int) -> Union[int, np.datetime64, np.timedelta64]:\n        \"\"\"\n        Box an integer unboxed via _unbox_scalar into the native type for\n        the underlying ndarray.\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    def _unbox_scalar(self, value: DTScalarOrNaT, setitem: bool = False) -> int:\n        \"\"\"\n        Unbox the integer value of a scalar `value`.\n\n        Parameters\n        ----------\n        value : Period, Timestamp, Timedelta, or NaT\n            Depending on subclass.\n        setitem : bool, default False\n            Whether to check compatiblity with setitem strictness.\n\n        Returns\n        -------\n        int\n\n        Examples\n        --------\n        >>> self._unbox_scalar(Timedelta(\"10s\"))  # doctest: +SKIP\n        10000000000\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _check_compatible_with(\n        self, other: DTScalarOrNaT, setitem: bool = False\n    ) -> None:\n        \"\"\"\n        Verify that `self` and `other` are compatible.\n\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n\n        In each case, NaT is considered compatible.\n\n        Parameters\n        ----------\n        other\n        setitem : bool, default False\n            For __setitem__ we may have stricter compatibility restrictions than\n            for comparisons.\n\n        Raises\n        ------\n        Exception\n        \"\"\"\n        raise AbstractMethodError(self)\n\n\nclass DatelikeOps:\n    \"\"\"\n    Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.\n    \"\"\"\n\n    @Substitution(\n        URL=\"https://docs.python.org/3/library/datetime.html\"\n        \"#strftime-and-strptime-behavior\"\n    )\n    def strftime(self, date_format):\n        \"\"\"\n        Convert to Index using specified date_format.\n\n        Return an Index of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format\n        doc <%(URL)s>`__.\n\n        Parameters\n        ----------\n        date_format : str\n            Date format string (e.g. \"%%Y-%%m-%%d\").\n\n        Returns\n        -------\n        ndarray\n            NumPy ndarray of formatted strings.\n\n        See Also\n        --------\n        to_datetime : Convert the given argument to datetime.\n        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\n        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.\n        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\n\n        Examples\n        --------\n        >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\n        ...                     periods=3, freq='s')\n        >>> rng.strftime('%%B %%d, %%Y, %%r')\n        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n               'March 10, 2018, 09:00:02 AM'],\n              dtype='object')\n        \"\"\"\n        result = self._format_native_types(date_format=date_format, na_rep=np.nan)\n        return result.astype(object)\n\n\nclass TimelikeOps:\n    \"\"\"\n    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.\n    \"\"\"\n\n    _round_doc = \"\"\"\n        Perform {op} operation on the data to the specified `freq`.\n\n        Parameters\n        ----------\n        freq : str or Offset\n            The frequency level to {op} the index to. Must be a fixed\n            frequency like 'S' (second) not 'ME' (month end). See\n            :ref:`frequency aliases <timeseries.offset_aliases>` for\n            a list of possible `freq` values.\n        ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n            Only relevant for DatetimeIndex:\n\n            - 'infer' will attempt to infer fall dst-transition hours based on\n              order\n            - bool-ndarray where True signifies a DST time, False designates\n              a non-DST time (note that this flag is only applicable for\n              ambiguous times)\n            - 'NaT' will return NaT where there are ambiguous times\n            - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n              times.\n\n            .. versionadded:: 0.24.0\n\n        nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \\\ndefault 'raise'\n            A nonexistent time does not exist in a particular timezone\n            where clocks moved forward due to DST.\n\n            - 'shift_forward' will shift the nonexistent time forward to the\n              closest existing time\n            - 'shift_backward' will shift the nonexistent time backward to the\n              closest existing time\n            - 'NaT' will return NaT where there are nonexistent times\n            - timedelta objects will shift nonexistent times by the timedelta\n            - 'raise' will raise an NonExistentTimeError if there are\n              nonexistent times.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        DatetimeIndex, TimedeltaIndex, or Series\n            Index of the same type for a DatetimeIndex or TimedeltaIndex,\n            or a Series with the same index for a Series.\n\n        Raises\n        ------\n        ValueError if the `freq` cannot be converted.\n\n        Examples\n        --------\n        **DatetimeIndex**\n\n        >>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n        >>> rng\n        DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:01:00'],\n                      dtype='datetime64[ns]', freq='T')\n        \"\"\"\n\n    _round_example = \"\"\">>> rng.round('H')\n        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n\n        **Series**\n\n        >>> pd.Series(rng).dt.round(\"H\")\n        0   2018-01-01 12:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 12:00:00\n        dtype: datetime64[ns]\n        \"\"\"\n\n    _floor_example = \"\"\">>> rng.floor('H')\n        DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 12:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n\n        **Series**\n\n        >>> pd.Series(rng).dt.floor(\"H\")\n        0   2018-01-01 11:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 12:00:00\n        dtype: datetime64[ns]\n        \"\"\"\n\n    _ceil_example = \"\"\">>> rng.ceil('H')\n        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                       '2018-01-01 13:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n\n        **Series**\n\n        >>> pd.Series(rng).dt.ceil(\"H\")\n        0   2018-01-01 12:00:00\n        1   2018-01-01 12:00:00\n        2   2018-01-01 13:00:00\n        dtype: datetime64[ns]\n        \"\"\"\n\n    def _round(self, freq, mode, ambiguous, nonexistent):\n        # round the local times\n        if is_datetime64tz_dtype(self.dtype):\n            # operate on naive timestamps, then convert back to aware\n            naive = self.tz_localize(None)\n            result = naive._round(freq, mode, ambiguous, nonexistent)\n            aware = result.tz_localize(\n                self.tz, ambiguous=ambiguous, nonexistent=nonexistent\n            )\n            return aware\n\n        values = self.view(\"i8\")\n        result = round_nsint64(values, mode, freq)\n        result = self._maybe_mask_results(result, fill_value=NaT)\n        return self._simple_new(result, dtype=self.dtype)\n\n    @Appender((_round_doc + _round_example).format(op=\"round\"))\n    def round(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)\n\n    @Appender((_round_doc + _floor_example).format(op=\"floor\"))\n    def floor(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)\n\n    @Appender((_round_doc + _ceil_example).format(op=\"ceil\"))\n    def ceil(self, freq, ambiguous=\"raise\", nonexistent=\"raise\"):\n        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)\n\n    def _with_freq(self, freq):\n        \"\"\"\n        Helper to get a view on the same data, with a new freq.\n\n        Parameters\n        ----------\n        freq : DateOffset, None, or \"infer\"\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        # GH#29843\n        if freq is None:\n            # Always valid\n            pass\n        elif len(self) == 0 and isinstance(freq, BaseOffset):\n            # Always valid.  In the TimedeltaArray case, we assume this\n            #  is a Tick offset.\n            pass\n        else:\n            # As an internal method, we can ensure this assertion always holds\n            assert freq == \"infer\"\n            freq = to_offset(self.inferred_freq)\n\n        arr = self.view()\n        arr._freq = freq\n        return arr\n\n\nDatetimeLikeArrayT = TypeVar(\"DatetimeLikeArrayT\", bound=\"DatetimeLikeArrayMixin\")\n\n\nclass DatetimeLikeArrayMixin(\n    ExtensionOpsMixin, AttributesMixin, NDArrayBackedExtensionArray\n):\n    \"\"\"\n    Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray\n\n    Assumes that __new__/__init__ defines:\n        _data\n        _freq\n\n    and that the inheriting class has methods:\n        _generate_range\n    \"\"\"\n\n    _is_recognized_dtype: Callable[[DtypeObj], bool]\n    _recognized_scalars: Tuple[Type, ...]\n\n    # ------------------------------------------------------------------\n    # NDArrayBackedExtensionArray compat\n\n    @cache_readonly\n    def _ndarray(self) -> np.ndarray:\n        return self._data\n\n    def _from_backing_data(\n        self: DatetimeLikeArrayT, arr: np.ndarray\n    ) -> DatetimeLikeArrayT:\n        # Note: we do not retain `freq`\n        return type(self)._simple_new(arr, dtype=self.dtype)\n\n    # ------------------------------------------------------------------\n\n    def _box_func(self, x):\n        \"\"\"\n        box function to get object from internal representation\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _box_values(self, values):\n        \"\"\"\n        apply box func to passed values\n        \"\"\"\n        return lib.map_infer(values, self._box_func)\n\n    def __iter__(self):\n        return (self._box_func(v) for v in self.asi8)\n\n    @property\n    def asi8(self) -> np.ndarray:\n        \"\"\"\n        Integer representation of the values.\n\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        # do not cache or you'll create a memory leak\n        return self._data.view(\"i8\")\n\n    # ----------------------------------------------------------------\n    # Rendering Methods\n\n    def _format_native_types(self, na_rep=\"NaT\", date_format=None):\n        \"\"\"\n        Helper method for astype when converting to strings.\n\n        Returns\n        -------\n        ndarray[str]\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _formatter(self, boxed=False):\n        # TODO: Remove Datetime & DatetimeTZ formatters.\n        return \"'{}'\".format\n\n    # ----------------------------------------------------------------\n    # Array-Like / EA-Interface Methods\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        # used for Timedelta/DatetimeArray, overwritten by PeriodArray\n        if is_object_dtype(dtype):\n            return np.array(list(self), dtype=object)\n        return self._ndarray\n\n    def __getitem__(self, key):\n        \"\"\"\n        This getitem defers to the underlying array, which by-definition can\n        only handle list-likes, slices, and integer scalars\n        \"\"\"\n\n        if lib.is_integer(key):\n            # fast-path\n            result = self._ndarray[key]\n            if self.ndim == 1:\n                return self._box_func(result)\n            return self._from_backing_data(result)\n\n        key = self._validate_getitem_key(key)\n        result = self._ndarray[key]\n        if lib.is_scalar(result):\n            return self._box_func(result)\n\n        result = self._from_backing_data(result)\n\n        freq = self._get_getitem_freq(key)\n        result._freq = freq\n        return result\n\n    def _validate_getitem_key(self, key):\n        if com.is_bool_indexer(key):\n            # first convert to boolean, because check_array_indexer doesn't\n            # allow object dtype\n            if is_object_dtype(key):\n                key = np.asarray(key, dtype=bool)\n\n            key = check_array_indexer(self, key)\n            key = lib.maybe_booleans_to_slice(key.view(np.uint8))\n        elif isinstance(key, list) and len(key) == 1 and isinstance(key[0], slice):\n            # see https://github.com/pandas-dev/pandas/issues/31299, need to allow\n            # this for now (would otherwise raise in check_array_indexer)\n            pass\n        else:\n            key = check_array_indexer(self, key)\n        return key\n\n    def _get_getitem_freq(self, key):\n        \"\"\"\n        Find the `freq` attribute to assign to the result of a __getitem__ lookup.\n        \"\"\"\n        is_period = is_period_dtype(self.dtype)\n        if is_period:\n            freq = self.freq\n        else:\n            freq = None\n            if isinstance(key, slice):\n                if self.freq is not None and key.step is not None:\n                    freq = key.step * self.freq\n                else:\n                    freq = self.freq\n            elif key is Ellipsis:\n                # GH#21282 indexing with Ellipsis is similar to a full slice,\n                #  should preserve `freq` attribute\n                freq = self.freq\n        return freq\n\n    def __setitem__(\n        self,\n        key: Union[int, Sequence[int], Sequence[bool], slice],\n        value: Union[NaTType, Any, Sequence[Any]],\n    ) -> None:\n        # I'm fudging the types a bit here. \"Any\" above really depends\n        # on type(self). For PeriodArray, it's Period (or stuff coercible\n        # to a period in from_sequence). For DatetimeArray, it's Timestamp...\n        # I don't know if mypy can do that, possibly with Generics.\n        # https://mypy.readthedocs.io/en/latest/generics.html\n        no_op = check_setitem_lengths(key, value, self)\n        if no_op:\n            return\n\n        super().__setitem__(key, value)\n        self._maybe_clear_freq()\n\n    def _maybe_clear_freq(self):\n        # inplace operations like __setitem__ may invalidate the freq of\n        # DatetimeArray and TimedeltaArray\n        pass\n\n    def astype(self, dtype, copy=True):\n        # Some notes on cases we don't have to handle here in the base class:\n        #   1. PeriodArray.astype handles period -> period\n        #   2. DatetimeArray.astype handles conversion between tz.\n        #   3. DatetimeArray.astype handles datetime -> period\n        dtype = pandas_dtype(dtype)\n\n        if is_object_dtype(dtype):\n            return self._box_values(self.asi8.ravel()).reshape(self.shape)\n        elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):\n            if is_extension_array_dtype(dtype):\n                arr_cls = dtype.construct_array_type()\n                return arr_cls._from_sequence(self, dtype=dtype)\n            else:\n                return self._format_native_types()\n        elif is_integer_dtype(dtype):\n            # we deliberately ignore int32 vs. int64 here.\n            # See https://github.com/pandas-dev/pandas/issues/24381 for more.\n            values = self.asi8\n\n            if is_unsigned_integer_dtype(dtype):\n                # Again, we ignore int32 vs. int64\n                values = values.view(\"uint64\")\n\n            if copy:\n                values = values.copy()\n            return values\n        elif (\n            is_datetime_or_timedelta_dtype(dtype)\n            and not is_dtype_equal(self.dtype, dtype)\n        ) or is_float_dtype(dtype):\n            # disallow conversion between datetime/timedelta,\n            # and conversions for any datetimelike to float\n            msg = f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n            raise TypeError(msg)\n        elif is_categorical_dtype(dtype):\n            arr_cls = dtype.construct_array_type()\n            return arr_cls(self, dtype=dtype)\n        else:\n            return np.asarray(self, dtype=dtype)\n\n    def view(self, dtype=None):\n        if dtype is None or dtype is self.dtype:\n            return type(self)(self._ndarray, dtype=self.dtype)\n        return self._ndarray.view(dtype=dtype)\n\n    # ------------------------------------------------------------------\n    # ExtensionArray Interface\n\n    @classmethod\n    def _concat_same_type(cls, to_concat, axis: int = 0):\n        new_obj = super()._concat_same_type(to_concat, axis)\n\n        obj = to_concat[0]\n        dtype = obj.dtype\n\n        new_freq = None\n        if is_period_dtype(dtype):\n            new_freq = obj.freq\n        elif axis == 0:\n            # GH 3232: If the concat result is evenly spaced, we can retain the\n            # original frequency\n            to_concat = [x for x in to_concat if len(x)]\n\n            if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):\n                pairs = zip(to_concat[:-1], to_concat[1:])\n                if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):\n                    new_freq = obj.freq\n\n        new_obj._freq = new_freq\n        return new_obj\n\n    def copy(self: DatetimeLikeArrayT) -> DatetimeLikeArrayT:\n        new_obj = super().copy()\n        new_obj._freq = self.freq\n        return new_obj\n\n    def _values_for_factorize(self):\n        return self._ndarray, iNaT\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values, dtype=original.dtype)\n\n    # ------------------------------------------------------------------\n    # Validation Methods\n    # TODO: try to de-duplicate these, ensure identical behavior\n\n    def _validate_fill_value(self, fill_value):\n        \"\"\"\n        If a fill_value is passed to `take` convert it to an i8 representation,\n        raising ValueError if this is not possible.\n\n        Parameters\n        ----------\n        fill_value : object\n\n        Returns\n        -------\n        fill_value : np.int64, np.datetime64, or np.timedelta64\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n        msg = (\n            f\"'fill_value' should be a {self._scalar_type}. \"\n            f\"Got '{str(fill_value)}'.\"\n        )\n        try:\n            fill_value = self._validate_scalar(fill_value, msg)\n        except TypeError as err:\n            raise ValueError(msg) from err\n        rv = self._unbox(fill_value)\n        return self._rebox_native(rv)\n\n    def _validate_shift_value(self, fill_value):\n        # TODO(2.0): once this deprecation is enforced, use _validate_fill_value\n        if is_valid_nat_for_dtype(fill_value, self.dtype):\n            fill_value = NaT\n        elif isinstance(fill_value, self._recognized_scalars):\n            fill_value = self._scalar_type(fill_value)\n        else:\n            # only warn if we're not going to raise\n            if self._scalar_type is Period and lib.is_integer(fill_value):\n                # kludge for #31971 since Period(integer) tries to cast to str\n                new_fill = Period._from_ordinal(fill_value, freq=self.dtype.freq)\n            else:\n                new_fill = self._scalar_type(fill_value)\n\n            # stacklevel here is chosen to be correct when called from\n            #  DataFrame.shift or Series.shift\n            warnings.warn(\n                f\"Passing {type(fill_value)} to shift is deprecated and \"\n                \"will raise in a future version, pass \"\n                f\"{self._scalar_type.__name__} instead.\",\n                FutureWarning,\n                stacklevel=8,\n            )\n            fill_value = new_fill\n\n        return self._unbox(fill_value)\n\n    def _validate_scalar(\n        self, value, msg: Optional[str] = None, cast_str: bool = False\n    ):\n        \"\"\"\n        Validate that the input value can be cast to our scalar_type.\n\n        Parameters\n        ----------\n        value : object\n        msg : str, optional.\n            Message to raise in TypeError on invalid input.\n            If not provided, `value` is cast to a str and used\n            as the message.\n        cast_str : bool, default False\n            Whether to try to parse string input to scalar_type.\n\n        Returns\n        -------\n        self._scalar_type or NaT\n        \"\"\"\n        if cast_str and isinstance(value, str):\n            # NB: Careful about tzawareness\n            try:\n                value = self._scalar_from_string(value)\n            except ValueError as err:\n                raise TypeError(msg) from err\n\n        elif is_valid_nat_for_dtype(value, self.dtype):\n            # GH#18295\n            value = NaT\n\n        elif isinstance(value, self._recognized_scalars):\n            # error: Too many arguments for \"object\"  [call-arg]\n            value = self._scalar_type(value)  # type: ignore[call-arg]\n\n        else:\n            if msg is None:\n                msg = str(value)\n            raise TypeError(msg)\n\n        return value\n\n    def _validate_listlike(\n        self, value, opname: str, cast_str: bool = False, allow_object: bool = False\n    ):\n        if isinstance(value, type(self)):\n            return value\n\n        # Do type inference if necessary up front\n        # e.g. we passed PeriodIndex.values and got an ndarray of Periods\n        value = array(value)\n        value = extract_array(value, extract_numpy=True)\n\n        if cast_str and is_dtype_equal(value.dtype, \"string\"):\n            # We got a StringArray\n            try:\n                # TODO: Could use from_sequence_of_strings if implemented\n                # Note: passing dtype is necessary for PeriodArray tests\n                value = type(self)._from_sequence(value, dtype=self.dtype)\n            except ValueError:\n                pass\n\n        if is_categorical_dtype(value.dtype):\n            # e.g. we have a Categorical holding self.dtype\n            if is_dtype_equal(value.categories.dtype, self.dtype):\n                # TODO: do we need equal dtype or just comparable?\n                value = value._internal_get_values()\n                value = extract_array(value, extract_numpy=True)\n\n        if allow_object and is_object_dtype(value.dtype):\n            pass\n\n        elif not type(self)._is_recognized_dtype(value.dtype):\n            raise TypeError(\n                f\"{opname} requires compatible dtype or scalar, \"\n                f\"not {type(value).__name__}\"\n            )\n\n        return value\n\n    def _validate_searchsorted_value(self, value):\n        msg = \"searchsorted requires compatible dtype or scalar\"\n        if not is_list_like(value):\n            value = self._validate_scalar(value, msg, cast_str=True)\n        else:\n            value = self._validate_listlike(value, \"searchsorted\", cast_str=True)\n\n        rv = self._unbox(value)\n        return self._rebox_native(rv)\n\n    def _validate_setitem_value(self, value):\n        msg = (\n            f\"'value' should be a '{self._scalar_type.__name__}', 'NaT', \"\n            f\"or array of those. Got '{type(value).__name__}' instead.\"\n        )\n        if is_list_like(value):\n            value = self._validate_listlike(value, \"setitem\", cast_str=True)\n        else:\n            value = self._validate_scalar(value, msg, cast_str=True)\n\n        return self._unbox(value, setitem=True)\n\n    def _validate_insert_value(self, value):\n        msg = f\"cannot insert {type(self).__name__} with incompatible label\"\n        value = self._validate_scalar(value, msg, cast_str=False)\n\n        self._check_compatible_with(value, setitem=True)\n        # TODO: if we dont have compat, should we raise or astype(object)?\n        #  PeriodIndex does astype(object)\n        return value\n        # Note: we do not unbox here because the caller needs boxed value\n        #  to check for freq.\n\n    def _validate_where_value(self, other):\n        msg = f\"Where requires matching dtype, not {type(other)}\"\n        if not is_list_like(other):\n            other = self._validate_scalar(other, msg)\n        else:\n            other = self._validate_listlike(other, \"where\")\n\n        return self._unbox(other, setitem=True)\n\n    def _unbox(self, other, setitem: bool = False) -> Union[np.int64, np.ndarray]:\n        \"\"\"\n        Unbox either a scalar with _unbox_scalar or an instance of our own type.\n        \"\"\"\n        if lib.is_scalar(other):\n            other = self._unbox_scalar(other, setitem=setitem)\n        else:\n            # same type as self\n            self._check_compatible_with(other, setitem=setitem)\n            other = other.view(\"i8\")\n        return other\n\n    # ------------------------------------------------------------------\n    # Additional array methods\n    #  These are not part of the EA API, but we implement them because\n    #  pandas assumes they're there.\n\n    def value_counts(self, dropna=False):\n        \"\"\"\n        Return a Series containing counts of unique values.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of NaT values.\n\n        Returns\n        -------\n        Series\n        \"\"\"\n        from pandas import Index, Series\n\n        if dropna:\n            values = self[~self.isna()]._ndarray\n        else:\n            values = self._ndarray\n\n        cls = type(self)\n\n        result = value_counts(values, sort=False, dropna=dropna)\n        index = Index(\n            cls(result.index.view(\"i8\"), dtype=self.dtype), name=result.index.name\n        )\n        return Series(result._values, index=index, name=result.name)\n\n    def map(self, mapper):\n        # TODO(GH-23179): Add ExtensionArray.map\n        # Need to figure out if we want ExtensionArray.map first.\n        # If so, then we can refactor IndexOpsMixin._map_values to\n        # a standalone function and call from here..\n        # Else, just rewrite _map_infer_values to do the right thing.\n        from pandas import Index\n\n        return Index(self).map(mapper).array\n\n    # ------------------------------------------------------------------\n    # Null Handling\n\n    def isna(self):\n        return self._isnan\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _isnan(self):\n        \"\"\"\n        return if each value is nan\n        \"\"\"\n        return self.asi8 == iNaT\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _hasnans(self):\n        \"\"\"\n        return if I have any nans; enables various perf speedups\n        \"\"\"\n        return bool(self._isnan.any())\n\n    def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):\n        \"\"\"\n        Parameters\n        ----------\n        result : a ndarray\n        fill_value : object, default iNaT\n        convert : str, dtype or None\n\n        Returns\n        -------\n        result : ndarray with values replace by the fill_value\n\n        mask the result if needed, convert to the provided dtype if its not\n        None\n\n        This is an internal routine.\n        \"\"\"\n        if self._hasnans:\n            if convert:\n                result = result.astype(convert)\n            if fill_value is None:\n                fill_value = np.nan\n            result[self._isnan] = fill_value\n        return result\n\n    def fillna(self, value=None, method=None, limit=None):\n        # TODO(GH-20300): remove this\n        # Just overriding to ensure that we avoid an astype(object).\n        # Either 20300 or a `_values_for_fillna` would avoid this duplication.\n        if isinstance(value, ABCSeries):\n            value = value.array\n\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n\n        if is_array_like(value):\n            if len(value) != len(self):\n                raise ValueError(\n                    f\"Length of 'value' does not match. Got ({len(value)}) \"\n                    f\" expected {len(self)}\"\n                )\n            value = value[mask]\n\n        if mask.any():\n            if method is not None:\n                if method == \"pad\":\n                    func = missing.pad_1d\n                else:\n                    func = missing.backfill_1d\n\n                values = self._ndarray\n                if not is_period_dtype(self.dtype):\n                    # For PeriodArray self._ndarray is i8, which gets copied\n                    #  by `func`.  Otherwise we need to make a copy manually\n                    # to avoid modifying `self` in-place.\n                    values = values.copy()\n\n                new_values = func(values, limit=limit, mask=mask)\n                if is_datetime64tz_dtype(self.dtype):\n                    # we need to pass int64 values to the constructor to avoid\n                    #  re-localizing incorrectly\n                    new_values = new_values.view(\"i8\")\n                new_values = type(self)(new_values, dtype=self.dtype)\n            else:\n                # fill with value\n                new_values = self.copy()\n                new_values[mask] = value\n        else:\n            new_values = self.copy()\n        return new_values\n\n    # ------------------------------------------------------------------\n    # Frequency Properties/Methods\n\n    @property\n    def freq(self):\n        \"\"\"\n        Return the frequency object if it is set, otherwise None.\n        \"\"\"\n        return self._freq\n\n    @freq.setter\n    def freq(self, value):\n        if value is not None:\n            value = to_offset(value)\n            self._validate_frequency(self, value)\n\n        self._freq = value\n\n    @property\n    def freqstr(self):\n        \"\"\"\n        Return the frequency object as a string if its set, otherwise None.\n        \"\"\"\n        if self.freq is None:\n            return None\n        return self.freq.freqstr\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def inferred_freq(self):\n        \"\"\"\n        Tries to return a string representing a frequency guess,\n        generated by infer_freq.  Returns None if it can't autodetect the\n        frequency.\n        \"\"\"\n        if self.ndim != 1:\n            return None\n        try:\n            return frequencies.infer_freq(self)\n        except ValueError:\n            return None\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _resolution_obj(self) -> Optional[Resolution]:\n        try:\n            return Resolution.get_reso_from_freq(self.freqstr)\n        except KeyError:\n            return None\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def resolution(self) -> str:\n        \"\"\"\n        Returns day, hour, minute, second, millisecond or microsecond\n        \"\"\"\n        # error: Item \"None\" of \"Optional[Any]\" has no attribute \"attrname\"\n        return self._resolution_obj.attrname  # type: ignore[union-attr]\n\n    @classmethod\n    def _validate_frequency(cls, index, freq, **kwargs):\n        \"\"\"\n        Validate that a frequency is compatible with the values of a given\n        Datetime Array/Index or Timedelta Array/Index\n\n        Parameters\n        ----------\n        index : DatetimeIndex or TimedeltaIndex\n            The index on which to determine if the given frequency is valid\n        freq : DateOffset\n            The frequency to validate\n        \"\"\"\n        # TODO: this is not applicable to PeriodArray, move to correct Mixin\n        inferred = index.inferred_freq\n        if index.size == 0 or inferred == freq.freqstr:\n            return None\n\n        try:\n            on_freq = cls._generate_range(\n                start=index[0], end=None, periods=len(index), freq=freq, **kwargs\n            )\n            if not np.array_equal(index.asi8, on_freq.asi8):\n                raise ValueError\n        except ValueError as e:\n            if \"non-fixed\" in str(e):\n                # non-fixed frequencies are not meaningful for timedelta64;\n                #  we retain that error message\n                raise e\n            # GH#11587 the main way this is reached is if the `np.array_equal`\n            #  check above is False.  This can also be reached if index[0]\n            #  is `NaT`, in which case the call to `cls._generate_range` will\n            #  raise a ValueError, which we re-raise with a more targeted\n            #  message.\n            raise ValueError(\n                f\"Inferred frequency {inferred} from passed values \"\n                f\"does not conform to passed frequency {freq.freqstr}\"\n            ) from e\n\n    # monotonicity/uniqueness properties are called via frequencies.infer_freq,\n    #  see GH#23789\n\n    @property\n    def _is_monotonic_increasing(self):\n        return algos.is_monotonic(self.asi8, timelike=True)[0]\n\n    @property\n    def _is_monotonic_decreasing(self):\n        return algos.is_monotonic(self.asi8, timelike=True)[1]\n\n    @property\n    def _is_unique(self):\n        return len(unique1d(self.asi8)) == len(self)\n\n    # ------------------------------------------------------------------\n    # Arithmetic Methods\n    _create_comparison_method = classmethod(_datetimelike_array_cmp)\n\n    # pow is invalid for all three subclasses; TimedeltaArray will override\n    #  the multiplication and division ops\n    __pow__ = make_invalid_op(\"__pow__\")\n    __rpow__ = make_invalid_op(\"__rpow__\")\n    __mul__ = make_invalid_op(\"__mul__\")\n    __rmul__ = make_invalid_op(\"__rmul__\")\n    __truediv__ = make_invalid_op(\"__truediv__\")\n    __rtruediv__ = make_invalid_op(\"__rtruediv__\")\n    __floordiv__ = make_invalid_op(\"__floordiv__\")\n    __rfloordiv__ = make_invalid_op(\"__rfloordiv__\")\n    __mod__ = make_invalid_op(\"__mod__\")\n    __rmod__ = make_invalid_op(\"__rmod__\")\n    __divmod__ = make_invalid_op(\"__divmod__\")\n    __rdivmod__ = make_invalid_op(\"__rdivmod__\")\n\n    def _add_datetimelike_scalar(self, other):\n        # Overridden by TimedeltaArray\n        raise TypeError(f\"cannot add {type(self).__name__} and {type(other).__name__}\")\n\n    _add_datetime_arraylike = _add_datetimelike_scalar\n\n    def _sub_datetimelike_scalar(self, other):\n        # Overridden by DatetimeArray\n        assert other is not NaT\n        raise TypeError(f\"cannot subtract a datelike from a {type(self).__name__}\")\n\n    _sub_datetime_arraylike = _sub_datetimelike_scalar\n\n    def _sub_period(self, other):\n        # Overridden by PeriodArray\n        raise TypeError(f\"cannot subtract Period from a {type(self).__name__}\")\n\n    def _add_period(self, other: Period):\n        # Overriden by TimedeltaArray\n        raise TypeError(f\"cannot add Period to a {type(self).__name__}\")\n\n    def _add_offset(self, offset):\n        raise AbstractMethodError(self)\n\n    def _add_timedeltalike_scalar(self, other):\n        \"\"\"\n        Add a delta of a timedeltalike\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        if isna(other):\n            # i.e np.timedelta64(\"NaT\"), not recognized by delta_to_nanoseconds\n            new_values = np.empty(self.shape, dtype=\"i8\")\n            new_values[:] = iNaT\n            return type(self)(new_values, dtype=self.dtype)\n\n        inc = delta_to_nanoseconds(other)\n        new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(\n            \"i8\"\n        )\n        new_values = self._maybe_mask_results(new_values)\n\n        new_freq = None\n        if isinstance(self.freq, Tick) or is_period_dtype(self.dtype):\n            # adding a scalar preserves freq\n            new_freq = self.freq\n\n        return type(self)(new_values, dtype=self.dtype, freq=new_freq)\n\n    def _add_timedelta_arraylike(self, other):\n        \"\"\"\n        Add a delta of a TimedeltaIndex\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        # overridden by PeriodArray\n\n        if len(self) != len(other):\n            raise ValueError(\"cannot add indices of unequal length\")\n\n        if isinstance(other, np.ndarray):\n            # ndarray[timedelta64]; wrap in TimedeltaIndex for op\n            from pandas.core.arrays import TimedeltaArray\n\n            other = TimedeltaArray._from_sequence(other)\n\n        self_i8 = self.asi8\n        other_i8 = other.asi8\n        new_values = checked_add_with_arr(\n            self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan\n        )\n        if self._hasnans or other._hasnans:\n            mask = (self._isnan) | (other._isnan)\n            new_values[mask] = iNaT\n\n        return type(self)(new_values, dtype=self.dtype)\n\n    def _add_nat(self):\n        \"\"\"\n        Add pd.NaT to self\n        \"\"\"\n        if is_period_dtype(self.dtype):\n            raise TypeError(\n                f\"Cannot add {type(self).__name__} and {type(NaT).__name__}\"\n            )\n\n        # GH#19124 pd.NaT is treated like a timedelta for both timedelta\n        # and datetime dtypes\n        result = np.zeros(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        return type(self)(result, dtype=self.dtype, freq=None)\n\n    def _sub_nat(self):\n        \"\"\"\n        Subtract pd.NaT from self\n        \"\"\"\n        # GH#19124 Timedelta - datetime is not in general well-defined.\n        # We make an exception for pd.NaT, which in this case quacks\n        # like a timedelta.\n        # For datetime64 dtypes by convention we treat NaT as a datetime, so\n        # this subtraction returns a timedelta64 dtype.\n        # For period dtype, timedelta64 is a close-enough return dtype.\n        result = np.zeros(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        return result.view(\"timedelta64[ns]\")\n\n    def _sub_period_array(self, other):\n        # Overridden by PeriodArray\n        raise TypeError(\n            f\"cannot subtract {other.dtype}-dtype from {type(self).__name__}\"\n        )\n\n    def _addsub_object_array(self, other: np.ndarray, op):\n        \"\"\"\n        Add or subtract array-like of DateOffset objects\n\n        Parameters\n        ----------\n        other : np.ndarray[object]\n        op : {operator.add, operator.sub}\n\n        Returns\n        -------\n        result : same class as self\n        \"\"\"\n        assert op in [operator.add, operator.sub]\n        if len(other) == 1:\n            # If both 1D then broadcasting is unambiguous\n            # TODO(EA2D): require self.ndim == other.ndim here\n            return op(self, other[0])\n\n        warnings.warn(\n            \"Adding/subtracting object-dtype array to \"\n            f\"{type(self).__name__} not vectorized\",\n            PerformanceWarning,\n        )\n\n        # Caller is responsible for broadcasting if necessary\n        assert self.shape == other.shape, (self.shape, other.shape)\n\n        res_values = op(self.astype(\"O\"), np.asarray(other))\n        result = array(res_values.ravel())\n        result = extract_array(result, extract_numpy=True).reshape(self.shape)\n        return result\n\n    def _time_shift(self, periods, freq=None):\n        \"\"\"\n        Shift each value by `periods`.\n\n        Note this is different from ExtensionArray.shift, which\n        shifts the *position* of each element, padding the end with\n        missing values.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to shift by.\n        freq : pandas.DateOffset, pandas.Timedelta, or str\n            Frequency increment to shift by.\n        \"\"\"\n        if freq is not None and freq != self.freq:\n            if isinstance(freq, str):\n                freq = to_offset(freq)\n            offset = periods * freq\n            result = self + offset\n            return result\n\n        if periods == 0:\n            # immutable so OK\n            return self.copy()\n\n        if self.freq is None:\n            raise NullFrequencyError(\"Cannot shift with no freq\")\n\n        start = self[0] + periods * self.freq\n        end = self[-1] + periods * self.freq\n\n        # Note: in the DatetimeTZ case, _generate_range will infer the\n        #  appropriate timezone from `start` and `end`, so tz does not need\n        #  to be passed explicitly.\n        return self._generate_range(start=start, end=end, periods=None, freq=self.freq)\n\n    @unpack_zerodim_and_defer(\"__add__\")\n    def __add__(self, other):\n        other_dtype = getattr(other, \"dtype\", None)\n\n        # scalar others\n        if other is NaT:\n            result = self._add_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_timedeltalike_scalar(other)\n        elif isinstance(other, BaseOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._add_datetimelike_scalar(other)\n        elif isinstance(other, Period) and is_timedelta64_dtype(self.dtype):\n            result = self._add_period(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not is_period_dtype(self.dtype):\n                raise integer_op_not_supported(self)\n            result = self._time_shift(other)\n\n        # array-like others\n        elif is_timedelta64_dtype(other_dtype):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_timedelta_arraylike(other)\n        elif is_object_dtype(other_dtype):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.add)\n        elif is_datetime64_dtype(other_dtype) or is_datetime64tz_dtype(other_dtype):\n            # DatetimeIndex, ndarray[datetime64]\n            return self._add_datetime_arraylike(other)\n        elif is_integer_dtype(other_dtype):\n            if not is_period_dtype(self.dtype):\n                raise integer_op_not_supported(self)\n            result = self._addsub_int_array(other, operator.add)\n        else:\n            # Includes Categorical, other ExtensionArrays\n            # For PeriodDtype, if self is a TimedeltaArray and other is a\n            #  PeriodArray with  a timedelta-like (i.e. Tick) freq, this\n            #  operation is valid.  Defer to the PeriodArray implementation.\n            #  In remaining cases, this will end up raising TypeError.\n            return NotImplemented\n\n        if isinstance(result, np.ndarray) and is_timedelta64_dtype(result.dtype):\n            from pandas.core.arrays import TimedeltaArray\n\n            return TimedeltaArray(result)\n        return result\n\n    def __radd__(self, other):\n        # alias for __add__\n        return self.__add__(other)\n\n    @unpack_zerodim_and_defer(\"__sub__\")\n    def __sub__(self, other):\n\n        other_dtype = getattr(other, \"dtype\", None)\n\n        # scalar others\n        if other is NaT:\n            result = self._sub_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_timedeltalike_scalar(-other)\n        elif isinstance(other, BaseOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(-other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._sub_datetimelike_scalar(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not is_period_dtype(self.dtype):\n                raise integer_op_not_supported(self)\n            result = self._time_shift(-other)\n\n        elif isinstance(other, Period):\n            result = self._sub_period(other)\n\n        # array-like others\n        elif is_timedelta64_dtype(other_dtype):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_timedelta_arraylike(-other)\n        elif is_object_dtype(other_dtype):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.sub)\n        elif is_datetime64_dtype(other_dtype) or is_datetime64tz_dtype(other_dtype):\n            # DatetimeIndex, ndarray[datetime64]\n            result = self._sub_datetime_arraylike(other)\n        elif is_period_dtype(other_dtype):\n            # PeriodIndex\n            result = self._sub_period_array(other)\n        elif is_integer_dtype(other_dtype):\n            if not is_period_dtype(self.dtype):\n                raise integer_op_not_supported(self)\n            result = self._addsub_int_array(other, operator.sub)\n        else:\n            # Includes ExtensionArrays, float_dtype\n            return NotImplemented\n\n        if isinstance(result, np.ndarray) and is_timedelta64_dtype(result.dtype):\n            from pandas.core.arrays import TimedeltaArray\n\n            return TimedeltaArray(result)\n        return result\n\n    def __rsub__(self, other):\n        other_dtype = getattr(other, \"dtype\", None)\n\n        if is_datetime64_any_dtype(other_dtype) and is_timedelta64_dtype(self.dtype):\n            # ndarray[datetime64] cannot be subtracted from self, so\n            # we need to wrap in DatetimeArray/Index and flip the operation\n            if lib.is_scalar(other):\n                # i.e. np.datetime64 object\n                return Timestamp(other) - self\n            if not isinstance(other, DatetimeLikeArrayMixin):\n                # Avoid down-casting DatetimeIndex\n                from pandas.core.arrays import DatetimeArray\n\n                other = DatetimeArray(other)\n            return other - self\n        elif (\n            is_datetime64_any_dtype(self.dtype)\n            and hasattr(other, \"dtype\")\n            and not is_datetime64_any_dtype(other.dtype)\n        ):\n            # GH#19959 datetime - datetime is well-defined as timedelta,\n            # but any other type - datetime is not well-defined.\n            raise TypeError(\n                f\"cannot subtract {type(self).__name__} from {type(other).__name__}\"\n            )\n        elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other_dtype):\n            # TODO: Can we simplify/generalize these cases at all?\n            raise TypeError(f\"cannot subtract {type(self).__name__} from {other.dtype}\")\n        elif is_timedelta64_dtype(self.dtype):\n            return (-self) + other\n\n        # We get here with e.g. datetime objects\n        return -(self - other)\n\n    def __iadd__(self, other):\n        result = self + other\n        self[:] = result[:]\n\n        if not is_period_dtype(self.dtype):\n            # restore freq, which is invalidated by setitem\n            self._freq = result._freq\n        return self\n\n    def __isub__(self, other):\n        result = self - other\n        self[:] = result[:]\n\n        if not is_period_dtype(self.dtype):\n            # restore freq, which is invalidated by setitem\n            self._freq = result._freq\n        return self\n\n    # --------------------------------------------------------------\n    # Reductions\n\n    def _reduce(self, name: str, skipna: bool = True, **kwargs):\n        op = getattr(self, name, None)\n        if op:\n            return op(skipna=skipna, **kwargs)\n        else:\n            return super()._reduce(name, skipna, **kwargs)\n\n    def min(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the minimum value of the Array or minimum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.min\n        Index.min : Return the minimum value in an Index.\n        Series.min : Return the minimum value in a Series.\n        \"\"\"\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())\n        if isna(result):\n            # Period._from_ordinal does not handle np.nan gracefully\n            return NaT\n        return self._box_func(result)\n\n    def max(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the maximum value of the Array or maximum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.max\n        Index.max : Return the maximum value in an Index.\n        Series.max : Return the maximum value in a Series.\n        \"\"\"\n        # TODO: skipna is broken with max.\n        # See https://github.com/pandas-dev/pandas/issues/24265\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        mask = self.isna()\n        if skipna:\n            values = self[~mask].asi8\n        elif mask.any():\n            return NaT\n        else:\n            values = self.asi8\n\n        if not len(values):\n            # short-circuit for empty max / min\n            return NaT\n\n        result = nanops.nanmax(values, skipna=skipna)\n        # Don't have to worry about NA `result`, since no NA went in.\n        return self._box_func(result)\n\n    def mean(self, skipna=True):\n        \"\"\"\n        Return the mean value of the Array.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Whether to ignore any NaT elements.\n\n        Returns\n        -------\n        scalar\n            Timestamp or Timedelta.\n\n        See Also\n        --------\n        numpy.ndarray.mean : Returns the average of array elements along a given axis.\n        Series.mean : Return the mean value in a Series.\n\n        Notes\n        -----\n        mean is only defined for Datetime and Timedelta dtypes, not for Period.\n        \"\"\"\n        if is_period_dtype(self.dtype):\n            # See discussion in GH#24757\n            raise TypeError(\n                f\"mean is not implemented for {type(self).__name__} since the \"\n                \"meaning is ambiguous.  An alternative is \"\n                \"obj.to_timestamp(how='start').mean()\"\n            )\n\n        mask = self.isna()\n        if skipna:\n            values = self[~mask]\n        elif mask.any():\n            return NaT\n        else:\n            values = self\n\n        if not len(values):\n            # short-circuit for empty max / min\n            return NaT\n\n        result = nanops.nanmean(values.view(\"i8\"), skipna=skipna)\n        # Don't have to worry about NA `result`, since no NA went in.\n        return self._box_func(result)\n\n\nDatetimeLikeArrayMixin._add_comparison_ops()\n\n# -------------------------------------------------------------------\n# Shared Constructor Helpers\n\n\ndef validate_periods(periods):\n    \"\"\"\n    If a `periods` argument is passed to the Datetime/Timedelta Array/Index\n    constructor, cast it to an integer.\n\n    Parameters\n    ----------\n    periods : None, float, int\n\n    Returns\n    -------\n    periods : None or int\n\n    Raises\n    ------\n    TypeError\n        if periods is None, float, or int\n    \"\"\"\n    if periods is not None:\n        if lib.is_float(periods):\n            periods = int(periods)\n        elif not lib.is_integer(periods):\n            raise TypeError(f\"periods must be a number, got {periods}\")\n    return periods\n\n\ndef validate_endpoints(closed):\n    \"\"\"\n    Check that the `closed` argument is among [None, \"left\", \"right\"]\n\n    Parameters\n    ----------\n    closed : {None, \"left\", \"right\"}\n\n    Returns\n    -------\n    left_closed : bool\n    right_closed : bool\n\n    Raises\n    ------\n    ValueError : if argument is not among valid values\n    \"\"\"\n    left_closed = False\n    right_closed = False\n\n    if closed is None:\n        left_closed = True\n        right_closed = True\n    elif closed == \"left\":\n        left_closed = True\n    elif closed == \"right\":\n        right_closed = True\n    else:\n        raise ValueError(\"Closed has to be either 'left', 'right' or None\")\n\n    return left_closed, right_closed\n\n\ndef validate_inferred_freq(freq, inferred_freq, freq_infer):\n    \"\"\"\n    If the user passes a freq and another freq is inferred from passed data,\n    require that they match.\n\n    Parameters\n    ----------\n    freq : DateOffset or None\n    inferred_freq : DateOffset or None\n    freq_infer : bool\n\n    Returns\n    -------\n    freq : DateOffset or None\n    freq_infer : bool\n\n    Notes\n    -----\n    We assume at this point that `maybe_infer_freq` has been called, so\n    `freq` is either a DateOffset object or None.\n    \"\"\"\n    if inferred_freq is not None:\n        if freq is not None and freq != inferred_freq:\n            raise ValueError(\n                f\"Inferred frequency {inferred_freq} from passed \"\n                \"values does not conform to passed frequency \"\n                f\"{freq.freqstr}\"\n            )\n        elif freq is None:\n            freq = inferred_freq\n        freq_infer = False\n\n    return freq, freq_infer\n\n\ndef maybe_infer_freq(freq):\n    \"\"\"\n    Comparing a DateOffset to the string \"infer\" raises, so we need to\n    be careful about comparisons.  Make a dummy variable `freq_infer` to\n    signify the case where the given freq is \"infer\" and set freq to None\n    to avoid comparison trouble later on.\n\n    Parameters\n    ----------\n    freq : {DateOffset, None, str}\n\n    Returns\n    -------\n    freq : {DateOffset, None}\n    freq_infer : bool\n        Whether we should inherit the freq of passed data.\n    \"\"\"\n    freq_infer = False\n    if not isinstance(freq, BaseOffset):\n        # if a passed freq is None, don't infer automatically\n        if freq != \"infer\":\n            freq = to_offset(freq)\n        else:\n            freq_infer = True\n            freq = None\n    return freq, freq_infer\n"
    },
    {
      "filename": "pandas/core/indexes/datetimelike.py",
      "content": "\"\"\"\nBase and utility classes for tseries type pandas objects.\n\"\"\"\nfrom datetime import datetime, tzinfo\nfrom typing import Any, List, Optional, TypeVar, Union, cast\n\nimport numpy as np\n\nfrom pandas._libs import NaT, Timedelta, iNaT, join as libjoin, lib\nfrom pandas._libs.tslibs import BaseOffset, Resolution, Tick, timezones\nfrom pandas._libs.tslibs.parsing import DateParseError\nfrom pandas._typing import Callable, Label\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, cache_readonly, doc\n\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    is_bool_dtype,\n    is_dtype_equal,\n    is_integer,\n    is_list_like,\n    is_period_dtype,\n    is_scalar,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.generic import ABCIndex, ABCSeries\n\nfrom pandas.core import algorithms\nfrom pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray\nfrom pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin\nfrom pandas.core.base import IndexOpsMixin\nimport pandas.core.common as com\nfrom pandas.core.construction import array as pd_array, extract_array\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import Index, _index_shared_docs\nfrom pandas.core.indexes.extension import (\n    ExtensionIndex,\n    inherit_names,\n    make_wrapped_arith_op,\n)\nfrom pandas.core.indexes.numeric import Int64Index\nfrom pandas.core.ops import get_op_result_name\nfrom pandas.core.tools.timedeltas import to_timedelta\n\n_index_doc_kwargs = dict(ibase._index_doc_kwargs)\n\n_T = TypeVar(\"_T\", bound=\"DatetimeIndexOpsMixin\")\n\n\ndef _join_i8_wrapper(joinf, with_indexers: bool = True):\n    \"\"\"\n    Create the join wrapper methods.\n    \"\"\"\n\n    # error: 'staticmethod' used with a non-method\n    @staticmethod  # type: ignore[misc]\n    def wrapper(left, right):\n        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):\n            left = left.view(\"i8\")\n        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):\n            right = right.view(\"i8\")\n\n        results = joinf(left, right)\n        if with_indexers:\n            # dtype should be timedelta64[ns] for TimedeltaIndex\n            #  and datetime64[ns] for DatetimeIndex\n            dtype = left.dtype.base\n\n            join_index, left_indexer, right_indexer = results\n            join_index = join_index.view(dtype)\n            return join_index, left_indexer, right_indexer\n        return results\n\n    return wrapper\n\n\n@inherit_names(\n    [\"inferred_freq\", \"_isnan\", \"_resolution_obj\", \"resolution\"],\n    DatetimeLikeArrayMixin,\n    cache=True,\n)\n@inherit_names([\"mean\", \"asi8\", \"freq\", \"freqstr\"], DatetimeLikeArrayMixin)\nclass DatetimeIndexOpsMixin(ExtensionIndex):\n    \"\"\"\n    Common ops mixin to support a unified interface datetimelike Index.\n    \"\"\"\n\n    _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]\n    freq: Optional[BaseOffset]\n    freqstr: Optional[str]\n    _resolution_obj: Resolution\n    _bool_ops: List[str] = []\n    _field_ops: List[str] = []\n\n    # error: \"Callable[[Any], Any]\" has no attribute \"fget\"\n    hasnans = cache_readonly(\n        DatetimeLikeArrayMixin._hasnans.fget  # type: ignore[attr-defined]\n    )\n    _hasnans = hasnans  # for index / array -agnostic code\n\n    @property\n    def is_all_dates(self) -> bool:\n        return True\n\n    # ------------------------------------------------------------------------\n    # Abstract data attributes\n\n    @property\n    def values(self):\n        # Note: PeriodArray overrides this to return an ndarray of objects.\n        return self._data._data\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc and other functions.\n        \"\"\"\n        result = lib.item_from_zerodim(result)\n        if is_bool_dtype(result) or lib.is_scalar(result):\n            return result\n\n        attrs = self._get_attributes_dict()\n        if not is_period_dtype(self.dtype) and attrs[\"freq\"]:\n            # no need to infer if freq is None\n            attrs[\"freq\"] = \"infer\"\n        return Index(result, **attrs)\n\n    # ------------------------------------------------------------------------\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Determines if two Index objects contain the same elements.\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n        elif not isinstance(other, type(self)):\n            try:\n                other = type(self)(other)\n            except (ValueError, TypeError, OverflowError):\n                # e.g.\n                #  ValueError -> cannot parse str entry, or OutOfBoundsDatetime\n                #  TypeError  -> trying to convert IntervalIndex to DatetimeIndex\n                #  OverflowError -> Index([very_large_timedeltas])\n                return False\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            # have different timezone\n            return False\n\n        return np.array_equal(self.asi8, other.asi8)\n\n    @Appender(Index.__contains__.__doc__)\n    def __contains__(self, key: Any) -> bool:\n        hash(key)\n        try:\n            res = self.get_loc(key)\n        except (KeyError, TypeError, ValueError):\n            return False\n        return bool(\n            is_scalar(res) or isinstance(res, slice) or (is_list_like(res) and len(res))\n        )\n\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):\n        nv.validate_take(tuple(), kwargs)\n        indices = ensure_int64(indices)\n\n        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))\n        if isinstance(maybe_slice, slice):\n            return self[maybe_slice]\n\n        return ExtensionIndex.take(\n            self, indices, axis, allow_fill, fill_value, **kwargs\n        )\n\n    @doc(IndexOpsMixin.searchsorted, klass=\"Datetime-like Index\")\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n\n    _can_hold_na = True\n\n    _na_value = NaT\n    \"\"\"The expected NA value to use with this index.\"\"\"\n\n    def _convert_tolerance(self, tolerance, target):\n        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())\n\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        return tolerance\n\n    def tolist(self) -> List:\n        \"\"\"\n        Return a list of the underlying data.\n        \"\"\"\n        return list(self.astype(object))\n\n    def min(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the minimum value of the Index or minimum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.min\n        Series.min : Return the minimum value in a Series.\n        \"\"\"\n        nv.validate_min(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        if not len(self):\n            return self._na_value\n\n        i8 = self.asi8\n        try:\n            # quick check\n            if len(i8) and self.is_monotonic:\n                if i8[0] != iNaT:\n                    return self._data._box_func(i8[0])\n\n            if self.hasnans:\n                if skipna:\n                    min_stamp = self[~self._isnan].asi8.min()\n                else:\n                    return self._na_value\n            else:\n                min_stamp = i8.min()\n            return self._data._box_func(min_stamp)\n        except ValueError:\n            return self._na_value\n\n    def argmin(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Returns the indices of the minimum values along an axis.\n\n        See `numpy.ndarray.argmin` for more information on the\n        `axis` parameter.\n\n        See Also\n        --------\n        numpy.ndarray.argmin\n        \"\"\"\n        nv.validate_argmin(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        i8 = self.asi8\n        if self.hasnans:\n            mask = self._isnan\n            if mask.all() or not skipna:\n                return -1\n            i8 = i8.copy()\n            i8[mask] = np.iinfo(\"int64\").max\n        return i8.argmin()\n\n    def max(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the maximum value of the Index or maximum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.max\n        Series.max : Return the maximum value in a Series.\n        \"\"\"\n        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        if not len(self):\n            return self._na_value\n\n        i8 = self.asi8\n        try:\n            # quick check\n            if len(i8) and self.is_monotonic:\n                if i8[-1] != iNaT:\n                    return self._data._box_func(i8[-1])\n\n            if self.hasnans:\n                if skipna:\n                    max_stamp = self[~self._isnan].asi8.max()\n                else:\n                    return self._na_value\n            else:\n                max_stamp = i8.max()\n            return self._data._box_func(max_stamp)\n        except ValueError:\n            return self._na_value\n\n    def argmax(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Returns the indices of the maximum values along an axis.\n\n        See `numpy.ndarray.argmax` for more information on the\n        `axis` parameter.\n\n        See Also\n        --------\n        numpy.ndarray.argmax\n        \"\"\"\n        nv.validate_argmax(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        i8 = self.asi8\n        if self.hasnans:\n            mask = self._isnan\n            if mask.all() or not skipna:\n                return -1\n            i8 = i8.copy()\n            i8[mask] = 0\n        return i8.argmax()\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    def format(\n        self,\n        name: bool = False,\n        formatter: Optional[Callable] = None,\n        na_rep: str = \"NaT\",\n        date_format: Optional[str] = None,\n    ) -> List[str]:\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        header = []\n        if name:\n            header.append(\n                ibase.pprint_thing(self.name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                if self.name is not None\n                else \"\"\n            )\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(header, na_rep=na_rep, date_format=date_format)\n\n    def _format_with_header(\n        self, header: List[str], na_rep: str = \"NaT\", date_format: Optional[str] = None\n    ) -> List[str]:\n        return header + list(\n            self._format_native_types(na_rep=na_rep, date_format=date_format)\n        )\n\n    @property\n    def _formatter_func(self):\n        raise AbstractMethodError(self)\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        attrs = super()._format_attrs()\n        for attrib in self._attributes:\n            if attrib == \"freq\":\n                freq = self.freqstr\n                if freq is not None:\n                    freq = repr(freq)\n                attrs.append((\"freq\", freq))\n        return attrs\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    def _validate_partial_date_slice(self, reso: Resolution):\n        raise NotImplementedError\n\n    def _parsed_string_to_bounds(self, reso: Resolution, parsed: datetime):\n        raise NotImplementedError\n\n    def _partial_date_slice(\n        self,\n        reso: Resolution,\n        parsed: datetime,\n        use_lhs: bool = True,\n        use_rhs: bool = True,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        reso : Resolution\n        parsed : datetime\n        use_lhs : bool, default True\n        use_rhs : bool, default True\n\n        Returns\n        -------\n        slice or ndarray[intp]\n        \"\"\"\n        self._validate_partial_date_slice(reso)\n\n        t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n        i8vals = self.asi8\n        unbox = self._data._unbox_scalar\n\n        if self.is_monotonic:\n\n            if len(self) and (\n                (use_lhs and t1 < self[0] and t2 < self[0])\n                or ((use_rhs and t1 > self[-1] and t2 > self[-1]))\n            ):\n                # we are out of range\n                raise KeyError\n\n            # TODO: does this depend on being monotonic _increasing_?\n\n            # a monotonic (sorted) series can be sliced\n            # Use asi8.searchsorted to avoid re-validating Periods/Timestamps\n            left = i8vals.searchsorted(unbox(t1), side=\"left\") if use_lhs else None\n            right = i8vals.searchsorted(unbox(t2), side=\"right\") if use_rhs else None\n            return slice(left, right)\n\n        else:\n            lhs_mask = (i8vals >= unbox(t1)) if use_lhs else True\n            rhs_mask = (i8vals <= unbox(t2)) if use_rhs else True\n\n            # try to find the dates\n            return (lhs_mask & rhs_mask).nonzero()[0]\n\n    # --------------------------------------------------------------------\n    # Arithmetic Methods\n\n    __add__ = make_wrapped_arith_op(\"__add__\")\n    __sub__ = make_wrapped_arith_op(\"__sub__\")\n    __radd__ = make_wrapped_arith_op(\"__radd__\")\n    __rsub__ = make_wrapped_arith_op(\"__rsub__\")\n    __pow__ = make_wrapped_arith_op(\"__pow__\")\n    __rpow__ = make_wrapped_arith_op(\"__rpow__\")\n    __mul__ = make_wrapped_arith_op(\"__mul__\")\n    __rmul__ = make_wrapped_arith_op(\"__rmul__\")\n    __floordiv__ = make_wrapped_arith_op(\"__floordiv__\")\n    __rfloordiv__ = make_wrapped_arith_op(\"__rfloordiv__\")\n    __mod__ = make_wrapped_arith_op(\"__mod__\")\n    __rmod__ = make_wrapped_arith_op(\"__rmod__\")\n    __divmod__ = make_wrapped_arith_op(\"__divmod__\")\n    __rdivmod__ = make_wrapped_arith_op(\"__rdivmod__\")\n    __truediv__ = make_wrapped_arith_op(\"__truediv__\")\n    __rtruediv__ = make_wrapped_arith_op(\"__rtruediv__\")\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Compute boolean array of whether each index value is found in the\n        passed set of values.\n\n        Parameters\n        ----------\n        values : set or sequence of values\n\n        Returns\n        -------\n        is_contained : ndarray (boolean dtype)\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n\n        if not isinstance(values, type(self)):\n            try:\n                values = type(self)(values)\n            except ValueError:\n                return self.astype(object).isin(values)\n\n        return algorithms.isin(self.asi8, values.asi8)\n\n    @Appender(Index.where.__doc__)\n    def where(self, cond, other=None):\n        values = self.view(\"i8\")\n\n        try:\n            other = self._data._validate_where_value(other)\n        except (TypeError, ValueError) as err:\n            # Includes tzawareness mismatch and IncompatibleFrequencyError\n            oth = getattr(other, \"dtype\", other)\n            raise TypeError(f\"Where requires matching dtype, not {oth}\") from err\n\n        result = np.where(cond, values, other).astype(\"i8\")\n        arr = type(self._data)._simple_new(result, dtype=self.dtype)\n        return type(self)._simple_new(arr, name=self.name)\n\n    def _summary(self, name=None) -> str:\n        \"\"\"\n        Return a summarized representation.\n\n        Parameters\n        ----------\n        name : str\n            Name to use in the summary representation.\n\n        Returns\n        -------\n        str\n            Summarized representation of the index.\n        \"\"\"\n        formatter = self._formatter_func\n        if len(self) > 0:\n            index_summary = f\", {formatter(self[0])} to {formatter(self[-1])}\"\n        else:\n            index_summary = \"\"\n\n        if name is None:\n            name = type(self).__name__\n        result = f\"{name}: {len(self)} entries{index_summary}\"\n        if self.freq:\n            result += f\"\\nFreq: {self.freqstr}\"\n\n        # display as values, not quoted\n        result = result.replace(\"'\", \"\")\n        return result\n\n    def shift(self, periods=1, freq=None):\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n\n            .. versionchanged:: 0.24.0\n\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.DatetimeIndex\n            Shifted index.\n\n        See Also\n        --------\n        Index.shift : Shift values of Index.\n        PeriodIndex.shift : Shift values of PeriodIndex.\n        \"\"\"\n        arr = self._data.view()\n        arr._freq = self.freq\n        result = arr._time_shift(periods, freq=freq)\n        return type(self)(result, name=self.name)\n\n    # --------------------------------------------------------------------\n    # List-like Methods\n\n    def delete(self, loc):\n        new_i8s = np.delete(self.asi8, loc)\n\n        freq = None\n        if is_period_dtype(self.dtype):\n            freq = self.freq\n        elif is_integer(loc):\n            if loc in (0, -len(self), -1, len(self) - 1):\n                freq = self.freq\n        else:\n            if is_list_like(loc):\n                loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))\n            if isinstance(loc, slice) and loc.step in (1, None):\n                if loc.start in (0, None) or loc.stop in (len(self), None):\n                    freq = self.freq\n\n        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)\n        return type(self)._simple_new(arr, name=self.name)\n\n    # --------------------------------------------------------------------\n    # Join/Set Methods\n\n    def _wrap_joined_index(self, joined: np.ndarray, other):\n        assert other.dtype == self.dtype, (other.dtype, self.dtype)\n        name = get_op_result_name(self, other)\n\n        if is_period_dtype(self.dtype):\n            freq = self.freq\n        else:\n            self = cast(DatetimeTimedeltaMixin, self)\n            freq = self.freq if self._can_fast_union(other) else None\n\n        new_data = self._data._from_backing_data(joined)\n        new_data._freq = freq\n\n        return type(self)._simple_new(new_data, name=name)\n\n    @doc(Index._convert_arr_indexer)\n    def _convert_arr_indexer(self, keyarr):\n        if lib.infer_dtype(keyarr) == \"string\":\n            # Weak reasoning that indexer is a list of strings\n            # representing datetime or timedelta or period\n            try:\n                extension_arr = pd_array(keyarr, self.dtype)\n            except (ValueError, DateParseError):\n                # Fail to infer keyarr from self.dtype\n                return keyarr\n\n            converted_arr = extract_array(extension_arr, extract_numpy=True)\n        else:\n            converted_arr = com.asarray_tuplesafe(keyarr)\n        return converted_arr\n\n\nclass DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n    \"\"\"\n    Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,\n    but not PeriodIndex\n    \"\"\"\n\n    tz: Optional[tzinfo]\n\n    # Compat for frequency inference, see GH#23789\n    _is_monotonic_increasing = Index.is_monotonic_increasing\n    _is_monotonic_decreasing = Index.is_monotonic_decreasing\n    _is_unique = Index.is_unique\n\n    def _with_freq(self, freq):\n        arr = self._data._with_freq(freq)\n        return type(self)._simple_new(arr, name=self.name)\n\n    def _shallow_copy(self, values=None, name: Label = lib.no_default):\n        name = self.name if name is lib.no_default else name\n        cache = self._cache.copy() if values is None else {}\n\n        if values is None:\n            values = self._data\n\n        if isinstance(values, np.ndarray):\n            # TODO: We would rather not get here\n            values = type(self._data)(values, dtype=self.dtype)\n\n        result = type(self)._simple_new(values, name=name)\n        result._cache = cache\n        return result\n\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n\n    @Appender(Index.difference.__doc__)\n    def difference(self, other, sort=None):\n        new_idx = super().difference(other, sort=sort)._with_freq(None)\n        return new_idx\n\n    def intersection(self, other, sort=False):\n        \"\"\"\n        Specialized intersection for DatetimeIndex/TimedeltaIndex.\n\n        May be much faster than Index.intersection\n\n        Parameters\n        ----------\n        other : Same type as self or array-like\n        sort : False or None, default False\n            Sort the resulting index if possible.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default to ``False`` to match the behaviour\n               from before 0.24.0.\n\n            .. versionchanged:: 0.25.0\n\n               The `sort` keyword is added\n\n        Returns\n        -------\n        y : Index or same type as self\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        res_name = get_op_result_name(self, other)\n\n        if self.equals(other):\n            return self._get_reconciled_name_object(other)\n\n        if len(self) == 0:\n            return self.copy()\n        if len(other) == 0:\n            return other.copy()\n\n        if not isinstance(other, type(self)):\n            result = Index.intersection(self, other, sort=sort)\n            if isinstance(result, type(self)):\n                if result.freq is None:\n                    # TODO: no tests rely on this; needed?\n                    result = result._with_freq(\"infer\")\n            result.name = res_name\n            return result\n\n        elif not self._can_fast_intersect(other):\n            result = Index.intersection(self, other, sort=sort)\n            # We need to invalidate the freq because Index.intersection\n            #  uses _shallow_copy on a view of self._data, which will preserve\n            #  self.freq if we're not careful.\n            result = result._with_freq(None)._with_freq(\"infer\")\n            result.name = res_name\n            return result\n\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n\n        # after sorting, the intersection always starts with the right index\n        # and ends with the index of which the last elements is smallest\n        end = min(left[-1], right[-1])\n        start = right[0]\n\n        if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq, name=res_name)\n        else:\n            lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n            return type(self)._simple_new(left_chunk, name=res_name)\n\n    def _can_fast_intersect(self: _T, other: _T) -> bool:\n        if self.freq is None:\n            return False\n\n        elif other.freq != self.freq:\n            return False\n\n        elif not self.is_monotonic_increasing:\n            # Because freq is not None, we must then be monotonic decreasing\n            return False\n\n        elif self.freq.is_anchored():\n            # this along with matching freqs ensure that we \"line up\",\n            #  so intersection will preserve freq\n            return True\n\n        elif isinstance(self.freq, Tick):\n            # We \"line up\" if and only if the difference between two of our points\n            #  is a multiple of our freq\n            diff = self[0] - other[0]\n            remainder = diff % self.freq.delta\n            return remainder == Timedelta(0)\n\n        return True\n\n    def _can_fast_union(self: _T, other: _T) -> bool:\n        # Assumes that type(self) == type(other), as per the annotation\n        # The ability to fast_union also implies that `freq` should be\n        #  retained on union.\n        if not isinstance(other, type(self)):\n            return False\n\n        freq = self.freq\n\n        if freq is None or freq != other.freq:\n            return False\n\n        if not self.is_monotonic_increasing:\n            # Because freq is not None, we must then be monotonic decreasing\n            # TODO: do union on the reversed indexes?\n            return False\n\n        if len(self) == 0 or len(other) == 0:\n            return True\n\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        else:\n            left, right = other, self\n\n        right_start = right[0]\n        left_end = left[-1]\n\n        # Only need to \"adjoin\", not overlap\n        return (right_start == left_end + freq) or right_start in left\n\n    def _fast_union(self, other, sort=None):\n        if len(other) == 0:\n            return self.view(type(self))\n\n        if len(self) == 0:\n            return other.view(type(self))\n\n        # to make our life easier, \"sort\" the two ranges\n        if self[0] <= other[0]:\n            left, right = self, other\n        elif sort is False:\n            # TDIs are not in the \"correct\" order and we don't want\n            #  to sort but want to remove overlaps\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right._values[:loc]\n            dates = concat_compat((left._values, right_chunk))\n            # With sort being False, we can't infer that result.freq == self.freq\n            # TODO: no tests rely on the _with_freq(\"infer\"); needed?\n            result = self._shallow_copy(dates)._with_freq(\"infer\")\n            return result\n        else:\n            left, right = other, self\n\n        left_end = left[-1]\n        right_end = right[-1]\n\n        # concatenate\n        if left_end < right_end:\n            loc = right.searchsorted(left_end, side=\"right\")\n            right_chunk = right._values[loc:]\n            dates = concat_compat([left._values, right_chunk])\n            # The can_fast_union check ensures that the result.freq\n            #  should match self.freq\n            dates = type(self._data)(dates, freq=self.freq)\n            result = type(self)._simple_new(dates, name=self.name)\n            return result\n        else:\n            return left\n\n    def _union(self, other, sort):\n        if not len(other) or self.equals(other) or not len(self):\n            return super()._union(other, sort=sort)\n\n        # We are called by `union`, which is responsible for this validation\n        assert isinstance(other, type(self))\n\n        this, other = self._maybe_utc_convert(other)\n\n        if this._can_fast_union(other):\n            result = this._fast_union(other, sort=sort)\n            if sort is None:\n                # In the case where sort is None, _can_fast_union\n                #  implies that result.freq should match self.freq\n                assert result.freq == self.freq, (result.freq, self.freq)\n            elif result.freq is None:\n                # TODO: no tests rely on this; needed?\n                result = result._with_freq(\"infer\")\n            return result\n        else:\n            i8self = Int64Index._simple_new(self.asi8, name=self.name)\n            i8other = Int64Index._simple_new(other.asi8, name=other.name)\n            i8result = i8self._union(i8other, sort=sort)\n            result = type(self)(i8result, dtype=self.dtype, freq=\"infer\")\n            return result\n\n    # --------------------------------------------------------------------\n    # Join Methods\n    _join_precedence = 10\n\n    _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)\n    _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)\n    _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)\n    _left_indexer_unique = _join_i8_wrapper(\n        libjoin.left_join_indexer_unique, with_indexers=False\n    )\n\n    def join(\n        self, other, how: str = \"left\", level=None, return_indexers=False, sort=False\n    ):\n        \"\"\"\n        See Index.join\n        \"\"\"\n        if self._is_convertible_to_index_for_join(other):\n            try:\n                other = type(self)(other)\n            except (TypeError, ValueError):\n                pass\n\n        this, other = self._maybe_utc_convert(other)\n        return Index.join(\n            this,\n            other,\n            how=how,\n            level=level,\n            return_indexers=return_indexers,\n            sort=sort,\n        )\n\n    def _maybe_utc_convert(self, other):\n        this = self\n        if not hasattr(self, \"tz\"):\n            return this, other\n\n        if isinstance(other, type(self)):\n            if self.tz is not None:\n                if other.tz is None:\n                    raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n            elif other.tz is not None:\n                raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n\n            if not timezones.tz_compare(self.tz, other.tz):\n                this = self.tz_convert(\"UTC\")\n                other = other.tz_convert(\"UTC\")\n        return this, other\n\n    @classmethod\n    def _is_convertible_to_index_for_join(cls, other: Index) -> bool:\n        \"\"\"\n        return a boolean whether I can attempt conversion to a\n        DatetimeIndex/TimedeltaIndex\n        \"\"\"\n        if isinstance(other, cls):\n            return False\n        elif len(other) > 0 and other.inferred_type not in (\n            \"floating\",\n            \"mixed-integer\",\n            \"integer\",\n            \"integer-na\",\n            \"mixed-integer-float\",\n            \"mixed\",\n        ):\n            return True\n        return False\n\n    # --------------------------------------------------------------------\n    # List-Like Methods\n\n    def insert(self, loc, item):\n        \"\"\"\n        Make new Index inserting new item at location\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n            if not either a Python datetime or a numpy integer-like, returned\n            Index dtype will be object rather than datetime.\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        if isinstance(item, str):\n            # TODO: Why are strings special?\n            # TODO: Should we attempt _scalar_from_string?\n            return self.astype(object).insert(loc, item)\n\n        item = self._data._validate_insert_value(item)\n\n        freq = None\n        # check freq can be preserved on edge cases\n        if self.freq is not None:\n            if self.size:\n                if item is NaT:\n                    pass\n                elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:\n                    freq = self.freq\n                elif (loc == len(self)) and item - self.freq == self[-1]:\n                    freq = self.freq\n            else:\n                # Adding a single item to an empty index may preserve freq\n                if self.freq.is_on_offset(item):\n                    freq = self.freq\n\n        item = self._data._unbox_scalar(item)\n\n        new_i8s = np.concatenate([self[:loc].asi8, [item], self[loc:].asi8])\n        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)\n        return type(self)._simple_new(arr, name=self.name)\n"
    },
    {
      "filename": "pandas/tests/arrays/test_datetimelike.py",
      "content": "from typing import Type, Union\n\nimport numpy as np\nimport pytest\nimport pytz\n\nfrom pandas._libs import OutOfBoundsDatetime\nfrom pandas.compat.numpy import np_version_under1p18\n\nimport pandas as pd\nimport pandas._testing as tm\nfrom pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.period import Period, PeriodIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\n\n\n# TODO: more freq variants\n@pytest.fixture(params=[\"D\", \"B\", \"W\", \"M\", \"Q\", \"Y\"])\ndef period_index(request):\n    \"\"\"\n    A fixture to provide PeriodIndex objects with different frequencies.\n\n    Most PeriodArray behavior is already tested in PeriodIndex tests,\n    so here we just test that the PeriodArray behavior matches\n    the PeriodIndex behavior.\n    \"\"\"\n    freqstr = request.param\n    # TODO: non-monotone indexes; NaTs, different start dates\n    pi = pd.period_range(start=pd.Timestamp(\"2000-01-01\"), periods=100, freq=freqstr)\n    return pi\n\n\n@pytest.fixture(params=[\"D\", \"B\", \"W\", \"M\", \"Q\", \"Y\"])\ndef datetime_index(request):\n    \"\"\"\n    A fixture to provide DatetimeIndex objects with different frequencies.\n\n    Most DatetimeArray behavior is already tested in DatetimeIndex tests,\n    so here we just test that the DatetimeArray behavior matches\n    the DatetimeIndex behavior.\n    \"\"\"\n    freqstr = request.param\n    # TODO: non-monotone indexes; NaTs, different start dates, timezones\n    dti = pd.date_range(start=pd.Timestamp(\"2000-01-01\"), periods=100, freq=freqstr)\n    return dti\n\n\n@pytest.fixture\ndef timedelta_index(request):\n    \"\"\"\n    A fixture to provide TimedeltaIndex objects with different frequencies.\n     Most TimedeltaArray behavior is already tested in TimedeltaIndex tests,\n    so here we just test that the TimedeltaArray behavior matches\n    the TimedeltaIndex behavior.\n    \"\"\"\n    # TODO: flesh this out\n    return pd.TimedeltaIndex([\"1 Day\", \"3 Hours\", \"NaT\"])\n\n\nclass SharedTests:\n    index_cls: Type[Union[DatetimeIndex, PeriodIndex, TimedeltaIndex]]\n\n    @pytest.fixture\n    def arr1d(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n        return arr\n\n    def test_compare_len1_raises(self):\n        # make sure we raise when comparing with different lengths, specific\n        #  to the case where one has length-1, which numpy would broadcast\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n\n        arr = self.array_cls._simple_new(data, freq=\"D\")\n        idx = self.index_cls(arr)\n\n        with pytest.raises(ValueError, match=\"Lengths must match\"):\n            arr == arr[:1]\n\n        # test the index classes while we're at it, GH#23078\n        with pytest.raises(ValueError, match=\"Lengths must match\"):\n            idx <= idx[[0]]\n\n    @pytest.mark.parametrize(\"reverse\", [True, False])\n    @pytest.mark.parametrize(\"as_index\", [True, False])\n    def test_compare_categorical_dtype(self, arr1d, as_index, reverse, ordered):\n        other = pd.Categorical(arr1d, ordered=ordered)\n        if as_index:\n            other = pd.CategoricalIndex(other)\n\n        left, right = arr1d, other\n        if reverse:\n            left, right = right, left\n\n        ones = np.ones(arr1d.shape, dtype=bool)\n        zeros = ~ones\n\n        result = left == right\n        tm.assert_numpy_array_equal(result, ones)\n\n        result = left != right\n        tm.assert_numpy_array_equal(result, zeros)\n\n        if not reverse and not as_index:\n            # Otherwise Categorical raises TypeError bc it is not ordered\n            # TODO: we should probably get the same behavior regardless?\n            result = left < right\n            tm.assert_numpy_array_equal(result, zeros)\n\n            result = left <= right\n            tm.assert_numpy_array_equal(result, ones)\n\n            result = left > right\n            tm.assert_numpy_array_equal(result, zeros)\n\n            result = left >= right\n            tm.assert_numpy_array_equal(result, ones)\n\n    def test_take(self):\n        data = np.arange(100, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        np.random.shuffle(data)\n\n        arr = self.array_cls._simple_new(data, freq=\"D\")\n        idx = self.index_cls._simple_new(arr)\n\n        takers = [1, 4, 94]\n        result = arr.take(takers)\n        expected = idx.take(takers)\n\n        tm.assert_index_equal(self.index_cls(result), expected)\n\n        takers = np.array([1, 4, 94])\n        result = arr.take(takers)\n        expected = idx.take(takers)\n\n        tm.assert_index_equal(self.index_cls(result), expected)\n\n    @pytest.mark.parametrize(\"fill_value\", [2, 2.0, pd.Timestamp.now().time])\n    def test_take_fill_raises(self, fill_value):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n\n        arr = self.array_cls._simple_new(data, freq=\"D\")\n\n        msg = f\"'fill_value' should be a {self.dtype}. Got '{fill_value}'\"\n        with pytest.raises(ValueError, match=msg):\n            arr.take([0, 1], allow_fill=True, fill_value=fill_value)\n\n    def test_take_fill(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n\n        arr = self.array_cls._simple_new(data, freq=\"D\")\n\n        result = arr.take([-1, 1], allow_fill=True, fill_value=None)\n        assert result[0] is pd.NaT\n\n        result = arr.take([-1, 1], allow_fill=True, fill_value=np.nan)\n        assert result[0] is pd.NaT\n\n        result = arr.take([-1, 1], allow_fill=True, fill_value=pd.NaT)\n        assert result[0] is pd.NaT\n\n    def test_concat_same_type(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n\n        arr = self.array_cls._simple_new(data, freq=\"D\")\n        idx = self.index_cls(arr)\n        idx = idx.insert(0, pd.NaT)\n        arr = self.array_cls(idx)\n\n        result = arr._concat_same_type([arr[:-1], arr[1:], arr])\n        arr2 = arr.astype(object)\n        expected = self.index_cls(np.concatenate([arr2[:-1], arr2[1:], arr2]), None)\n\n        tm.assert_index_equal(self.index_cls(result), expected)\n\n    def test_unbox_scalar(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n        result = arr._unbox_scalar(arr[0])\n        assert isinstance(result, int)\n\n        result = arr._unbox_scalar(pd.NaT)\n        assert isinstance(result, int)\n\n        msg = f\"'value' should be a {self.dtype.__name__}.\"\n        with pytest.raises(ValueError, match=msg):\n            arr._unbox_scalar(\"foo\")\n\n    def test_check_compatible_with(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n\n        arr._check_compatible_with(arr[0])\n        arr._check_compatible_with(arr[:1])\n        arr._check_compatible_with(pd.NaT)\n\n    def test_scalar_from_string(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n        result = arr._scalar_from_string(str(arr[0]))\n        assert result == arr[0]\n\n    def test_reduce_invalid(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n\n        with pytest.raises(TypeError, match=\"cannot perform\"):\n            arr._reduce(\"not a method\")\n\n    @pytest.mark.parametrize(\"method\", [\"pad\", \"backfill\"])\n    def test_fillna_method_doesnt_change_orig(self, method):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n        arr[4] = pd.NaT\n\n        fill_value = arr[3] if method == \"pad\" else arr[5]\n\n        result = arr.fillna(method=method)\n        assert result[4] == fill_value\n\n        # check that the original was not changed\n        assert arr[4] is pd.NaT\n\n    def test_searchsorted(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n\n        # scalar\n        result = arr.searchsorted(arr[1])\n        assert result == 1\n\n        result = arr.searchsorted(arr[2], side=\"right\")\n        assert result == 3\n\n        # own-type\n        result = arr.searchsorted(arr[1:3])\n        expected = np.array([1, 2], dtype=np.intp)\n        tm.assert_numpy_array_equal(result, expected)\n\n        result = arr.searchsorted(arr[1:3], side=\"right\")\n        expected = np.array([2, 3], dtype=np.intp)\n        tm.assert_numpy_array_equal(result, expected)\n\n        # GH#29884 match numpy convention on whether NaT goes\n        #  at the end or the beginning\n        result = arr.searchsorted(pd.NaT)\n        if np_version_under1p18:\n            # Following numpy convention, NaT goes at the beginning\n            #  (unlike NaN which goes at the end)\n            assert result == 0\n        else:\n            assert result == 10\n\n    @pytest.mark.parametrize(\"box\", [None, \"index\", \"series\"])\n    def test_searchsorted_castable_strings(self, arr1d, box):\n        if isinstance(arr1d, DatetimeArray):\n            tz = arr1d.tz\n            if (\n                tz is not None\n                and tz is not pytz.UTC\n                and not isinstance(tz, pytz._FixedOffset)\n            ):\n                # If we have e.g. tzutc(), when we cast to string and parse\n                #  back we get pytz.UTC, and then consider them different timezones\n                #  so incorrectly raise.\n                pytest.xfail(reason=\"timezone comparisons inconsistent\")\n\n        arr = arr1d\n        if box is None:\n            pass\n        elif box == \"index\":\n            # Test the equivalent Index.searchsorted method while we're here\n            arr = self.index_cls(arr)\n        else:\n            # Test the equivalent Series.searchsorted method while we're here\n            arr = pd.Series(arr)\n\n        # scalar\n        result = arr.searchsorted(str(arr[1]))\n        assert result == 1\n\n        result = arr.searchsorted(str(arr[2]), side=\"right\")\n        assert result == 3\n\n        result = arr.searchsorted([str(x) for x in arr[1:3]])\n        expected = np.array([1, 2], dtype=np.intp)\n        tm.assert_numpy_array_equal(result, expected)\n\n        with pytest.raises(TypeError):\n            arr.searchsorted(\"foo\")\n\n        with pytest.raises(TypeError):\n            arr.searchsorted([str(arr[1]), \"baz\"])\n\n    def test_getitem_2d(self, arr1d):\n        # 2d slicing on a 1D array\n        expected = type(arr1d)(arr1d._data[:, np.newaxis], dtype=arr1d.dtype)\n        result = arr1d[:, np.newaxis]\n        tm.assert_equal(result, expected)\n\n        # Lookup on a 2D array\n        arr2d = expected\n        expected = type(arr2d)(arr2d._data[:3, 0], dtype=arr2d.dtype)\n        result = arr2d[:3, 0]\n        tm.assert_equal(result, expected)\n\n        # Scalar lookup\n        result = arr2d[-1, 0]\n        expected = arr1d[-1]\n        assert result == expected\n\n    def test_setitem(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n\n        arr[0] = arr[1]\n        expected = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        expected[0] = expected[1]\n\n        tm.assert_numpy_array_equal(arr.asi8, expected)\n\n        arr[:2] = arr[-2:]\n        expected[:2] = expected[-2:]\n        tm.assert_numpy_array_equal(arr.asi8, expected)\n\n    def test_setitem_strs(self, arr1d):\n        # Check that we parse strs in both scalar and listlike\n        if isinstance(arr1d, DatetimeArray):\n            tz = arr1d.tz\n            if (\n                tz is not None\n                and tz is not pytz.UTC\n                and not isinstance(tz, pytz._FixedOffset)\n            ):\n                # If we have e.g. tzutc(), when we cast to string and parse\n                #  back we get pytz.UTC, and then consider them different timezones\n                #  so incorrectly raise.\n                pytest.xfail(reason=\"timezone comparisons inconsistent\")\n\n        # Setting list-like of strs\n        expected = arr1d.copy()\n        expected[[0, 1]] = arr1d[-2:]\n\n        result = arr1d.copy()\n        result[:2] = [str(x) for x in arr1d[-2:]]\n        tm.assert_equal(result, expected)\n\n        # Same thing but now for just a scalar str\n        expected = arr1d.copy()\n        expected[0] = arr1d[-1]\n\n        result = arr1d.copy()\n        result[0] = str(arr1d[-1])\n        tm.assert_equal(result, expected)\n\n    @pytest.mark.parametrize(\"as_index\", [True, False])\n    def test_setitem_categorical(self, arr1d, as_index):\n        expected = arr1d.copy()[::-1]\n        if not isinstance(expected, PeriodArray):\n            expected = expected._with_freq(None)\n\n        cat = pd.Categorical(arr1d)\n        if as_index:\n            cat = pd.CategoricalIndex(cat)\n\n        arr1d[:] = cat[::-1]\n\n        tm.assert_equal(arr1d, expected)\n\n    def test_setitem_raises(self):\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n        val = arr[0]\n\n        with pytest.raises(IndexError, match=\"index 12 is out of bounds\"):\n            arr[12] = val\n\n        with pytest.raises(TypeError, match=\"'value' should be a.* 'object'\"):\n            arr[0] = object()\n\n        msg = \"cannot set using a list-like indexer with a different length\"\n        with pytest.raises(ValueError, match=msg):\n            # GH#36339\n            arr[[]] = [arr[1]]\n\n        msg = \"cannot set using a slice indexer with a different length than\"\n        with pytest.raises(ValueError, match=msg):\n            # GH#36339\n            arr[1:1] = arr[:3]\n\n    @pytest.mark.parametrize(\"box\", [list, np.array, pd.Index, pd.Series])\n    def test_setitem_numeric_raises(self, arr1d, box):\n        # We dont case e.g. int64 to our own dtype for setitem\n\n        msg = \"requires compatible dtype\"\n        with pytest.raises(TypeError, match=msg):\n            arr1d[:2] = box([0, 1])\n\n        with pytest.raises(TypeError, match=msg):\n            arr1d[:2] = box([0.0, 1.0])\n\n    def test_inplace_arithmetic(self):\n        # GH#24115 check that iadd and isub are actually in-place\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n\n        expected = arr + pd.Timedelta(days=1)\n        arr += pd.Timedelta(days=1)\n        tm.assert_equal(arr, expected)\n\n        expected = arr - pd.Timedelta(days=1)\n        arr -= pd.Timedelta(days=1)\n        tm.assert_equal(arr, expected)\n\n    def test_shift_fill_int_deprecated(self):\n        # GH#31971\n        data = np.arange(10, dtype=\"i8\") * 24 * 3600 * 10 ** 9\n        arr = self.array_cls(data, freq=\"D\")\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = arr.shift(1, fill_value=1)\n\n        expected = arr.copy()\n        if self.array_cls is PeriodArray:\n            fill_val = PeriodArray._scalar_type._from_ordinal(1, freq=arr.freq)\n        else:\n            fill_val = arr._scalar_type(1)\n        expected[0] = fill_val\n        expected[1:] = arr[:-1]\n        tm.assert_equal(result, expected)\n\n\nclass TestDatetimeArray(SharedTests):\n    index_cls = pd.DatetimeIndex\n    array_cls = DatetimeArray\n    dtype = pd.Timestamp\n\n    @pytest.fixture\n    def arr1d(self, tz_naive_fixture):\n        tz = tz_naive_fixture\n        dti = pd.date_range(\"2016-01-01 01:01:00\", periods=3, freq=\"H\", tz=tz)\n        dta = dti._data\n        return dta\n\n    def test_round(self, tz_naive_fixture):\n        # GH#24064\n        tz = tz_naive_fixture\n        dti = pd.date_range(\"2016-01-01 01:01:00\", periods=3, freq=\"H\", tz=tz)\n\n        result = dti.round(freq=\"2T\")\n        expected = dti - pd.Timedelta(minutes=1)\n        expected = expected._with_freq(None)\n        tm.assert_index_equal(result, expected)\n\n        dta = dti._data\n        result = dta.round(freq=\"2T\")\n        expected = expected._data._with_freq(None)\n        tm.assert_datetime_array_equal(result, expected)\n\n    def test_array_interface(self, datetime_index):\n        arr = DatetimeArray(datetime_index)\n\n        # default asarray gives the same underlying data (for tz naive)\n        result = np.asarray(arr)\n        expected = arr._data\n        assert result is expected\n        tm.assert_numpy_array_equal(result, expected)\n        result = np.array(arr, copy=False)\n        assert result is expected\n        tm.assert_numpy_array_equal(result, expected)\n\n        # specifying M8[ns] gives the same result as default\n        result = np.asarray(arr, dtype=\"datetime64[ns]\")\n        expected = arr._data\n        assert result is expected\n        tm.assert_numpy_array_equal(result, expected)\n        result = np.array(arr, dtype=\"datetime64[ns]\", copy=False)\n        assert result is expected\n        tm.assert_numpy_array_equal(result, expected)\n        result = np.array(arr, dtype=\"datetime64[ns]\")\n        assert result is not expected\n        tm.assert_numpy_array_equal(result, expected)\n\n        # to object dtype\n        result = np.asarray(arr, dtype=object)\n        expected = np.array(list(arr), dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n        # to other dtype always copies\n        result = np.asarray(arr, dtype=\"int64\")\n        assert result is not arr.asi8\n        assert not np.may_share_memory(arr, result)\n        expected = arr.asi8.copy()\n        tm.assert_numpy_array_equal(result, expected)\n\n        # other dtypes handled by numpy\n        for dtype in [\"float64\", str]:\n            result = np.asarray(arr, dtype=dtype)\n            expected = np.asarray(arr).astype(dtype)\n            tm.assert_numpy_array_equal(result, expected)\n\n    def test_array_object_dtype(self, tz_naive_fixture):\n        # GH#23524\n        tz = tz_naive_fixture\n        dti = pd.date_range(\"2016-01-01\", periods=3, tz=tz)\n        arr = DatetimeArray(dti)\n\n        expected = np.array(list(dti))\n\n        result = np.array(arr, dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n        # also test the DatetimeIndex method while we're at it\n        result = np.array(dti, dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_array_tz(self, tz_naive_fixture):\n        # GH#23524\n        tz = tz_naive_fixture\n        dti = pd.date_range(\"2016-01-01\", periods=3, tz=tz)\n        arr = DatetimeArray(dti)\n\n        expected = dti.asi8.view(\"M8[ns]\")\n        result = np.array(arr, dtype=\"M8[ns]\")\n        tm.assert_numpy_array_equal(result, expected)\n\n        result = np.array(arr, dtype=\"datetime64[ns]\")\n        tm.assert_numpy_array_equal(result, expected)\n\n        # check that we are not making copies when setting copy=False\n        result = np.array(arr, dtype=\"M8[ns]\", copy=False)\n        assert result.base is expected.base\n        assert result.base is not None\n        result = np.array(arr, dtype=\"datetime64[ns]\", copy=False)\n        assert result.base is expected.base\n        assert result.base is not None\n\n    def test_array_i8_dtype(self, tz_naive_fixture):\n        tz = tz_naive_fixture\n        dti = pd.date_range(\"2016-01-01\", periods=3, tz=tz)\n        arr = DatetimeArray(dti)\n\n        expected = dti.asi8\n        result = np.array(arr, dtype=\"i8\")\n        tm.assert_numpy_array_equal(result, expected)\n\n        result = np.array(arr, dtype=np.int64)\n        tm.assert_numpy_array_equal(result, expected)\n\n        # check that we are still making copies when setting copy=False\n        result = np.array(arr, dtype=\"i8\", copy=False)\n        assert result.base is not expected.base\n        assert result.base is None\n\n    def test_from_array_keeps_base(self):\n        # Ensure that DatetimeArray._data.base isn't lost.\n        arr = np.array([\"2000-01-01\", \"2000-01-02\"], dtype=\"M8[ns]\")\n        dta = DatetimeArray(arr)\n\n        assert dta._data is arr\n        dta = DatetimeArray(arr[:0])\n        assert dta._data.base is arr\n\n    def test_from_dti(self, tz_naive_fixture):\n        tz = tz_naive_fixture\n        dti = pd.date_range(\"2016-01-01\", periods=3, tz=tz)\n        arr = DatetimeArray(dti)\n        assert list(dti) == list(arr)\n\n        # Check that Index.__new__ knows what to do with DatetimeArray\n        dti2 = pd.Index(arr)\n        assert isinstance(dti2, pd.DatetimeIndex)\n        assert list(dti2) == list(arr)\n\n    def test_astype_object(self, tz_naive_fixture):\n        tz = tz_naive_fixture\n        dti = pd.date_range(\"2016-01-01\", periods=3, tz=tz)\n        arr = DatetimeArray(dti)\n        asobj = arr.astype(\"O\")\n        assert isinstance(asobj, np.ndarray)\n        assert asobj.dtype == \"O\"\n        assert list(asobj) == list(dti)\n\n    @pytest.mark.parametrize(\"freqstr\", [\"D\", \"B\", \"W\", \"M\", \"Q\", \"Y\"])\n    def test_to_perioddelta(self, datetime_index, freqstr):\n        # GH#23113\n        dti = datetime_index\n        arr = DatetimeArray(dti)\n\n        with tm.assert_produces_warning(FutureWarning):\n            # Deprecation GH#34853\n            expected = dti.to_perioddelta(freq=freqstr)\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            # stacklevel is chosen to be \"correct\" for DatetimeIndex, not\n            #  DatetimeArray\n            result = arr.to_perioddelta(freq=freqstr)\n        assert isinstance(result, TimedeltaArray)\n\n        # placeholder until these become actual EA subclasses and we can use\n        #  an EA-specific tm.assert_ function\n        tm.assert_index_equal(pd.Index(result), pd.Index(expected))\n\n    @pytest.mark.parametrize(\"freqstr\", [\"D\", \"B\", \"W\", \"M\", \"Q\", \"Y\"])\n    def test_to_period(self, datetime_index, freqstr):\n        dti = datetime_index\n        arr = DatetimeArray(dti)\n\n        expected = dti.to_period(freq=freqstr)\n        result = arr.to_period(freq=freqstr)\n        assert isinstance(result, PeriodArray)\n\n        # placeholder until these become actual EA subclasses and we can use\n        #  an EA-specific tm.assert_ function\n        tm.assert_index_equal(pd.Index(result), pd.Index(expected))\n\n    @pytest.mark.parametrize(\"propname\", pd.DatetimeIndex._bool_ops)\n    def test_bool_properties(self, datetime_index, propname):\n        # in this case _bool_ops is just `is_leap_year`\n        dti = datetime_index\n        arr = DatetimeArray(dti)\n        assert dti.freq == arr.freq\n\n        result = getattr(arr, propname)\n        expected = np.array(getattr(dti, propname), dtype=result.dtype)\n\n        tm.assert_numpy_array_equal(result, expected)\n\n    @pytest.mark.parametrize(\"propname\", pd.DatetimeIndex._field_ops)\n    def test_int_properties(self, datetime_index, propname):\n        if propname in [\"week\", \"weekofyear\"]:\n            # GH#33595 Deprecate week and weekofyear\n            return\n        dti = datetime_index\n        arr = DatetimeArray(dti)\n\n        result = getattr(arr, propname)\n        expected = np.array(getattr(dti, propname), dtype=result.dtype)\n\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_take_fill_valid(self, datetime_index, tz_naive_fixture):\n        dti = datetime_index.tz_localize(tz_naive_fixture)\n        arr = DatetimeArray(dti)\n\n        now = pd.Timestamp.now().tz_localize(dti.tz)\n        result = arr.take([-1, 1], allow_fill=True, fill_value=now)\n        assert result[0] == now\n\n        msg = f\"'fill_value' should be a {self.dtype}. Got '0 days 00:00:00'.\"\n        with pytest.raises(ValueError, match=msg):\n            # fill_value Timedelta invalid\n            arr.take([-1, 1], allow_fill=True, fill_value=now - now)\n\n        msg = f\"'fill_value' should be a {self.dtype}. Got '2014Q1'.\"\n        with pytest.raises(ValueError, match=msg):\n            # fill_value Period invalid\n            arr.take([-1, 1], allow_fill=True, fill_value=pd.Period(\"2014Q1\"))\n\n        tz = None if dti.tz is not None else \"US/Eastern\"\n        now = pd.Timestamp.now().tz_localize(tz)\n        msg = \"Cannot compare tz-naive and tz-aware datetime-like objects\"\n        with pytest.raises(TypeError, match=msg):\n            # Timestamp with mismatched tz-awareness\n            arr.take([-1, 1], allow_fill=True, fill_value=now)\n\n        value = pd.NaT.value\n        msg = f\"'fill_value' should be a {self.dtype}. Got '{value}'.\"\n        with pytest.raises(ValueError, match=msg):\n            # require NaT, not iNaT, as it could be confused with an integer\n            arr.take([-1, 1], allow_fill=True, fill_value=value)\n\n        value = np.timedelta64(\"NaT\", \"ns\")\n        msg = f\"'fill_value' should be a {self.dtype}. Got '{str(value)}'.\"\n        with pytest.raises(ValueError, match=msg):\n            # require appropriate-dtype if we have a NA value\n            arr.take([-1, 1], allow_fill=True, fill_value=value)\n\n    def test_concat_same_type_invalid(self, datetime_index):\n        # different timezones\n        dti = datetime_index\n        arr = DatetimeArray(dti)\n\n        if arr.tz is None:\n            other = arr.tz_localize(\"UTC\")\n        else:\n            other = arr.tz_localize(None)\n\n        with pytest.raises(ValueError, match=\"to_concat must have the same\"):\n            arr._concat_same_type([arr, other])\n\n    def test_concat_same_type_different_freq(self):\n        # we *can* concatenate DTI with different freqs.\n        a = DatetimeArray(pd.date_range(\"2000\", periods=2, freq=\"D\", tz=\"US/Central\"))\n        b = DatetimeArray(pd.date_range(\"2000\", periods=2, freq=\"H\", tz=\"US/Central\"))\n        result = DatetimeArray._concat_same_type([a, b])\n        expected = DatetimeArray(\n            pd.to_datetime(\n                [\n                    \"2000-01-01 00:00:00\",\n                    \"2000-01-02 00:00:00\",\n                    \"2000-01-01 00:00:00\",\n                    \"2000-01-01 01:00:00\",\n                ]\n            ).tz_localize(\"US/Central\")\n        )\n\n        tm.assert_datetime_array_equal(result, expected)\n\n    def test_strftime(self, datetime_index):\n        arr = DatetimeArray(datetime_index)\n\n        result = arr.strftime(\"%Y %b\")\n        expected = np.array([ts.strftime(\"%Y %b\") for ts in arr], dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_strftime_nat(self):\n        # GH 29578\n        arr = DatetimeArray(DatetimeIndex([\"2019-01-01\", pd.NaT]))\n\n        result = arr.strftime(\"%Y-%m-%d\")\n        expected = np.array([\"2019-01-01\", np.nan], dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n\nclass TestTimedeltaArray(SharedTests):\n    index_cls = pd.TimedeltaIndex\n    array_cls = TimedeltaArray\n    dtype = pd.Timedelta\n\n    def test_from_tdi(self):\n        tdi = pd.TimedeltaIndex([\"1 Day\", \"3 Hours\"])\n        arr = TimedeltaArray(tdi)\n        assert list(arr) == list(tdi)\n\n        # Check that Index.__new__ knows what to do with TimedeltaArray\n        tdi2 = pd.Index(arr)\n        assert isinstance(tdi2, pd.TimedeltaIndex)\n        assert list(tdi2) == list(arr)\n\n    def test_astype_object(self):\n        tdi = pd.TimedeltaIndex([\"1 Day\", \"3 Hours\"])\n        arr = TimedeltaArray(tdi)\n        asobj = arr.astype(\"O\")\n        assert isinstance(asobj, np.ndarray)\n        assert asobj.dtype == \"O\"\n        assert list(asobj) == list(tdi)\n\n    def test_to_pytimedelta(self, timedelta_index):\n        tdi = timedelta_index\n        arr = TimedeltaArray(tdi)\n\n        expected = tdi.to_pytimedelta()\n        result = arr.to_pytimedelta()\n\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_total_seconds(self, timedelta_index):\n        tdi = timedelta_index\n        arr = TimedeltaArray(tdi)\n\n        expected = tdi.total_seconds()\n        result = arr.total_seconds()\n\n        tm.assert_numpy_array_equal(result, expected.values)\n\n    @pytest.mark.parametrize(\"propname\", pd.TimedeltaIndex._field_ops)\n    def test_int_properties(self, timedelta_index, propname):\n        tdi = timedelta_index\n        arr = TimedeltaArray(tdi)\n\n        result = getattr(arr, propname)\n        expected = np.array(getattr(tdi, propname), dtype=result.dtype)\n\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_array_interface(self, timedelta_index):\n        arr = TimedeltaArray(timedelta_index)\n\n        # default asarray gives the same underlying data\n        result = np.asarray(arr)\n        expected = arr._data\n        assert result is expected\n        tm.assert_numpy_array_equal(result, expected)\n        result = np.array(arr, copy=False)\n        assert result is expected\n        tm.assert_numpy_array_equal(result, expected)\n\n        # specifying m8[ns] gives the same result as default\n        result = np.asarray(arr, dtype=\"timedelta64[ns]\")\n        expected = arr._data\n        assert result is expected\n        tm.assert_numpy_array_equal(result, expected)\n        result = np.array(arr, dtype=\"timedelta64[ns]\", copy=False)\n        assert result is expected\n        tm.assert_numpy_array_equal(result, expected)\n        result = np.array(arr, dtype=\"timedelta64[ns]\")\n        assert result is not expected\n        tm.assert_numpy_array_equal(result, expected)\n\n        # to object dtype\n        result = np.asarray(arr, dtype=object)\n        expected = np.array(list(arr), dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n        # to other dtype always copies\n        result = np.asarray(arr, dtype=\"int64\")\n        assert result is not arr.asi8\n        assert not np.may_share_memory(arr, result)\n        expected = arr.asi8.copy()\n        tm.assert_numpy_array_equal(result, expected)\n\n        # other dtypes handled by numpy\n        for dtype in [\"float64\", str]:\n            result = np.asarray(arr, dtype=dtype)\n            expected = np.asarray(arr).astype(dtype)\n            tm.assert_numpy_array_equal(result, expected)\n\n    def test_take_fill_valid(self, timedelta_index):\n        tdi = timedelta_index\n        arr = TimedeltaArray(tdi)\n\n        td1 = pd.Timedelta(days=1)\n        result = arr.take([-1, 1], allow_fill=True, fill_value=td1)\n        assert result[0] == td1\n\n        now = pd.Timestamp.now()\n        value = now\n        msg = f\"'fill_value' should be a {self.dtype}. Got '{value}'.\"\n        with pytest.raises(ValueError, match=msg):\n            # fill_value Timestamp invalid\n            arr.take([0, 1], allow_fill=True, fill_value=value)\n\n        value = now.to_period(\"D\")\n        msg = f\"'fill_value' should be a {self.dtype}. Got '{value}'.\"\n        with pytest.raises(ValueError, match=msg):\n            # fill_value Period invalid\n            arr.take([0, 1], allow_fill=True, fill_value=value)\n\n        value = np.datetime64(\"NaT\", \"ns\")\n        msg = f\"'fill_value' should be a {self.dtype}. Got '{str(value)}'.\"\n        with pytest.raises(ValueError, match=msg):\n            # require appropriate-dtype if we have a NA value\n            arr.take([-1, 1], allow_fill=True, fill_value=value)\n\n\nclass TestPeriodArray(SharedTests):\n    index_cls = pd.PeriodIndex\n    array_cls = PeriodArray\n    dtype = pd.Period\n\n    @pytest.fixture\n    def arr1d(self, period_index):\n        return period_index._data\n\n    def test_from_pi(self, period_index):\n        pi = period_index\n        arr = PeriodArray(pi)\n        assert list(arr) == list(pi)\n\n        # Check that Index.__new__ knows what to do with PeriodArray\n        pi2 = pd.Index(arr)\n        assert isinstance(pi2, pd.PeriodIndex)\n        assert list(pi2) == list(arr)\n\n    def test_astype_object(self, period_index):\n        pi = period_index\n        arr = PeriodArray(pi)\n        asobj = arr.astype(\"O\")\n        assert isinstance(asobj, np.ndarray)\n        assert asobj.dtype == \"O\"\n        assert list(asobj) == list(pi)\n\n    def test_take_fill_valid(self, period_index):\n        pi = period_index\n        arr = PeriodArray(pi)\n\n        value = pd.NaT.value\n        msg = f\"'fill_value' should be a {self.dtype}. Got '{value}'.\"\n        with pytest.raises(ValueError, match=msg):\n            # require NaT, not iNaT, as it could be confused with an integer\n            arr.take([-1, 1], allow_fill=True, fill_value=value)\n\n        value = np.timedelta64(\"NaT\", \"ns\")\n        msg = f\"'fill_value' should be a {self.dtype}. Got '{str(value)}'.\"\n        with pytest.raises(ValueError, match=msg):\n            # require appropriate-dtype if we have a NA value\n            arr.take([-1, 1], allow_fill=True, fill_value=value)\n\n    @pytest.mark.parametrize(\"how\", [\"S\", \"E\"])\n    def test_to_timestamp(self, how, period_index):\n        pi = period_index\n        arr = PeriodArray(pi)\n\n        expected = DatetimeArray(pi.to_timestamp(how=how))\n        result = arr.to_timestamp(how=how)\n        assert isinstance(result, DatetimeArray)\n\n        # placeholder until these become actual EA subclasses and we can use\n        #  an EA-specific tm.assert_ function\n        tm.assert_index_equal(pd.Index(result), pd.Index(expected))\n\n    def test_to_timestamp_out_of_bounds(self):\n        # GH#19643 previously overflowed silently\n        pi = pd.period_range(\"1500\", freq=\"Y\", periods=3)\n        msg = \"Out of bounds nanosecond timestamp: 1500-01-01 00:00:00\"\n        with pytest.raises(OutOfBoundsDatetime, match=msg):\n            pi.to_timestamp()\n\n        with pytest.raises(OutOfBoundsDatetime, match=msg):\n            pi._data.to_timestamp()\n\n    @pytest.mark.parametrize(\"propname\", PeriodArray._bool_ops)\n    def test_bool_properties(self, period_index, propname):\n        # in this case _bool_ops is just `is_leap_year`\n        pi = period_index\n        arr = PeriodArray(pi)\n\n        result = getattr(arr, propname)\n        expected = np.array(getattr(pi, propname))\n\n        tm.assert_numpy_array_equal(result, expected)\n\n    @pytest.mark.parametrize(\"propname\", PeriodArray._field_ops)\n    def test_int_properties(self, period_index, propname):\n        pi = period_index\n        arr = PeriodArray(pi)\n\n        result = getattr(arr, propname)\n        expected = np.array(getattr(pi, propname))\n\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_array_interface(self, period_index):\n        arr = PeriodArray(period_index)\n\n        # default asarray gives objects\n        result = np.asarray(arr)\n        expected = np.array(list(arr), dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n        # to object dtype (same as default)\n        result = np.asarray(arr, dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n        result = np.asarray(arr, dtype=\"int64\")\n        tm.assert_numpy_array_equal(result, arr.asi8)\n\n        # to other dtypes\n        msg = r\"float\\(\\) argument must be a string or a number, not 'Period'\"\n        with pytest.raises(TypeError, match=msg):\n            np.asarray(arr, dtype=\"float64\")\n\n        result = np.asarray(arr, dtype=\"S20\")\n        expected = np.asarray(arr).astype(\"S20\")\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_strftime(self, period_index):\n        arr = PeriodArray(period_index)\n\n        result = arr.strftime(\"%Y\")\n        expected = np.array([per.strftime(\"%Y\") for per in arr], dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_strftime_nat(self):\n        # GH 29578\n        arr = PeriodArray(PeriodIndex([\"2019-01-01\", pd.NaT], dtype=\"period[D]\"))\n\n        result = arr.strftime(\"%Y-%m-%d\")\n        expected = np.array([\"2019-01-01\", np.nan], dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"array,casting_nats\",\n    [\n        (\n            pd.TimedeltaIndex([\"1 Day\", \"3 Hours\", \"NaT\"])._data,\n            (pd.NaT, np.timedelta64(\"NaT\", \"ns\")),\n        ),\n        (\n            pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")._data,\n            (pd.NaT, np.datetime64(\"NaT\", \"ns\")),\n        ),\n        (pd.period_range(\"2000-01-01\", periods=3, freq=\"D\")._data, (pd.NaT,)),\n    ],\n    ids=lambda x: type(x).__name__,\n)\ndef test_casting_nat_setitem_array(array, casting_nats):\n    expected = type(array)._from_sequence([pd.NaT, array[1], array[2]])\n\n    for nat in casting_nats:\n        arr = array.copy()\n        arr[0] = nat\n        tm.assert_equal(arr, expected)\n\n\n@pytest.mark.parametrize(\n    \"array,non_casting_nats\",\n    [\n        (\n            pd.TimedeltaIndex([\"1 Day\", \"3 Hours\", \"NaT\"])._data,\n            (np.datetime64(\"NaT\", \"ns\"), pd.NaT.value),\n        ),\n        (\n            pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")._data,\n            (np.timedelta64(\"NaT\", \"ns\"), pd.NaT.value),\n        ),\n        (\n            pd.period_range(\"2000-01-01\", periods=3, freq=\"D\")._data,\n            (np.datetime64(\"NaT\", \"ns\"), np.timedelta64(\"NaT\", \"ns\"), pd.NaT.value),\n        ),\n    ],\n    ids=lambda x: type(x).__name__,\n)\ndef test_invalid_nat_setitem_array(array, non_casting_nats):\n    msg = (\n        \"'value' should be a '(Timestamp|Timedelta|Period)', 'NaT', or array of those. \"\n        \"Got '(timedelta64|datetime64|int)' instead.\"\n    )\n\n    for nat in non_casting_nats:\n        with pytest.raises(TypeError, match=msg):\n            array[0] = nat\n\n\n@pytest.mark.parametrize(\n    \"array\",\n    [\n        pd.date_range(\"2000\", periods=4).array,\n        pd.timedelta_range(\"2000\", periods=4).array,\n    ],\n)\ndef test_to_numpy_extra(array):\n    if np_version_under1p18:\n        # np.isnan(NaT) raises, so use pandas'\n        isnan = pd.isna\n    else:\n        isnan = np.isnan\n\n    array[0] = pd.NaT\n    original = array.copy()\n\n    result = array.to_numpy()\n    assert isnan(result[0])\n\n    result = array.to_numpy(dtype=\"int64\")\n    assert result[0] == -9223372036854775808\n\n    result = array.to_numpy(dtype=\"int64\", na_value=0)\n    assert result[0] == 0\n\n    result = array.to_numpy(na_value=array[1].to_numpy())\n    assert result[0] == result[1]\n\n    result = array.to_numpy(na_value=array[1].to_numpy(copy=False))\n    assert result[0] == result[1]\n\n    tm.assert_equal(array, original)\n\n\n@pytest.mark.parametrize(\"as_index\", [True, False])\n@pytest.mark.parametrize(\n    \"values\",\n    [\n        pd.to_datetime([\"2020-01-01\", \"2020-02-01\"]),\n        pd.TimedeltaIndex([1, 2], unit=\"D\"),\n        pd.PeriodIndex([\"2020-01-01\", \"2020-02-01\"], freq=\"D\"),\n    ],\n)\n@pytest.mark.parametrize(\n    \"klass\",\n    [\n        list,\n        np.array,\n        pd.array,\n        pd.Series,\n        pd.Index,\n        pd.Categorical,\n        pd.CategoricalIndex,\n    ],\n)\ndef test_searchsorted_datetimelike_with_listlike(values, klass, as_index):\n    # https://github.com/pandas-dev/pandas/issues/32762\n    if not as_index:\n        values = values._data\n\n    result = values.searchsorted(klass(values))\n    expected = np.array([0, 1], dtype=result.dtype)\n\n    tm.assert_numpy_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"values\",\n    [\n        pd.to_datetime([\"2020-01-01\", \"2020-02-01\"]),\n        pd.TimedeltaIndex([1, 2], unit=\"D\"),\n        pd.PeriodIndex([\"2020-01-01\", \"2020-02-01\"], freq=\"D\"),\n    ],\n)\n@pytest.mark.parametrize(\n    \"arg\", [[1, 2], [\"a\", \"b\"], [pd.Timestamp(\"2020-01-01\", tz=\"Europe/London\")] * 2]\n)\ndef test_searchsorted_datetimelike_with_listlike_invalid_dtype(values, arg):\n    # https://github.com/pandas-dev/pandas/issues/32762\n    msg = \"[Unexpected type|Cannot compare]\"\n    with pytest.raises(TypeError, match=msg):\n        values.searchsorted(arg)\n\n\n@pytest.mark.parametrize(\"klass\", [list, tuple, np.array, pd.Series])\ndef test_period_index_construction_from_strings(klass):\n    # https://github.com/pandas-dev/pandas/issues/26109\n    strings = [\"2020Q1\", \"2020Q2\"] * 2\n    data = klass(strings)\n    result = PeriodIndex(data, freq=\"Q\")\n    expected = PeriodIndex([Period(s) for s in strings])\n    tm.assert_index_equal(result, expected)\n"
    }
  ],
  "questions": [],
  "golden_answers": [],
  "questions_generated": [
    "What specific update was made to the `.searchsorted()` method in the `pandas-dev/pandas` repository?",
    "In which file would you find the definition of the `.searchsorted()` method for datetimelike arrays in the `pandas` library?",
    "Why is it important to add examples to the doc-string of the `.searchsorted()` method?",
    "What are some of the modules imported in the `pandas/core/arrays/datetimelike.py` file that are relevant to the modification of the `.searchsorted()` method?",
    "How can a developer take on the task of adding searchsorted examples to the documentation in the `pandas-dev/pandas` repository?"
  ],
  "golden_answers_generated": [
    "The `.searchsorted()` method on the datetimelike indexes was updated to accept string and list-like inputs as part of the changes made in PR https://github.com/pandas-dev/pandas/pull/36346.",
    "The definition of the `.searchsorted()` method for datetimelike arrays can be found in the `pandas/core/arrays/datetimelike.py` file.",
    "Adding examples to the doc-string is important because it helps users understand how to use the updated functionality of the `.searchsorted()` method, specifically how to pass string and list-like inputs, thereby improving the overall usability and documentation quality of the pandas library.",
    "Some relevant modules imported in the `pandas/core/arrays/datetimelike.py` file include `numpy`, `pandas._libs.tslibs`, and various pandas core and utility modules such as `pandas.core.algorithms` and `pandas.util._decorators`. These imports may be utilized for handling datetime operations and array manipulations within the `.searchsorted()` method.",
    "A developer can take on the task by commenting 'take' on the issue in the GitHub repository, as indicated by the discussion where a bot assigns the issue to the first person who comments 'take'. This is a common practice in open-source projects to manage contributions and ensure that tasks are not duplicated."
  ]
}