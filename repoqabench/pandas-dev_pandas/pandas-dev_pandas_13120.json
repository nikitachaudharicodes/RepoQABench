{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "13120",
  "issue_description": "# BUG: PeriodIndex construction fails (or produces erroneous result) with memoryview as input.\n\n#### Code Sample, a copy-pastable example if possible\n\nFirst noted in discussion here: https://github.com/quantopian/zipline/pull/1190/files/fe5a2a888a498d838c3cc43de2c33ac08b20d2a7#r62506342.  This most often matters when passing a value typed in Cython as something like `int64_t[:]`.\n\nSimple repro cases:\n\nDatetimeIndex construction fails (looks like trying to parse strings?), and vanilla Index construction returns an array of unicode strings.\n\n```\nIn [6]: import pandas as pd; import numpy as np\n\nIn [7]: pd.__version__, np.__version__\nOut[7]: ('0.18.1+20.gaf7bdd3', '1.11.0')\n\nIn [8]: m = memoryview(np.arange(5))\n\nIn [9]: pd.DatetimeIndex(m)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-9-09b855a663a4> in <module>()\n----> 1 pd.DatetimeIndex(m)\n\n/home/ssanderson/clones/pandas/pandas/util/decorators.pyc in wrapper(*args, **kwargs)\n     89                 else:\n     90                     kwargs[new_arg_name] = new_arg_value\n---> 91             return func(*args, **kwargs)\n     92         return wrapper\n     93     return _deprecate_kwarg\n\n/home/ssanderson/clones/pandas/pandas/tseries/index.pyc in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, closed, ambiguous, dtype, **kwargs)\n    283                 data = tslib.parse_str_array_to_datetime(data, freq=freq,\n    284                                                          dayfirst=dayfirst,\n--> 285                                                          yearfirst=yearfirst)\n    286             else:\n    287                 data = tools.to_datetime(data, errors='raise')\n\n/home/ssanderson/clones/pandas/pandas/tslib.pyx in pandas.tslib.parse_str_array_to_datetime (pandas/tslib.c:39610)()\n   2175                         except ValueError:\n   2176                             if is_coerce:\n-> 2177                                 iresult[i] = NPY_NAT\n   2178                                 continue\n   2179                             raise\n\nValueError:\n\nIn [10]: pd.Index(m)\nOut[10]: Index([u'', u'', u'', u'', u''], dtype='object')\n```\n#### Expected Output\n\nI'd expect this to either error immediately or provide the same result as coercing the provided memoryview into a numpy array.\n#### output of `pd.show_versions()`\n\n```\nIn [6]: pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: af7bdd3883c8d61e9d9388d3aa699930eee7fff8\npython: 2.7.10.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-16-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1+20.gaf7bdd3\nnose: None\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.24\nnumpy: 1.11.0\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: None\npandas_datareader: None\n```\n\ncc @jbredeche\n",
  "issue_comments": [
    {
      "id": 217905248,
      "user": "jreback",
      "body": "I suppose. You are much better off just doing `np.array(m)` before sending this in. If you want to push a PR which checks if its a memory buffer then ok.\n"
    },
    {
      "id": 217905548,
      "user": "jreback",
      "body": "note that `_simple_new` methods are NOT public, you should really really not sue internal/private things.\n"
    },
    {
      "id": 217905809,
      "user": "jreback",
      "body": "We prob don't handle construction for Series/DataFrame with a memory buffer either.\n"
    },
    {
      "id": 217916629,
      "user": "ssanderson",
      "body": "> You are much better off just doing np.array(m) before sending this in\n\nThis is what we're currently doing (and what we'll probably continue to do for a while since we support older versions of pandas).\n\n> note that _simple_new methods are NOT public, you should really really not sue internal/private things.\n\nYep, I assumed as much.  What would you think about making public versions of some of the Pandas object constructors that accept limited, specific arguments and don't do any coercion?  We frequently have the problem in Zipline that we have an object of a well-known type and we want to construct an Index/Series/DataFrame without incurring the extra overhead of Pandas having to infer the type of our input.\n"
    },
    {
      "id": 217917508,
      "user": "jreback",
      "body": "@ssanderson not sure what you mean. The inference is generally very light so you shouldn't incur costs with anything. If you have a specific example I can look.\n"
    },
    {
      "id": 217921900,
      "user": "ssanderson",
      "body": "> If you want to push a PR which checks if its a memory buffer then ok.\n\nWould the preferred behavior here be to coerce to `ndarray`, or to treat this as an error and barf?  It seems like Pandas' general philosophy is to try its best to do reasonable coercions when possible.\n"
    },
    {
      "id": 217922047,
      "user": "ssanderson",
      "body": "> The inference is generally very light so you shouldn't incur costs with anything. If you have a specific example I can look.\n\nThe case that comes to mind offhand is something like `DataFrame.from_records` when reading values out of a tabular database.  In that case, we know that all our records will have the same columns and dtypes, but there isn't a good way to tell pandas that information.\n"
    },
    {
      "id": 217923752,
      "user": "jreback",
      "body": "@ssanderson no you could certainly add it as a clause somewhere here: https://github.com/pydata/pandas/blob/master/pandas/indexes/base.py#L124\n\neasy enough to add a: \n\n```\ndef coerce_memory_view_to_ndarray(data):\n    if isinstance(data, memoryview):\n           data = np.array(data)\n   return data\n```\n\nthough better to put in the if/else's if possible (as it reduces the checks needed) e.g. right before the scalar check.\n\n```\n        elif isinstance(data, memoryview)\n               return cls(np.array(data), ......)\n```\n"
    },
    {
      "id": 217925219,
      "user": "jreback",
      "body": "btw `.from_records` is trivial, give it a structured array\n\n```\nIn [9]: arr = np.zeros((2,), dtype=('i4,f4,a10'))\n\nIn [10]: arr[:] = [(1, 2., 'Hello'), (2, 3., \"World\")]\n\nIn [11]: arr\nOut[11]: \narray([(1, 2.0, 'Hello'), (2, 3.0, 'World')], \n      dtype=[('f0', '<i4'), ('f1', '<f4'), ('f2', 'S10')])\n\nIn [12]: DataFrame.from_records(arr)\nOut[12]: \n   f0   f1     f2\n0   1  2.0  Hello\n1   2  3.0  World\n```\n"
    },
    {
      "id": 217930719,
      "user": "ssanderson",
      "body": "> btw .from_records is trivial, give it a structured array\n\nTIL\n"
    },
    {
      "id": 217933248,
      "user": "jreback",
      "body": "@ssanderson this is what all of the read-sql type things do (and pytables as well), these are row-structured arrays essentially.\n"
    },
    {
      "id": 445392510,
      "user": "jbrockmendel",
      "body": "FWIW this works in master for DatetimeIndex and TimedeltaIndex (though not for PeriodIndex):\r\n\r\n```\r\nm = memoryview(np.arange(5))\r\n\r\ndti = pd.DatetimeIndex(m)\r\ntdi = pd.TimedeltaIndex(m)\r\n\r\npd.PeriodIndex(m, freq='Y')  # <-- pandas._libs.tslibs.parsing.DateParseError: Unable to parse datetime string: \r\npd.Int64Index(m)   # <-- TypeError: String dtype not supported, you may need to explicitly cast to a numeric type\r\npd.Float64Index(m) # <-- TypeError: String dtype not supported, you may need to explicitly cast to a numeric type\r\n\r\n>>> pd.Index(m)\r\nIndex([u'', u'', u'', u'', u''], dtype='object')\r\n```"
    },
    {
      "id": 623844838,
      "user": "mroeschke",
      "body": "Only fails with `PeriodIndex` now on master\r\n\r\n```\r\nIn [25]: pd.Int64Index(m)\r\nOut[25]: Int64Index([0, 1, 2, 3, 4], dtype='int64')\r\n\r\nIn [26]: pd.Float64Index(m)\r\nOut[26]: Float64Index([0.0, 1.0, 2.0, 3.0, 4.0], dtype='float64')\r\n\r\nIn [27]: pd.Index(m)\r\nOut[27]: Int64Index([0, 1, 2, 3, 4], dtype='int64')\r\n\r\nIn [28]: pd.PeriodIndex(m, freq='Y')\r\nDateParseError: day is out of range for month\r\n\r\nIn [29]: pd.__version__\r\nOut[29]: '1.1.0.dev0+1466.ga3477c769.dirty'\r\n```"
    },
    {
      "id": 625478168,
      "user": "mroeschke",
      "body": "Actually this fully works for a PeriodIndex. Could all use a test\r\n\r\n```\r\nIn [1]: m = memoryview(np.arange(2000, 2005))\r\n\r\nIn [2]: pd.PeriodIndex(m, freq='Y')\r\nOut[2]: PeriodIndex(['2000', '2001', '2002', '2003', '2004'], dtype='period[A-DEC]', freq='A-DEC')\r\n```"
    }
  ],
  "text_context": "# BUG: PeriodIndex construction fails (or produces erroneous result) with memoryview as input.\n\n#### Code Sample, a copy-pastable example if possible\n\nFirst noted in discussion here: https://github.com/quantopian/zipline/pull/1190/files/fe5a2a888a498d838c3cc43de2c33ac08b20d2a7#r62506342.  This most often matters when passing a value typed in Cython as something like `int64_t[:]`.\n\nSimple repro cases:\n\nDatetimeIndex construction fails (looks like trying to parse strings?), and vanilla Index construction returns an array of unicode strings.\n\n```\nIn [6]: import pandas as pd; import numpy as np\n\nIn [7]: pd.__version__, np.__version__\nOut[7]: ('0.18.1+20.gaf7bdd3', '1.11.0')\n\nIn [8]: m = memoryview(np.arange(5))\n\nIn [9]: pd.DatetimeIndex(m)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-9-09b855a663a4> in <module>()\n----> 1 pd.DatetimeIndex(m)\n\n/home/ssanderson/clones/pandas/pandas/util/decorators.pyc in wrapper(*args, **kwargs)\n     89                 else:\n     90                     kwargs[new_arg_name] = new_arg_value\n---> 91             return func(*args, **kwargs)\n     92         return wrapper\n     93     return _deprecate_kwarg\n\n/home/ssanderson/clones/pandas/pandas/tseries/index.pyc in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, closed, ambiguous, dtype, **kwargs)\n    283                 data = tslib.parse_str_array_to_datetime(data, freq=freq,\n    284                                                          dayfirst=dayfirst,\n--> 285                                                          yearfirst=yearfirst)\n    286             else:\n    287                 data = tools.to_datetime(data, errors='raise')\n\n/home/ssanderson/clones/pandas/pandas/tslib.pyx in pandas.tslib.parse_str_array_to_datetime (pandas/tslib.c:39610)()\n   2175                         except ValueError:\n   2176                             if is_coerce:\n-> 2177                                 iresult[i] = NPY_NAT\n   2178                                 continue\n   2179                             raise\n\nValueError:\n\nIn [10]: pd.Index(m)\nOut[10]: Index([u'', u'', u'', u'', u''], dtype='object')\n```\n#### Expected Output\n\nI'd expect this to either error immediately or provide the same result as coercing the provided memoryview into a numpy array.\n#### output of `pd.show_versions()`\n\n```\nIn [6]: pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: af7bdd3883c8d61e9d9388d3aa699930eee7fff8\npython: 2.7.10.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.2.0-16-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.18.1+20.gaf7bdd3\nnose: None\npip: 8.1.0\nsetuptools: 20.2.2\nCython: 0.24\nnumpy: 1.11.0\nscipy: None\nstatsmodels: None\nxarray: None\nIPython: 4.2.0\nsphinx: None\npatsy: None\ndateutil: 2.5.3\npytz: 2016.3\nblosc: None\nbottleneck: None\ntables: None\nnumexpr: None\nmatplotlib: None\nopenpyxl: None\nxlrd: None\nxlwt: None\nxlsxwriter: None\nlxml: None\nbs4: None\nhtml5lib: None\nhttplib2: None\napiclient: None\nsqlalchemy: None\npymysql: None\npsycopg2: None\njinja2: None\nboto: None\npandas_datareader: None\n```\n\ncc @jbredeche\n\n\nI suppose. You are much better off just doing `np.array(m)` before sending this in. If you want to push a PR which checks if its a memory buffer then ok.\n\n\nnote that `_simple_new` methods are NOT public, you should really really not sue internal/private things.\n\n\nWe prob don't handle construction for Series/DataFrame with a memory buffer either.\n\n\n> You are much better off just doing np.array(m) before sending this in\n\nThis is what we're currently doing (and what we'll probably continue to do for a while since we support older versions of pandas).\n\n> note that _simple_new methods are NOT public, you should really really not sue internal/private things.\n\nYep, I assumed as much.  What would you think about making public versions of some of the Pandas object constructors that accept limited, specific arguments and don't do any coercion?  We frequently have the problem in Zipline that we have an object of a well-known type and we want to construct an Index/Series/DataFrame without incurring the extra overhead of Pandas having to infer the type of our input.\n\n\n@ssanderson not sure what you mean. The inference is generally very light so you shouldn't incur costs with anything. If you have a specific example I can look.\n\n\n> If you want to push a PR which checks if its a memory buffer then ok.\n\nWould the preferred behavior here be to coerce to `ndarray`, or to treat this as an error and barf?  It seems like Pandas' general philosophy is to try its best to do reasonable coercions when possible.\n\n\n> The inference is generally very light so you shouldn't incur costs with anything. If you have a specific example I can look.\n\nThe case that comes to mind offhand is something like `DataFrame.from_records` when reading values out of a tabular database.  In that case, we know that all our records will have the same columns and dtypes, but there isn't a good way to tell pandas that information.\n\n\n@ssanderson no you could certainly add it as a clause somewhere here: https://github.com/pydata/pandas/blob/master/pandas/indexes/base.py#L124\n\neasy enough to add a: \n\n```\ndef coerce_memory_view_to_ndarray(data):\n    if isinstance(data, memoryview):\n           data = np.array(data)\n   return data\n```\n\nthough better to put in the if/else's if possible (as it reduces the checks needed) e.g. right before the scalar check.\n\n```\n        elif isinstance(data, memoryview)\n               return cls(np.array(data), ......)\n```\n\n\nbtw `.from_records` is trivial, give it a structured array\n\n```\nIn [9]: arr = np.zeros((2,), dtype=('i4,f4,a10'))\n\nIn [10]: arr[:] = [(1, 2., 'Hello'), (2, 3., \"World\")]\n\nIn [11]: arr\nOut[11]: \narray([(1, 2.0, 'Hello'), (2, 3.0, 'World')], \n      dtype=[('f0', '<i4'), ('f1', '<f4'), ('f2', 'S10')])\n\nIn [12]: DataFrame.from_records(arr)\nOut[12]: \n   f0   f1     f2\n0   1  2.0  Hello\n1   2  3.0  World\n```\n\n\n> btw .from_records is trivial, give it a structured array\n\nTIL\n\n\n@ssanderson this is what all of the read-sql type things do (and pytables as well), these are row-structured arrays essentially.\n\n\nFWIW this works in master for DatetimeIndex and TimedeltaIndex (though not for PeriodIndex):\r\n\r\n```\r\nm = memoryview(np.arange(5))\r\n\r\ndti = pd.DatetimeIndex(m)\r\ntdi = pd.TimedeltaIndex(m)\r\n\r\npd.PeriodIndex(m, freq='Y')  # <-- pandas._libs.tslibs.parsing.DateParseError: Unable to parse datetime string: \r\npd.Int64Index(m)   # <-- TypeError: String dtype not supported, you may need to explicitly cast to a numeric type\r\npd.Float64Index(m) # <-- TypeError: String dtype not supported, you may need to explicitly cast to a numeric type\r\n\r\n>>> pd.Index(m)\r\nIndex([u'', u'', u'', u'', u''], dtype='object')\r\n```\n\nOnly fails with `PeriodIndex` now on master\r\n\r\n```\r\nIn [25]: pd.Int64Index(m)\r\nOut[25]: Int64Index([0, 1, 2, 3, 4], dtype='int64')\r\n\r\nIn [26]: pd.Float64Index(m)\r\nOut[26]: Float64Index([0.0, 1.0, 2.0, 3.0, 4.0], dtype='float64')\r\n\r\nIn [27]: pd.Index(m)\r\nOut[27]: Int64Index([0, 1, 2, 3, 4], dtype='int64')\r\n\r\nIn [28]: pd.PeriodIndex(m, freq='Y')\r\nDateParseError: day is out of range for month\r\n\r\nIn [29]: pd.__version__\r\nOut[29]: '1.1.0.dev0+1466.ga3477c769.dirty'\r\n```\n\nActually this fully works for a PeriodIndex. Could all use a test\r\n\r\n```\r\nIn [1]: m = memoryview(np.arange(2000, 2005))\r\n\r\nIn [2]: pd.PeriodIndex(m, freq='Y')\r\nOut[2]: PeriodIndex(['2000', '2001', '2002', '2003', '2004'], dtype='period[A-DEC]', freq='A-DEC')\r\n```",
  "pr_link": "https://github.com/quantopian/zipline/pull/1190",
  "code_context": [
    {
      "filename": "zipline/algorithm.py",
      "content": "#\n# Copyright 2015 Quantopian, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom copy import copy\nimport operator as op\nimport warnings\n\nimport logbook\nimport pytz\nimport pandas as pd\nfrom contextlib2 import ExitStack\nfrom pandas.tseries.tools import normalize_date\nimport numpy as np\n\nfrom itertools import chain, repeat\nfrom numbers import Integral\n\nfrom six import (\n    exec_,\n    iteritems,\n    itervalues,\n    string_types,\n)\n\nfrom zipline._protocol import handle_non_market_minutes\nfrom zipline.assets.synthetic import make_simple_equity_info\nfrom zipline.data.data_portal import DataPortal\nfrom zipline.errors import (\n    AttachPipelineAfterInitialize,\n    HistoryInInitialize,\n    NoSuchPipeline,\n    OrderDuringInitialize,\n    PipelineOutputDuringInitialize,\n    RegisterAccountControlPostInit,\n    RegisterTradingControlPostInit,\n    SetBenchmarkOutsideInitialize,\n    SetCommissionPostInit,\n    SetSlippagePostInit,\n    UnsupportedCommissionModel,\n    UnsupportedDatetimeFormat,\n    UnsupportedOrderParameters,\n    UnsupportedSlippageModel,\n    CannotOrderDelistedAsset, UnsupportedCancelPolicy, SetCancelPolicyPostInit,\n    OrderInBeforeTradingStart)\nfrom zipline.finance.trading import TradingEnvironment\nfrom zipline.finance.blotter import Blotter\nfrom zipline.finance.commission import PerShare, PerTrade, PerDollar\nfrom zipline.finance.controls import (\n    LongOnly,\n    MaxOrderCount,\n    MaxOrderSize,\n    MaxPositionSize,\n    MaxLeverage,\n    RestrictedListOrder\n)\nfrom zipline.finance.execution import (\n    LimitOrder,\n    MarketOrder,\n    StopLimitOrder,\n    StopOrder,\n)\nfrom zipline.finance.performance import PerformanceTracker\nfrom zipline.finance.slippage import (\n    VolumeShareSlippage,\n    SlippageModel\n)\nfrom zipline.finance.cancel_policy import NeverCancel, CancelPolicy\nfrom zipline.assets import Asset, Equity, Future\nfrom zipline.assets.futures import FutureChain\nfrom zipline.gens.tradesimulation import AlgorithmSimulator\nfrom zipline.pipeline.engine import (\n    NoOpPipelineEngine,\n    SimplePipelineEngine,\n)\nfrom zipline.utils.api_support import (\n    api_method,\n    require_initialized,\n    require_not_initialized,\n    ZiplineAPI,\n    disallowed_in_before_trading_start)\n\nfrom zipline.utils.input_validation import ensure_upper_case, error_keywords\nfrom zipline.utils.cache import CachedObject, Expired\nimport zipline.utils.events\nfrom zipline.utils.events import (\n    EventManager,\n    make_eventrule,\n    DateRuleFactory,\n    TimeRuleFactory,\n)\nfrom zipline.utils.factory import create_simulation_parameters\nfrom zipline.utils.math_utils import (\n    tolerant_equals,\n    round_if_near_integer\n)\nfrom zipline.utils.preprocess import preprocess\n\nimport zipline.protocol\nfrom zipline.sources.requests_csv import PandasRequestsCSV\n\nfrom zipline.gens.sim_engine import (\n    MinuteSimulationClock,\n    DailySimulationClock,\n)\nfrom zipline.sources.benchmark_source import BenchmarkSource\nfrom zipline.zipline_warnings import ZiplineDeprecationWarning\n\nDEFAULT_CAPITAL_BASE = 1e5\n\n\nlog = logbook.Logger(\"ZiplineLog\")\n\n\nclass TradingAlgorithm(object):\n    \"\"\"A class that represents a trading strategy and parameters to execute\n    the strategy.\n\n    Parameters\n    ----------\n    *args, **kwargs\n        Forwarded to ``initialize`` unless listed below.\n    initialize : callable[context -> None], optional\n        Function that is called at the start of the simulation to\n        setup the initial context.\n    handle_data : callable[(context, data) -> None], optional\n        Function called on every bar. This is where most logic should be\n        implemented.\n    before_trading_start : callable[(context, data) -> None], optional\n        Function that is called before any bars have been processed each\n        day.\n    analyze : callable[(context, DataFrame) -> None], optional\n        Function that is called at the end of the backtest. This is passed\n        the context and the performance results for the backtest.\n    script : str, optional\n        Algoscript that contains the definitions for the four algorithm\n        lifecycle functions and any supporting code.\n    namespace : dict, optional\n        The namespace to execute the algoscript in. By default this is an\n        empty namespace that will include only python built ins.\n    algo_filename : str, optional\n        The filename for the algoscript. This will be used in exception\n        tracebacks. default: '<string>'.\n    data_frequency : {'daily', 'minute'}, optional\n        The duration of the bars.\n    capital_base : float, optional\n        How much capital to start with. default: 1.0e5\n    instant_fill : bool, optional\n        Whether to fill orders immediately or on next bar. default: False\n    equities_metadata : dict or DataFrame or file-like object, optional\n        If dict is provided, it must have the following structure:\n        * keys are the identifiers\n        * values are dicts containing the metadata, with the metadata\n          field name as the key\n        If pandas.DataFrame is provided, it must have the\n        following structure:\n        * column names must be the metadata fields\n        * index must be the different asset identifiers\n        * array contents should be the metadata value\n        If an object with a ``read`` method is provided, ``read`` must\n        return rows containing at least one of 'sid' or 'symbol' along\n        with the other metadata fields.\n    futures_metadata : dict or DataFrame or file-like object, optional\n        The same layout as ``equities_metadata`` except that it is used\n        for futures information.\n    identifiers : list, optional\n        Any asset identifiers that are not provided in the\n        equities_metadata, but will be traded by this TradingAlgorithm.\n    get_pipeline_loader : callable[BoundColumn -> PipelineLoader], optional\n        The function that maps pipeline columns to their loaders.\n    create_event_context : callable[BarData -> context manager], optional\n        A function used to create a context mananger that wraps the\n        execution of all events that are scheduled for a bar.\n        This function will be passed the data for the bar and should\n        return the actual context manager that will be entered.\n    history_container_class : type, optional\n        The type of history container to use. default: HistoryContainer\n    platform : str, optional\n        The platform the simulation is running on. This can be queried for\n        in the simulation with ``get_environment``. This allows algorithms\n        to conditionally execute code based on platform it is running on.\n        default: 'zipline'\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize sids and other state variables.\n\n        :Arguments:\n        :Optional:\n            initialize : function\n                Function that is called with a single\n                argument at the begninning of the simulation.\n            handle_data : function\n                Function that is called with 2 arguments\n                (context and data) on every bar.\n            script : str\n                Algoscript that contains initialize and\n                handle_data function definition.\n            data_frequency : {'daily', 'minute'}\n               The duration of the bars.\n            capital_base : float <default: 1.0e5>\n               How much capital to start with.\n            asset_finder : An AssetFinder object\n                A new AssetFinder object to be used in this TradingEnvironment\n            equities_metadata : can be either:\n                            - dict\n                            - pandas.DataFrame\n                            - object with 'read' property\n                If dict is provided, it must have the following structure:\n                * keys are the identifiers\n                * values are dicts containing the metadata, with the metadata\n                  field name as the key\n                If pandas.DataFrame is provided, it must have the\n                following structure:\n                * column names must be the metadata fields\n                * index must be the different asset identifiers\n                * array contents should be the metadata value\n                If an object with a 'read' property is provided, 'read' must\n                return rows containing at least one of 'sid' or 'symbol' along\n                with the other metadata fields.\n            identifiers : List\n                Any asset identifiers that are not provided in the\n                equities_metadata, but will be traded by this TradingAlgorithm\n        \"\"\"\n        self.sources = []\n\n        # List of trading controls to be used to validate orders.\n        self.trading_controls = []\n\n        # List of account controls to be checked on each bar.\n        self.account_controls = []\n\n        self._recorded_vars = {}\n        self.namespace = kwargs.pop('namespace', {})\n\n        self._platform = kwargs.pop('platform', 'zipline')\n\n        self.logger = None\n\n        self.data_portal = kwargs.pop('data_portal', None)\n\n        # If an env has been provided, pop it\n        self.trading_environment = kwargs.pop('env', None)\n\n        if self.trading_environment is None:\n            self.trading_environment = TradingEnvironment()\n\n        # Update the TradingEnvironment with the provided asset metadata\n        if 'equities_metadata' in kwargs or 'futures_metadata' in kwargs:\n            warnings.warn(\n                'passing metadata to TradingAlgorithm is deprecated; please'\n                ' write this data into the asset db before passing it to the'\n                ' trading environment',\n                DeprecationWarning,\n                stacklevel=1,\n            )\n            self.trading_environment.write_data(\n                equities=kwargs.pop('equities_metadata', None),\n                futures=kwargs.pop('futures_metadata', None),\n            )\n\n        # set the capital base\n        self.capital_base = kwargs.pop('capital_base', DEFAULT_CAPITAL_BASE)\n        self.sim_params = kwargs.pop('sim_params', None)\n        if self.sim_params is None:\n            self.sim_params = create_simulation_parameters(\n                capital_base=self.capital_base,\n                start=kwargs.pop('start', None),\n                end=kwargs.pop('end', None),\n                env=self.trading_environment,\n            )\n        else:\n            self.sim_params.update_internal_from_env(self.trading_environment)\n\n        self.perf_tracker = None\n        # Pull in the environment's new AssetFinder for quick reference\n        self.asset_finder = self.trading_environment.asset_finder\n\n        # Initialize Pipeline API data.\n        self.init_engine(kwargs.pop('get_pipeline_loader', None))\n        self._pipelines = {}\n        # Create an always-expired cache so that we compute the first time data\n        # is requested.\n        self._pipeline_cache = CachedObject(None, pd.Timestamp(0, tz='UTC'))\n\n        self.blotter = kwargs.pop('blotter', None)\n        self.cancel_policy = kwargs.pop('cancel_policy', NeverCancel())\n        if not self.blotter:\n            self.blotter = Blotter(\n                data_frequency=self.data_frequency,\n                asset_finder=self.asset_finder,\n                slippage_func=VolumeShareSlippage(),\n                commission=PerShare(),\n                # Default to NeverCancel in zipline\n                cancel_policy=self.cancel_policy\n            )\n\n        # The symbol lookup date specifies the date to use when resolving\n        # symbols to sids, and can be set using set_symbol_lookup_date()\n        self._symbol_lookup_date = None\n\n        self.portfolio_needs_update = True\n        self.account_needs_update = True\n        self.performance_needs_update = True\n        self._portfolio = None\n        self._account = None\n\n        # If string is passed in, execute and get reference to\n        # functions.\n        self.algoscript = kwargs.pop('script', None)\n\n        self._initialize = None\n        self._before_trading_start = None\n        self._analyze = None\n\n        self._in_before_trading_start = False\n\n        self.event_manager = EventManager(\n            create_context=kwargs.pop('create_event_context', None),\n        )\n\n        self._handle_data = None\n\n        if self.algoscript is not None:\n            filename = kwargs.pop('algo_filename', None)\n            if filename is None:\n                filename = '<string>'\n            code = compile(self.algoscript, filename, 'exec')\n            exec_(code, self.namespace)\n            self._initialize = self.namespace.get('initialize')\n            if 'handle_data' in self.namespace:\n                self._handle_data = self.namespace['handle_data']\n\n            self._before_trading_start = \\\n                self.namespace.get('before_trading_start')\n            # Optional analyze function, gets called after run\n            self._analyze = self.namespace.get('analyze')\n\n        elif kwargs.get('initialize') and kwargs.get('handle_data'):\n            if self.algoscript is not None:\n                raise ValueError('You can not set script and \\\n                initialize/handle_data.')\n            self._initialize = kwargs.pop('initialize')\n            self._handle_data = kwargs.pop('handle_data')\n            self._before_trading_start = kwargs.pop('before_trading_start',\n                                                    None)\n            self._analyze = kwargs.pop('analyze', None)\n\n        self.event_manager.add_event(\n            zipline.utils.events.Event(\n                zipline.utils.events.Always(),\n                # We pass handle_data.__func__ to get the unbound method.\n                # We will explicitly pass the algorithm to bind it again.\n                self.handle_data.__func__,\n            ),\n            prepend=True,\n        )\n\n        # If method not defined, NOOP\n        if self._initialize is None:\n            self._initialize = lambda x: None\n\n        # Alternative way of setting data_frequency for backwards\n        # compatibility.\n        if 'data_frequency' in kwargs:\n            self.data_frequency = kwargs.pop('data_frequency')\n\n        # Prepare the algo for initialization\n        self.initialized = False\n        self.initialize_args = args\n        self.initialize_kwargs = kwargs\n\n        self.benchmark_sid = kwargs.pop('benchmark_sid', None)\n\n    def init_engine(self, get_loader):\n        \"\"\"\n        Construct and store a PipelineEngine from loader.\n\n        If get_loader is None, constructs a NoOpPipelineEngine.\n        \"\"\"\n        if get_loader is not None:\n            self.engine = SimplePipelineEngine(\n                get_loader,\n                self.trading_environment.trading_days,\n                self.asset_finder,\n            )\n        else:\n            self.engine = NoOpPipelineEngine()\n\n    def initialize(self, *args, **kwargs):\n        \"\"\"\n        Call self._initialize with `self` made available to Zipline API\n        functions.\n        \"\"\"\n        with ZiplineAPI(self):\n            self._initialize(self, *args, **kwargs)\n\n    def before_trading_start(self, data):\n        if self._before_trading_start is None:\n            return\n\n        self._in_before_trading_start = True\n\n        with handle_non_market_minutes(data) if \\\n                self.data_frequency == \"minute\" else ExitStack():\n            self._before_trading_start(self, data)\n\n        self._in_before_trading_start = False\n\n    def handle_data(self, data):\n        if self._handle_data:\n            self._handle_data(self, data)\n\n        # Unlike trading controls which remain constant unless placing an\n        # order, account controls can change each bar. Thus, must check\n        # every bar no matter if the algorithm places an order or not.\n        self.validate_account_controls()\n\n    def analyze(self, perf):\n        if self._analyze is None:\n            return\n\n        with ZiplineAPI(self):\n            self._analyze(self, perf)\n\n    def __repr__(self):\n        \"\"\"\n        N.B. this does not yet represent a string that can be used\n        to instantiate an exact copy of an algorithm.\n\n        However, it is getting close, and provides some value as something\n        that can be inspected interactively.\n        \"\"\"\n        return \"\"\"\n{class_name}(\n    capital_base={capital_base}\n    sim_params={sim_params},\n    initialized={initialized},\n    slippage={slippage},\n    commission={commission},\n    blotter={blotter},\n    recorded_vars={recorded_vars})\n\"\"\".strip().format(class_name=self.__class__.__name__,\n                   capital_base=self.capital_base,\n                   sim_params=repr(self.sim_params),\n                   initialized=self.initialized,\n                   slippage=repr(self.blotter.slippage_func),\n                   commission=repr(self.blotter.commission),\n                   blotter=repr(self.blotter),\n                   recorded_vars=repr(self.recorded_vars))\n\n    def _create_clock(self):\n        \"\"\"\n        If the clock property is not set, then create one based on frequency.\n        \"\"\"\n        if self.sim_params.data_frequency == 'minute':\n            env = self.trading_environment\n            trading_o_and_c = env.open_and_closes.ix[\n                self.sim_params.trading_days]\n            market_opens = trading_o_and_c['market_open'].values.astype(\n                'datetime64[ns]').astype(np.int64)\n            market_closes = trading_o_and_c['market_close'].values.astype(\n                'datetime64[ns]').astype(np.int64)\n\n            minutely_emission = self.sim_params.emission_rate == \"minute\"\n\n            clock = MinuteSimulationClock(\n                self.sim_params.trading_days,\n                market_opens,\n                market_closes,\n                minutely_emission\n            )\n            return clock\n        else:\n            return DailySimulationClock(self.sim_params.trading_days)\n\n    def _create_benchmark_source(self):\n        return BenchmarkSource(\n            self.benchmark_sid,\n            self.trading_environment,\n            self.sim_params.trading_days,\n            self.data_portal,\n            emission_rate=self.sim_params.emission_rate,\n        )\n\n    def _create_generator(self, sim_params):\n        if sim_params is not None:\n            self.sim_params = sim_params\n\n        if self.perf_tracker is None:\n            # HACK: When running with the `run` method, we set perf_tracker to\n            # None so that it will be overwritten here.\n            self.perf_tracker = PerformanceTracker(\n                sim_params=self.sim_params,\n                env=self.trading_environment,\n            )\n\n            # Set the dt initially to the period start by forcing it to change.\n            self.on_dt_changed(self.sim_params.period_start)\n\n        if not self.initialized:\n            self.initialize(*self.initialize_args, **self.initialize_kwargs)\n            self.initialized = True\n\n        self.trading_client = AlgorithmSimulator(\n            self,\n            sim_params,\n            self.data_portal,\n            self._create_clock(),\n            self._create_benchmark_source(),\n            universe_func=self._calculate_universe\n        )\n\n        return self.trading_client.transform()\n\n    def _calculate_universe(self):\n        # this exists to provide backwards compatibility for older,\n        # deprecated APIs, particularly around the iterability of\n        # BarData (ie, 'for sid in data`).\n\n        # our universe is all the assets passed into `run`.\n        return self._assets_from_source\n\n    def get_generator(self):\n        \"\"\"\n        Override this method to add new logic to the construction\n        of the generator. Overrides can use the _create_generator\n        method to get a standard construction generator.\n        \"\"\"\n        return self._create_generator(self.sim_params)\n\n    def run(self, data=None, overwrite_sim_params=True):\n        \"\"\"Run the algorithm.\n\n        :Arguments:\n            source : DataPortal\n\n        :Returns:\n            daily_stats : pandas.DataFrame\n              Daily performance metrics such as returns, alpha etc.\n\n        \"\"\"\n        self._assets_from_source = []\n\n        if isinstance(data, DataPortal):\n            self.data_portal = data\n\n            # define the universe as all the assets in the assetfinder\n            # This is not great, because multiple runs can accumulate assets\n            # in the assetfinder, but it's better than spending time adding\n            # functionality in the dataportal to report all the assets it\n            # knows about.\n            self._assets_from_source = \\\n                self.trading_environment.asset_finder.retrieve_all(\n                    self.trading_environment.asset_finder.sids\n                )\n\n        else:\n            if isinstance(data, pd.DataFrame):\n                # If a DataFrame is passed. Promote it to a Panel.\n                # The reader will fake volume values.\n                data = pd.Panel({'close': data.copy()})\n                data = data.swapaxes(0, 2)\n\n            if isinstance(data, pd.Panel):\n                # For compatibility with existing examples allow start/end\n                # to be inferred.\n                if overwrite_sim_params:\n                    self.sim_params.period_start = data.major_axis[0]\n                    self.sim_params.period_end = data.major_axis[-1]\n                    # Changing period_start and period_close might require\n                    # updating of first_open and last_close.\n                    self.sim_params.update_internal_from_env(\n                        env=self.trading_environment\n                    )\n\n                copy_panel = data.rename(\n                    # These were the old names for the close/open columns.  We\n                    # need to make a copy anyway, so swap these for backwards\n                    # compat while we're here.\n                    minor_axis={'close_price': 'close', 'open_price': 'open'},\n                    copy=True,\n                )\n                copy_panel.items = self._write_and_map_id_index_to_sids(\n                    copy_panel.items, copy_panel.major_axis[0],\n                )\n                self._assets_from_source = \\\n                    set(self.trading_environment.asset_finder.retrieve_all(\n                        copy_panel.items\n                    ))\n                equities = []\n                for asset in self._assets_from_source:\n                    if isinstance(asset, Equity):\n                        equities.append(asset)\n                if equities:\n                    from zipline.data.us_equity_pricing import \\\n                        PanelDailyBarReader\n                    equity_daily_reader = PanelDailyBarReader(\n                        self.trading_environment.trading_days, copy_panel)\n                else:\n                    equity_daily_reader = None\n                self.data_portal = DataPortal(\n                    self.trading_environment,\n                    equity_daily_reader=equity_daily_reader)\n\n        # Force a reset of the performance tracker, in case\n        # this is a repeat run of the algorithm.\n        self.perf_tracker = None\n\n        # Create zipline and loop through simulated_trading.\n        # Each iteration returns a perf dictionary\n        try:\n            perfs = []\n            for perf in self.get_generator():\n                perfs.append(perf)\n\n            # convert perf dict to pandas dataframe\n            daily_stats = self._create_daily_stats(perfs)\n\n            self.analyze(daily_stats)\n        finally:\n            self.data_portal = None\n\n        return daily_stats\n\n    def _write_and_map_id_index_to_sids(self, identifiers, as_of_date):\n        # Build new Assets for identifiers that can't be resolved as\n        # sids/Assets\n        def is_unknown(asset_or_sid):\n            sid = op.index(asset_or_sid)\n            return self.asset_finder.retrieve_asset(\n                sid=sid,\n                default_none=True\n            ) is None\n\n        new_assets = set()\n        new_sids = set()\n        new_symbols = set()\n        for identifier in identifiers:\n            if isinstance(identifier, Asset) and is_unknown(identifier):\n                new_assets.add(identifier)\n            elif isinstance(identifier, Integral) and is_unknown(identifier):\n                new_sids.add(identifier)\n            elif isinstance(identifier, (string_types)):\n                new_symbols.add(identifier)\n            else:\n                try:\n                    new_sids.add(op.index(identifier))\n                except TypeError:\n                    raise TypeError(\n                        \"Can't convert %s to an asset.\" % identifier\n                    )\n\n        new_assets = tuple(new_assets)\n        new_sids = tuple(new_sids)\n        new_symbols = tuple(new_symbols)\n\n        number_of_kinds_of_new_things = (\n            sum((bool(new_assets), bool(new_sids), bool(new_symbols)))\n        )\n\n        # Nothing to insert, bail early.\n        if not number_of_kinds_of_new_things:\n            return self.asset_finder.map_identifier_index_to_sids(\n                identifiers, as_of_date,\n            )\n        elif number_of_kinds_of_new_things == 1:\n            warnings.warn(\n                'writing unknown identifiers into the assets db of the trading'\n                ' environment is deprecated; please write this information'\n                ' to the assets db before constructing the environment',\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        else:\n            raise ValueError(\n                \"Mixed types in DataFrame or Panel index.\\n\"\n                \"Asset Count: %d, Sid Count: %d, Symbol Count: %d.\\n\"\n                \"Choose one type and stick with it.\" % (\n                    len(new_assets),\n                    len(new_sids),\n                    len(new_symbols),\n                )\n            )\n\n        def map_getattr(iterable, attr):\n            return [getattr(i, attr) for i in iterable]\n\n        if new_assets:\n            frame_to_write = pd.DataFrame(\n                data=dict(\n                    symbol=map_getattr(new_assets, 'symbol'),\n                    start_date=map_getattr(new_assets, 'start_date'),\n                    end_date=map_getattr(new_assets, 'end_date'),\n                    exchange=map_getattr(new_assets, 'exchange'),\n                ),\n                index=map_getattr(new_assets, 'sid'),\n            )\n        elif new_sids:\n            frame_to_write = make_simple_equity_info(\n                new_sids,\n                start_date=self.sim_params.period_start,\n                end_date=self.sim_params.period_end,\n                symbols=map(str, new_sids),\n            )\n        elif new_symbols:\n            existing_sids = self.asset_finder.sids\n            first_sid = max(existing_sids) + 1 if existing_sids else 0\n            fake_sids = range(first_sid, first_sid + len(new_symbols))\n            frame_to_write = make_simple_equity_info(\n                sids=fake_sids,\n                start_date=self.sim_params.period_start,\n                end_date=self.sim_params.period_end,\n                symbols=new_symbols,\n            )\n        else:\n            raise AssertionError(\"This should never happen.\")\n\n        self.trading_environment.write_data(equities=frame_to_write)\n\n        # We need to clear out any cache misses that were stored while trying\n        # to do lookups.  The real fix for this problem is to not construct an\n        # AssetFinder until we `run()` when we actually have all the data we\n        # need to so.\n        self.asset_finder._reset_caches()\n\n        return self.asset_finder.map_identifier_index_to_sids(\n            identifiers, as_of_date,\n        )\n\n    def _create_daily_stats(self, perfs):\n        # create daily and cumulative stats dataframe\n        daily_perfs = []\n        # TODO: the loop here could overwrite expected properties\n        # of daily_perf. Could potentially raise or log a\n        # warning.\n        for perf in perfs:\n            if 'daily_perf' in perf:\n\n                perf['daily_perf'].update(\n                    perf['daily_perf'].pop('recorded_vars')\n                )\n                perf['daily_perf'].update(perf['cumulative_risk_metrics'])\n                daily_perfs.append(perf['daily_perf'])\n            else:\n                self.risk_report = perf\n\n        daily_dts = [np.datetime64(perf['period_close'], utc=True)\n                     for perf in daily_perfs]\n        daily_stats = pd.DataFrame(daily_perfs, index=daily_dts)\n\n        return daily_stats\n\n    @api_method\n    def get_environment(self, field='platform'):\n        env = {\n            'arena': self.sim_params.arena,\n            'data_frequency': self.sim_params.data_frequency,\n            'start': self.sim_params.first_open,\n            'end': self.sim_params.last_close,\n            'capital_base': self.sim_params.capital_base,\n            'platform': self._platform\n        }\n        if field == '*':\n            return env\n        else:\n            return env[field]\n\n    @api_method\n    def fetch_csv(self, url,\n                  pre_func=None,\n                  post_func=None,\n                  date_column='date',\n                  date_format=None,\n                  timezone=pytz.utc.zone,\n                  symbol=None,\n                  mask=True,\n                  symbol_column=None,\n                  special_params_checker=None,\n                  **kwargs):\n\n        # Show all the logs every time fetcher is used.\n        csv_data_source = PandasRequestsCSV(\n            url,\n            pre_func,\n            post_func,\n            self.trading_environment,\n            self.sim_params.period_start,\n            self.sim_params.period_end,\n            date_column,\n            date_format,\n            timezone,\n            symbol,\n            mask,\n            symbol_column,\n            data_frequency=self.data_frequency,\n            special_params_checker=special_params_checker,\n            **kwargs\n        )\n\n        # ingest this into dataportal\n        self.data_portal.handle_extra_source(csv_data_source.df,\n                                             self.sim_params)\n\n        return csv_data_source\n\n    def add_event(self, rule=None, callback=None):\n        \"\"\"\n        Adds an event to the algorithm's EventManager.\n        \"\"\"\n        self.event_manager.add_event(\n            zipline.utils.events.Event(rule, callback),\n        )\n\n    @api_method\n    def schedule_function(self,\n                          func,\n                          date_rule=None,\n                          time_rule=None,\n                          half_days=True):\n        \"\"\"\n        Schedules a function to be called with some timed rules.\n        \"\"\"\n        date_rule = date_rule or DateRuleFactory.every_day()\n        time_rule = ((time_rule or TimeRuleFactory.market_open())\n                     if self.sim_params.data_frequency == 'minute' else\n                     # If we are in daily mode the time_rule is ignored.\n                     zipline.utils.events.Always())\n\n        self.add_event(\n            make_eventrule(date_rule, time_rule, half_days),\n            func,\n        )\n\n    @api_method\n    def record(self, *args, **kwargs):\n        \"\"\"\n        Track and record local variable (i.e. attributes) each day.\n        \"\"\"\n        # Make 2 objects both referencing the same iterator\n        args = [iter(args)] * 2\n\n        # Zip generates list entries by calling `next` on each iterator it\n        # receives.  In this case the two iterators are the same object, so the\n        # call to next on args[0] will also advance args[1], resulting in zip\n        # returning (a,b) (c,d) (e,f) rather than (a,a) (b,b) (c,c) etc.\n        positionals = zip(*args)\n        for name, value in chain(positionals, iteritems(kwargs)):\n            self._recorded_vars[name] = value\n\n    @api_method\n    def set_benchmark(self, benchmark_sid):\n        if self.initialized:\n            raise SetBenchmarkOutsideInitialize()\n\n        self.benchmark_sid = benchmark_sid\n\n    @api_method\n    @preprocess(symbol_str=ensure_upper_case)\n    def symbol(self, symbol_str):\n        \"\"\"\n        Default symbol lookup for any source that directly maps the\n        symbol to the Asset (e.g. yahoo finance).\n        \"\"\"\n        # If the user has not set the symbol lookup date,\n        # use the period_end as the date for sybmol->sid resolution.\n        _lookup_date = self._symbol_lookup_date if self._symbol_lookup_date is not None \\\n            else self.sim_params.period_end\n\n        return self.asset_finder.lookup_symbol(\n            symbol_str,\n            as_of_date=_lookup_date,\n        )\n\n    @api_method\n    def symbols(self, *args):\n        \"\"\"\n        Default symbols lookup for any source that directly maps the\n        symbol to the Asset (e.g. yahoo finance).\n        \"\"\"\n        return [self.symbol(identifier) for identifier in args]\n\n    @api_method\n    def sid(self, a_sid):\n        \"\"\"\n        Default sid lookup for any source that directly maps the integer sid\n        to the Asset.\n        \"\"\"\n        return self.asset_finder.retrieve_asset(a_sid)\n\n    @api_method\n    @preprocess(symbol=ensure_upper_case)\n    def future_symbol(self, symbol):\n        \"\"\" Lookup a futures contract with a given symbol.\n\n        Parameters\n        ----------\n        symbol : str\n            The symbol of the desired contract.\n\n        Returns\n        -------\n        Future\n            A Future object.\n\n        Raises\n        ------\n        SymbolNotFound\n            Raised when no contract named 'symbol' is found.\n\n        \"\"\"\n        return self.asset_finder.lookup_future_symbol(symbol)\n\n    @api_method\n    @preprocess(root_symbol=ensure_upper_case)\n    def future_chain(self, root_symbol, as_of_date=None):\n        \"\"\" Look up a future chain with the specified parameters.\n\n        Parameters\n        ----------\n        root_symbol : str\n            The root symbol of a future chain.\n        as_of_date : datetime.datetime or pandas.Timestamp or str, optional\n            Date at which the chain determination is rooted. I.e. the\n            existing contract whose notice date is first after this date is\n            the primary contract, etc.\n\n        Returns\n        -------\n        FutureChain\n            The future chain matching the specified parameters.\n\n        Raises\n        ------\n        RootSymbolNotFound\n            If a future chain could not be found for the given root symbol.\n        \"\"\"\n        if as_of_date:\n            try:\n                as_of_date = pd.Timestamp(as_of_date, tz='UTC')\n            except ValueError:\n                raise UnsupportedDatetimeFormat(input=as_of_date,\n                                                method='future_chain')\n        return FutureChain(\n            asset_finder=self.asset_finder,\n            get_datetime=self.get_datetime,\n            root_symbol=root_symbol,\n            as_of_date=as_of_date\n        )\n\n    def _calculate_order_value_amount(self, asset, value):\n        \"\"\"\n        Calculates how many shares/contracts to order based on the type of\n        asset being ordered.\n        \"\"\"\n        # Make sure the asset exists, and that there is a last price for it.\n        # FIXME: we should use BarData's can_trade logic here, but I haven't\n        # yet found a good way to do that.\n        normalized_date = normalize_date(self.datetime)\n\n        if normalized_date < asset.start_date:\n            raise CannotOrderDelistedAsset(\n                msg=\"Cannot order {0}, as it started trading on\"\n                    \" {1}.\".format(asset.symbol, asset.start_date)\n            )\n        elif normalized_date > asset.end_date:\n            raise CannotOrderDelistedAsset(\n                msg=\"Cannot order {0}, as it stopped trading on\"\n                    \" {1}.\".format(asset.symbol, asset.end_date)\n            )\n        else:\n            last_price = \\\n                self.trading_client.current_data.current(asset, \"price\")\n\n            if np.isnan(last_price):\n                raise CannotOrderDelistedAsset(\n                    msg=\"Cannot order {0} on {1} as there is no last \"\n                        \"price for the security.\".format(asset.symbol,\n                                                         self.datetime)\n                )\n\n        if tolerant_equals(last_price, 0):\n            zero_message = \"Price of 0 for {psid}; can't infer value\".format(\n                psid=asset\n            )\n            if self.logger:\n                self.logger.debug(zero_message)\n            # Don't place any order\n            return 0\n\n        if isinstance(asset, Future):\n            value_multiplier = asset.multiplier\n        else:\n            value_multiplier = 1\n\n        return value / (last_price * value_multiplier)\n\n    def _can_order_asset(self, asset):\n        if not isinstance(asset, Asset):\n            raise UnsupportedOrderParameters(\n                msg=\"Passing non-Asset argument to 'order()' is not supported.\"\n                    \" Use 'sid()' or 'symbol()' methods to look up an Asset.\"\n            )\n\n        if asset.auto_close_date:\n            day = normalize_date(self.get_datetime())\n\n            if asset.end_date < day < asset.auto_close_date:\n                # we are between the asset's end date and auto close date,\n                # so warn the user that they can't place an order for this\n                # asset, and return None.\n                log.warn(\"Cannot place order for {0}, as it has de-listed. \"\n                         \"Any existing positions for this asset will be \"\n                         \"liquidated on \"\n                         \"{1}.\".format(asset.symbol, asset.auto_close_date))\n\n                return False\n\n        return True\n\n    @api_method\n    @disallowed_in_before_trading_start(OrderInBeforeTradingStart())\n    def order(self, asset, amount,\n              limit_price=None,\n              stop_price=None,\n              style=None):\n        \"\"\"\n        Place an order using the specified parameters.\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        # Truncate to the integer share count that's either within .0001 of\n        # amount or closer to zero.\n        # E.g. 3.9999 -> 4.0; 5.5 -> 5.0; -5.5 -> -5.0\n        amount = int(round_if_near_integer(amount))\n\n        # Raises a ZiplineError if invalid parameters are detected.\n        self.validate_order_params(asset,\n                                   amount,\n                                   limit_price,\n                                   stop_price,\n                                   style)\n\n        # Convert deprecated limit_price and stop_price parameters to use\n        # ExecutionStyle objects.\n        style = self.__convert_order_params_for_blotter(limit_price,\n                                                        stop_price,\n                                                        style)\n        return self.blotter.order(asset, amount, style)\n\n    def validate_order_params(self,\n                              asset,\n                              amount,\n                              limit_price,\n                              stop_price,\n                              style):\n        \"\"\"\n        Helper method for validating parameters to the order API function.\n\n        Raises an UnsupportedOrderParameters if invalid arguments are found.\n        \"\"\"\n\n        if not self.initialized:\n            raise OrderDuringInitialize(\n                msg=\"order() can only be called from within handle_data()\"\n            )\n\n        if style:\n            if limit_price:\n                raise UnsupportedOrderParameters(\n                    msg=\"Passing both limit_price and style is not supported.\"\n                )\n\n            if stop_price:\n                raise UnsupportedOrderParameters(\n                    msg=\"Passing both stop_price and style is not supported.\"\n                )\n\n        for control in self.trading_controls:\n            control.validate(asset,\n                             amount,\n                             self.updated_portfolio(),\n                             self.get_datetime(),\n                             self.trading_client.current_data)\n\n    @staticmethod\n    def __convert_order_params_for_blotter(limit_price, stop_price, style):\n        \"\"\"\n        Helper method for converting deprecated limit_price and stop_price\n        arguments into ExecutionStyle instances.\n\n        This function assumes that either style == None or (limit_price,\n        stop_price) == (None, None).\n        \"\"\"\n        # TODO_SS: DeprecationWarning for usage of limit_price and stop_price.\n        if style:\n            assert (limit_price, stop_price) == (None, None)\n            return style\n        if limit_price and stop_price:\n            return StopLimitOrder(limit_price, stop_price)\n        if limit_price:\n            return LimitOrder(limit_price)\n        if stop_price:\n            return StopOrder(stop_price)\n        else:\n            return MarketOrder()\n\n    @api_method\n    @disallowed_in_before_trading_start(OrderInBeforeTradingStart())\n    def order_value(self, asset, value,\n                    limit_price=None, stop_price=None, style=None):\n        \"\"\"\n        Place an order by desired value rather than desired number of shares.\n        If the requested asset exists, the requested value is\n        divided by its price to imply the number of shares to transact.\n        If the Asset being ordered is a Future, the 'value' calculated\n        is actually the exposure, as Futures have no 'value'.\n\n        value > 0 :: Buy/Cover\n        value < 0 :: Sell/Short\n        Market order:    order(sid, value)\n        Limit order:     order(sid, value, limit_price)\n        Stop order:      order(sid, value, None, stop_price)\n        StopLimit order: order(sid, value, limit_price, stop_price)\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        amount = self._calculate_order_value_amount(asset, value)\n        return self.order(asset, amount,\n                          limit_price=limit_price,\n                          stop_price=stop_price,\n                          style=style)\n\n    @property\n    def recorded_vars(self):\n        return copy(self._recorded_vars)\n\n    @property\n    def portfolio(self):\n        return self.updated_portfolio()\n\n    def updated_portfolio(self):\n        if self.portfolio_needs_update:\n            self.perf_tracker.position_tracker.sync_last_sale_prices(\n                self.datetime, self._in_before_trading_start, self.data_portal)\n            self._portfolio = \\\n                self.perf_tracker.get_portfolio(self.performance_needs_update)\n            self.portfolio_needs_update = False\n            self.performance_needs_update = False\n        return self._portfolio\n\n    @property\n    def account(self):\n        return self.updated_account()\n\n    def updated_account(self):\n        if self.account_needs_update:\n            self.perf_tracker.position_tracker.sync_last_sale_prices(\n                self.datetime, self._in_before_trading_start, self.data_portal)\n            self._account = \\\n                self.perf_tracker.get_account(self.performance_needs_update)\n            self.account_needs_update = False\n            self.performance_needs_update = False\n        return self._account\n\n    def set_logger(self, logger):\n        self.logger = logger\n\n    def on_dt_changed(self, dt):\n        \"\"\"\n        Callback triggered by the simulation loop whenever the current dt\n        changes.\n\n        Any logic that should happen exactly once at the start of each datetime\n        group should happen here.\n        \"\"\"\n        self.datetime = dt\n        self.perf_tracker.set_date(dt)\n        self.blotter.set_date(dt)\n\n        self.portfolio_needs_update = True\n        self.account_needs_update = True\n        self.performance_needs_update = True\n\n    @api_method\n    def get_datetime(self, tz=None):\n        \"\"\"\n        Returns the simulation datetime.\n        \"\"\"\n        dt = self.datetime\n        assert dt.tzinfo == pytz.utc, \"Algorithm should have a utc datetime\"\n\n        if tz is not None:\n            # Convert to the given timezone passed as a string or tzinfo.\n            if isinstance(tz, string_types):\n                tz = pytz.timezone(tz)\n            dt = dt.astimezone(tz)\n\n        return dt  # datetime.datetime objects are immutable.\n\n    def update_dividends(self, dividend_frame):\n        \"\"\"\n        Set DataFrame used to process dividends.  DataFrame columns should\n        contain at least the entries in zp.DIVIDEND_FIELDS.\n        \"\"\"\n        self.perf_tracker.update_dividends(dividend_frame)\n\n    @api_method\n    def set_slippage(self, slippage):\n        if not isinstance(slippage, SlippageModel):\n            raise UnsupportedSlippageModel()\n        if self.initialized:\n            raise SetSlippagePostInit()\n        self.blotter.slippage_func = slippage\n\n    @api_method\n    def set_commission(self, commission):\n        if not isinstance(commission, (PerShare, PerTrade, PerDollar)):\n            raise UnsupportedCommissionModel()\n\n        if self.initialized:\n            raise SetCommissionPostInit()\n        self.blotter.commission = commission\n\n    @api_method\n    def set_cancel_policy(self, cancel_policy):\n        if not isinstance(cancel_policy, CancelPolicy):\n            raise UnsupportedCancelPolicy()\n\n        if self.initialized:\n            raise SetCancelPolicyPostInit()\n\n        self.blotter.cancel_policy = cancel_policy\n\n    @api_method\n    def set_symbol_lookup_date(self, dt):\n        \"\"\"\n        Set the date for which symbols will be resolved to their assets\n        (symbols may map to different firms or underlying assets at\n        different times)\n        \"\"\"\n        try:\n            self._symbol_lookup_date = pd.Timestamp(dt, tz='UTC')\n        except ValueError:\n            raise UnsupportedDatetimeFormat(input=dt,\n                                            method='set_symbol_lookup_date')\n\n    # Remain backwards compatibility\n    @property\n    def data_frequency(self):\n        return self.sim_params.data_frequency\n\n    @data_frequency.setter\n    def data_frequency(self, value):\n        assert value in ('daily', 'minute')\n        self.sim_params.data_frequency = value\n\n    @api_method\n    @disallowed_in_before_trading_start(OrderInBeforeTradingStart())\n    def order_percent(self, asset, percent,\n                      limit_price=None, stop_price=None, style=None):\n        \"\"\"\n        Place an order in the specified asset corresponding to the given\n        percent of the current portfolio value.\n\n        Note that percent must expressed as a decimal (0.50 means 50\\%).\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        value = self.portfolio.portfolio_value * percent\n        return self.order_value(asset, value,\n                                limit_price=limit_price,\n                                stop_price=stop_price,\n                                style=style)\n\n    @api_method\n    @disallowed_in_before_trading_start(OrderInBeforeTradingStart())\n    def order_target(self, asset, target,\n                     limit_price=None, stop_price=None, style=None):\n        \"\"\"\n        Place an order to adjust a position to a target number of shares. If\n        the position doesn't already exist, this is equivalent to placing a new\n        order. If the position does exist, this is equivalent to placing an\n        order for the difference between the target number of shares and the\n        current number of shares.\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        if asset in self.portfolio.positions:\n            current_position = self.portfolio.positions[asset].amount\n            req_shares = target - current_position\n            return self.order(asset, req_shares,\n                              limit_price=limit_price,\n                              stop_price=stop_price,\n                              style=style)\n        else:\n            return self.order(asset, target,\n                              limit_price=limit_price,\n                              stop_price=stop_price,\n                              style=style)\n\n    @api_method\n    @disallowed_in_before_trading_start(OrderInBeforeTradingStart())\n    def order_target_value(self, asset, target,\n                           limit_price=None, stop_price=None, style=None):\n        \"\"\"\n        Place an order to adjust a position to a target value. If\n        the position doesn't already exist, this is equivalent to placing a new\n        order. If the position does exist, this is equivalent to placing an\n        order for the difference between the target value and the\n        current value.\n        If the Asset being ordered is a Future, the 'target value' calculated\n        is actually the target exposure, as Futures have no 'value'.\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        target_amount = self._calculate_order_value_amount(asset, target)\n        return self.order_target(asset, target_amount,\n                                 limit_price=limit_price,\n                                 stop_price=stop_price,\n                                 style=style)\n\n    @api_method\n    @disallowed_in_before_trading_start(OrderInBeforeTradingStart())\n    def order_target_percent(self, asset, target,\n                             limit_price=None, stop_price=None, style=None):\n        \"\"\"\n        Place an order to adjust a position to a target percent of the\n        current portfolio value. If the position doesn't already exist, this is\n        equivalent to placing a new order. If the position does exist, this is\n        equivalent to placing an order for the difference between the target\n        percent and the current percent.\n\n        Note that target must expressed as a decimal (0.50 means 50\\%).\n        \"\"\"\n        if not self._can_order_asset(asset):\n            return None\n\n        target_value = self.portfolio.portfolio_value * target\n        return self.order_target_value(asset, target_value,\n                                       limit_price=limit_price,\n                                       stop_price=stop_price,\n                                       style=style)\n\n    @error_keywords(sid='Keyword argument `sid` is no longer supported for '\n                        'get_open_orders. Use `asset` instead.')\n    @api_method\n    def get_open_orders(self, asset=None):\n        if asset is None:\n            return {\n                key: [order.to_api_obj() for order in orders]\n                for key, orders in iteritems(self.blotter.open_orders)\n                if orders\n            }\n        if asset in self.blotter.open_orders:\n            orders = self.blotter.open_orders[asset]\n            return [order.to_api_obj() for order in orders]\n        return []\n\n    @api_method\n    def get_order(self, order_id):\n        if order_id in self.blotter.orders:\n            return self.blotter.orders[order_id].to_api_obj()\n\n    @api_method\n    def cancel_order(self, order_param):\n        order_id = order_param\n        if isinstance(order_param, zipline.protocol.Order):\n            order_id = order_param.id\n\n        self.blotter.cancel(order_id)\n\n    @api_method\n    @require_initialized(HistoryInInitialize())\n    def history(self, bar_count, frequency, field, ffill=True):\n        warnings.warn(\n            \"The `history` method is deprecated.  Use `data.history` instead.\",\n            category=ZiplineDeprecationWarning,\n            stacklevel=4\n        )\n\n        return self.get_history_window(\n            bar_count,\n            frequency,\n            self._calculate_universe(),\n            field,\n            ffill\n        )\n\n    def get_history_window(self, bar_count, frequency, assets, field, ffill):\n        if not self._in_before_trading_start:\n            return self.data_portal.get_history_window(\n                assets,\n                self.datetime,\n                bar_count,\n                frequency,\n                field,\n                ffill,\n            )\n        else:\n            # If we are in before_trading_start, we need to get the window\n            # as of the previous market minute\n            adjusted_dt = \\\n                self.data_portal.env.previous_market_minute(self.datetime)\n\n            window = self.data_portal.get_history_window(\n                assets,\n                adjusted_dt,\n                bar_count,\n                frequency,\n                field,\n                ffill,\n            )\n\n            # Get the adjustments between the last market minute and the\n            # current before_trading_start dt and apply to the window\n            adjs = self.data_portal.get_adjustments(\n                assets,\n                field,\n                adjusted_dt,\n                self.datetime\n            )\n            window = window * adjs\n\n            return window\n\n    ####################\n    # Account Controls #\n    ####################\n\n    def register_account_control(self, control):\n        \"\"\"\n        Register a new AccountControl to be checked on each bar.\n        \"\"\"\n        if self.initialized:\n            raise RegisterAccountControlPostInit()\n        self.account_controls.append(control)\n\n    def validate_account_controls(self):\n        for control in self.account_controls:\n            control.validate(self.updated_portfolio(),\n                             self.updated_account(),\n                             self.get_datetime(),\n                             self.trading_client.current_data)\n\n    @api_method\n    def set_max_leverage(self, max_leverage=None):\n        \"\"\"\n        Set a limit on the maximum leverage of the algorithm.\n        \"\"\"\n        control = MaxLeverage(max_leverage)\n        self.register_account_control(control)\n\n    ####################\n    # Trading Controls #\n    ####################\n\n    def register_trading_control(self, control):\n        \"\"\"\n        Register a new TradingControl to be checked prior to order calls.\n        \"\"\"\n        if self.initialized:\n            raise RegisterTradingControlPostInit()\n        self.trading_controls.append(control)\n\n    @api_method\n    def set_max_position_size(self,\n                              asset=None,\n                              max_shares=None,\n                              max_notional=None):\n        \"\"\"\n        Set a limit on the number of shares and/or dollar value held for the\n        given sid. Limits are treated as absolute values and are enforced at\n        the time that the algo attempts to place an order for sid. This means\n        that it's possible to end up with more than the max number of shares\n        due to splits/dividends, and more than the max notional due to price\n        improvement.\n\n        If an algorithm attempts to place an order that would result in\n        increasing the absolute value of shares/dollar value exceeding one of\n        these limits, raise a TradingControlException.\n        \"\"\"\n        control = MaxPositionSize(asset=asset,\n                                  max_shares=max_shares,\n                                  max_notional=max_notional)\n        self.register_trading_control(control)\n\n    @api_method\n    def set_max_order_size(self, asset=None, max_shares=None,\n                           max_notional=None):\n        \"\"\"\n        Set a limit on the number of shares and/or dollar value of any single\n        order placed for sid.  Limits are treated as absolute values and are\n        enforced at the time that the algo attempts to place an order for sid.\n\n        If an algorithm attempts to place an order that would result in\n        exceeding one of these limits, raise a TradingControlException.\n        \"\"\"\n        control = MaxOrderSize(asset=asset,\n                               max_shares=max_shares,\n                               max_notional=max_notional)\n        self.register_trading_control(control)\n\n    @api_method\n    def set_max_order_count(self, max_count):\n        \"\"\"\n        Set a limit on the number of orders that can be placed within the given\n        time interval.\n        \"\"\"\n        control = MaxOrderCount(max_count)\n        self.register_trading_control(control)\n\n    @api_method\n    def set_do_not_order_list(self, restricted_list):\n        \"\"\"\n        Set a restriction on which assets can be ordered.\n        \"\"\"\n        control = RestrictedListOrder(restricted_list)\n        self.register_trading_control(control)\n\n    @api_method\n    def set_long_only(self):\n        \"\"\"\n        Set a rule specifying that this algorithm cannot take short positions.\n        \"\"\"\n        self.register_trading_control(LongOnly())\n\n    ##############\n    # Pipeline API\n    ##############\n    @api_method\n    @require_not_initialized(AttachPipelineAfterInitialize())\n    def attach_pipeline(self, pipeline, name, chunksize=None):\n        \"\"\"\n        Register a pipeline to be computed at the start of each day.\n        \"\"\"\n        if self._pipelines:\n            raise NotImplementedError(\"Multiple pipelines are not supported.\")\n        if chunksize is None:\n            # Make the first chunk smaller to get more immediate results:\n            # (one week, then every half year)\n            chunks = iter(chain([5], repeat(126)))\n        else:\n            chunks = iter(repeat(int(chunksize)))\n        self._pipelines[name] = pipeline, chunks\n\n        # Return the pipeline to allow expressions like\n        # p = attach_pipeline(Pipeline(), 'name')\n        return pipeline\n\n    @api_method\n    @require_initialized(PipelineOutputDuringInitialize())\n    def pipeline_output(self, name):\n        \"\"\"\n        Get the results of pipeline with name `name`.\n\n        Parameters\n        ----------\n        name : str\n            Name of the pipeline for which results are requested.\n\n        Returns\n        -------\n        results : pd.DataFrame\n            DataFrame containing the results of the requested pipeline for\n            the current simulation date.\n\n        Raises\n        ------\n        NoSuchPipeline\n            Raised when no pipeline with the name `name` has been registered.\n\n        See Also\n        --------\n        :meth:`zipline.pipeline.engine.PipelineEngine.run_pipeline`\n        \"\"\"\n        # NOTE: We don't currently support multiple pipelines, but we plan to\n        # in the future.\n        try:\n            p, chunks = self._pipelines[name]\n        except KeyError:\n            raise NoSuchPipeline(\n                name=name,\n                valid=list(self._pipelines.keys()),\n            )\n        return self._pipeline_output(p, chunks)\n\n    def _pipeline_output(self, pipeline, chunks):\n        \"\"\"\n        Internal implementation of `pipeline_output`.\n        \"\"\"\n        today = normalize_date(self.get_datetime())\n        try:\n            data = self._pipeline_cache.unwrap(today)\n        except Expired:\n            data, valid_until = self._run_pipeline(\n                pipeline, today, next(chunks),\n            )\n            self._pipeline_cache = CachedObject(data, valid_until)\n\n        # Now that we have a cached result, try to return the data for today.\n        try:\n            return data.loc[today]\n        except KeyError:\n            # This happens if no assets passed the pipeline screen on a given\n            # day.\n            return pd.DataFrame(index=[], columns=data.columns)\n\n    def _run_pipeline(self, pipeline, start_date, chunksize):\n        \"\"\"\n        Compute `pipeline`, providing values for at least `start_date`.\n\n        Produces a DataFrame containing data for days between `start_date` and\n        `end_date`, where `end_date` is defined by:\n\n            `end_date = min(start_date + chunksize trading days,\n                            simulation_end)`\n\n        Returns\n        -------\n        (data, valid_until) : tuple (pd.DataFrame, pd.Timestamp)\n\n        See Also\n        --------\n        PipelineEngine.run_pipeline\n        \"\"\"\n        days = self.trading_environment.trading_days\n\n        # Load data starting from the previous trading day...\n        start_date_loc = days.get_loc(start_date)\n\n        # ...continuing until either the day before the simulation end, or\n        # until chunksize days of data have been loaded.\n        sim_end = self.sim_params.last_close.normalize()\n        end_loc = min(start_date_loc + chunksize, days.get_loc(sim_end))\n        end_date = days[end_loc]\n\n        return \\\n            self.engine.run_pipeline(pipeline, start_date, end_date), end_date\n\n    ##################\n    # End Pipeline API\n    ##################\n\n    @classmethod\n    def all_api_methods(cls):\n        \"\"\"\n        Return a list of all the TradingAlgorithm API methods.\n        \"\"\"\n        return [\n            fn for fn in itervalues(vars(cls))\n            if getattr(fn, 'is_api_method', False)\n        ]\n"
    }
  ],
  "questions": [
    "> You are much better off just doing np.array(m) before sending this in\n\nThis is what we're currently doing (and what we'll probably continue to do for a while since we support older versions of pandas).\n\n> note that _simple_new methods are NOT public, you should really really not sue internal/private things.\n\nYep, I assumed as much.  What would you think about making public versions of some of the Pandas object constructors that accept limited, specific arguments and don't do any coercion?  We frequently have the problem in Zipline that we have an object of a well-known type and we want to construct an Index/Series/DataFrame without incurring the extra overhead of Pandas having to infer the type of our input.",
    "> If you want to push a PR which checks if its a memory buffer then ok.\n\nWould the preferred behavior here be to coerce to `ndarray`, or to treat this as an error and barf?  It seems like Pandas' general philosophy is to try its best to do reasonable coercions when possible."
  ],
  "golden_answers": [
    "> If you want to push a PR which checks if its a memory buffer then ok.\n\nWould the preferred behavior here be to coerce to `ndarray`, or to treat this as an error and barf?  It seems like Pandas' general philosophy is to try its best to do reasonable coercions when possible.",
    "> The inference is generally very light so you shouldn't incur costs with anything. If you have a specific example I can look.\n\nThe case that comes to mind offhand is something like `DataFrame.from_records` when reading values out of a tabular database.  In that case, we know that all our records will have the same columns and dtypes, but there isn't a good way to tell pandas that information."
  ],
  "questions_generated": [
    "What is the primary cause of the erroneous behavior when constructing a PeriodIndex with a memoryview as input in the pandas-dev/pandas repository?",
    "In the provided issue context, what is the expected behavior when constructing a DatetimeIndex using a memoryview of numpy array data?",
    "Why does the construction of a vanilla Index with a memoryview return an array of unicode strings, and what would be a more appropriate outcome?",
    "Based on the code sample provided in the issue, what specific part of the pandas codebase is responsible for the erroneous parsing of memoryview objects during DatetimeIndex construction?",
    "Considering the repository's code structure and the provided context, what changes would you propose to handle memoryview inputs correctly in pandas index constructors?"
  ],
  "golden_answers_generated": [
    "The erroneous behavior occurs because the pandas library does not handle memoryview objects correctly during the construction of PeriodIndex, DatetimeIndex, and Index. The library attempts to parse the memoryview as a string or provides an incorrect result, such as an array of empty unicode strings, instead of converting it to a numpy array first.",
    "The expected behavior when constructing a DatetimeIndex using a memoryview is for pandas to coerce the memoryview into a numpy array and then create the index based on the numpy array's integer values. If this coercion is not possible, it should raise an error immediately, rather than attempting to parse the data as strings.",
    "The construction of a vanilla Index with a memoryview returns an array of unicode strings because pandas attempts to treat the memoryview as an iterable of objects, converting each element to a string. A more appropriate outcome would be to convert the memoryview into a numpy array first, using its integer values to construct the Index directly, similar to how pandas handles numpy arrays.",
    "The erroneous parsing occurs in the `pandas/tseries/index.py` file within the `__new__` method of `DatetimeIndex`. Specifically, the issue arises when the method calls `tslib.parse_str_array_to_datetime`, which incorrectly attempts to parse the memoryview as an array of strings instead of converting it to a numpy array first.",
    "To handle memoryview inputs correctly, the pandas index constructors should first check if the input data is a memoryview. If it is, they should coerce the memoryview into a numpy array before proceeding with the usual index construction process. This could involve adding a utility function to detect and convert memoryviews uniformly across different index types, ensuring consistent behavior and error handling."
  ]
}