{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "38138",
  "issue_description": "# CLN: C408 Unnecessary dict call - rewrite as a literal\n\nThe following files contain flake8 C408 Unnecessary dict call - rewrite as a literal. Good for first time contributors. See #38078 and #38116 for how to fix them. Use `flake8 [file_path]` to check for each file.\r\n\r\n### C408\r\n- [x] ./pandas/core/generic.py\r\n- [x] ./pandas/core/series.py\r\n- [x] ./pandas/core/frame.py\r\n- [x] ./pandas/core/strings/accessor.py\r\n- [x] ./pandas/core/dtypes/dtypes.py\r\n- [x] ./pandas/core/computation/pytables.py\r\n- [x] ./pandas/core/computation/expressions.py\r\n- [x] ./pandas/core/arrays/floating.py\r\n- [x] ./pandas/core/arrays/interval.py\r\n- [x] ./pandas/core/arrays/numpy_.py\r\n- [x] ./pandas/core/arrays/base.py\r\n- [x] ./pandas/core/ops/methods.py\r\n- [x] ./pandas/core/indexes/interval.py\r\n- [x] ./pandas/core/indexes/multi.py\r\n- [x] ./pandas/core/indexes/numeric.py\r\n- [x] ./pandas/core/indexes/base.py\r\n- [x] ./pandas/io/pytables.py\r\n- [x] ./pandas/tests/reshape/concat/test_datetimes.py\r\n- [x] ./pandas/tests/reshape/merge/test_merge.py\r\n- [x] ./pandas/tests/reshape/merge/test_multi.py\r\n- [x] ./pandas/tests/tools/test_to_numeric.py\r\n- [x] ./pandas/tests/resample/test_time_grouper.py\r\n- [x] ./pandas/tests/util/test_assert_series_equal.py\r\n- [x] ./pandas/tests/util/test_assert_interval_array_equal.py\r\n- [x] ./pandas/tests/util/test_assert_index_equal.py\r\n- [x] ./pandas/tests/util/test_assert_extension_array_equal.py\r\n- [x] ./pandas/tests/io/test_parquet.py\r\n- [x] ./pandas/tests/io/generate_legacy_storage_files.py\r\n- [x] ./pandas/tests/io/formats/test_format.py\r\n- [x] ./pandas/tests/io/parser/test_comment.py\r\n- [x] ./pandas/tests/io/parser/test_parse_dates.py\r\n- [x] ./pandas/tests/io/parser/test_c_parser_only.py\r\n- [x] ./pandas/tests/io/parser/test_dialect.py\r\n- [x] ./pandas/tests/io/parser/test_skiprows.py\r\n- [x] ./pandas/tests/io/parser/test_encoding.py\r\n- [x] ./pandas/tests/io/parser/test_read_fwf.py\r\n- [x] ./pandas/tests/io/parser/test_python_parser_only.py\r\n- [x] ./pandas/tests/io/parser/test_na_values.py\r\n- [x] ./pandas/tests/io/parser/test_usecols.py\r\n- [x] ./pandas/tests/io/parser/test_quoting.py\r\n- [x] ./pandas/tests/io/parser/test_header.py\r\n- [x] ./pandas/tests/io/parser/test_index_col.py\r\n- [x] ./pandas/tests/io/json/test_ujson.py\r\n- [x] ./pandas/tests/io/json/test_normalize.py\r\n- [x] ./pandas/tests/io/pytables/test_timezones.py\r\n- [x] ./pandas/tests/io/pytables/test_store.py\r\n- [x] ./pandas/tests/frame/test_reductions.py\r\n- [x] ./pandas/tests/frame/test_arithmetic.py\r\n- [x] ./pandas/tests/frame/methods/test_to_records.py\r\n- [x] ./pandas/tests/frame/methods/test_sort_values.py\r\n- [x] ./pandas/tests/frame/methods/test_fillna.py\r\n- [x] ./pandas/tests/frame/methods/test_to_csv.py\r\n- [x] ./pandas/tests/frame/indexing/test_indexing.py\r\n- [x] ./pandas/tests/frame/indexing/test_where.py\r\n- [x] ./pandas/tests/dtypes/test_inference.py\r\n- [x] ./pandas/tests/groupby/test_function.py\r\n- [x] ./pandas/tests/groupby/transform/test_transform.py\r\n- [x] ./pandas/tests/plotting/test_boxplot_method.py\r\n- [x] ./pandas/tests/plotting/frame/test_frame_subplots.py\r\n- [x] ./pandas/tests/io/parser/test_common.py\r\n\r\n### Remove ignore setting from flake8 section\r\n- [x] setup.cfg",
  "issue_comments": [
    {
      "id": 735492266,
      "user": "liudj2008",
      "body": "I would like to take the followings as the first step\r\n----------------------------------------\r\n ./pandas/core/generic.py\r\n ./pandas/core/series.py\r\n ./pandas/core/frame.py"
    },
    {
      "id": 735678821,
      "user": "skvrahul",
      "body": "Created a PR (#38180 )that addresses the following files:\r\n\r\n./pandas/tests/reshape/concat/test_datetimes.py\r\n./pandas/tests/reshape/merge/test_merge.py\r\n./pandas/tests/reshape/merge/test_multi.py\r\n\r\n@fangchenli Is a **whatsnew** entry necessary for such a change in the code?"
    },
    {
      "id": 735900398,
      "user": "Qbiwan",
      "body": "first-time contributor here; please let me know if I'd missed anything"
    },
    {
      "id": 736149294,
      "user": "pillemer",
      "body": "Hi, I'm new to contributing and would like to help on this issue. Do I need to request to work on specific files before I start?"
    },
    {
      "id": 736153860,
      "user": "fangchenli",
      "body": "> Hi, I'm new to contributing and would like to help on this issue. Do I need to request to work on specific files before I start?\r\n\r\nNo, that is not necessary. "
    },
    {
      "id": 736960588,
      "user": "jreback",
      "body": "@fangchenli if you have a chance can you check things off from the merged PRs"
    },
    {
      "id": 737542820,
      "user": "austiezr",
      "body": "Just submitted a [PR](https://github.com/pandas-dev/pandas/pull/38249) to fix another swath of this."
    },
    {
      "id": 737718220,
      "user": "gerardjorgensen",
      "body": "I would like to start working on the following\r\n ./pandas/tests/io/json/test_ujson.py\r\n ./pandas/tests/io/json/test_normalize.py"
    },
    {
      "id": 737904838,
      "user": "vmdhhh",
      "body": "Hi @jreback \r\nFirst time contributing\r\nI would like to take up ./pandas/tests/io/pytables/test_store.py\r\nI will start working on it, please let me know otherwise\r\n\r\nI would also take up  and push to the same PR #38263 \r\n\r\nEDIT: `./pandas/tests/io/pytables/test_store.py`\r\n `./pandas/core/arrays/interval.py` work complete as part of #38263 \r\nPlease review."
    },
    {
      "id": 738083361,
      "user": "mavismonica",
      "body": "Hi,\r\nI would like to start working on  ./pandas/core/indexes/numeric.py"
    },
    {
      "id": 738331366,
      "user": "joao-zanutto",
      "body": "Hello, I'm Joao and I would like to take  \r\n\r\n./pandas/core/arrays/floating.py\r\n\r\nto start working as my first issue\r\n\r\n**EDIT:** I'll also take\r\n\r\n ./pandas/core/arrays/interval.py\r\n ./pandas/core/arrays/numpy_.py\r\n\r\nto work on as the first one only had two modifications to be made"
    },
    {
      "id": 738359248,
      "user": "vmdhhh",
      "body": "@zanuttin hey, `./pandas/core/arrays/interval.py` is already up for merge in #38263.\r\nJust to avoid rework, maybe you could take something else if possible?"
    },
    {
      "id": 738418772,
      "user": "joao-zanutto",
      "body": "I have already pushed the modifications @vmdhhh, sorry for not paying attention on your first comment. \r\nFrom what I've checked we did the same modifications, not sure if this is enough to not generate a conflict though."
    },
    {
      "id": 738491334,
      "user": "UrielMaD",
      "body": "Hello @jreback , I just submitted a PR for ./pandas/core/strings/accessor.py and it passed all checks! \r\nif you have a chance could you please check it?\r\n\r\n#38138"
    },
    {
      "id": 739028373,
      "user": "UrielMaD",
      "body": "Hi again @jreback, I checked the file ./pandas/core/dtypes/dtypes.py and it has no unnecessary dict calls\r\n\r\nalso added 5 more files that passed all checks:\r\n\r\npandas/core/arrays/base.py\r\npandas/core/computation/pytables.py\r\npandas/core/indexes/base.py\r\npandas/core/ops/methods.py\r\npandas/io/pytables.py"
    },
    {
      "id": 739372945,
      "user": "fangchenli",
      "body": "I've checked everything in #38320 (not merged yet) off. Please select from the remaining files to work on."
    },
    {
      "id": 739575085,
      "user": "shrutiguptabu",
      "body": "Hello, \r\n\r\nI would like to take \r\n\r\n./pandas/tests/io/parser/test_parse_dates.py\r\n./pandas/tests/io/generate_legacy_storage_files.py\r\n\r\nas my first issue."
    },
    {
      "id": 739577715,
      "user": "pillemer",
      "body": "I will do /pandas/tests/io/parser/test_index_col.py\r\n\r\n"
    },
    {
      "id": 741609500,
      "user": "UrielMaD",
      "body": "Hi @jreback I just updated the last 10 files remaining, this will close the issue, \r\nthere's just the setup.cfg file to be changed, I tried to do it but it throws some flake8 errors and didn't passed the checks"
    },
    {
      "id": 741665947,
      "user": "jreback",
      "body": "close the issue when the checks are updated"
    },
    {
      "id": 741668212,
      "user": "MarcoGorelli",
      "body": "sorry didn't realise there was the \"closes\" keyword in the PR, will check more carefully next time"
    }
  ],
  "text_context": "# CLN: C408 Unnecessary dict call - rewrite as a literal\n\nThe following files contain flake8 C408 Unnecessary dict call - rewrite as a literal. Good for first time contributors. See #38078 and #38116 for how to fix them. Use `flake8 [file_path]` to check for each file.\r\n\r\n### C408\r\n- [x] ./pandas/core/generic.py\r\n- [x] ./pandas/core/series.py\r\n- [x] ./pandas/core/frame.py\r\n- [x] ./pandas/core/strings/accessor.py\r\n- [x] ./pandas/core/dtypes/dtypes.py\r\n- [x] ./pandas/core/computation/pytables.py\r\n- [x] ./pandas/core/computation/expressions.py\r\n- [x] ./pandas/core/arrays/floating.py\r\n- [x] ./pandas/core/arrays/interval.py\r\n- [x] ./pandas/core/arrays/numpy_.py\r\n- [x] ./pandas/core/arrays/base.py\r\n- [x] ./pandas/core/ops/methods.py\r\n- [x] ./pandas/core/indexes/interval.py\r\n- [x] ./pandas/core/indexes/multi.py\r\n- [x] ./pandas/core/indexes/numeric.py\r\n- [x] ./pandas/core/indexes/base.py\r\n- [x] ./pandas/io/pytables.py\r\n- [x] ./pandas/tests/reshape/concat/test_datetimes.py\r\n- [x] ./pandas/tests/reshape/merge/test_merge.py\r\n- [x] ./pandas/tests/reshape/merge/test_multi.py\r\n- [x] ./pandas/tests/tools/test_to_numeric.py\r\n- [x] ./pandas/tests/resample/test_time_grouper.py\r\n- [x] ./pandas/tests/util/test_assert_series_equal.py\r\n- [x] ./pandas/tests/util/test_assert_interval_array_equal.py\r\n- [x] ./pandas/tests/util/test_assert_index_equal.py\r\n- [x] ./pandas/tests/util/test_assert_extension_array_equal.py\r\n- [x] ./pandas/tests/io/test_parquet.py\r\n- [x] ./pandas/tests/io/generate_legacy_storage_files.py\r\n- [x] ./pandas/tests/io/formats/test_format.py\r\n- [x] ./pandas/tests/io/parser/test_comment.py\r\n- [x] ./pandas/tests/io/parser/test_parse_dates.py\r\n- [x] ./pandas/tests/io/parser/test_c_parser_only.py\r\n- [x] ./pandas/tests/io/parser/test_dialect.py\r\n- [x] ./pandas/tests/io/parser/test_skiprows.py\r\n- [x] ./pandas/tests/io/parser/test_encoding.py\r\n- [x] ./pandas/tests/io/parser/test_read_fwf.py\r\n- [x] ./pandas/tests/io/parser/test_python_parser_only.py\r\n- [x] ./pandas/tests/io/parser/test_na_values.py\r\n- [x] ./pandas/tests/io/parser/test_usecols.py\r\n- [x] ./pandas/tests/io/parser/test_quoting.py\r\n- [x] ./pandas/tests/io/parser/test_header.py\r\n- [x] ./pandas/tests/io/parser/test_index_col.py\r\n- [x] ./pandas/tests/io/json/test_ujson.py\r\n- [x] ./pandas/tests/io/json/test_normalize.py\r\n- [x] ./pandas/tests/io/pytables/test_timezones.py\r\n- [x] ./pandas/tests/io/pytables/test_store.py\r\n- [x] ./pandas/tests/frame/test_reductions.py\r\n- [x] ./pandas/tests/frame/test_arithmetic.py\r\n- [x] ./pandas/tests/frame/methods/test_to_records.py\r\n- [x] ./pandas/tests/frame/methods/test_sort_values.py\r\n- [x] ./pandas/tests/frame/methods/test_fillna.py\r\n- [x] ./pandas/tests/frame/methods/test_to_csv.py\r\n- [x] ./pandas/tests/frame/indexing/test_indexing.py\r\n- [x] ./pandas/tests/frame/indexing/test_where.py\r\n- [x] ./pandas/tests/dtypes/test_inference.py\r\n- [x] ./pandas/tests/groupby/test_function.py\r\n- [x] ./pandas/tests/groupby/transform/test_transform.py\r\n- [x] ./pandas/tests/plotting/test_boxplot_method.py\r\n- [x] ./pandas/tests/plotting/frame/test_frame_subplots.py\r\n- [x] ./pandas/tests/io/parser/test_common.py\r\n\r\n### Remove ignore setting from flake8 section\r\n- [x] setup.cfg\n\nI would like to take the followings as the first step\r\n----------------------------------------\r\n ./pandas/core/generic.py\r\n ./pandas/core/series.py\r\n ./pandas/core/frame.py\n\nCreated a PR (#38180 )that addresses the following files:\r\n\r\n./pandas/tests/reshape/concat/test_datetimes.py\r\n./pandas/tests/reshape/merge/test_merge.py\r\n./pandas/tests/reshape/merge/test_multi.py\r\n\r\n@fangchenli Is a **whatsnew** entry necessary for such a change in the code?\n\nfirst-time contributor here; please let me know if I'd missed anything\n\nHi, I'm new to contributing and would like to help on this issue. Do I need to request to work on specific files before I start?\n\n> Hi, I'm new to contributing and would like to help on this issue. Do I need to request to work on specific files before I start?\r\n\r\nNo, that is not necessary. \n\n@fangchenli if you have a chance can you check things off from the merged PRs\n\nJust submitted a [PR](https://github.com/pandas-dev/pandas/pull/38249) to fix another swath of this.\n\nI would like to start working on the following\r\n ./pandas/tests/io/json/test_ujson.py\r\n ./pandas/tests/io/json/test_normalize.py\n\nHi @jreback \r\nFirst time contributing\r\nI would like to take up ./pandas/tests/io/pytables/test_store.py\r\nI will start working on it, please let me know otherwise\r\n\r\nI would also take up  and push to the same PR #38263 \r\n\r\nEDIT: `./pandas/tests/io/pytables/test_store.py`\r\n `./pandas/core/arrays/interval.py` work complete as part of #38263 \r\nPlease review.\n\nHi,\r\nI would like to start working on  ./pandas/core/indexes/numeric.py\n\nHello, I'm Joao and I would like to take  \r\n\r\n./pandas/core/arrays/floating.py\r\n\r\nto start working as my first issue\r\n\r\n**EDIT:** I'll also take\r\n\r\n ./pandas/core/arrays/interval.py\r\n ./pandas/core/arrays/numpy_.py\r\n\r\nto work on as the first one only had two modifications to be made\n\n@zanuttin hey, `./pandas/core/arrays/interval.py` is already up for merge in #38263.\r\nJust to avoid rework, maybe you could take something else if possible?\n\nI have already pushed the modifications @vmdhhh, sorry for not paying attention on your first comment. \r\nFrom what I've checked we did the same modifications, not sure if this is enough to not generate a conflict though.\n\nHello @jreback , I just submitted a PR for ./pandas/core/strings/accessor.py and it passed all checks! \r\nif you have a chance could you please check it?\r\n\r\n#38138\n\nHi again @jreback, I checked the file ./pandas/core/dtypes/dtypes.py and it has no unnecessary dict calls\r\n\r\nalso added 5 more files that passed all checks:\r\n\r\npandas/core/arrays/base.py\r\npandas/core/computation/pytables.py\r\npandas/core/indexes/base.py\r\npandas/core/ops/methods.py\r\npandas/io/pytables.py\n\nI've checked everything in #38320 (not merged yet) off. Please select from the remaining files to work on.\n\nHello, \r\n\r\nI would like to take \r\n\r\n./pandas/tests/io/parser/test_parse_dates.py\r\n./pandas/tests/io/generate_legacy_storage_files.py\r\n\r\nas my first issue.\n\nI will do /pandas/tests/io/parser/test_index_col.py\r\n\r\n\n\nHi @jreback I just updated the last 10 files remaining, this will close the issue, \r\nthere's just the setup.cfg file to be changed, I tried to do it but it throws some flake8 errors and didn't passed the checks\n\nclose the issue when the checks are updated\n\nsorry didn't realise there was the \"closes\" keyword in the PR, will check more carefully next time",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/38249",
  "code_context": [
    {
      "filename": "pandas/core/arrays/base.py",
      "content": "\"\"\"\nAn interface for extending pandas with custom arrays.\n\n.. warning::\n\n   This is an experimental API and subject to breaking changes\n   without warning.\n\"\"\"\nfrom __future__ import annotations\n\nimport operator\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport numpy as np\n\nfrom pandas._libs import lib\nfrom pandas._typing import ArrayLike, Shape\nfrom pandas.compat import set_function_name\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import AbstractMethodError\nfrom pandas.util._decorators import Appender, Substitution\nfrom pandas.util._validators import validate_fillna_kwargs\n\nfrom pandas.core.dtypes.cast import maybe_cast_to_extension_array\nfrom pandas.core.dtypes.common import (\n    is_array_like,\n    is_dtype_equal,\n    is_list_like,\n    is_scalar,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCIndexClass, ABCSeries\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import ops\nfrom pandas.core.algorithms import factorize_array, unique\nfrom pandas.core.missing import get_fill_func\nfrom pandas.core.sorting import nargminmax, nargsort\n\n_extension_array_shared_docs: Dict[str, str] = {}\n\nExtensionArrayT = TypeVar(\"ExtensionArrayT\", bound=\"ExtensionArray\")\n\n\nclass ExtensionArray:\n    \"\"\"\n    Abstract base class for custom 1-D array types.\n\n    pandas will recognize instances of this class as proper arrays\n    with a custom type and will not attempt to coerce them to objects. They\n    may be stored directly inside a :class:`DataFrame` or :class:`Series`.\n\n    Attributes\n    ----------\n    dtype\n    nbytes\n    ndim\n    shape\n\n    Methods\n    -------\n    argsort\n    astype\n    copy\n    dropna\n    factorize\n    fillna\n    equals\n    isna\n    ravel\n    repeat\n    searchsorted\n    shift\n    take\n    unique\n    view\n    _concat_same_type\n    _formatter\n    _from_factorized\n    _from_sequence\n    _from_sequence_of_strings\n    _reduce\n    _values_for_argsort\n    _values_for_factorize\n\n    Notes\n    -----\n    The interface includes the following abstract methods that must be\n    implemented by subclasses:\n\n    * _from_sequence\n    * _from_factorized\n    * __getitem__\n    * __len__\n    * __eq__\n    * dtype\n    * nbytes\n    * isna\n    * take\n    * copy\n    * _concat_same_type\n\n    A default repr displaying the type, (truncated) data, length,\n    and dtype is provided. It can be customized or replaced by\n    by overriding:\n\n    * __repr__ : A default repr for the ExtensionArray.\n    * _formatter : Print scalars inside a Series or DataFrame.\n\n    Some methods require casting the ExtensionArray to an ndarray of Python\n    objects with ``self.astype(object)``, which may be expensive. When\n    performance is a concern, we highly recommend overriding the following\n    methods:\n\n    * fillna\n    * dropna\n    * unique\n    * factorize / _values_for_factorize\n    * argsort / _values_for_argsort\n    * searchsorted\n\n    The remaining methods implemented on this class should be performant,\n    as they only compose abstract methods. Still, a more efficient\n    implementation may be available, and these methods can be overridden.\n\n    One can implement methods to handle array reductions.\n\n    * _reduce\n\n    One can implement methods to handle parsing from strings that will be used\n    in methods such as ``pandas.io.parsers.read_csv``.\n\n    * _from_sequence_of_strings\n\n    This class does not inherit from 'abc.ABCMeta' for performance reasons.\n    Methods and properties required by the interface raise\n    ``pandas.errors.AbstractMethodError`` and no ``register`` method is\n    provided for registering virtual subclasses.\n\n    ExtensionArrays are limited to 1 dimension.\n\n    They may be backed by none, one, or many NumPy arrays. For example,\n    ``pandas.Categorical`` is an extension array backed by two arrays,\n    one for codes and one for categories. An array of IPv6 address may\n    be backed by a NumPy structured array with two fields, one for the\n    lower 64 bits and one for the upper 64 bits. Or they may be backed\n    by some other storage type, like Python lists. Pandas makes no\n    assumptions on how the data are stored, just that it can be converted\n    to a NumPy array.\n    The ExtensionArray interface does not impose any rules on how this data\n    is stored. However, currently, the backing data cannot be stored in\n    attributes called ``.values`` or ``._values`` to ensure full compatibility\n    with pandas internals. But other names as ``.data``, ``._data``,\n    ``._items``, ... can be freely used.\n\n    If implementing NumPy's ``__array_ufunc__`` interface, pandas expects\n    that\n\n    1. You defer by returning ``NotImplemented`` when any Series are present\n       in `inputs`. Pandas will extract the arrays and call the ufunc again.\n    2. You define a ``_HANDLED_TYPES`` tuple as an attribute on the class.\n       Pandas inspect this to determine whether the ufunc is valid for the\n       types present.\n\n    See :ref:`extending.extension.ufunc` for more.\n\n    By default, ExtensionArrays are not hashable.  Immutable subclasses may\n    override this behavior.\n    \"\"\"\n\n    # '_typ' is for pandas.core.dtypes.generic.ABCExtensionArray.\n    # Don't override this.\n    _typ = \"extension\"\n\n    # ------------------------------------------------------------------------\n    # Constructors\n    # ------------------------------------------------------------------------\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype=None, copy=False):\n        \"\"\"\n        Construct a new ExtensionArray from a sequence of scalars.\n\n        Parameters\n        ----------\n        scalars : Sequence\n            Each element will be an instance of the scalar type for this\n            array, ``cls.dtype.type`` or be converted into this type in this method.\n        dtype : dtype, optional\n            Construct for this particular dtype. This should be a Dtype\n            compatible with the ExtensionArray.\n        copy : bool, default False\n            If True, copy the underlying data.\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _from_sequence_of_strings(cls, strings, *, dtype=None, copy=False):\n        \"\"\"\n        Construct a new ExtensionArray from a sequence of strings.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        strings : Sequence\n            Each element will be an instance of the scalar type for this\n            array, ``cls.dtype.type``.\n        dtype : dtype, optional\n            Construct for this particular dtype. This should be a Dtype\n            compatible with the ExtensionArray.\n        copy : bool, default False\n            If True, copy the underlying data.\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        \"\"\"\n        Reconstruct an ExtensionArray after factorization.\n\n        Parameters\n        ----------\n        values : ndarray\n            An integer ndarray with the factorized values.\n        original : ExtensionArray\n            The original ExtensionArray that factorize was called on.\n\n        See Also\n        --------\n        factorize : Top-level factorize method that dispatches here.\n        ExtensionArray.factorize : Encode the extension array as an enumerated type.\n        \"\"\"\n        raise AbstractMethodError(cls)\n\n    # ------------------------------------------------------------------------\n    # Must be a Sequence\n    # ------------------------------------------------------------------------\n\n    def __getitem__(\n        self, item: Union[int, slice, np.ndarray]\n    ) -> Union[ExtensionArray, Any]:\n        \"\"\"\n        Select a subset of self.\n\n        Parameters\n        ----------\n        item : int, slice, or ndarray\n            * int: The position in 'self' to get.\n\n            * slice: A slice object, where 'start', 'stop', and 'step' are\n              integers or None\n\n            * ndarray: A 1-d boolean NumPy ndarray the same length as 'self'\n\n        Returns\n        -------\n        item : scalar or ExtensionArray\n\n        Notes\n        -----\n        For scalar ``item``, return a scalar value suitable for the array's\n        type. This should be an instance of ``self.dtype.type``.\n\n        For slice ``key``, return an instance of ``ExtensionArray``, even\n        if the slice is length 0 or 1.\n\n        For a boolean mask, return an instance of ``ExtensionArray``, filtered\n        to the values where ``item`` is True.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def __setitem__(self, key: Union[int, np.ndarray], value: Any) -> None:\n        \"\"\"\n        Set one or more values inplace.\n\n        This method is not required to satisfy the pandas extension array\n        interface.\n\n        Parameters\n        ----------\n        key : int, ndarray, or slice\n            When called from, e.g. ``Series.__setitem__``, ``key`` will be\n            one of\n\n            * scalar int\n            * ndarray of integers.\n            * boolean ndarray\n            * slice object\n\n        value : ExtensionDtype.type, Sequence[ExtensionDtype.type], or object\n            value or values to be set of ``key``.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # Some notes to the ExtensionArray implementor who may have ended up\n        # here. While this method is not required for the interface, if you\n        # *do* choose to implement __setitem__, then some semantics should be\n        # observed:\n        #\n        # * Setting multiple values : ExtensionArrays should support setting\n        #   multiple values at once, 'key' will be a sequence of integers and\n        #  'value' will be a same-length sequence.\n        #\n        # * Broadcasting : For a sequence 'key' and a scalar 'value',\n        #   each position in 'key' should be set to 'value'.\n        #\n        # * Coercion : Most users will expect basic coercion to work. For\n        #   example, a string like '2018-01-01' is coerced to a datetime\n        #   when setting on a datetime64ns array. In general, if the\n        #   __init__ method coerces that value, then so should __setitem__\n        # Note, also, that Series/DataFrame.where internally use __setitem__\n        # on a copy of the data.\n        raise NotImplementedError(f\"{type(self)} does not implement __setitem__.\")\n\n    def __len__(self) -> int:\n        \"\"\"\n        Length of this array\n\n        Returns\n        -------\n        length : int\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over elements of the array.\n        \"\"\"\n        # This needs to be implemented so that pandas recognizes extension\n        # arrays as list-like. The default implementation makes successive\n        # calls to ``__getitem__``, which may be slower than necessary.\n        for i in range(len(self)):\n            yield self[i]\n\n    def __contains__(self, item) -> bool:\n        \"\"\"\n        Return for `item in self`.\n        \"\"\"\n        # GH37867\n        # comparisons of any item to pd.NA always return pd.NA, so e.g. \"a\" in [pd.NA]\n        # would raise a TypeError. The implementation below works around that.\n        if is_scalar(item) and isna(item):\n            if not self._can_hold_na:\n                return False\n            elif item is self.dtype.na_value or isinstance(item, self.dtype.type):\n                return self.isna().any()\n            else:\n                return False\n        else:\n            return (item == self).any()\n\n    def __eq__(self, other: Any) -> ArrayLike:\n        \"\"\"\n        Return for `self == other` (element-wise equality).\n        \"\"\"\n        # Implementer note: this should return a boolean numpy ndarray or\n        # a boolean ExtensionArray.\n        # When `other` is one of Series, Index, or DataFrame, this method should\n        # return NotImplemented (to ensure that those objects are responsible for\n        # first unpacking the arrays, and then dispatch the operation to the\n        # underlying arrays)\n        raise AbstractMethodError(self)\n\n    def __ne__(self, other: Any) -> ArrayLike:\n        \"\"\"\n        Return for `self != other` (element-wise in-equality).\n        \"\"\"\n        return ~(self == other)\n\n    def to_numpy(\n        self, dtype=None, copy: bool = False, na_value=lib.no_default\n    ) -> np.ndarray:\n        \"\"\"\n        Convert to a NumPy ndarray.\n\n        .. versionadded:: 1.0.0\n\n        This is similar to :meth:`numpy.asarray`, but may provide additional control\n        over how the conversion is done.\n\n        Parameters\n        ----------\n        dtype : str or numpy.dtype, optional\n            The dtype to pass to :meth:`numpy.asarray`.\n        copy : bool, default False\n            Whether to ensure that the returned value is a not a view on\n            another array. Note that ``copy=False`` does not *ensure* that\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n            a copy is made, even if not strictly necessary.\n        na_value : Any, optional\n            The value to use for missing values. The default value depends\n            on `dtype` and the type of the array.\n\n        Returns\n        -------\n        numpy.ndarray\n        \"\"\"\n        result = np.asarray(self, dtype=dtype)\n        if copy or na_value is not lib.no_default:\n            result = result.copy()\n        if na_value is not lib.no_default:\n            result[self.isna()] = na_value\n        return result\n\n    # ------------------------------------------------------------------------\n    # Required attributes\n    # ------------------------------------------------------------------------\n\n    @property\n    def dtype(self) -> ExtensionDtype:\n        \"\"\"\n        An instance of 'ExtensionDtype'.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    @property\n    def shape(self) -> Shape:\n        \"\"\"\n        Return a tuple of the array dimensions.\n        \"\"\"\n        return (len(self),)\n\n    @property\n    def size(self) -> int:\n        \"\"\"\n        The number of elements in the array.\n        \"\"\"\n        return np.prod(self.shape)\n\n    @property\n    def ndim(self) -> int:\n        \"\"\"\n        Extension Arrays are only allowed to be 1-dimensional.\n        \"\"\"\n        return 1\n\n    @property\n    def nbytes(self) -> int:\n        \"\"\"\n        The number of bytes needed to store this object in memory.\n        \"\"\"\n        # If this is expensive to compute, return an approximate lower bound\n        # on the number of bytes needed.\n        raise AbstractMethodError(self)\n\n    # ------------------------------------------------------------------------\n    # Additional Methods\n    # ------------------------------------------------------------------------\n\n    def astype(self, dtype, copy=True):\n        \"\"\"\n        Cast to a NumPy array with 'dtype'.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n        copy : bool, default True\n            Whether to copy the data, even if not necessary. If False,\n            a copy is made only if the old dtype does not match the\n            new dtype.\n\n        Returns\n        -------\n        array : ndarray\n            NumPy ndarray with 'dtype' for its dtype.\n        \"\"\"\n        from pandas.core.arrays.string_ import StringDtype\n        from pandas.core.arrays.string_arrow import ArrowStringDtype\n\n        dtype = pandas_dtype(dtype)\n        if is_dtype_equal(dtype, self.dtype):\n            if not copy:\n                return self\n            else:\n                return self.copy()\n\n        # FIXME: Really hard-code here?\n        if isinstance(\n            dtype, (ArrowStringDtype, StringDtype)\n        ):  # allow conversion to StringArrays\n            return dtype.construct_array_type()._from_sequence(self, copy=False)\n\n        return np.array(self, dtype=dtype, copy=copy)\n\n    def isna(self) -> ArrayLike:\n        \"\"\"\n        A 1-D array indicating if each value is missing.\n\n        Returns\n        -------\n        na_values : Union[np.ndarray, ExtensionArray]\n            In most cases, this should return a NumPy ndarray. For\n            exceptional cases like ``SparseArray``, where returning\n            an ndarray would be expensive, an ExtensionArray may be\n            returned.\n\n        Notes\n        -----\n        If returning an ExtensionArray, then\n\n        * ``na_values._is_boolean`` should be True\n        * `na_values` should implement :func:`ExtensionArray._reduce`\n        * ``na_values.any`` and ``na_values.all`` should be implemented\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _values_for_argsort(self) -> np.ndarray:\n        \"\"\"\n        Return values for sorting.\n\n        Returns\n        -------\n        ndarray\n            The transformed values should maintain the ordering between values\n            within the array.\n\n        See Also\n        --------\n        ExtensionArray.argsort : Return the indices that would sort this array.\n        \"\"\"\n        # Note: this is used in `ExtensionArray.argsort`.\n        return np.array(self)\n\n    def argsort(\n        self,\n        ascending: bool = True,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        *args,\n        **kwargs,\n    ) -> np.ndarray:\n        \"\"\"\n        Return the indices that would sort this array.\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            Whether the indices should result in an ascending\n            or descending sort.\n        kind : {'quicksort', 'mergesort', 'heapsort'}, optional\n            Sorting algorithm.\n        *args, **kwargs:\n            Passed through to :func:`numpy.argsort`.\n\n        Returns\n        -------\n        ndarray\n            Array of indices that sort ``self``. If NaN values are contained,\n            NaN values are placed at the end.\n\n        See Also\n        --------\n        numpy.argsort : Sorting implementation used internally.\n        \"\"\"\n        # Implementor note: You have two places to override the behavior of\n        # argsort.\n        # 1. _values_for_argsort : construct the values passed to np.argsort\n        # 2. argsort : total control over sorting.\n        ascending = nv.validate_argsort_with_ascending(ascending, args, kwargs)\n\n        values = self._values_for_argsort()\n        return nargsort(\n            values,\n            kind=kind,\n            ascending=ascending,\n            na_position=na_position,\n            mask=np.asarray(self.isna()),\n        )\n\n    def argmin(self):\n        \"\"\"\n        Return the index of minimum value.\n\n        In case of multiple occurrences of the minimum value, the index\n        corresponding to the first occurrence is returned.\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        ExtensionArray.argmax\n        \"\"\"\n        return nargminmax(self, \"argmin\")\n\n    def argmax(self):\n        \"\"\"\n        Return the index of maximum value.\n\n        In case of multiple occurrences of the maximum value, the index\n        corresponding to the first occurrence is returned.\n\n        Returns\n        -------\n        int\n\n        See Also\n        --------\n        ExtensionArray.argmin\n        \"\"\"\n        return nargminmax(self, \"argmax\")\n\n    def fillna(self, value=None, method=None, limit=None):\n        \"\"\"\n        Fill NA/NaN values using the specified method.\n\n        Parameters\n        ----------\n        value : scalar, array-like\n            If a scalar value is passed it is used to fill all missing values.\n            Alternatively, an array-like 'value' can be given. It's expected\n            that the array-like have the same length as 'self'.\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n            Method to use for filling holes in reindexed Series\n            pad / ffill: propagate last valid observation forward to next valid\n            backfill / bfill: use NEXT valid observation to fill gap.\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled.\n\n        Returns\n        -------\n        ExtensionArray\n            With NA/NaN filled.\n        \"\"\"\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n\n        if is_array_like(value):\n            if len(value) != len(self):\n                raise ValueError(\n                    f\"Length of 'value' does not match. Got ({len(value)}) \"\n                    f\"expected {len(self)}\"\n                )\n            value = value[mask]\n\n        if mask.any():\n            if method is not None:\n                func = get_fill_func(method)\n                new_values = func(self.astype(object), limit=limit, mask=mask)\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n            else:\n                # fill with value\n                new_values = self.copy()\n                new_values[mask] = value\n        else:\n            new_values = self.copy()\n        return new_values\n\n    def dropna(self):\n        \"\"\"\n        Return ExtensionArray without NA values.\n\n        Returns\n        -------\n        valid : ExtensionArray\n        \"\"\"\n        return self[~self.isna()]\n\n    def shift(self, periods: int = 1, fill_value: object = None) -> ExtensionArray:\n        \"\"\"\n        Shift values by desired number.\n\n        Newly introduced missing values are filled with\n        ``self.dtype.na_value``.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        periods : int, default 1\n            The number of periods to shift. Negative values are allowed\n            for shifting backwards.\n\n        fill_value : object, optional\n            The scalar value to use for newly introduced missing values.\n            The default is ``self.dtype.na_value``.\n\n            .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        ExtensionArray\n            Shifted.\n\n        Notes\n        -----\n        If ``self`` is empty or ``periods`` is 0, a copy of ``self`` is\n        returned.\n\n        If ``periods > len(self)``, then an array of size\n        len(self) is returned, with all values filled with\n        ``self.dtype.na_value``.\n        \"\"\"\n        # Note: this implementation assumes that `self.dtype.na_value` can be\n        # stored in an instance of your ExtensionArray with `self.dtype`.\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n\n        empty = self._from_sequence(\n            [fill_value] * min(abs(periods), len(self)), dtype=self.dtype\n        )\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def unique(self):\n        \"\"\"\n        Compute the ExtensionArray of unique values.\n\n        Returns\n        -------\n        uniques : ExtensionArray\n        \"\"\"\n        uniques = unique(self.astype(object))\n        return self._from_sequence(uniques, dtype=self.dtype)\n\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        \"\"\"\n        Find indices where elements should be inserted to maintain order.\n\n        .. versionadded:: 0.24.0\n\n        Find the indices into a sorted array `self` (a) such that, if the\n        corresponding elements in `value` were inserted before the indices,\n        the order of `self` would be preserved.\n\n        Assuming that `self` is sorted:\n\n        ======  ================================\n        `side`  returned index `i` satisfies\n        ======  ================================\n        left    ``self[i-1] < value <= self[i]``\n        right   ``self[i-1] <= value < self[i]``\n        ======  ================================\n\n        Parameters\n        ----------\n        value : array_like\n            Values to insert into `self`.\n        side : {'left', 'right'}, optional\n            If 'left', the index of the first suitable location found is given.\n            If 'right', return the last such index.  If there is no suitable\n            index, return either 0 or N (where N is the length of `self`).\n        sorter : 1-D array_like, optional\n            Optional array of integer indices that sort array a into ascending\n            order. They are typically the result of argsort.\n\n        Returns\n        -------\n        array of ints\n            Array of insertion points with the same shape as `value`.\n\n        See Also\n        --------\n        numpy.searchsorted : Similar method from NumPy.\n        \"\"\"\n        # Note: the base tests provided by pandas only test the basics.\n        # We do not test\n        # 1. Values outside the range of the `data_for_sorting` fixture\n        # 2. Values between the values in the `data_for_sorting` fixture\n        # 3. Missing values.\n        arr = self.astype(object)\n        return arr.searchsorted(value, side=side, sorter=sorter)\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Return if another array is equivalent to this array.\n\n        Equivalent means that both arrays have the same shape and dtype, and\n        all values compare equal. Missing values in the same location are\n        considered equal (in contrast with normal equality).\n\n        Parameters\n        ----------\n        other : ExtensionArray\n            Array to compare to this Array.\n\n        Returns\n        -------\n        boolean\n            Whether the arrays are equivalent.\n        \"\"\"\n        if type(self) != type(other):\n            return False\n        other = cast(ExtensionArray, other)\n        if not is_dtype_equal(self.dtype, other.dtype):\n            return False\n        elif len(self) != len(other):\n            return False\n        else:\n            equal_values = self == other\n            if isinstance(equal_values, ExtensionArray):\n                # boolean array with NA -> fill with False\n                equal_values = equal_values.fillna(False)\n            equal_na = self.isna() & other.isna()\n            return bool((equal_values | equal_na).all())\n\n    def _values_for_factorize(self) -> Tuple[np.ndarray, Any]:\n        \"\"\"\n        Return an array and missing value suitable for factorization.\n\n        Returns\n        -------\n        values : ndarray\n\n            An array suitable for factorization. This should maintain order\n            and be a supported dtype (Float64, Int64, UInt64, String, Object).\n            By default, the extension array is cast to object dtype.\n        na_value : object\n            The value in `values` to consider missing. This will be treated\n            as NA in the factorization routines, so it will be coded as\n            `na_sentinel` and not included in `uniques`. By default,\n            ``np.nan`` is used.\n\n        Notes\n        -----\n        The values returned by this method are also used in\n        :func:`pandas.util.hash_pandas_object`.\n        \"\"\"\n        return self.astype(object), np.nan\n\n    def factorize(self, na_sentinel: int = -1) -> Tuple[np.ndarray, ExtensionArray]:\n        \"\"\"\n        Encode the extension array as an enumerated type.\n\n        Parameters\n        ----------\n        na_sentinel : int, default -1\n            Value to use in the `codes` array to indicate missing values.\n\n        Returns\n        -------\n        codes : ndarray\n            An integer NumPy array that's an indexer into the original\n            ExtensionArray.\n        uniques : ExtensionArray\n            An ExtensionArray containing the unique values of `self`.\n\n            .. note::\n\n               uniques will *not* contain an entry for the NA value of\n               the ExtensionArray if there are any missing values present\n               in `self`.\n\n        See Also\n        --------\n        factorize : Top-level factorize method that dispatches here.\n\n        Notes\n        -----\n        :meth:`pandas.factorize` offers a `sort` keyword as well.\n        \"\"\"\n        # Implementer note: There are two ways to override the behavior of\n        # pandas.factorize\n        # 1. _values_for_factorize and _from_factorize.\n        #    Specify the values passed to pandas' internal factorization\n        #    routines, and how to convert from those values back to the\n        #    original ExtensionArray.\n        # 2. ExtensionArray.factorize.\n        #    Complete control over factorization.\n        arr, na_value = self._values_for_factorize()\n\n        codes, uniques = factorize_array(\n            arr, na_sentinel=na_sentinel, na_value=na_value\n        )\n\n        uniques = self._from_factorized(uniques, self)\n        return codes, uniques\n\n    _extension_array_shared_docs[\n        \"repeat\"\n    ] = \"\"\"\n        Repeat elements of a %(klass)s.\n\n        Returns a new %(klass)s where each element of the current %(klass)s\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            %(klass)s.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        repeated_array : %(klass)s\n            Newly created %(klass)s with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        Index.repeat : Equivalent function for Index.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n        ExtensionArray.take : Take arbitrary positions.\n\n        Examples\n        --------\n        >>> cat = pd.Categorical(['a', 'b', 'c'])\n        >>> cat\n        ['a', 'b', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> cat.repeat(2)\n        ['a', 'a', 'b', 'b', 'c', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        >>> cat.repeat([1, 2, 3])\n        ['a', 'b', 'b', 'c', 'c', 'c']\n        Categories (3, object): ['a', 'b', 'c']\n        \"\"\"\n\n    @Substitution(klass=\"ExtensionArray\")\n    @Appender(_extension_array_shared_docs[\"repeat\"])\n    def repeat(self, repeats, axis=None):\n        nv.validate_repeat((), {\"axis\": axis})\n        ind = np.arange(len(self)).repeat(repeats)\n        return self.take(ind)\n\n    # ------------------------------------------------------------------------\n    # Indexing methods\n    # ------------------------------------------------------------------------\n\n    def take(\n        self,\n        indices: Sequence[int],\n        *,\n        allow_fill: bool = False,\n        fill_value: Any = None,\n    ) -> ExtensionArray:\n        \"\"\"\n        Take elements from an array.\n\n        Parameters\n        ----------\n        indices : sequence of int\n            Indices to be taken.\n        allow_fill : bool, default False\n            How to handle negative values in `indices`.\n\n            * False: negative values in `indices` indicate positional indices\n              from the right (the default). This is similar to\n              :func:`numpy.take`.\n\n            * True: negative values in `indices` indicate\n              missing values. These values are set to `fill_value`. Any other\n              other negative values raise a ``ValueError``.\n\n        fill_value : any, optional\n            Fill value to use for NA-indices when `allow_fill` is True.\n            This may be ``None``, in which case the default NA value for\n            the type, ``self.dtype.na_value``, is used.\n\n            For many ExtensionArrays, there will be two representations of\n            `fill_value`: a user-facing \"boxed\" scalar, and a low-level\n            physical NA value. `fill_value` should be the user-facing version,\n            and the implementation should handle translating that to the\n            physical version for processing the take if necessary.\n\n        Returns\n        -------\n        ExtensionArray\n\n        Raises\n        ------\n        IndexError\n            When the indices are out of bounds for the array.\n        ValueError\n            When `indices` contains negative values other than ``-1``\n            and `allow_fill` is True.\n\n        See Also\n        --------\n        numpy.take : Take elements from an array along an axis.\n        api.extensions.take : Take elements from an array.\n\n        Notes\n        -----\n        ExtensionArray.take is called by ``Series.__getitem__``, ``.loc``,\n        ``iloc``, when `indices` is a sequence of values. Additionally,\n        it's called by :meth:`Series.reindex`, or any other method\n        that causes realignment, with a `fill_value`.\n\n        Examples\n        --------\n        Here's an example implementation, which relies on casting the\n        extension array to object dtype. This uses the helper method\n        :func:`pandas.api.extensions.take`.\n\n        .. code-block:: python\n\n           def take(self, indices, allow_fill=False, fill_value=None):\n               from pandas.core.algorithms import take\n\n               # If the ExtensionArray is backed by an ndarray, then\n               # just pass that here instead of coercing to object.\n               data = self.astype(object)\n\n               if allow_fill and fill_value is None:\n                   fill_value = self.dtype.na_value\n\n               # fill value should always be translated from the scalar\n               # type for the array, to the physical storage type for\n               # the data, before passing to take.\n\n               result = take(data, indices, fill_value=fill_value,\n                             allow_fill=allow_fill)\n               return self._from_sequence(result, dtype=self.dtype)\n        \"\"\"\n        # Implementer note: The `fill_value` parameter should be a user-facing\n        # value, an instance of self.dtype.type. When passed `fill_value=None`,\n        # the default of `self.dtype.na_value` should be used.\n        # This may differ from the physical storage type your ExtensionArray\n        # uses. In this case, your implementation is responsible for casting\n        # the user-facing type to the storage type, before using\n        # pandas.api.extensions.take\n        raise AbstractMethodError(self)\n\n    def copy(self: ExtensionArrayT) -> ExtensionArrayT:\n        \"\"\"\n        Return a copy of the array.\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def view(self, dtype=None) -> ArrayLike:\n        \"\"\"\n        Return a view on the array.\n\n        Parameters\n        ----------\n        dtype : str, np.dtype, or ExtensionDtype, optional\n            Default None.\n\n        Returns\n        -------\n        ExtensionArray or np.ndarray\n            A view on the :class:`ExtensionArray`'s data.\n        \"\"\"\n        # NB:\n        # - This must return a *new* object referencing the same data, not self.\n        # - The only case that *must* be implemented is with dtype=None,\n        #   giving a view with the same dtype as self.\n        if dtype is not None:\n            raise NotImplementedError(dtype)\n        return self[:]\n\n    # ------------------------------------------------------------------------\n    # Printing\n    # ------------------------------------------------------------------------\n\n    def __repr__(self) -> str:\n        from pandas.io.formats.printing import format_object_summary\n\n        # the short repr has no trailing newline, while the truncated\n        # repr does. So we include a newline in our template, and strip\n        # any trailing newlines from format_object_summary\n        data = format_object_summary(\n            self, self._formatter(), indent_for_name=False\n        ).rstrip(\", \\n\")\n        class_name = f\"<{type(self).__name__}>\\n\"\n        return f\"{class_name}{data}\\nLength: {len(self)}, dtype: {self.dtype}\"\n\n    def _formatter(self, boxed: bool = False) -> Callable[[Any], Optional[str]]:\n        \"\"\"\n        Formatting function for scalar values.\n\n        This is used in the default '__repr__'. The returned formatting\n        function receives instances of your scalar type.\n\n        Parameters\n        ----------\n        boxed : bool, default False\n            An indicated for whether or not your array is being printed\n            within a Series, DataFrame, or Index (True), or just by\n            itself (False). This may be useful if you want scalar values\n            to appear differently within a Series versus on its own (e.g.\n            quoted or not).\n\n        Returns\n        -------\n        Callable[[Any], str]\n            A callable that gets instances of the scalar type and\n            returns a string. By default, :func:`repr` is used\n            when ``boxed=False`` and :func:`str` is used when\n            ``boxed=True``.\n        \"\"\"\n        if boxed:\n            return str\n        return repr\n\n    # ------------------------------------------------------------------------\n    # Reshaping\n    # ------------------------------------------------------------------------\n\n    def transpose(self, *axes) -> ExtensionArray:\n        \"\"\"\n        Return a transposed view on this array.\n\n        Because ExtensionArrays are always 1D, this is a no-op.  It is included\n        for compatibility with np.ndarray.\n        \"\"\"\n        return self[:]\n\n    @property\n    def T(self) -> ExtensionArray:\n        return self.transpose()\n\n    def ravel(self, order=\"C\") -> ExtensionArray:\n        \"\"\"\n        Return a flattened view on this array.\n\n        Parameters\n        ----------\n        order : {None, 'C', 'F', 'A', 'K'}, default 'C'\n\n        Returns\n        -------\n        ExtensionArray\n\n        Notes\n        -----\n        - Because ExtensionArrays are 1D-only, this is a no-op.\n        - The \"order\" argument is ignored, is for compatibility with NumPy.\n        \"\"\"\n        return self\n\n    @classmethod\n    def _concat_same_type(\n        cls: Type[ExtensionArrayT], to_concat: Sequence[ExtensionArrayT]\n    ) -> ExtensionArrayT:\n        \"\"\"\n        Concatenate multiple array of this dtype.\n\n        Parameters\n        ----------\n        to_concat : sequence of this type\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        # Implementer note: this method will only be called with a sequence of\n        # ExtensionArrays of this class and with the same dtype as self. This\n        # should allow \"easy\" concatenation (no upcasting needed), and result\n        # in a new ExtensionArray of the same dtype.\n        # Note: this strict behaviour is only guaranteed starting with pandas 1.1\n        raise AbstractMethodError(cls)\n\n    # The _can_hold_na attribute is set to True so that pandas internals\n    # will use the ExtensionDtype.na_value as the NA value in operations\n    # such as take(), reindex(), shift(), etc.  In addition, those results\n    # will then be of the ExtensionArray subclass rather than an array\n    # of objects\n    _can_hold_na = True\n\n    def _reduce(self, name: str, *, skipna: bool = True, **kwargs):\n        \"\"\"\n        Return a scalar result of performing the reduction operation.\n\n        Parameters\n        ----------\n        name : str\n            Name of the function, supported values are:\n            { any, all, min, max, sum, mean, median, prod,\n            std, var, sem, kurt, skew }.\n        skipna : bool, default True\n            If True, skip NaN values.\n        **kwargs\n            Additional keyword arguments passed to the reduction function.\n            Currently, `ddof` is the only supported kwarg.\n\n        Returns\n        -------\n        scalar\n\n        Raises\n        ------\n        TypeError : subclass does not define reductions\n        \"\"\"\n        raise TypeError(f\"cannot perform {name} with type {self.dtype}\")\n\n    def __hash__(self):\n        raise TypeError(f\"unhashable type: {repr(type(self).__name__)}\")\n\n\nclass ExtensionOpsMixin:\n    \"\"\"\n    A base class for linking the operators to their dunder names.\n\n    .. note::\n\n       You may want to set ``__array_priority__`` if you want your\n       implementation to be called when involved in binary operations\n       with NumPy arrays.\n    \"\"\"\n\n    @classmethod\n    def _create_arithmetic_method(cls, op):\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _add_arithmetic_ops(cls):\n        setattr(cls, \"__add__\", cls._create_arithmetic_method(operator.add))\n        setattr(cls, \"__radd__\", cls._create_arithmetic_method(ops.radd))\n        setattr(cls, \"__sub__\", cls._create_arithmetic_method(operator.sub))\n        setattr(cls, \"__rsub__\", cls._create_arithmetic_method(ops.rsub))\n        setattr(cls, \"__mul__\", cls._create_arithmetic_method(operator.mul))\n        setattr(cls, \"__rmul__\", cls._create_arithmetic_method(ops.rmul))\n        setattr(cls, \"__pow__\", cls._create_arithmetic_method(operator.pow))\n        setattr(cls, \"__rpow__\", cls._create_arithmetic_method(ops.rpow))\n        setattr(cls, \"__mod__\", cls._create_arithmetic_method(operator.mod))\n        setattr(cls, \"__rmod__\", cls._create_arithmetic_method(ops.rmod))\n        setattr(cls, \"__floordiv__\", cls._create_arithmetic_method(operator.floordiv))\n        setattr(cls, \"__rfloordiv__\", cls._create_arithmetic_method(ops.rfloordiv))\n        setattr(cls, \"__truediv__\", cls._create_arithmetic_method(operator.truediv))\n        setattr(cls, \"__rtruediv__\", cls._create_arithmetic_method(ops.rtruediv))\n        setattr(cls, \"__divmod__\", cls._create_arithmetic_method(divmod))\n        setattr(cls, \"__rdivmod__\", cls._create_arithmetic_method(ops.rdivmod))\n\n    @classmethod\n    def _create_comparison_method(cls, op):\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _add_comparison_ops(cls):\n        setattr(cls, \"__eq__\", cls._create_comparison_method(operator.eq))\n        setattr(cls, \"__ne__\", cls._create_comparison_method(operator.ne))\n        setattr(cls, \"__lt__\", cls._create_comparison_method(operator.lt))\n        setattr(cls, \"__gt__\", cls._create_comparison_method(operator.gt))\n        setattr(cls, \"__le__\", cls._create_comparison_method(operator.le))\n        setattr(cls, \"__ge__\", cls._create_comparison_method(operator.ge))\n\n    @classmethod\n    def _create_logical_method(cls, op):\n        raise AbstractMethodError(cls)\n\n    @classmethod\n    def _add_logical_ops(cls):\n        setattr(cls, \"__and__\", cls._create_logical_method(operator.and_))\n        setattr(cls, \"__rand__\", cls._create_logical_method(ops.rand_))\n        setattr(cls, \"__or__\", cls._create_logical_method(operator.or_))\n        setattr(cls, \"__ror__\", cls._create_logical_method(ops.ror_))\n        setattr(cls, \"__xor__\", cls._create_logical_method(operator.xor))\n        setattr(cls, \"__rxor__\", cls._create_logical_method(ops.rxor))\n\n\nclass ExtensionScalarOpsMixin(ExtensionOpsMixin):\n    \"\"\"\n    A mixin for defining  ops on an ExtensionArray.\n\n    It is assumed that the underlying scalar objects have the operators\n    already defined.\n\n    Notes\n    -----\n    If you have defined a subclass MyExtensionArray(ExtensionArray), then\n    use MyExtensionArray(ExtensionArray, ExtensionScalarOpsMixin) to\n    get the arithmetic operators.  After the definition of MyExtensionArray,\n    insert the lines\n\n    MyExtensionArray._add_arithmetic_ops()\n    MyExtensionArray._add_comparison_ops()\n\n    to link the operators to your class.\n\n    .. note::\n\n       You may want to set ``__array_priority__`` if you want your\n       implementation to be called when involved in binary operations\n       with NumPy arrays.\n    \"\"\"\n\n    @classmethod\n    def _create_method(cls, op, coerce_to_dtype=True, result_dtype=None):\n        \"\"\"\n        A class method that returns a method that will correspond to an\n        operator for an ExtensionArray subclass, by dispatching to the\n        relevant operator defined on the individual elements of the\n        ExtensionArray.\n\n        Parameters\n        ----------\n        op : function\n            An operator that takes arguments op(a, b)\n        coerce_to_dtype : bool, default True\n            boolean indicating whether to attempt to convert\n            the result to the underlying ExtensionArray dtype.\n            If it's not possible to create a new ExtensionArray with the\n            values, an ndarray is returned instead.\n\n        Returns\n        -------\n        Callable[[Any, Any], Union[ndarray, ExtensionArray]]\n            A method that can be bound to a class. When used, the method\n            receives the two arguments, one of which is the instance of\n            this class, and should return an ExtensionArray or an ndarray.\n\n            Returning an ndarray may be necessary when the result of the\n            `op` cannot be stored in the ExtensionArray. The dtype of the\n            ndarray uses NumPy's normal inference rules.\n\n        Examples\n        --------\n        Given an ExtensionArray subclass called MyExtensionArray, use\n\n            __add__ = cls._create_method(operator.add)\n\n        in the class definition of MyExtensionArray to create the operator\n        for addition, that will be based on the operator implementation\n        of the underlying elements of the ExtensionArray\n        \"\"\"\n\n        def _binop(self, other):\n            def convert_values(param):\n                if isinstance(param, ExtensionArray) or is_list_like(param):\n                    ovalues = param\n                else:  # Assume its an object\n                    ovalues = [param] * len(self)\n                return ovalues\n\n            if isinstance(other, (ABCSeries, ABCIndexClass, ABCDataFrame)):\n                # rely on pandas to unbox and dispatch to us\n                return NotImplemented\n\n            lvalues = self\n            rvalues = convert_values(other)\n\n            # If the operator is not defined for the underlying objects,\n            # a TypeError should be raised\n            res = [op(a, b) for (a, b) in zip(lvalues, rvalues)]\n\n            def _maybe_convert(arr):\n                if coerce_to_dtype:\n                    # https://github.com/pandas-dev/pandas/issues/22850\n                    # We catch all regular exceptions here, and fall back\n                    # to an ndarray.\n                    res = maybe_cast_to_extension_array(type(self), arr)\n                    if not isinstance(res, type(self)):\n                        # exception raised in _from_sequence; ensure we have ndarray\n                        res = np.asarray(arr)\n                else:\n                    res = np.asarray(arr, dtype=result_dtype)\n                return res\n\n            if op.__name__ in {\"divmod\", \"rdivmod\"}:\n                a, b = zip(*res)\n                return _maybe_convert(a), _maybe_convert(b)\n\n            return _maybe_convert(res)\n\n        op_name = f\"__{op.__name__}__\"\n        return set_function_name(_binop, op_name, cls)\n\n    @classmethod\n    def _create_arithmetic_method(cls, op):\n        return cls._create_method(op)\n\n    @classmethod\n    def _create_comparison_method(cls, op):\n        return cls._create_method(op, coerce_to_dtype=False, result_dtype=bool)\n"
    },
    {
      "filename": "pandas/core/arrays/floating.py",
      "content": "import numbers\nfrom typing import TYPE_CHECKING, List, Optional, Tuple, Type, Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import lib, missing as libmissing\nfrom pandas._typing import ArrayLike, DtypeObj\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import cache_readonly\n\nfrom pandas.core.dtypes.cast import astype_nansafe\nfrom pandas.core.dtypes.common import (\n    is_bool_dtype,\n    is_datetime64_dtype,\n    is_float,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_list_like,\n    is_object_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import register_extension_dtype\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import ops\nfrom pandas.core.ops import invalid_comparison\nfrom pandas.core.tools.numeric import to_numeric\n\nfrom .masked import BaseMaskedArray, BaseMaskedDtype\n\nif TYPE_CHECKING:\n    import pyarrow\n\n\nclass FloatingDtype(BaseMaskedDtype):\n    \"\"\"\n    An ExtensionDtype to hold a single size of floating dtype.\n\n    These specific implementations are subclasses of the non-public\n    FloatingDtype. For example we have Float32Dtype to represent float32.\n\n    The attributes name & type are set when these subclasses are created.\n    \"\"\"\n\n    def __repr__(self) -> str:\n        return f\"{self.name}Dtype()\"\n\n    @property\n    def _is_numeric(self) -> bool:\n        return True\n\n    @classmethod\n    def construct_array_type(cls) -> Type[\"FloatingArray\"]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        return FloatingArray\n\n    def _get_common_dtype(self, dtypes: List[DtypeObj]) -> Optional[DtypeObj]:\n        # for now only handle other floating types\n        if not all(isinstance(t, FloatingDtype) for t in dtypes):\n            return None\n        np_dtype = np.find_common_type(\n            [t.numpy_dtype for t in dtypes], []  # type: ignore[union-attr]\n        )\n        if np.issubdtype(np_dtype, np.floating):\n            return FLOAT_STR_TO_DTYPE[str(np_dtype)]\n        return None\n\n    def __from_arrow__(\n        self, array: Union[\"pyarrow.Array\", \"pyarrow.ChunkedArray\"]\n    ) -> \"FloatingArray\":\n        \"\"\"\n        Construct FloatingArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays._arrow_utils import pyarrow_array_to_numpy_and_mask\n\n        pyarrow_type = pyarrow.from_numpy_dtype(self.type)\n        if not array.type.equals(pyarrow_type):\n            array = array.cast(pyarrow_type)\n\n        if isinstance(array, pyarrow.Array):\n            chunks = [array]\n        else:\n            # pyarrow.ChunkedArray\n            chunks = array.chunks\n\n        results = []\n        for arr in chunks:\n            data, mask = pyarrow_array_to_numpy_and_mask(arr, dtype=self.type)\n            float_arr = FloatingArray(data.copy(), ~mask, copy=False)\n            results.append(float_arr)\n\n        return FloatingArray._concat_same_type(results)\n\n\ndef coerce_to_array(\n    values, dtype=None, mask=None, copy: bool = False\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Coerce the input values array to numpy arrays with a mask.\n\n    Parameters\n    ----------\n    values : 1D list-like\n    dtype : float dtype\n    mask : bool 1D array, optional\n    copy : bool, default False\n        if True, copy the input\n\n    Returns\n    -------\n    tuple of (values, mask)\n    \"\"\"\n    # if values is floating numpy array, preserve its dtype\n    if dtype is None and hasattr(values, \"dtype\"):\n        if is_float_dtype(values.dtype):\n            dtype = values.dtype\n\n    if dtype is not None:\n        if isinstance(dtype, str) and dtype.startswith(\"Float\"):\n            # Avoid DeprecationWarning from NumPy about np.dtype(\"Float64\")\n            # https://github.com/numpy/numpy/pull/7476\n            dtype = dtype.lower()\n\n        if not issubclass(type(dtype), FloatingDtype):\n            try:\n                dtype = FLOAT_STR_TO_DTYPE[str(np.dtype(dtype))]\n            except KeyError as err:\n                raise ValueError(f\"invalid dtype specified {dtype}\") from err\n\n    if isinstance(values, FloatingArray):\n        values, mask = values._data, values._mask\n        if dtype is not None:\n            values = values.astype(dtype.numpy_dtype, copy=False)\n\n        if copy:\n            values = values.copy()\n            mask = mask.copy()\n        return values, mask\n\n    values = np.array(values, copy=copy)\n    if is_object_dtype(values):\n        inferred_type = lib.infer_dtype(values, skipna=True)\n        if inferred_type == \"empty\":\n            values = np.empty(len(values))\n            values.fill(np.nan)\n        elif inferred_type not in [\n            \"floating\",\n            \"integer\",\n            \"mixed-integer\",\n            \"integer-na\",\n            \"mixed-integer-float\",\n        ]:\n            raise TypeError(f\"{values.dtype} cannot be converted to a FloatingDtype\")\n\n    elif is_bool_dtype(values) and is_float_dtype(dtype):\n        values = np.array(values, dtype=float, copy=copy)\n\n    elif not (is_integer_dtype(values) or is_float_dtype(values)):\n        raise TypeError(f\"{values.dtype} cannot be converted to a FloatingDtype\")\n\n    if mask is None:\n        mask = isna(values)\n    else:\n        assert len(mask) == len(values)\n\n    if not values.ndim == 1:\n        raise TypeError(\"values must be a 1D list-like\")\n    if not mask.ndim == 1:\n        raise TypeError(\"mask must be a 1D list-like\")\n\n    # infer dtype if needed\n    if dtype is None:\n        dtype = np.dtype(\"float64\")\n    else:\n        dtype = dtype.type\n\n    # if we are float, let's make sure that we can\n    # safely cast\n\n    # we copy as need to coerce here\n    # TODO should this be a safe cast?\n    if mask.any():\n        values = values.copy()\n        values[mask] = np.nan\n        values = values.astype(dtype, copy=False)  # , casting=\"safe\")\n    else:\n        values = values.astype(dtype, copy=False)  # , casting=\"safe\")\n\n    return values, mask\n\n\nclass FloatingArray(BaseMaskedArray):\n    \"\"\"\n    Array of floating (optional missing) values.\n\n    .. versionadded:: 1.2.0\n\n    .. warning::\n\n       FloatingArray is currently experimental, and its API or internal\n       implementation may change without warning. Expecially the behaviour\n       regarding NaN (distinct from NA missing values) is subject to change.\n\n    We represent a FloatingArray with 2 numpy arrays:\n\n    - data: contains a numpy float array of the appropriate dtype\n    - mask: a boolean array holding a mask on the data, True is missing\n\n    To construct an FloatingArray from generic array-like input, use\n    :func:`pandas.array` with one of the float dtypes (see examples).\n\n    See :ref:`integer_na` for more.\n\n    Parameters\n    ----------\n    values : numpy.ndarray\n        A 1-d float-dtype array.\n    mask : numpy.ndarray\n        A 1-d boolean-dtype array indicating missing values.\n    copy : bool, default False\n        Whether to copy the `values` and `mask`.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    FloatingArray\n\n    Examples\n    --------\n    Create an FloatingArray with :func:`pandas.array`:\n\n    >>> pd.array([0.1, None, 0.3], dtype=pd.Float32Dtype())\n    <FloatingArray>\n    [0.1, <NA>, 0.3]\n    Length: 3, dtype: Float32\n\n    String aliases for the dtypes are also available. They are capitalized.\n\n    >>> pd.array([0.1, None, 0.3], dtype=\"Float32\")\n    <FloatingArray>\n    [0.1, <NA>, 0.3]\n    Length: 3, dtype: Float32\n    \"\"\"\n\n    # The value used to fill '_data' to avoid upcasting\n    _internal_fill_value = 0.0\n\n    @cache_readonly\n    def dtype(self) -> FloatingDtype:\n        return FLOAT_STR_TO_DTYPE[str(self._data.dtype)]\n\n    def __init__(self, values: np.ndarray, mask: np.ndarray, copy: bool = False):\n        if not (isinstance(values, np.ndarray) and values.dtype.kind == \"f\"):\n            raise TypeError(\n                \"values should be floating numpy array. Use \"\n                \"the 'pd.array' function instead\"\n            )\n        super().__init__(values, mask, copy=copy)\n\n    @classmethod\n    def _from_sequence(\n        cls, scalars, *, dtype=None, copy: bool = False\n    ) -> \"FloatingArray\":\n        values, mask = coerce_to_array(scalars, dtype=dtype, copy=copy)\n        return FloatingArray(values, mask)\n\n    @classmethod\n    def _from_sequence_of_strings(\n        cls, strings, *, dtype=None, copy: bool = False\n    ) -> \"FloatingArray\":\n        scalars = to_numeric(strings, errors=\"raise\")\n        return cls._from_sequence(scalars, dtype=dtype, copy=copy)\n\n    _HANDLED_TYPES = (np.ndarray, numbers.Number)\n\n    def __array_ufunc__(self, ufunc, method: str, *inputs, **kwargs):\n        # For FloatingArray inputs, we apply the ufunc to ._data\n        # and mask the result.\n        if method == \"reduce\":\n            # Not clear how to handle missing values in reductions. Raise.\n            raise NotImplementedError(\"The 'reduce' method is not supported.\")\n        out = kwargs.get(\"out\", ())\n\n        for x in inputs + out:\n            if not isinstance(x, self._HANDLED_TYPES + (FloatingArray,)):\n                return NotImplemented\n\n        # for binary ops, use our custom dunder methods\n        result = ops.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        mask = np.zeros(len(self), dtype=bool)\n        inputs2 = []\n        for x in inputs:\n            if isinstance(x, FloatingArray):\n                mask |= x._mask\n                inputs2.append(x._data)\n            else:\n                inputs2.append(x)\n\n        def reconstruct(x):\n            # we don't worry about scalar `x` here, since we\n            # raise for reduce up above.\n\n            # TODO\n            if is_float_dtype(x.dtype):\n                m = mask.copy()\n                return FloatingArray(x, m)\n            else:\n                x[mask] = np.nan\n            return x\n\n        result = getattr(ufunc, method)(*inputs2, **kwargs)\n        if isinstance(result, tuple):\n            tuple(reconstruct(x) for x in result)\n        else:\n            return reconstruct(result)\n\n    def _coerce_to_array(self, value) -> Tuple[np.ndarray, np.ndarray]:\n        return coerce_to_array(value, dtype=self.dtype)\n\n    def astype(self, dtype, copy: bool = True) -> ArrayLike:\n        \"\"\"\n        Cast to a NumPy array or ExtensionArray with 'dtype'.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n        copy : bool, default True\n            Whether to copy the data, even if not necessary. If False,\n            a copy is made only if the old dtype does not match the\n            new dtype.\n\n        Returns\n        -------\n        ndarray or ExtensionArray\n            NumPy ndarray, or BooleanArray, IntegerArray or FloatingArray with\n            'dtype' for its dtype.\n\n        Raises\n        ------\n        TypeError\n            if incompatible type with an FloatingDtype, equivalent of same_kind\n            casting\n        \"\"\"\n        from pandas.core.arrays.string_ import StringArray, StringDtype\n\n        dtype = pandas_dtype(dtype)\n\n        # if the dtype is exactly the same, we can fastpath\n        if self.dtype == dtype:\n            # return the same object for copy=False\n            return self.copy() if copy else self\n        # if we are astyping to another nullable masked dtype, we can fastpath\n        if isinstance(dtype, BaseMaskedDtype):\n            # TODO deal with NaNs\n            data = self._data.astype(dtype.numpy_dtype, copy=copy)\n            # mask is copied depending on whether the data was copied, and\n            # not directly depending on the `copy` keyword\n            mask = self._mask if data is self._data else self._mask.copy()\n            return dtype.construct_array_type()(data, mask, copy=False)\n        elif isinstance(dtype, StringDtype):\n            return StringArray._from_sequence(self, copy=False)\n\n        # coerce\n        if is_float_dtype(dtype):\n            # In astype, we consider dtype=float to also mean na_value=np.nan\n            kwargs = {\"na_value\": np.nan}\n        elif is_datetime64_dtype(dtype):\n            kwargs = {\"na_value\": np.datetime64(\"NaT\")}\n        else:\n            kwargs = {}\n\n        data = self.to_numpy(dtype=dtype, **kwargs)\n        return astype_nansafe(data, dtype, copy=False)\n\n    def _values_for_argsort(self) -> np.ndarray:\n        return self._data\n\n    def _cmp_method(self, other, op):\n        from pandas.arrays import BooleanArray, IntegerArray\n\n        mask = None\n\n        if isinstance(other, (BooleanArray, IntegerArray, FloatingArray)):\n            other, mask = other._data, other._mask\n\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n\n        if other is libmissing.NA:\n            # numpy does not handle pd.NA well as \"other\" scalar (it returns\n            # a scalar False instead of an array)\n            # This may be fixed by NA.__array_ufunc__. Revisit this check\n            # once that's implemented.\n            result = np.zeros(self._data.shape, dtype=\"bool\")\n            mask = np.ones(self._data.shape, dtype=\"bool\")\n        else:\n            with warnings.catch_warnings():\n                # numpy may show a FutureWarning:\n                #     elementwise comparison failed; returning scalar instead,\n                #     but in the future will perform elementwise comparison\n                # before returning NotImplemented. We fall back to the correct\n                # behavior today, so that should be fine to ignore.\n                warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op.__name__}__\")\n                    result = method(other)\n\n                if result is NotImplemented:\n                    result = invalid_comparison(self._data, other, op)\n\n        # nans propagate\n        if mask is None:\n            mask = self._mask.copy()\n        else:\n            mask = self._mask | mask\n\n        return BooleanArray(result, mask)\n\n    def sum(self, *, skipna=True, min_count=0, **kwargs):\n        nv.validate_sum((), kwargs)\n        return super()._reduce(\"sum\", skipna=skipna, min_count=min_count)\n\n    def prod(self, *, skipna=True, min_count=0, **kwargs):\n        nv.validate_prod((), kwargs)\n        return super()._reduce(\"prod\", skipna=skipna, min_count=min_count)\n\n    def min(self, *, skipna=True, **kwargs):\n        nv.validate_min((), kwargs)\n        return super()._reduce(\"min\", skipna=skipna)\n\n    def max(self, *, skipna=True, **kwargs):\n        nv.validate_max((), kwargs)\n        return super()._reduce(\"max\", skipna=skipna)\n\n    def _maybe_mask_result(self, result, mask, other, op_name: str):\n        \"\"\"\n        Parameters\n        ----------\n        result : array-like\n        mask : array-like bool\n        other : scalar or array-like\n        op_name : str\n        \"\"\"\n        # TODO are there cases we don't end up with float?\n        # if we have a float operand we are by-definition\n        # a float result\n        # or our op is a divide\n        # if (is_float_dtype(other) or is_float(other)) or (\n        #     op_name in [\"rtruediv\", \"truediv\"]\n        # ):\n        #     result[mask] = np.nan\n        #     return result\n\n        return type(self)(result, mask, copy=False)\n\n    def _arith_method(self, other, op):\n        from pandas.arrays import IntegerArray\n\n        omask = None\n\n        if getattr(other, \"ndim\", 0) > 1:\n            raise NotImplementedError(\"can only perform ops with 1-d structures\")\n\n        if isinstance(other, (IntegerArray, FloatingArray)):\n            other, omask = other._data, other._mask\n\n        elif is_list_like(other):\n            other = np.asarray(other)\n            if other.ndim > 1:\n                raise NotImplementedError(\"can only perform ops with 1-d structures\")\n            if len(self) != len(other):\n                raise ValueError(\"Lengths must match\")\n            if not (is_float_dtype(other) or is_integer_dtype(other)):\n                raise TypeError(\"can only perform ops with numeric values\")\n\n        else:\n            if not (is_float(other) or is_integer(other) or other is libmissing.NA):\n                raise TypeError(\"can only perform ops with numeric values\")\n\n        if omask is None:\n            mask = self._mask.copy()\n            if other is libmissing.NA:\n                mask |= True\n        else:\n            mask = self._mask | omask\n\n        if op.__name__ == \"pow\":\n            # 1 ** x is 1.\n            mask = np.where((self._data == 1) & ~self._mask, False, mask)\n            # x ** 0 is 1.\n            if omask is not None:\n                mask = np.where((other == 0) & ~omask, False, mask)\n            elif other is not libmissing.NA:\n                mask = np.where(other == 0, False, mask)\n\n        elif op.__name__ == \"rpow\":\n            # 1 ** x is 1.\n            if omask is not None:\n                mask = np.where((other == 1) & ~omask, False, mask)\n            elif other is not libmissing.NA:\n                mask = np.where(other == 1, False, mask)\n            # x ** 0 is 1.\n            mask = np.where((self._data == 0) & ~self._mask, False, mask)\n\n        if other is libmissing.NA:\n            result = np.ones_like(self._data)\n        else:\n            with np.errstate(all=\"ignore\"):\n                result = op(self._data, other)\n\n        # divmod returns a tuple\n        if op.__name__ == \"divmod\":\n            div, mod = result\n            return (\n                self._maybe_mask_result(div, mask, other, \"floordiv\"),\n                self._maybe_mask_result(mod, mask, other, \"mod\"),\n            )\n\n        return self._maybe_mask_result(result, mask, other, op.__name__)\n\n\n_dtype_docstring = \"\"\"\nAn ExtensionDtype for {dtype} data.\n\nThis dtype uses ``pd.NA`` as missing value indicator.\n\nAttributes\n----------\nNone\n\nMethods\n-------\nNone\n\"\"\"\n\n# create the Dtype\n\n\n@register_extension_dtype\nclass Float32Dtype(FloatingDtype):\n    type = np.float32\n    name = \"Float32\"\n    __doc__ = _dtype_docstring.format(dtype=\"float32\")\n\n\n@register_extension_dtype\nclass Float64Dtype(FloatingDtype):\n    type = np.float64\n    name = \"Float64\"\n    __doc__ = _dtype_docstring.format(dtype=\"float64\")\n\n\nFLOAT_STR_TO_DTYPE = {\n    \"float32\": Float32Dtype(),\n    \"float64\": Float64Dtype(),\n}\n"
    },
    {
      "filename": "pandas/core/arrays/interval.py",
      "content": "import operator\nfrom operator import le, lt\nimport textwrap\nfrom typing import Sequence, Type, TypeVar\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs.interval import (\n    VALID_CLOSED,\n    Interval,\n    IntervalMixin,\n    intervals_to_interval_bounds,\n)\nfrom pandas._libs.missing import NA\nfrom pandas.compat.numpy import function as nv\nfrom pandas.util._decorators import Appender\n\nfrom pandas.core.dtypes.cast import maybe_convert_platform\nfrom pandas.core.dtypes.common import (\n    is_categorical_dtype,\n    is_datetime64_any_dtype,\n    is_float_dtype,\n    is_integer_dtype,\n    is_interval_dtype,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    is_string_dtype,\n    is_timedelta64_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import IntervalDtype\nfrom pandas.core.dtypes.generic import (\n    ABCDatetimeIndex,\n    ABCIntervalIndex,\n    ABCPeriodIndex,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna, notna\n\nfrom pandas.core.algorithms import take, value_counts\nfrom pandas.core.arrays.base import ExtensionArray, _extension_array_shared_docs\nfrom pandas.core.arrays.categorical import Categorical\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    array,\n    ensure_wrapped_if_datetimelike,\n    extract_array,\n)\nfrom pandas.core.indexers import check_array_indexer\nfrom pandas.core.indexes.base import ensure_index\nfrom pandas.core.ops import invalid_comparison, unpack_zerodim_and_defer\n\nIntervalArrayT = TypeVar(\"IntervalArrayT\", bound=\"IntervalArray\")\n\n_interval_shared_docs = {}\n\n_shared_docs_kwargs = {\n    \"klass\": \"IntervalArray\",\n    \"qualname\": \"arrays.IntervalArray\",\n    \"name\": \"\",\n}\n\n\n_interval_shared_docs[\n    \"class\"\n] = \"\"\"\n%(summary)s\n\n.. versionadded:: %(versionadded)s\n\nParameters\n----------\ndata : array-like (1-dimensional)\n    Array-like containing Interval objects from which to build the\n    %(klass)s.\nclosed : {'left', 'right', 'both', 'neither'}, default 'right'\n    Whether the intervals are closed on the left-side, right-side, both or\n    neither.\ndtype : dtype or None, default None\n    If None, dtype will be inferred.\ncopy : bool, default False\n    Copy the input data.\n%(name)s\\\nverify_integrity : bool, default True\n    Verify that the %(klass)s is valid.\n\nAttributes\n----------\nleft\nright\nclosed\nmid\nlength\nis_empty\nis_non_overlapping_monotonic\n%(extra_attributes)s\\\n\nMethods\n-------\nfrom_arrays\nfrom_tuples\nfrom_breaks\ncontains\noverlaps\nset_closed\nto_tuples\n%(extra_methods)s\\\n\nSee Also\n--------\nIndex : The base pandas Index type.\nInterval : A bounded slice-like interval; the elements of an %(klass)s.\ninterval_range : Function to create a fixed frequency IntervalIndex.\ncut : Bin values into discrete Intervals.\nqcut : Bin values into equal-sized Intervals based on rank or sample quantiles.\n\nNotes\n-----\nSee the `user guide\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#intervalindex>`_\nfor more.\n\n%(examples)s\\\n\"\"\"\n\n\n@Appender(\n    _interval_shared_docs[\"class\"]\n    % {\n        \"klass\": \"IntervalArray\",\n        \"summary\": \"Pandas array for interval data that are closed on the same side.\",\n        \"versionadded\": \"0.24.0\",\n        \"name\": \"\",\n        \"extra_attributes\": \"\",\n        \"extra_methods\": \"\",\n        \"examples\": textwrap.dedent(\n            \"\"\"\\\n    Examples\n    --------\n    A new ``IntervalArray`` can be constructed directly from an array-like of\n    ``Interval`` objects:\n\n    >>> pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(1, 5)])\n    <IntervalArray>\n    [(0, 1], (1, 5]]\n    Length: 2, closed: right, dtype: interval[int64]\n\n    It may also be constructed using one of the constructor\n    methods: :meth:`IntervalArray.from_arrays`,\n    :meth:`IntervalArray.from_breaks`, and :meth:`IntervalArray.from_tuples`.\n    \"\"\"\n        ),\n    }\n)\nclass IntervalArray(IntervalMixin, ExtensionArray):\n    ndim = 1\n    can_hold_na = True\n    _na_value = _fill_value = np.nan\n\n    # ---------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls,\n        data,\n        closed=None,\n        dtype=None,\n        copy: bool = False,\n        verify_integrity: bool = True,\n    ):\n\n        if isinstance(data, (ABCSeries, ABCIntervalIndex)) and is_interval_dtype(\n            data.dtype\n        ):\n            data = data._values  # TODO: extract_array?\n\n        if isinstance(data, cls):\n            left = data._left\n            right = data._right\n            closed = closed or data.closed\n        else:\n\n            # don't allow scalars\n            if is_scalar(data):\n                msg = (\n                    f\"{cls.__name__}(...) must be called with a collection \"\n                    f\"of some kind, {data} was passed\"\n                )\n                raise TypeError(msg)\n\n            # might need to convert empty or purely na data\n            data = maybe_convert_platform_interval(data)\n            left, right, infer_closed = intervals_to_interval_bounds(\n                data, validate_closed=closed is None\n            )\n            closed = closed or infer_closed\n\n        return cls._simple_new(\n            left,\n            right,\n            closed,\n            copy=copy,\n            dtype=dtype,\n            verify_integrity=verify_integrity,\n        )\n\n    @classmethod\n    def _simple_new(\n        cls, left, right, closed=None, copy=False, dtype=None, verify_integrity=True\n    ):\n        result = IntervalMixin.__new__(cls)\n\n        closed = closed or \"right\"\n        left = ensure_index(left, copy=copy)\n        right = ensure_index(right, copy=copy)\n\n        if dtype is not None:\n            # GH 19262: dtype must be an IntervalDtype to override inferred\n            dtype = pandas_dtype(dtype)\n            if not is_interval_dtype(dtype):\n                msg = f\"dtype must be an IntervalDtype, got {dtype}\"\n                raise TypeError(msg)\n            elif dtype.subtype is not None:\n                left = left.astype(dtype.subtype)\n                right = right.astype(dtype.subtype)\n\n        # coerce dtypes to match if needed\n        if is_float_dtype(left) and is_integer_dtype(right):\n            right = right.astype(left.dtype)\n        elif is_float_dtype(right) and is_integer_dtype(left):\n            left = left.astype(right.dtype)\n\n        if type(left) != type(right):\n            msg = (\n                f\"must not have differing left [{type(left).__name__}] and \"\n                f\"right [{type(right).__name__}] types\"\n            )\n            raise ValueError(msg)\n        elif is_categorical_dtype(left.dtype) or is_string_dtype(left.dtype):\n            # GH 19016\n            msg = (\n                \"category, object, and string subtypes are not supported \"\n                \"for IntervalArray\"\n            )\n            raise TypeError(msg)\n        elif isinstance(left, ABCPeriodIndex):\n            msg = \"Period dtypes are not supported, use a PeriodIndex instead\"\n            raise ValueError(msg)\n        elif isinstance(left, ABCDatetimeIndex) and str(left.tz) != str(right.tz):\n            msg = (\n                \"left and right must have the same time zone, got \"\n                f\"'{left.tz}' and '{right.tz}'\"\n            )\n            raise ValueError(msg)\n\n        # For dt64/td64 we want DatetimeArray/TimedeltaArray instead of ndarray\n        left = ensure_wrapped_if_datetimelike(left)\n        left = extract_array(left, extract_numpy=True)\n        right = ensure_wrapped_if_datetimelike(right)\n        right = extract_array(right, extract_numpy=True)\n\n        lbase = getattr(left, \"_ndarray\", left).base\n        rbase = getattr(right, \"_ndarray\", right).base\n        if lbase is not None and lbase is rbase:\n            # If these share data, then setitem could corrupt our IA\n            right = right.copy()\n\n        result._left = left\n        result._right = right\n        result._closed = closed\n        if verify_integrity:\n            result._validate()\n        return result\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype=None, copy=False):\n        return cls(scalars, dtype=dtype, copy=copy)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        if len(values) == 0:\n            # An empty array returns object-dtype here. We can't create\n            # a new IA from an (empty) object-dtype array, so turn it into the\n            # correct dtype.\n            values = values.astype(original.dtype.subtype)\n        return cls(values, closed=original.closed)\n\n    _interval_shared_docs[\"from_breaks\"] = textwrap.dedent(\n        \"\"\"\n        Construct an %(klass)s from an array of splits.\n\n        Parameters\n        ----------\n        breaks : array-like (1-dimensional)\n            Left and right bounds for each interval.\n        closed : {'left', 'right', 'both', 'neither'}, default 'right'\n            Whether the intervals are closed on the left-side, right-side, both\n            or neither.\n        copy : bool, default False\n            Copy the data.\n        dtype : dtype or None, default None\n            If None, dtype will be inferred.\n\n        Returns\n        -------\n        %(klass)s\n\n        See Also\n        --------\n        interval_range : Function to create a fixed frequency IntervalIndex.\n        %(klass)s.from_arrays : Construct from a left and right array.\n        %(klass)s.from_tuples : Construct from a sequence of tuples.\n\n        %(examples)s\\\n        \"\"\"\n    )\n\n    @classmethod\n    @Appender(\n        _interval_shared_docs[\"from_breaks\"]\n        % {\n            \"klass\": \"IntervalArray\",\n            \"examples\": textwrap.dedent(\n                \"\"\"\\\n        Examples\n        --------\n        >>> pd.arrays.IntervalArray.from_breaks([0, 1, 2, 3])\n        <IntervalArray>\n        [(0, 1], (1, 2], (2, 3]]\n        Length: 3, closed: right, dtype: interval[int64]\n        \"\"\"\n            ),\n        }\n    )\n    def from_breaks(cls, breaks, closed=\"right\", copy=False, dtype=None):\n        breaks = maybe_convert_platform_interval(breaks)\n\n        return cls.from_arrays(breaks[:-1], breaks[1:], closed, copy=copy, dtype=dtype)\n\n    _interval_shared_docs[\"from_arrays\"] = textwrap.dedent(\n        \"\"\"\n        Construct from two arrays defining the left and right bounds.\n\n        Parameters\n        ----------\n        left : array-like (1-dimensional)\n            Left bounds for each interval.\n        right : array-like (1-dimensional)\n            Right bounds for each interval.\n        closed : {'left', 'right', 'both', 'neither'}, default 'right'\n            Whether the intervals are closed on the left-side, right-side, both\n            or neither.\n        copy : bool, default False\n            Copy the data.\n        dtype : dtype, optional\n            If None, dtype will be inferred.\n\n        Returns\n        -------\n        %(klass)s\n\n        Raises\n        ------\n        ValueError\n            When a value is missing in only one of `left` or `right`.\n            When a value in `left` is greater than the corresponding value\n            in `right`.\n\n        See Also\n        --------\n        interval_range : Function to create a fixed frequency IntervalIndex.\n        %(klass)s.from_breaks : Construct an %(klass)s from an array of\n            splits.\n        %(klass)s.from_tuples : Construct an %(klass)s from an\n            array-like of tuples.\n\n        Notes\n        -----\n        Each element of `left` must be less than or equal to the `right`\n        element at the same position. If an element is missing, it must be\n        missing in both `left` and `right`. A TypeError is raised when\n        using an unsupported type for `left` or `right`. At the moment,\n        'category', 'object', and 'string' subtypes are not supported.\n\n        %(examples)s\\\n        \"\"\"\n    )\n\n    @classmethod\n    @Appender(\n        _interval_shared_docs[\"from_arrays\"]\n        % {\n            \"klass\": \"IntervalArray\",\n            \"examples\": textwrap.dedent(\n                \"\"\"\\\n        >>> pd.arrays.IntervalArray.from_arrays([0, 1, 2], [1, 2, 3])\n        <IntervalArray>\n        [(0, 1], (1, 2], (2, 3]]\n        Length: 3, closed: right, dtype: interval[int64]\n        \"\"\"\n            ),\n        }\n    )\n    def from_arrays(cls, left, right, closed=\"right\", copy=False, dtype=None):\n        left = maybe_convert_platform_interval(left)\n        right = maybe_convert_platform_interval(right)\n\n        return cls._simple_new(\n            left, right, closed, copy=copy, dtype=dtype, verify_integrity=True\n        )\n\n    _interval_shared_docs[\"from_tuples\"] = textwrap.dedent(\n        \"\"\"\n        Construct an %(klass)s from an array-like of tuples.\n\n        Parameters\n        ----------\n        data : array-like (1-dimensional)\n            Array of tuples.\n        closed : {'left', 'right', 'both', 'neither'}, default 'right'\n            Whether the intervals are closed on the left-side, right-side, both\n            or neither.\n        copy : bool, default False\n            By-default copy the data, this is compat only and ignored.\n        dtype : dtype or None, default None\n            If None, dtype will be inferred.\n\n        Returns\n        -------\n        %(klass)s\n\n        See Also\n        --------\n        interval_range : Function to create a fixed frequency IntervalIndex.\n        %(klass)s.from_arrays : Construct an %(klass)s from a left and\n                                    right array.\n        %(klass)s.from_breaks : Construct an %(klass)s from an array of\n                                    splits.\n\n        %(examples)s\\\n        \"\"\"\n    )\n\n    @classmethod\n    @Appender(\n        _interval_shared_docs[\"from_tuples\"]\n        % {\n            \"klass\": \"IntervalArray\",\n            \"examples\": textwrap.dedent(\n                \"\"\"\\\n        Examples\n        --------\n        >>> pd.arrays.IntervalArray.from_tuples([(0, 1), (1, 2)])\n        <IntervalArray>\n        [(0, 1], (1, 2]]\n        Length: 2, closed: right, dtype: interval[int64]\n        \"\"\"\n            ),\n        }\n    )\n    def from_tuples(cls, data, closed=\"right\", copy=False, dtype=None):\n        if len(data):\n            left, right = [], []\n        else:\n            # ensure that empty data keeps input dtype\n            left = right = data\n\n        for d in data:\n            if isna(d):\n                lhs = rhs = np.nan\n            else:\n                name = cls.__name__\n                try:\n                    # need list of length 2 tuples, e.g. [(0, 1), (1, 2), ...]\n                    lhs, rhs = d\n                except ValueError as err:\n                    msg = f\"{name}.from_tuples requires tuples of length 2, got {d}\"\n                    raise ValueError(msg) from err\n                except TypeError as err:\n                    msg = f\"{name}.from_tuples received an invalid item, {d}\"\n                    raise TypeError(msg) from err\n            left.append(lhs)\n            right.append(rhs)\n\n        return cls.from_arrays(left, right, closed, copy=False, dtype=dtype)\n\n    def _validate(self):\n        \"\"\"\n        Verify that the IntervalArray is valid.\n\n        Checks that\n\n        * closed is valid\n        * left and right match lengths\n        * left and right have the same missing values\n        * left is always below right\n        \"\"\"\n        if self.closed not in VALID_CLOSED:\n            msg = f\"invalid option for 'closed': {self.closed}\"\n            raise ValueError(msg)\n        if len(self._left) != len(self._right):\n            msg = \"left and right must have the same length\"\n            raise ValueError(msg)\n        left_mask = notna(self._left)\n        right_mask = notna(self._right)\n        if not (left_mask == right_mask).all():\n            msg = (\n                \"missing values must be missing in the same \"\n                \"location both left and right sides\"\n            )\n            raise ValueError(msg)\n        if not (self._left[left_mask] <= self._right[left_mask]).all():\n            msg = \"left side of interval must be <= right side\"\n            raise ValueError(msg)\n\n    def _shallow_copy(self, left, right):\n        \"\"\"\n        Return a new IntervalArray with the replacement attributes\n\n        Parameters\n        ----------\n        left : Index\n            Values to be used for the left-side of the intervals.\n        right : Index\n            Values to be used for the right-side of the intervals.\n        \"\"\"\n        return self._simple_new(left, right, closed=self.closed, verify_integrity=False)\n\n    # ---------------------------------------------------------------------\n    # Descriptive\n\n    @property\n    def dtype(self):\n        return IntervalDtype(self.left.dtype)\n\n    @property\n    def nbytes(self) -> int:\n        return self.left.nbytes + self.right.nbytes\n\n    @property\n    def size(self) -> int:\n        # Avoid materializing self.values\n        return self.left.size\n\n    # ---------------------------------------------------------------------\n    # EA Interface\n\n    def __iter__(self):\n        return iter(np.asarray(self))\n\n    def __len__(self) -> int:\n        return len(self._left)\n\n    def __getitem__(self, key):\n        key = check_array_indexer(self, key)\n        left = self._left[key]\n        right = self._right[key]\n\n        if not isinstance(left, (np.ndarray, ExtensionArray)):\n            # scalar\n            if is_scalar(left) and isna(left):\n                return self._fill_value\n            return Interval(left, right, self.closed)\n        if np.ndim(left) > 1:\n            # GH#30588 multi-dimensional indexer disallowed\n            raise ValueError(\"multi-dimensional indexing not allowed\")\n        return self._shallow_copy(left, right)\n\n    def __setitem__(self, key, value):\n        value_left, value_right = self._validate_setitem_value(value)\n        key = check_array_indexer(self, key)\n\n        self._left[key] = value_left\n        self._right[key] = value_right\n\n    def _cmp_method(self, other, op):\n        # ensure pandas array for list-like and eliminate non-interval scalars\n        if is_list_like(other):\n            if len(self) != len(other):\n                raise ValueError(\"Lengths must match to compare\")\n            other = array(other)\n        elif not isinstance(other, Interval):\n            # non-interval scalar -> no matches\n            return invalid_comparison(self, other, op)\n\n        # determine the dtype of the elements we want to compare\n        if isinstance(other, Interval):\n            other_dtype = pandas_dtype(\"interval\")\n        elif not is_categorical_dtype(other.dtype):\n            other_dtype = other.dtype\n        else:\n            # for categorical defer to categories for dtype\n            other_dtype = other.categories.dtype\n\n            # extract intervals if we have interval categories with matching closed\n            if is_interval_dtype(other_dtype):\n                if self.closed != other.categories.closed:\n                    return invalid_comparison(self, other, op)\n\n                other = other.categories.take(\n                    other.codes, allow_fill=True, fill_value=other.categories._na_value\n                )\n\n        # interval-like -> need same closed and matching endpoints\n        if is_interval_dtype(other_dtype):\n            if self.closed != other.closed:\n                return invalid_comparison(self, other, op)\n            elif not isinstance(other, Interval):\n                other = type(self)(other)\n\n            if op is operator.eq:\n                return (self._left == other.left) & (self._right == other.right)\n            elif op is operator.ne:\n                return (self._left != other.left) | (self._right != other.right)\n            elif op is operator.gt:\n                return (self._left > other.left) | (\n                    (self._left == other.left) & (self._right > other.right)\n                )\n            elif op is operator.ge:\n                return (self == other) | (self > other)\n            elif op is operator.lt:\n                return (self._left < other.left) | (\n                    (self._left == other.left) & (self._right < other.right)\n                )\n            else:\n                # operator.lt\n                return (self == other) | (self < other)\n\n        # non-interval/non-object dtype -> no matches\n        if not is_object_dtype(other_dtype):\n            return invalid_comparison(self, other, op)\n\n        # object dtype -> iteratively check for intervals\n        result = np.zeros(len(self), dtype=bool)\n        for i, obj in enumerate(other):\n            try:\n                result[i] = op(self[i], obj)\n            except TypeError:\n                if obj is NA:\n                    # comparison with np.nan returns NA\n                    # github.com/pandas-dev/pandas/pull/37124#discussion_r509095092\n                    result[i] = op is operator.ne\n                else:\n                    raise\n        return result\n\n    @unpack_zerodim_and_defer(\"__eq__\")\n    def __eq__(self, other):\n        return self._cmp_method(other, operator.eq)\n\n    @unpack_zerodim_and_defer(\"__ne__\")\n    def __ne__(self, other):\n        return self._cmp_method(other, operator.ne)\n\n    @unpack_zerodim_and_defer(\"__gt__\")\n    def __gt__(self, other):\n        return self._cmp_method(other, operator.gt)\n\n    @unpack_zerodim_and_defer(\"__ge__\")\n    def __ge__(self, other):\n        return self._cmp_method(other, operator.ge)\n\n    @unpack_zerodim_and_defer(\"__lt__\")\n    def __lt__(self, other):\n        return self._cmp_method(other, operator.lt)\n\n    @unpack_zerodim_and_defer(\"__le__\")\n    def __le__(self, other):\n        return self._cmp_method(other, operator.le)\n\n    def argsort(\n        self,\n        ascending: bool = True,\n        kind: str = \"quicksort\",\n        na_position: str = \"last\",\n        *args,\n        **kwargs,\n    ) -> np.ndarray:\n        ascending = nv.validate_argsort_with_ascending(ascending, args, kwargs)\n\n        if ascending and kind == \"quicksort\" and na_position == \"last\":\n            return np.lexsort((self.right, self.left))\n\n        # TODO: other cases we can use lexsort for?  much more performant.\n        return super().argsort(\n            ascending=ascending, kind=kind, na_position=na_position, **kwargs\n        )\n\n    def fillna(self, value=None, method=None, limit=None):\n        \"\"\"\n        Fill NA/NaN values using the specified method.\n\n        Parameters\n        ----------\n        value : scalar, dict, Series\n            If a scalar value is passed it is used to fill all missing values.\n            Alternatively, a Series or dict can be used to fill in different\n            values for each index. The value should not be a list. The\n            value(s) passed should be either Interval objects or NA/NaN.\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n            (Not implemented yet for IntervalArray)\n            Method to use for filling holes in reindexed Series\n        limit : int, default None\n            (Not implemented yet for IntervalArray)\n            If method is specified, this is the maximum number of consecutive\n            NaN values to forward/backward fill. In other words, if there is\n            a gap with more than this number of consecutive NaNs, it will only\n            be partially filled. If method is not specified, this is the\n            maximum number of entries along the entire axis where NaNs will be\n            filled.\n\n        Returns\n        -------\n        filled : IntervalArray with NA/NaN filled\n        \"\"\"\n        if method is not None:\n            raise TypeError(\"Filling by method is not supported for IntervalArray.\")\n        if limit is not None:\n            raise TypeError(\"limit is not supported for IntervalArray.\")\n\n        value_left, value_right = self._validate_fill_value(value)\n\n        left = self.left.fillna(value=value_left)\n        right = self.right.fillna(value=value_right)\n        return self._shallow_copy(left, right)\n\n    def astype(self, dtype, copy=True):\n        \"\"\"\n        Cast to an ExtensionArray or NumPy array with dtype 'dtype'.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n\n        copy : bool, default True\n            Whether to copy the data, even if not necessary. If False,\n            a copy is made only if the old dtype does not match the\n            new dtype.\n\n        Returns\n        -------\n        array : ExtensionArray or ndarray\n            ExtensionArray or NumPy ndarray with 'dtype' for its dtype.\n        \"\"\"\n        from pandas import Index\n        from pandas.core.arrays.string_ import StringDtype\n\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        if is_interval_dtype(dtype):\n            if dtype == self.dtype:\n                return self.copy() if copy else self\n\n            # need to cast to different subtype\n            try:\n                # We need to use Index rules for astype to prevent casting\n                #  np.nan entries to int subtypes\n                new_left = Index(self._left, copy=False).astype(dtype.subtype)\n                new_right = Index(self._right, copy=False).astype(dtype.subtype)\n            except TypeError as err:\n                msg = (\n                    f\"Cannot convert {self.dtype} to {dtype}; subtypes are incompatible\"\n                )\n                raise TypeError(msg) from err\n            return self._shallow_copy(new_left, new_right)\n        elif is_categorical_dtype(dtype):\n            return Categorical(np.asarray(self), dtype=dtype)\n        elif isinstance(dtype, StringDtype):\n            return dtype.construct_array_type()._from_sequence(self, copy=False)\n\n        # TODO: This try/except will be repeated.\n        try:\n            return np.asarray(self).astype(dtype, copy=copy)\n        except (TypeError, ValueError) as err:\n            msg = f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n            raise TypeError(msg) from err\n\n    def equals(self, other) -> bool:\n        if type(self) != type(other):\n            return False\n\n        return bool(\n            self.closed == other.closed\n            and self.left.equals(other.left)\n            and self.right.equals(other.right)\n        )\n\n    @classmethod\n    def _concat_same_type(\n        cls: Type[IntervalArrayT], to_concat: Sequence[IntervalArrayT]\n    ) -> IntervalArrayT:\n        \"\"\"\n        Concatenate multiple IntervalArray\n\n        Parameters\n        ----------\n        to_concat : sequence of IntervalArray\n\n        Returns\n        -------\n        IntervalArray\n        \"\"\"\n        closed = {interval.closed for interval in to_concat}\n        if len(closed) != 1:\n            raise ValueError(\"Intervals must all be closed on the same side.\")\n        closed = closed.pop()\n\n        left = np.concatenate([interval.left for interval in to_concat])\n        right = np.concatenate([interval.right for interval in to_concat])\n        return cls._simple_new(left, right, closed=closed, copy=False)\n\n    def copy(self: IntervalArrayT) -> IntervalArrayT:\n        \"\"\"\n        Return a copy of the array.\n\n        Returns\n        -------\n        IntervalArray\n        \"\"\"\n        left = self._left.copy()\n        right = self._right.copy()\n        closed = self.closed\n        # TODO: Could skip verify_integrity here.\n        return type(self).from_arrays(left, right, closed=closed)\n\n    def isna(self) -> np.ndarray:\n        return isna(self._left)\n\n    def shift(self, periods: int = 1, fill_value: object = None) -> \"IntervalArray\":\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n\n        # ExtensionArray.shift doesn't work for two reasons\n        # 1. IntervalArray.dtype.na_value may not be correct for the dtype.\n        # 2. IntervalArray._from_sequence only accepts NaN for missing values,\n        #    not other values like NaT\n\n        empty_len = min(abs(periods), len(self))\n        if isna(fill_value):\n            from pandas import Index\n\n            fill_value = Index(self._left, copy=False)._na_value\n            empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))\n        else:\n            empty = self._from_sequence([fill_value] * empty_len)\n\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def take(self, indices, *, allow_fill=False, fill_value=None, axis=None, **kwargs):\n        \"\"\"\n        Take elements from the IntervalArray.\n\n        Parameters\n        ----------\n        indices : sequence of integers\n            Indices to be taken.\n\n        allow_fill : bool, default False\n            How to handle negative values in `indices`.\n\n            * False: negative values in `indices` indicate positional indices\n              from the right (the default). This is similar to\n              :func:`numpy.take`.\n\n            * True: negative values in `indices` indicate\n              missing values. These values are set to `fill_value`. Any other\n              other negative values raise a ``ValueError``.\n\n        fill_value : Interval or NA, optional\n            Fill value to use for NA-indices when `allow_fill` is True.\n            This may be ``None``, in which case the default NA value for\n            the type, ``self.dtype.na_value``, is used.\n\n            For many ExtensionArrays, there will be two representations of\n            `fill_value`: a user-facing \"boxed\" scalar, and a low-level\n            physical NA value. `fill_value` should be the user-facing version,\n            and the implementation should handle translating that to the\n            physical version for processing the take if necessary.\n\n        axis : any, default None\n            Present for compat with IntervalIndex; does nothing.\n\n        Returns\n        -------\n        IntervalArray\n\n        Raises\n        ------\n        IndexError\n            When the indices are out of bounds for the array.\n        ValueError\n            When `indices` contains negative values other than ``-1``\n            and `allow_fill` is True.\n        \"\"\"\n        nv.validate_take((), kwargs)\n\n        fill_left = fill_right = fill_value\n        if allow_fill:\n            fill_left, fill_right = self._validate_fill_value(fill_value)\n\n        left_take = take(\n            self._left, indices, allow_fill=allow_fill, fill_value=fill_left\n        )\n        right_take = take(\n            self._right, indices, allow_fill=allow_fill, fill_value=fill_right\n        )\n\n        return self._shallow_copy(left_take, right_take)\n\n    def _validate_listlike(self, value):\n        # list-like of intervals\n        try:\n            array = IntervalArray(value)\n            # TODO: self._check_closed_matches(array, name=\"value\")\n            value_left, value_right = array.left, array.right\n        except TypeError as err:\n            # wrong type: not interval or NA\n            msg = f\"'value' should be an interval type, got {type(value)} instead.\"\n            raise TypeError(msg) from err\n        return value_left, value_right\n\n    def _validate_scalar(self, value):\n        if isinstance(value, Interval):\n            self._check_closed_matches(value, name=\"value\")\n            left, right = value.left, value.right\n        elif is_valid_nat_for_dtype(value, self.left.dtype):\n            # GH#18295\n            left = right = value\n        else:\n            raise TypeError(\n                \"can only insert Interval objects and NA into an IntervalArray\"\n            )\n        return left, right\n\n    def _validate_fill_value(self, value):\n        return self._validate_scalar(value)\n\n    def _validate_setitem_value(self, value):\n        needs_float_conversion = False\n\n        if is_valid_nat_for_dtype(value, self.left.dtype):\n            # na value: need special casing to set directly on numpy arrays\n            if is_integer_dtype(self.dtype.subtype):\n                # can't set NaN on a numpy integer array\n                needs_float_conversion = True\n            elif is_datetime64_any_dtype(self.dtype.subtype):\n                # need proper NaT to set directly on the numpy array\n                value = np.datetime64(\"NaT\")\n            elif is_timedelta64_dtype(self.dtype.subtype):\n                # need proper NaT to set directly on the numpy array\n                value = np.timedelta64(\"NaT\")\n            value_left, value_right = value, value\n\n        elif is_interval_dtype(value) or isinstance(value, Interval):\n            # scalar interval\n            self._check_closed_matches(value, name=\"value\")\n            value_left, value_right = value.left, value.right\n\n        else:\n            return self._validate_listlike(value)\n\n        if needs_float_conversion:\n            raise ValueError(\"Cannot set float NaN to integer-backed IntervalArray\")\n        return value_left, value_right\n\n    def value_counts(self, dropna=True):\n        \"\"\"\n        Returns a Series containing counts of each interval.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        counts : Series\n\n        See Also\n        --------\n        Series.value_counts\n        \"\"\"\n        # TODO: implement this is a non-naive way!\n        return value_counts(np.asarray(self), dropna=dropna)\n\n    # ---------------------------------------------------------------------\n    # Rendering Methods\n\n    def _format_data(self):\n\n        # TODO: integrate with categorical and make generic\n        # name argument is unused here; just for compat with base / categorical\n        n = len(self)\n        max_seq_items = min((get_option(\"display.max_seq_items\") or n) // 10, 10)\n\n        formatter = str\n\n        if n == 0:\n            summary = \"[]\"\n        elif n == 1:\n            first = formatter(self[0])\n            summary = f\"[{first}]\"\n        elif n == 2:\n            first = formatter(self[0])\n            last = formatter(self[-1])\n            summary = f\"[{first}, {last}]\"\n        else:\n\n            if n > max_seq_items:\n                n = min(max_seq_items // 2, 10)\n                head = [formatter(x) for x in self[:n]]\n                tail = [formatter(x) for x in self[-n:]]\n                head_str = \", \".join(head)\n                tail_str = \", \".join(tail)\n                summary = f\"[{head_str} ... {tail_str}]\"\n            else:\n                tail = [formatter(x) for x in self]\n                tail_str = \", \".join(tail)\n                summary = f\"[{tail_str}]\"\n\n        return summary\n\n    def __repr__(self) -> str:\n        # the short repr has no trailing newline, while the truncated\n        # repr does. So we include a newline in our template, and strip\n        # any trailing newlines from format_object_summary\n        data = self._format_data()\n        class_name = f\"<{type(self).__name__}>\\n\"\n\n        template = (\n            f\"{class_name}\"\n            f\"{data}\\n\"\n            f\"Length: {len(self)}, closed: {self.closed}, dtype: {self.dtype}\"\n        )\n        return template\n\n    def _format_space(self):\n        space = \" \" * (len(type(self).__name__) + 1)\n        return f\"\\n{space}\"\n\n    # ---------------------------------------------------------------------\n    # Vectorized Interval Properties/Attributes\n\n    @property\n    def left(self):\n        \"\"\"\n        Return the left endpoints of each Interval in the IntervalArray as\n        an Index.\n        \"\"\"\n        from pandas import Index\n\n        return Index(self._left, copy=False)\n\n    @property\n    def right(self):\n        \"\"\"\n        Return the right endpoints of each Interval in the IntervalArray as\n        an Index.\n        \"\"\"\n        from pandas import Index\n\n        return Index(self._right, copy=False)\n\n    @property\n    def length(self):\n        \"\"\"\n        Return an Index with entries denoting the length of each Interval in\n        the IntervalArray.\n        \"\"\"\n        try:\n            return self.right - self.left\n        except TypeError as err:\n            # length not defined for some types, e.g. string\n            msg = (\n                \"IntervalArray contains Intervals without defined length, \"\n                \"e.g. Intervals with string endpoints\"\n            )\n            raise TypeError(msg) from err\n\n    @property\n    def mid(self):\n        \"\"\"\n        Return the midpoint of each Interval in the IntervalArray as an Index.\n        \"\"\"\n        try:\n            return 0.5 * (self.left + self.right)\n        except TypeError:\n            # datetime safe version\n            return self.left + 0.5 * self.length\n\n    _interval_shared_docs[\"overlaps\"] = textwrap.dedent(\n        \"\"\"\n        Check elementwise if an Interval overlaps the values in the %(klass)s.\n\n        Two intervals overlap if they share a common point, including closed\n        endpoints. Intervals that only have an open endpoint in common do not\n        overlap.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        other : %(klass)s\n            Interval to check against for an overlap.\n\n        Returns\n        -------\n        ndarray\n            Boolean array positionally indicating where an overlap occurs.\n\n        See Also\n        --------\n        Interval.overlaps : Check whether two Interval objects overlap.\n\n        Examples\n        --------\n        %(examples)s\n        >>> intervals.overlaps(pd.Interval(0.5, 1.5))\n        array([ True,  True, False])\n\n        Intervals that share closed endpoints overlap:\n\n        >>> intervals.overlaps(pd.Interval(1, 3, closed='left'))\n        array([ True,  True, True])\n\n        Intervals that only have an open endpoint in common do not overlap:\n\n        >>> intervals.overlaps(pd.Interval(1, 2, closed='right'))\n        array([False,  True, False])\n        \"\"\"\n    )\n\n    @Appender(\n        _interval_shared_docs[\"overlaps\"]\n        % {\n            \"klass\": \"IntervalArray\",\n            \"examples\": textwrap.dedent(\n                \"\"\"\\\n        >>> data = [(0, 1), (1, 3), (2, 4)]\n        >>> intervals = pd.arrays.IntervalArray.from_tuples(data)\n        >>> intervals\n        <IntervalArray>\n        [(0, 1], (1, 3], (2, 4]]\n        Length: 3, closed: right, dtype: interval[int64]\n        \"\"\"\n            ),\n        }\n    )\n    def overlaps(self, other):\n        if isinstance(other, (IntervalArray, ABCIntervalIndex)):\n            raise NotImplementedError\n        elif not isinstance(other, Interval):\n            msg = f\"`other` must be Interval-like, got {type(other).__name__}\"\n            raise TypeError(msg)\n\n        # equality is okay if both endpoints are closed (overlap at a point)\n        op1 = le if (self.closed_left and other.closed_right) else lt\n        op2 = le if (other.closed_left and self.closed_right) else lt\n\n        # overlaps is equivalent negation of two interval being disjoint:\n        # disjoint = (A.left > B.right) or (B.left > A.right)\n        # (simplifying the negation allows this to be done in less operations)\n        return op1(self.left, other.right) & op2(other.left, self.right)\n\n    # ---------------------------------------------------------------------\n\n    @property\n    def closed(self):\n        \"\"\"\n        Whether the intervals are closed on the left-side, right-side, both or\n        neither.\n        \"\"\"\n        return self._closed\n\n    _interval_shared_docs[\"set_closed\"] = textwrap.dedent(\n        \"\"\"\n        Return an %(klass)s identical to the current one, but closed on the\n        specified side.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        closed : {'left', 'right', 'both', 'neither'}\n            Whether the intervals are closed on the left-side, right-side, both\n            or neither.\n\n        Returns\n        -------\n        new_index : %(klass)s\n\n        %(examples)s\\\n        \"\"\"\n    )\n\n    @Appender(\n        _interval_shared_docs[\"set_closed\"]\n        % {\n            \"klass\": \"IntervalArray\",\n            \"examples\": textwrap.dedent(\n                \"\"\"\\\n        Examples\n        --------\n        >>> index = pd.arrays.IntervalArray.from_breaks(range(4))\n        >>> index\n        <IntervalArray>\n        [(0, 1], (1, 2], (2, 3]]\n        Length: 3, closed: right, dtype: interval[int64]\n        >>> index.set_closed('both')\n        <IntervalArray>\n        [[0, 1], [1, 2], [2, 3]]\n        Length: 3, closed: both, dtype: interval[int64]\n        \"\"\"\n            ),\n        }\n    )\n    def set_closed(self, closed):\n        if closed not in VALID_CLOSED:\n            msg = f\"invalid option for 'closed': {closed}\"\n            raise ValueError(msg)\n\n        return type(self)._simple_new(\n            left=self._left, right=self._right, closed=closed, verify_integrity=False\n        )\n\n    _interval_shared_docs[\n        \"is_non_overlapping_monotonic\"\n    ] = \"\"\"\n        Return True if the %(klass)s is non-overlapping (no Intervals share\n        points) and is either monotonic increasing or monotonic decreasing,\n        else False.\n        \"\"\"\n\n    # https://github.com/python/mypy/issues/1362\n    # Mypy does not support decorated properties\n    @property  # type: ignore[misc]\n    @Appender(\n        _interval_shared_docs[\"is_non_overlapping_monotonic\"] % _shared_docs_kwargs\n    )\n    def is_non_overlapping_monotonic(self):\n        # must be increasing  (e.g., [0, 1), [1, 2), [2, 3), ... )\n        # or decreasing (e.g., [-1, 0), [-2, -1), [-3, -2), ...)\n        # we already require left <= right\n\n        # strict inequality for closed == 'both'; equality implies overlapping\n        # at a point when both sides of intervals are included\n        if self.closed == \"both\":\n            return bool(\n                (self._right[:-1] < self._left[1:]).all()\n                or (self._left[:-1] > self._right[1:]).all()\n            )\n\n        # non-strict inequality when closed != 'both'; at least one side is\n        # not included in the intervals, so equality does not imply overlapping\n        return bool(\n            (self._right[:-1] <= self._left[1:]).all()\n            or (self._left[:-1] >= self._right[1:]).all()\n        )\n\n    # ---------------------------------------------------------------------\n    # Conversion\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\"\n        Return the IntervalArray's data as a numpy array of Interval\n        objects (with dtype='object')\n        \"\"\"\n        left = self._left\n        right = self._right\n        mask = self.isna()\n        closed = self._closed\n\n        result = np.empty(len(left), dtype=object)\n        for i in range(len(left)):\n            if mask[i]:\n                result[i] = np.nan\n            else:\n                result[i] = Interval(left[i], right[i], closed)\n        return result\n\n    def __arrow_array__(self, type=None):\n        \"\"\"\n        Convert myself into a pyarrow Array.\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays._arrow_utils import ArrowIntervalType\n\n        try:\n            subtype = pyarrow.from_numpy_dtype(self.dtype.subtype)\n        except TypeError as err:\n            raise TypeError(\n                f\"Conversion to arrow with subtype '{self.dtype.subtype}' \"\n                \"is not supported\"\n            ) from err\n        interval_type = ArrowIntervalType(subtype, self.closed)\n        storage_array = pyarrow.StructArray.from_arrays(\n            [\n                pyarrow.array(self._left, type=subtype, from_pandas=True),\n                pyarrow.array(self._right, type=subtype, from_pandas=True),\n            ],\n            names=[\"left\", \"right\"],\n        )\n        mask = self.isna()\n        if mask.any():\n            # if there are missing values, set validity bitmap also on the array level\n            null_bitmap = pyarrow.array(~mask).buffers()[1]\n            storage_array = pyarrow.StructArray.from_buffers(\n                storage_array.type,\n                len(storage_array),\n                [null_bitmap],\n                children=[storage_array.field(0), storage_array.field(1)],\n            )\n\n        if type is not None:\n            if type.equals(interval_type.storage_type):\n                return storage_array\n            elif isinstance(type, ArrowIntervalType):\n                # ensure we have the same subtype and closed attributes\n                if not type.equals(interval_type):\n                    raise TypeError(\n                        \"Not supported to convert IntervalArray to type with \"\n                        f\"different 'subtype' ({self.dtype.subtype} vs {type.subtype}) \"\n                        f\"and 'closed' ({self.closed} vs {type.closed}) attributes\"\n                    )\n            else:\n                raise TypeError(\n                    f\"Not supported to convert IntervalArray to '{type}' type\"\n                )\n\n        return pyarrow.ExtensionArray.from_storage(interval_type, storage_array)\n\n    _interval_shared_docs[\n        \"to_tuples\"\n    ] = \"\"\"\n        Return an %(return_type)s of tuples of the form (left, right).\n\n        Parameters\n        ----------\n        na_tuple : bool, default True\n            Returns NA as a tuple if True, ``(nan, nan)``, or just as the NA\n            value itself if False, ``nan``.\n\n        Returns\n        -------\n        tuples: %(return_type)s\n        %(examples)s\\\n        \"\"\"\n\n    @Appender(\n        _interval_shared_docs[\"to_tuples\"] % {\"return_type\": \"ndarray\", \"examples\": \"\"}\n    )\n    def to_tuples(self, na_tuple=True):\n        tuples = com.asarray_tuplesafe(zip(self._left, self._right))\n        if not na_tuple:\n            # GH 18756\n            tuples = np.where(~self.isna(), tuples, np.nan)\n        return tuples\n\n    # ---------------------------------------------------------------------\n\n    @Appender(_extension_array_shared_docs[\"repeat\"] % _shared_docs_kwargs)\n    def repeat(self, repeats, axis=None):\n        nv.validate_repeat((), {\"axis\": axis})\n        left_repeat = self.left.repeat(repeats)\n        right_repeat = self.right.repeat(repeats)\n        return self._shallow_copy(left=left_repeat, right=right_repeat)\n\n    _interval_shared_docs[\"contains\"] = textwrap.dedent(\n        \"\"\"\n        Check elementwise if the Intervals contain the value.\n\n        Return a boolean mask whether the value is contained in the Intervals\n        of the %(klass)s.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        other : scalar\n            The value to check whether it is contained in the Intervals.\n\n        Returns\n        -------\n        boolean array\n\n        See Also\n        --------\n        Interval.contains : Check whether Interval object contains value.\n        %(klass)s.overlaps : Check if an Interval overlaps the values in the\n            %(klass)s.\n\n        Examples\n        --------\n        %(examples)s\n        >>> intervals.contains(0.5)\n        array([ True, False, False])\n    \"\"\"\n    )\n\n    @Appender(\n        _interval_shared_docs[\"contains\"]\n        % {\n            \"klass\": \"IntervalArray\",\n            \"examples\": textwrap.dedent(\n                \"\"\"\\\n        >>> intervals = pd.arrays.IntervalArray.from_tuples([(0, 1), (1, 3), (2, 4)])\n        >>> intervals\n        <IntervalArray>\n        [(0, 1], (1, 3], (2, 4]]\n        Length: 3, closed: right, dtype: interval[int64]\n        \"\"\"\n            ),\n        }\n    )\n    def contains(self, other):\n        if isinstance(other, Interval):\n            raise NotImplementedError(\"contains not implemented for two intervals\")\n\n        return (self._left < other if self.open_left else self._left <= other) & (\n            other < self._right if self.open_right else other <= self._right\n        )\n\n\ndef maybe_convert_platform_interval(values):\n    \"\"\"\n    Try to do platform conversion, with special casing for IntervalArray.\n    Wrapper around maybe_convert_platform that alters the default return\n    dtype in certain cases to be compatible with IntervalArray.  For example,\n    empty lists return with integer dtype instead of object dtype, which is\n    prohibited for IntervalArray.\n\n    Parameters\n    ----------\n    values : array-like\n\n    Returns\n    -------\n    array\n    \"\"\"\n    if isinstance(values, (list, tuple)) and len(values) == 0:\n        # GH 19016\n        # empty lists/tuples get object dtype by default, but this is\n        # prohibited for IntervalArray, so coerce to integer instead\n        return np.array([], dtype=np.int64)\n    elif is_categorical_dtype(values):\n        values = np.asarray(values)\n\n    return maybe_convert_platform(values)\n"
    },
    {
      "filename": "pandas/core/arrays/numpy_.py",
      "content": "import numbers\nfrom typing import Tuple, Type, Union\n\nimport numpy as np\nfrom numpy.lib.mixins import NDArrayOperatorsMixin\n\nfrom pandas._libs import lib\nfrom pandas._typing import Scalar\nfrom pandas.compat.numpy import function as nv\n\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core import nanops, ops\nfrom pandas.core.arraylike import OpsMixin\nfrom pandas.core.arrays._mixins import NDArrayBackedExtensionArray\nfrom pandas.core.strings.object_array import ObjectStringArrayMixin\n\n\nclass PandasDtype(ExtensionDtype):\n    \"\"\"\n    A Pandas ExtensionDtype for NumPy dtypes.\n\n    .. versionadded:: 0.24.0\n\n    This is mostly for internal compatibility, and is not especially\n    useful on its own.\n\n    Parameters\n    ----------\n    dtype : object\n        Object to be converted to a NumPy data type object.\n\n    See Also\n    --------\n    numpy.dtype\n    \"\"\"\n\n    _metadata = (\"_dtype\",)\n\n    def __init__(self, dtype: object):\n        self._dtype = np.dtype(dtype)\n\n    def __repr__(self) -> str:\n        return f\"PandasDtype({repr(self.name)})\"\n\n    @property\n    def numpy_dtype(self) -> np.dtype:\n        \"\"\"\n        The NumPy dtype this PandasDtype wraps.\n        \"\"\"\n        return self._dtype\n\n    @property\n    def name(self) -> str:\n        \"\"\"\n        A bit-width name for this data-type.\n        \"\"\"\n        return self._dtype.name\n\n    @property\n    def type(self) -> Type[np.generic]:\n        \"\"\"\n        The type object used to instantiate a scalar of this NumPy data-type.\n        \"\"\"\n        return self._dtype.type\n\n    @property\n    def _is_numeric(self) -> bool:\n        # exclude object, str, unicode, void.\n        return self.kind in set(\"biufc\")\n\n    @property\n    def _is_boolean(self) -> bool:\n        return self.kind == \"b\"\n\n    @classmethod\n    def construct_from_string(cls, string: str) -> \"PandasDtype\":\n        try:\n            dtype = np.dtype(string)\n        except TypeError as err:\n            if not isinstance(string, str):\n                msg = f\"'construct_from_string' expects a string, got {type(string)}\"\n            else:\n                msg = f\"Cannot construct a 'PandasDtype' from '{string}'\"\n            raise TypeError(msg) from err\n        return cls(dtype)\n\n    @classmethod\n    def construct_array_type(cls) -> Type[\"PandasArray\"]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        return PandasArray\n\n    @property\n    def kind(self) -> str:\n        \"\"\"\n        A character code (one of 'biufcmMOSUV') identifying the general kind of data.\n        \"\"\"\n        return self._dtype.kind\n\n    @property\n    def itemsize(self) -> int:\n        \"\"\"\n        The element size of this data-type object.\n        \"\"\"\n        return self._dtype.itemsize\n\n\nclass PandasArray(\n    OpsMixin,\n    NDArrayBackedExtensionArray,\n    NDArrayOperatorsMixin,\n    ObjectStringArrayMixin,\n):\n    \"\"\"\n    A pandas ExtensionArray for NumPy data.\n\n    .. versionadded:: 0.24.0\n\n    This is mostly for internal compatibility, and is not especially\n    useful on its own.\n\n    Parameters\n    ----------\n    values : ndarray\n        The NumPy ndarray to wrap. Must be 1-dimensional.\n    copy : bool, default False\n        Whether to copy `values`.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n    \"\"\"\n\n    # If you're wondering why pd.Series(cls) doesn't put the array in an\n    # ExtensionBlock, search for `ABCPandasArray`. We check for\n    # that _typ to ensure that users don't unnecessarily use EAs inside\n    # pandas internals, which turns off things like block consolidation.\n    _typ = \"npy_extension\"\n    __array_priority__ = 1000\n    _ndarray: np.ndarray\n\n    # ------------------------------------------------------------------------\n    # Constructors\n\n    def __init__(self, values: Union[np.ndarray, \"PandasArray\"], copy: bool = False):\n        if isinstance(values, type(self)):\n            values = values._ndarray\n        if not isinstance(values, np.ndarray):\n            raise ValueError(\n                f\"'values' must be a NumPy array, not {type(values).__name__}\"\n            )\n\n        if values.ndim != 1:\n            raise ValueError(\"PandasArray must be 1-dimensional.\")\n\n        if copy:\n            values = values.copy()\n\n        self._ndarray = values\n        self._dtype = PandasDtype(values.dtype)\n\n    @classmethod\n    def _from_sequence(\n        cls, scalars, *, dtype=None, copy: bool = False\n    ) -> \"PandasArray\":\n        if isinstance(dtype, PandasDtype):\n            dtype = dtype._dtype\n\n        result = np.asarray(scalars, dtype=dtype)\n        if copy and result is scalars:\n            result = result.copy()\n        return cls(result)\n\n    @classmethod\n    def _from_factorized(cls, values, original) -> \"PandasArray\":\n        return cls(values)\n\n    def _from_backing_data(self, arr: np.ndarray) -> \"PandasArray\":\n        return type(self)(arr)\n\n    # ------------------------------------------------------------------------\n    # Data\n\n    @property\n    def dtype(self) -> PandasDtype:\n        return self._dtype\n\n    # ------------------------------------------------------------------------\n    # NumPy Array Interface\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        return np.asarray(self._ndarray, dtype=dtype)\n\n    _HANDLED_TYPES = (np.ndarray, numbers.Number)\n\n    def __array_ufunc__(self, ufunc, method: str, *inputs, **kwargs):\n        # Lightly modified version of\n        # https://numpy.org/doc/stable/reference/generated/numpy.lib.mixins.NDArrayOperatorsMixin.html\n        # The primary modification is not boxing scalar return values\n        # in PandasArray, since pandas' ExtensionArrays are 1-d.\n        out = kwargs.get(\"out\", ())\n        for x in inputs + out:\n            # Only support operations with instances of _HANDLED_TYPES.\n            # Use PandasArray instead of type(self) for isinstance to\n            # allow subclasses that don't override __array_ufunc__ to\n            # handle PandasArray objects.\n            if not isinstance(x, self._HANDLED_TYPES + (PandasArray,)):\n                return NotImplemented\n\n        if ufunc not in [np.logical_or, np.bitwise_or, np.bitwise_xor]:\n            # For binary ops, use our custom dunder methods\n            # We haven't implemented logical dunder funcs, so exclude these\n            #  to avoid RecursionError\n            result = ops.maybe_dispatch_ufunc_to_dunder_op(\n                self, ufunc, method, *inputs, **kwargs\n            )\n            if result is not NotImplemented:\n                return result\n\n        # Defer to the implementation of the ufunc on unwrapped values.\n        inputs = tuple(x._ndarray if isinstance(x, PandasArray) else x for x in inputs)\n        if out:\n            kwargs[\"out\"] = tuple(\n                x._ndarray if isinstance(x, PandasArray) else x for x in out\n            )\n        result = getattr(ufunc, method)(*inputs, **kwargs)\n\n        if type(result) is tuple and len(result):\n            # multiple return values\n            if not lib.is_scalar(result[0]):\n                # re-box array-like results\n                return tuple(type(self)(x) for x in result)\n            else:\n                # but not scalar reductions\n                return result\n        elif method == \"at\":\n            # no return value\n            return None\n        else:\n            # one return value\n            if not lib.is_scalar(result):\n                # re-box array-like results, but not scalar reductions\n                result = type(self)(result)\n            return result\n\n    # ------------------------------------------------------------------------\n    # Pandas ExtensionArray Interface\n\n    def isna(self) -> np.ndarray:\n        return isna(self._ndarray)\n\n    def _validate_fill_value(self, fill_value):\n        if fill_value is None:\n            # Primarily for subclasses\n            fill_value = self.dtype.na_value\n        return fill_value\n\n    def _values_for_factorize(self) -> Tuple[np.ndarray, int]:\n        return self._ndarray, -1\n\n    # ------------------------------------------------------------------------\n    # Reductions\n\n    def any(self, *, axis=None, out=None, keepdims=False, skipna=True):\n        nv.validate_any((), {\"out\": out, \"keepdims\": keepdims})\n        result = nanops.nanany(self._ndarray, axis=axis, skipna=skipna)\n        return self._wrap_reduction_result(axis, result)\n\n    def all(self, *, axis=None, out=None, keepdims=False, skipna=True):\n        nv.validate_all((), {\"out\": out, \"keepdims\": keepdims})\n        result = nanops.nanall(self._ndarray, axis=axis, skipna=skipna)\n        return self._wrap_reduction_result(axis, result)\n\n    def min(self, *, axis=None, skipna: bool = True, **kwargs) -> Scalar:\n        nv.validate_min((), kwargs)\n        result = nanops.nanmin(\n            values=self._ndarray, axis=axis, mask=self.isna(), skipna=skipna\n        )\n        return self._wrap_reduction_result(axis, result)\n\n    def max(self, *, axis=None, skipna: bool = True, **kwargs) -> Scalar:\n        nv.validate_max((), kwargs)\n        result = nanops.nanmax(\n            values=self._ndarray, axis=axis, mask=self.isna(), skipna=skipna\n        )\n        return self._wrap_reduction_result(axis, result)\n\n    def sum(self, *, axis=None, skipna=True, min_count=0, **kwargs) -> Scalar:\n        nv.validate_sum((), kwargs)\n        result = nanops.nansum(\n            self._ndarray, axis=axis, skipna=skipna, min_count=min_count\n        )\n        return self._wrap_reduction_result(axis, result)\n\n    def prod(self, *, axis=None, skipna=True, min_count=0, **kwargs) -> Scalar:\n        nv.validate_prod((), kwargs)\n        result = nanops.nanprod(\n            self._ndarray, axis=axis, skipna=skipna, min_count=min_count\n        )\n        return self._wrap_reduction_result(axis, result)\n\n    def mean(self, *, axis=None, dtype=None, out=None, keepdims=False, skipna=True):\n        nv.validate_mean((), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims})\n        result = nanops.nanmean(self._ndarray, axis=axis, skipna=skipna)\n        return self._wrap_reduction_result(axis, result)\n\n    def median(\n        self, *, axis=None, out=None, overwrite_input=False, keepdims=False, skipna=True\n    ):\n        nv.validate_median(\n            (), {\"out\": out, \"overwrite_input\": overwrite_input, \"keepdims\": keepdims}\n        )\n        result = nanops.nanmedian(self._ndarray, axis=axis, skipna=skipna)\n        return self._wrap_reduction_result(axis, result)\n\n    def std(\n        self, *, axis=None, dtype=None, out=None, ddof=1, keepdims=False, skipna=True\n    ):\n        nv.validate_stat_ddof_func(\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"std\"\n        )\n        result = nanops.nanstd(self._ndarray, axis=axis, skipna=skipna, ddof=ddof)\n        return self._wrap_reduction_result(axis, result)\n\n    def var(\n        self, *, axis=None, dtype=None, out=None, ddof=1, keepdims=False, skipna=True\n    ):\n        nv.validate_stat_ddof_func(\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"var\"\n        )\n        result = nanops.nanvar(self._ndarray, axis=axis, skipna=skipna, ddof=ddof)\n        return self._wrap_reduction_result(axis, result)\n\n    def sem(\n        self, *, axis=None, dtype=None, out=None, ddof=1, keepdims=False, skipna=True\n    ):\n        nv.validate_stat_ddof_func(\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"sem\"\n        )\n        result = nanops.nansem(self._ndarray, axis=axis, skipna=skipna, ddof=ddof)\n        return self._wrap_reduction_result(axis, result)\n\n    def kurt(self, *, axis=None, dtype=None, out=None, keepdims=False, skipna=True):\n        nv.validate_stat_ddof_func(\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"kurt\"\n        )\n        result = nanops.nankurt(self._ndarray, axis=axis, skipna=skipna)\n        return self._wrap_reduction_result(axis, result)\n\n    def skew(self, *, axis=None, dtype=None, out=None, keepdims=False, skipna=True):\n        nv.validate_stat_ddof_func(\n            (), {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}, fname=\"skew\"\n        )\n        result = nanops.nanskew(self._ndarray, axis=axis, skipna=skipna)\n        return self._wrap_reduction_result(axis, result)\n\n    # ------------------------------------------------------------------------\n    # Additional Methods\n\n    def to_numpy(\n        self, dtype=None, copy: bool = False, na_value=lib.no_default\n    ) -> np.ndarray:\n        result = np.asarray(self._ndarray, dtype=dtype)\n\n        if (copy or na_value is not lib.no_default) and result is self._ndarray:\n            result = result.copy()\n\n        if na_value is not lib.no_default:\n            result[self.isna()] = na_value\n\n        return result\n\n    # ------------------------------------------------------------------------\n    # Ops\n\n    def __invert__(self):\n        return type(self)(~self._ndarray)\n\n    def _cmp_method(self, other, op):\n        if isinstance(other, PandasArray):\n            other = other._ndarray\n\n        pd_op = ops.get_array_op(op)\n        result = pd_op(self._ndarray, other)\n\n        if op is divmod or op is ops.rdivmod:\n            a, b = result\n            if isinstance(a, np.ndarray):\n                # for e.g. op vs TimedeltaArray, we may already\n                #  have an ExtensionArray, in which case we do not wrap\n                return self._wrap_ndarray_result(a), self._wrap_ndarray_result(b)\n            return a, b\n\n        if isinstance(result, np.ndarray):\n            # for e.g. multiplication vs TimedeltaArray, we may already\n            #  have an ExtensionArray, in which case we do not wrap\n            return self._wrap_ndarray_result(result)\n        return result\n\n    _arith_method = _cmp_method\n\n    def _wrap_ndarray_result(self, result: np.ndarray):\n        # If we have timedelta64[ns] result, return a TimedeltaArray instead\n        #  of a PandasArray\n        if result.dtype == \"timedelta64[ns]\":\n            from pandas.core.arrays import TimedeltaArray\n\n            return TimedeltaArray._simple_new(result)\n        return type(self)(result)\n\n    # ------------------------------------------------------------------------\n    # String methods interface\n    _str_na_value = np.nan\n"
    },
    {
      "filename": "pandas/core/computation/expressions.py",
      "content": "\"\"\"\nExpressions\n-----------\n\nOffer fast expression evaluation through numexpr\n\n\"\"\"\nimport operator\nfrom typing import List, Set\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas.core.dtypes.generic import ABCDataFrame\n\nfrom pandas.core.computation.check import NUMEXPR_INSTALLED\nfrom pandas.core.ops import roperator\n\nif NUMEXPR_INSTALLED:\n    import numexpr as ne\n\n_TEST_MODE = None\n_TEST_RESULT: List[bool] = []\nUSE_NUMEXPR = NUMEXPR_INSTALLED\n_evaluate = None\n_where = None\n\n# the set of dtypes that we will allow pass to numexpr\n_ALLOWED_DTYPES = {\n    \"evaluate\": {\"int64\", \"int32\", \"float64\", \"float32\", \"bool\"},\n    \"where\": {\"int64\", \"float64\", \"bool\"},\n}\n\n# the minimum prod shape that we will use numexpr\n_MIN_ELEMENTS = 10000\n\n\ndef set_use_numexpr(v=True):\n    # set/unset to use numexpr\n    global USE_NUMEXPR\n    if NUMEXPR_INSTALLED:\n        USE_NUMEXPR = v\n\n    # choose what we are going to do\n    global _evaluate, _where\n\n    _evaluate = _evaluate_numexpr if USE_NUMEXPR else _evaluate_standard\n    _where = _where_numexpr if USE_NUMEXPR else _where_standard\n\n\ndef set_numexpr_threads(n=None):\n    # if we are using numexpr, set the threads to n\n    # otherwise reset\n    if NUMEXPR_INSTALLED and USE_NUMEXPR:\n        if n is None:\n            n = ne.detect_number_of_cores()\n        ne.set_num_threads(n)\n\n\ndef _evaluate_standard(op, op_str, a, b):\n    \"\"\"\n    Standard evaluation.\n    \"\"\"\n    if _TEST_MODE:\n        _store_test_result(False)\n    with np.errstate(all=\"ignore\"):\n        return op(a, b)\n\n\ndef _can_use_numexpr(op, op_str, a, b, dtype_check):\n    \"\"\" return a boolean if we WILL be using numexpr \"\"\"\n    if op_str is not None:\n\n        # required min elements (otherwise we are adding overhead)\n        if np.prod(a.shape) > _MIN_ELEMENTS:\n            # check for dtype compatibility\n            dtypes: Set[str] = set()\n            for o in [a, b]:\n                # Series implements dtypes, check for dimension count as well\n                if hasattr(o, \"dtypes\") and o.ndim > 1:\n                    s = o.dtypes.value_counts()\n                    if len(s) > 1:\n                        return False\n                    dtypes |= set(s.index.astype(str))\n                # ndarray and Series Case\n                elif hasattr(o, \"dtype\"):\n                    dtypes |= {o.dtype.name}\n\n            # allowed are a superset\n            if not len(dtypes) or _ALLOWED_DTYPES[dtype_check] >= dtypes:\n                return True\n\n    return False\n\n\ndef _evaluate_numexpr(op, op_str, a, b):\n    result = None\n\n    if _can_use_numexpr(op, op_str, a, b, \"evaluate\"):\n        is_reversed = op.__name__.strip(\"_\").startswith(\"r\")\n        if is_reversed:\n            # we were originally called by a reversed op method\n            a, b = b, a\n\n        a_value = a\n        b_value = b\n\n        result = ne.evaluate(\n            f\"a_value {op_str} b_value\",\n            local_dict={\"a_value\": a_value, \"b_value\": b_value},\n            casting=\"safe\",\n        )\n\n    if _TEST_MODE:\n        _store_test_result(result is not None)\n\n    if result is None:\n        result = _evaluate_standard(op, op_str, a, b)\n\n    return result\n\n\n_op_str_mapping = {\n    operator.add: \"+\",\n    roperator.radd: \"+\",\n    operator.mul: \"*\",\n    roperator.rmul: \"*\",\n    operator.sub: \"-\",\n    roperator.rsub: \"-\",\n    operator.truediv: \"/\",\n    roperator.rtruediv: \"/\",\n    operator.floordiv: \"//\",\n    roperator.rfloordiv: \"//\",\n    # we require Python semantics for mod of negative for backwards compatibility\n    # see https://github.com/pydata/numexpr/issues/365\n    # so sticking with unaccelerated for now\n    operator.mod: None,\n    roperator.rmod: \"%\",\n    operator.pow: \"**\",\n    roperator.rpow: \"**\",\n    operator.eq: \"==\",\n    operator.ne: \"!=\",\n    operator.le: \"<=\",\n    operator.lt: \"<\",\n    operator.ge: \">=\",\n    operator.gt: \">\",\n    operator.and_: \"&\",\n    roperator.rand_: \"&\",\n    operator.or_: \"|\",\n    roperator.ror_: \"|\",\n    operator.xor: \"^\",\n    roperator.rxor: \"^\",\n    divmod: None,\n    roperator.rdivmod: None,\n}\n\n\ndef _where_standard(cond, a, b):\n    # Caller is responsible for extracting ndarray if necessary\n    return np.where(cond, a, b)\n\n\ndef _where_numexpr(cond, a, b):\n    # Caller is responsible for extracting ndarray if necessary\n    result = None\n\n    if _can_use_numexpr(None, \"where\", a, b, \"where\"):\n\n        result = ne.evaluate(\n            \"where(cond_value, a_value, b_value)\",\n            local_dict={\"cond_value\": cond, \"a_value\": a, \"b_value\": b},\n            casting=\"safe\",\n        )\n\n    if result is None:\n        result = _where_standard(cond, a, b)\n\n    return result\n\n\n# turn myself on\nset_use_numexpr(get_option(\"compute.use_numexpr\"))\n\n\ndef _has_bool_dtype(x):\n    if isinstance(x, ABCDataFrame):\n        return \"bool\" in x.dtypes\n    try:\n        return x.dtype == bool\n    except AttributeError:\n        return isinstance(x, (bool, np.bool_))\n\n\ndef _bool_arith_check(\n    op_str, a, b, not_allowed=frozenset((\"/\", \"//\", \"**\")), unsupported=None\n):\n    if unsupported is None:\n        unsupported = {\"+\": \"|\", \"*\": \"&\", \"-\": \"^\"}\n\n    if _has_bool_dtype(a) and _has_bool_dtype(b):\n        if op_str in unsupported:\n            warnings.warn(\n                f\"evaluating in Python space because the {repr(op_str)} \"\n                \"operator is not supported by numexpr for \"\n                f\"the bool dtype, use {repr(unsupported[op_str])} instead\"\n            )\n            return False\n\n        if op_str in not_allowed:\n            raise NotImplementedError(\n                f\"operator {repr(op_str)} not implemented for bool dtypes\"\n            )\n    return True\n\n\ndef evaluate(op, a, b, use_numexpr: bool = True):\n    \"\"\"\n    Evaluate and return the expression of the op on a and b.\n\n    Parameters\n    ----------\n    op : the actual operand\n    a : left operand\n    b : right operand\n    use_numexpr : bool, default True\n        Whether to try to use numexpr.\n    \"\"\"\n    op_str = _op_str_mapping[op]\n    if op_str is not None:\n        use_numexpr = use_numexpr and _bool_arith_check(op_str, a, b)\n        if use_numexpr:\n            # error: \"None\" not callable\n            return _evaluate(op, op_str, a, b)  # type: ignore[misc]\n    return _evaluate_standard(op, op_str, a, b)\n\n\ndef where(cond, a, b, use_numexpr=True):\n    \"\"\"\n    Evaluate the where condition cond on a and b.\n\n    Parameters\n    ----------\n    cond : np.ndarray[bool]\n    a : return if cond is True\n    b : return if cond is False\n    use_numexpr : bool, default True\n        Whether to try to use numexpr.\n    \"\"\"\n    assert _where is not None\n    return _where(cond, a, b) if use_numexpr else _where_standard(cond, a, b)\n\n\ndef set_test_mode(v: bool = True) -> None:\n    \"\"\"\n    Keeps track of whether numexpr was used.\n\n    Stores an additional ``True`` for every successful use of evaluate with\n    numexpr since the last ``get_test_result``.\n    \"\"\"\n    global _TEST_MODE, _TEST_RESULT\n    _TEST_MODE = v\n    _TEST_RESULT = []\n\n\ndef _store_test_result(used_numexpr: bool) -> None:\n    global _TEST_RESULT\n    if used_numexpr:\n        _TEST_RESULT.append(used_numexpr)\n\n\ndef get_test_result() -> List[bool]:\n    \"\"\"\n    Get test result and reset test_results.\n    \"\"\"\n    global _TEST_RESULT\n    res = _TEST_RESULT\n    _TEST_RESULT = []\n    return res\n"
    },
    {
      "filename": "pandas/core/computation/pytables.py",
      "content": "\"\"\" manage PyTables query interface via Expressions \"\"\"\n\nimport ast\nfrom functools import partial\nfrom typing import Any, Dict, Optional, Tuple\n\nimport numpy as np\n\nfrom pandas._libs.tslibs import Timedelta, Timestamp\nfrom pandas.compat.chainmap import DeepChainMap\n\nfrom pandas.core.dtypes.common import is_list_like\n\nimport pandas as pd\nimport pandas.core.common as com\nfrom pandas.core.computation import expr, ops, scope as _scope\nfrom pandas.core.computation.common import ensure_decoded\nfrom pandas.core.computation.expr import BaseExprVisitor\nfrom pandas.core.computation.ops import UndefinedVariableError, is_term\nfrom pandas.core.construction import extract_array\n\nfrom pandas.io.formats.printing import pprint_thing, pprint_thing_encoded\n\n\nclass PyTablesScope(_scope.Scope):\n    __slots__ = (\"queryables\",)\n\n    queryables: Dict[str, Any]\n\n    def __init__(\n        self,\n        level: int,\n        global_dict=None,\n        local_dict=None,\n        queryables: Optional[Dict[str, Any]] = None,\n    ):\n        super().__init__(level + 1, global_dict=global_dict, local_dict=local_dict)\n        self.queryables = queryables or {}\n\n\nclass Term(ops.Term):\n    env: PyTablesScope\n\n    def __new__(cls, name, env, side=None, encoding=None):\n        if isinstance(name, str):\n            klass = cls\n        else:\n            klass = Constant\n        return object.__new__(klass)\n\n    def __init__(self, name, env: PyTablesScope, side=None, encoding=None):\n        super().__init__(name, env, side=side, encoding=encoding)\n\n    def _resolve_name(self):\n        # must be a queryables\n        if self.side == \"left\":\n            # Note: The behavior of __new__ ensures that self.name is a str here\n            if self.name not in self.env.queryables:\n                raise NameError(f\"name {repr(self.name)} is not defined\")\n            return self.name\n\n        # resolve the rhs (and allow it to be None)\n        try:\n            return self.env.resolve(self.name, is_local=False)\n        except UndefinedVariableError:\n            return self.name\n\n    # read-only property overwriting read/write property\n    @property  # type: ignore[misc]\n    def value(self):\n        return self._value\n\n\nclass Constant(Term):\n    def __init__(self, value, env: PyTablesScope, side=None, encoding=None):\n        assert isinstance(env, PyTablesScope), type(env)\n        super().__init__(value, env, side=side, encoding=encoding)\n\n    def _resolve_name(self):\n        return self._name\n\n\nclass BinOp(ops.BinOp):\n\n    _max_selectors = 31\n\n    op: str\n    queryables: Dict[str, Any]\n    condition: Optional[str]\n\n    def __init__(self, op: str, lhs, rhs, queryables: Dict[str, Any], encoding):\n        super().__init__(op, lhs, rhs)\n        self.queryables = queryables\n        self.encoding = encoding\n        self.condition = None\n\n    def _disallow_scalar_only_bool_ops(self):\n        pass\n\n    def prune(self, klass):\n        def pr(left, right):\n            \"\"\" create and return a new specialized BinOp from myself \"\"\"\n            if left is None:\n                return right\n            elif right is None:\n                return left\n\n            k = klass\n            if isinstance(left, ConditionBinOp):\n                if isinstance(right, ConditionBinOp):\n                    k = JointConditionBinOp\n                elif isinstance(left, k):\n                    return left\n                elif isinstance(right, k):\n                    return right\n\n            elif isinstance(left, FilterBinOp):\n                if isinstance(right, FilterBinOp):\n                    k = JointFilterBinOp\n                elif isinstance(left, k):\n                    return left\n                elif isinstance(right, k):\n                    return right\n\n            return k(\n                self.op, left, right, queryables=self.queryables, encoding=self.encoding\n            ).evaluate()\n\n        left, right = self.lhs, self.rhs\n\n        if is_term(left) and is_term(right):\n            res = pr(left.value, right.value)\n        elif not is_term(left) and is_term(right):\n            res = pr(left.prune(klass), right.value)\n        elif is_term(left) and not is_term(right):\n            res = pr(left.value, right.prune(klass))\n        elif not (is_term(left) or is_term(right)):\n            res = pr(left.prune(klass), right.prune(klass))\n\n        return res\n\n    def conform(self, rhs):\n        \"\"\" inplace conform rhs \"\"\"\n        if not is_list_like(rhs):\n            rhs = [rhs]\n        if isinstance(rhs, np.ndarray):\n            rhs = rhs.ravel()\n        return rhs\n\n    @property\n    def is_valid(self) -> bool:\n        \"\"\" return True if this is a valid field \"\"\"\n        return self.lhs in self.queryables\n\n    @property\n    def is_in_table(self) -> bool:\n        \"\"\"\n        return True if this is a valid column name for generation (e.g. an\n        actual column in the table)\n        \"\"\"\n        return self.queryables.get(self.lhs) is not None\n\n    @property\n    def kind(self):\n        \"\"\" the kind of my field \"\"\"\n        return getattr(self.queryables.get(self.lhs), \"kind\", None)\n\n    @property\n    def meta(self):\n        \"\"\" the meta of my field \"\"\"\n        return getattr(self.queryables.get(self.lhs), \"meta\", None)\n\n    @property\n    def metadata(self):\n        \"\"\" the metadata of my field \"\"\"\n        return getattr(self.queryables.get(self.lhs), \"metadata\", None)\n\n    def generate(self, v) -> str:\n        \"\"\" create and return the op string for this TermValue \"\"\"\n        val = v.tostring(self.encoding)\n        return f\"({self.lhs} {self.op} {val})\"\n\n    def convert_value(self, v) -> \"TermValue\":\n        \"\"\"\n        convert the expression that is in the term to something that is\n        accepted by pytables\n        \"\"\"\n\n        def stringify(value):\n            if self.encoding is not None:\n                return pprint_thing_encoded(value, encoding=self.encoding)\n            return pprint_thing(value)\n\n        kind = ensure_decoded(self.kind)\n        meta = ensure_decoded(self.meta)\n        if kind == \"datetime64\" or kind == \"datetime\":\n            if isinstance(v, (int, float)):\n                v = stringify(v)\n            v = ensure_decoded(v)\n            v = Timestamp(v)\n            if v.tz is not None:\n                v = v.tz_convert(\"UTC\")\n            return TermValue(v, v.value, kind)\n        elif kind == \"timedelta64\" or kind == \"timedelta\":\n            if isinstance(v, str):\n                v = Timedelta(v).value\n            else:\n                v = Timedelta(v, unit=\"s\").value\n            return TermValue(int(v), v, kind)\n        elif meta == \"category\":\n            metadata = extract_array(self.metadata, extract_numpy=True)\n            result = metadata.searchsorted(v, side=\"left\")\n\n            # result returns 0 if v is first element or if v is not in metadata\n            # check that metadata contains v\n            if not result and v not in metadata:\n                result = -1\n            return TermValue(result, result, \"integer\")\n        elif kind == \"integer\":\n            v = int(float(v))\n            return TermValue(v, v, kind)\n        elif kind == \"float\":\n            v = float(v)\n            return TermValue(v, v, kind)\n        elif kind == \"bool\":\n            if isinstance(v, str):\n                v = not v.strip().lower() in [\n                    \"false\",\n                    \"f\",\n                    \"no\",\n                    \"n\",\n                    \"none\",\n                    \"0\",\n                    \"[]\",\n                    \"{}\",\n                    \"\",\n                ]\n            else:\n                v = bool(v)\n            return TermValue(v, v, kind)\n        elif isinstance(v, str):\n            # string quoting\n            return TermValue(v, stringify(v), \"string\")\n        else:\n            raise TypeError(f\"Cannot compare {v} of type {type(v)} to {kind} column\")\n\n    def convert_values(self):\n        pass\n\n\nclass FilterBinOp(BinOp):\n    filter: Optional[Tuple[Any, Any, pd.Index]] = None\n\n    def __repr__(self) -> str:\n        if self.filter is None:\n            return \"Filter: Not Initialized\"\n        return pprint_thing(f\"[Filter : [{self.filter[0]}] -> [{self.filter[1]}]\")\n\n    def invert(self):\n        \"\"\" invert the filter \"\"\"\n        if self.filter is not None:\n            self.filter = (\n                self.filter[0],\n                self.generate_filter_op(invert=True),\n                self.filter[2],\n            )\n        return self\n\n    def format(self):\n        \"\"\" return the actual filter format \"\"\"\n        return [self.filter]\n\n    def evaluate(self):\n\n        if not self.is_valid:\n            raise ValueError(f\"query term is not valid [{self}]\")\n\n        rhs = self.conform(self.rhs)\n        values = list(rhs)\n\n        if self.is_in_table:\n\n            # if too many values to create the expression, use a filter instead\n            if self.op in [\"==\", \"!=\"] and len(values) > self._max_selectors:\n\n                filter_op = self.generate_filter_op()\n                self.filter = (self.lhs, filter_op, pd.Index(values))\n\n                return self\n            return None\n\n        # equality conditions\n        if self.op in [\"==\", \"!=\"]:\n\n            filter_op = self.generate_filter_op()\n            self.filter = (self.lhs, filter_op, pd.Index(values))\n\n        else:\n            raise TypeError(\n                f\"passing a filterable condition to a non-table indexer [{self}]\"\n            )\n\n        return self\n\n    def generate_filter_op(self, invert: bool = False):\n        if (self.op == \"!=\" and not invert) or (self.op == \"==\" and invert):\n            return lambda axis, vals: ~axis.isin(vals)\n        else:\n            return lambda axis, vals: axis.isin(vals)\n\n\nclass JointFilterBinOp(FilterBinOp):\n    def format(self):\n        raise NotImplementedError(\"unable to collapse Joint Filters\")\n\n    def evaluate(self):\n        return self\n\n\nclass ConditionBinOp(BinOp):\n    def __repr__(self) -> str:\n        return pprint_thing(f\"[Condition : [{self.condition}]]\")\n\n    def invert(self):\n        \"\"\" invert the condition \"\"\"\n        # if self.condition is not None:\n        #    self.condition = \"~(%s)\" % self.condition\n        # return self\n        raise NotImplementedError(\n            \"cannot use an invert condition when passing to numexpr\"\n        )\n\n    def format(self):\n        \"\"\" return the actual ne format \"\"\"\n        return self.condition\n\n    def evaluate(self):\n\n        if not self.is_valid:\n            raise ValueError(f\"query term is not valid [{self}]\")\n\n        # convert values if we are in the table\n        if not self.is_in_table:\n            return None\n\n        rhs = self.conform(self.rhs)\n        values = [self.convert_value(v) for v in rhs]\n\n        # equality conditions\n        if self.op in [\"==\", \"!=\"]:\n\n            # too many values to create the expression?\n            if len(values) <= self._max_selectors:\n                vs = [self.generate(v) for v in values]\n                self.condition = f\"({' | '.join(vs)})\"\n\n            # use a filter after reading\n            else:\n                return None\n        else:\n            self.condition = self.generate(values[0])\n\n        return self\n\n\nclass JointConditionBinOp(ConditionBinOp):\n    def evaluate(self):\n        self.condition = f\"({self.lhs.condition} {self.op} {self.rhs.condition})\"\n        return self\n\n\nclass UnaryOp(ops.UnaryOp):\n    def prune(self, klass):\n\n        if self.op != \"~\":\n            raise NotImplementedError(\"UnaryOp only support invert type ops\")\n\n        operand = self.operand\n        operand = operand.prune(klass)\n\n        if operand is not None and (\n            issubclass(klass, ConditionBinOp)\n            and operand.condition is not None\n            or not issubclass(klass, ConditionBinOp)\n            and issubclass(klass, FilterBinOp)\n            and operand.filter is not None\n        ):\n            return operand.invert()\n        return None\n\n\nclass PyTablesExprVisitor(BaseExprVisitor):\n    const_type = Constant\n    term_type = Term\n\n    def __init__(self, env, engine, parser, **kwargs):\n        super().__init__(env, engine, parser)\n        for bin_op in self.binary_ops:\n            bin_node = self.binary_op_nodes_map[bin_op]\n            setattr(\n                self,\n                f\"visit_{bin_node}\",\n                lambda node, bin_op=bin_op: partial(BinOp, bin_op, **kwargs),\n            )\n\n    def visit_UnaryOp(self, node, **kwargs):\n        if isinstance(node.op, (ast.Not, ast.Invert)):\n            return UnaryOp(\"~\", self.visit(node.operand))\n        elif isinstance(node.op, ast.USub):\n            return self.const_type(-self.visit(node.operand).value, self.env)\n        elif isinstance(node.op, ast.UAdd):\n            raise NotImplementedError(\"Unary addition not supported\")\n\n    def visit_Index(self, node, **kwargs):\n        return self.visit(node.value).value\n\n    def visit_Assign(self, node, **kwargs):\n        cmpr = ast.Compare(\n            ops=[ast.Eq()], left=node.targets[0], comparators=[node.value]\n        )\n        return self.visit(cmpr)\n\n    def visit_Subscript(self, node, **kwargs):\n        # only allow simple subscripts\n\n        value = self.visit(node.value)\n        slobj = self.visit(node.slice)\n        try:\n            value = value.value\n        except AttributeError:\n            pass\n\n        if isinstance(slobj, Term):\n            # In py39 np.ndarray lookups with Term containing int raise\n            slobj = slobj.value\n\n        try:\n            return self.const_type(value[slobj], self.env)\n        except TypeError as err:\n            raise ValueError(\n                f\"cannot subscript {repr(value)} with {repr(slobj)}\"\n            ) from err\n\n    def visit_Attribute(self, node, **kwargs):\n        attr = node.attr\n        value = node.value\n\n        ctx = type(node.ctx)\n        if ctx == ast.Load:\n            # resolve the value\n            resolved = self.visit(value)\n\n            # try to get the value to see if we are another expression\n            try:\n                resolved = resolved.value\n            except (AttributeError):\n                pass\n\n            try:\n                return self.term_type(getattr(resolved, attr), self.env)\n            except AttributeError:\n\n                # something like datetime.datetime where scope is overridden\n                if isinstance(value, ast.Name) and value.id == attr:\n                    return resolved\n\n        raise ValueError(f\"Invalid Attribute context {ctx.__name__}\")\n\n    def translate_In(self, op):\n        return ast.Eq() if isinstance(op, ast.In) else op\n\n    def _rewrite_membership_op(self, node, left, right):\n        return self.visit(node.op), node.op, left, right\n\n\ndef _validate_where(w):\n    \"\"\"\n    Validate that the where statement is of the right type.\n\n    The type may either be String, Expr, or list-like of Exprs.\n\n    Parameters\n    ----------\n    w : String term expression, Expr, or list-like of Exprs.\n\n    Returns\n    -------\n    where : The original where clause if the check was successful.\n\n    Raises\n    ------\n    TypeError : An invalid data type was passed in for w (e.g. dict).\n    \"\"\"\n    if not (isinstance(w, (PyTablesExpr, str)) or is_list_like(w)):\n        raise TypeError(\n            \"where must be passed as a string, PyTablesExpr, \"\n            \"or list-like of PyTablesExpr\"\n        )\n\n    return w\n\n\nclass PyTablesExpr(expr.Expr):\n    \"\"\"\n    Hold a pytables-like expression, comprised of possibly multiple 'terms'.\n\n    Parameters\n    ----------\n    where : string term expression, PyTablesExpr, or list-like of PyTablesExprs\n    queryables : a \"kinds\" map (dict of column name -> kind), or None if column\n        is non-indexable\n    encoding : an encoding that will encode the query terms\n\n    Returns\n    -------\n    a PyTablesExpr object\n\n    Examples\n    --------\n    'index>=date'\n    \"columns=['A', 'D']\"\n    'columns=A'\n    'columns==A'\n    \"~(columns=['A','B'])\"\n    'index>df.index[3] & string=\"bar\"'\n    '(index>df.index[3] & index<=df.index[6]) | string=\"bar\"'\n    \"ts>=Timestamp('2012-02-01')\"\n    \"major_axis>=20130101\"\n    \"\"\"\n\n    _visitor: Optional[PyTablesExprVisitor]\n    env: PyTablesScope\n\n    def __init__(\n        self,\n        where,\n        queryables: Optional[Dict[str, Any]] = None,\n        encoding=None,\n        scope_level: int = 0,\n    ):\n\n        where = _validate_where(where)\n\n        self.encoding = encoding\n        self.condition = None\n        self.filter = None\n        self.terms = None\n        self._visitor = None\n\n        # capture the environment if needed\n        local_dict: DeepChainMap[Any, Any] = DeepChainMap()\n\n        if isinstance(where, PyTablesExpr):\n            local_dict = where.env.scope\n            _where = where.expr\n\n        elif isinstance(where, (list, tuple)):\n            where = list(where)\n            for idx, w in enumerate(where):\n                if isinstance(w, PyTablesExpr):\n                    local_dict = w.env.scope\n                else:\n                    w = _validate_where(w)\n                    where[idx] = w\n            _where = \" & \".join(f\"({w})\" for w in com.flatten(where))\n        else:\n            _where = where\n\n        self.expr = _where\n        self.env = PyTablesScope(scope_level + 1, local_dict=local_dict)\n\n        if queryables is not None and isinstance(self.expr, str):\n            self.env.queryables.update(queryables)\n            self._visitor = PyTablesExprVisitor(\n                self.env,\n                queryables=queryables,\n                parser=\"pytables\",\n                engine=\"pytables\",\n                encoding=encoding,\n            )\n            self.terms = self.parse()\n\n    def __repr__(self) -> str:\n        if self.terms is not None:\n            return pprint_thing(self.terms)\n        return pprint_thing(self.expr)\n\n    def evaluate(self):\n        \"\"\" create and return the numexpr condition and filter \"\"\"\n        try:\n            self.condition = self.terms.prune(ConditionBinOp)\n        except AttributeError as err:\n            raise ValueError(\n                f\"cannot process expression [{self.expr}], [{self}] \"\n                \"is not a valid condition\"\n            ) from err\n        try:\n            self.filter = self.terms.prune(FilterBinOp)\n        except AttributeError as err:\n            raise ValueError(\n                f\"cannot process expression [{self.expr}], [{self}] \"\n                \"is not a valid filter\"\n            ) from err\n\n        return self.condition, self.filter\n\n\nclass TermValue:\n    \"\"\" hold a term value the we use to construct a condition/filter \"\"\"\n\n    def __init__(self, value, converted, kind: str):\n        assert isinstance(kind, str), kind\n        self.value = value\n        self.converted = converted\n        self.kind = kind\n\n    def tostring(self, encoding) -> str:\n        \"\"\" quote the string if not encoded else encode and return \"\"\"\n        if self.kind == \"string\":\n            if encoding is not None:\n                return str(self.converted)\n            return f'\"{self.converted}\"'\n        elif self.kind == \"float\":\n            # python 2 str(float) is not always\n            # round-trippable so use repr()\n            return repr(self.converted)\n        return str(self.converted)\n\n\ndef maybe_expression(s) -> bool:\n    \"\"\" loose checking if s is a pytables-acceptable expression \"\"\"\n    if not isinstance(s, str):\n        return False\n    ops = PyTablesExprVisitor.binary_ops + PyTablesExprVisitor.unary_ops + (\"=\",)\n\n    # make sure we have an op at least\n    return any(op in s for op in ops)\n"
    },
    {
      "filename": "pandas/core/dtypes/dtypes.py",
      "content": "\"\"\"\nDefine extension dtypes.\n\"\"\"\n\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    List,\n    MutableMapping,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport numpy as np\nimport pytz\n\nfrom pandas._libs.interval import Interval\nfrom pandas._libs.tslibs import NaT, Period, Timestamp, dtypes, timezones, to_offset\nfrom pandas._libs.tslibs.offsets import BaseOffset\nfrom pandas._typing import DtypeObj, Ordered\n\nfrom pandas.core.dtypes.base import ExtensionDtype, register_extension_dtype\nfrom pandas.core.dtypes.generic import ABCCategoricalIndex, ABCIndexClass\nfrom pandas.core.dtypes.inference import is_bool, is_list_like\n\nif TYPE_CHECKING:\n    import pyarrow\n\n    from pandas import Categorical\n    from pandas.core.arrays import DatetimeArray, IntervalArray, PeriodArray\n\nstr_type = str\n\n\nclass PandasExtensionDtype(ExtensionDtype):\n    \"\"\"\n    A np.dtype duck-typed class, suitable for holding a custom dtype.\n\n    THIS IS NOT A REAL NUMPY DTYPE\n    \"\"\"\n\n    type: Any\n    kind: Any\n    # The Any type annotations above are here only because mypy seems to have a\n    # problem dealing with multiple inheritance from PandasExtensionDtype\n    # and ExtensionDtype's @properties in the subclasses below. The kind and\n    # type variables in those subclasses are explicitly typed below.\n    subdtype = None\n    str: str_type\n    num = 100\n    shape: Tuple[int, ...] = ()\n    itemsize = 8\n    base = None\n    isbuiltin = 0\n    isnative = 0\n    _cache: Dict[str_type, \"PandasExtensionDtype\"] = {}\n\n    def __str__(self) -> str_type:\n        \"\"\"\n        Return a string representation for a particular Object\n        \"\"\"\n        return self.name\n\n    def __repr__(self) -> str_type:\n        \"\"\"\n        Return a string representation for a particular object.\n        \"\"\"\n        return str(self)\n\n    def __hash__(self) -> int:\n        raise NotImplementedError(\"sub-classes should implement an __hash__ method\")\n\n    def __getstate__(self) -> Dict[str_type, Any]:\n        # pickle support; we don't want to pickle the cache\n        return {k: getattr(self, k, None) for k in self._metadata}\n\n    @classmethod\n    def reset_cache(cls) -> None:\n        \"\"\" clear the cache \"\"\"\n        cls._cache = {}\n\n\nclass CategoricalDtypeType(type):\n    \"\"\"\n    the type of CategoricalDtype, this metaclass determines subclass ability\n    \"\"\"\n\n    pass\n\n\n@register_extension_dtype\nclass CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n    \"\"\"\n    Type for categorical data with the categories and orderedness.\n\n    Parameters\n    ----------\n    categories : sequence, optional\n        Must be unique, and must not contain any nulls.\n        The categories are stored in an Index,\n        and if an index is provided the dtype of that index will be used.\n    ordered : bool or None, default False\n        Whether or not this categorical is treated as a ordered categorical.\n        None can be used to maintain the ordered value of existing categoricals when\n        used in operations that combine categoricals, e.g. astype, and will resolve to\n        False if there is no existing ordered to maintain.\n\n    Attributes\n    ----------\n    categories\n    ordered\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    Categorical : Represent a categorical variable in classic R / S-plus fashion.\n\n    Notes\n    -----\n    This class is useful for specifying the type of a ``Categorical``\n    independent of the values. See :ref:`categorical.categoricaldtype`\n    for more.\n\n    Examples\n    --------\n    >>> t = pd.CategoricalDtype(categories=['b', 'a'], ordered=True)\n    >>> pd.Series(['a', 'b', 'a', 'c'], dtype=t)\n    0      a\n    1      b\n    2      a\n    3    NaN\n    dtype: category\n    Categories (2, object): ['b' < 'a']\n\n    An empty CategoricalDtype with a specific dtype can be created\n    by providing an empty index. As follows,\n\n    >>> pd.CategoricalDtype(pd.DatetimeIndex([])).categories.dtype\n    dtype('<M8[ns]')\n    \"\"\"\n\n    # TODO: Document public vs. private API\n    name = \"category\"\n    type: Type[CategoricalDtypeType] = CategoricalDtypeType\n    kind: str_type = \"O\"\n    str = \"|O08\"\n    base = np.dtype(\"O\")\n    _metadata = (\"categories\", \"ordered\")\n    _cache: Dict[str_type, PandasExtensionDtype] = {}\n\n    def __init__(self, categories=None, ordered: Ordered = False):\n        self._finalize(categories, ordered, fastpath=False)\n\n    @classmethod\n    def _from_fastpath(\n        cls, categories=None, ordered: Optional[bool] = None\n    ) -> \"CategoricalDtype\":\n        self = cls.__new__(cls)\n        self._finalize(categories, ordered, fastpath=True)\n        return self\n\n    @classmethod\n    def _from_categorical_dtype(\n        cls, dtype: \"CategoricalDtype\", categories=None, ordered: Ordered = None\n    ) -> \"CategoricalDtype\":\n        if categories is ordered is None:\n            return dtype\n        if categories is None:\n            categories = dtype.categories\n        if ordered is None:\n            ordered = dtype.ordered\n        return cls(categories, ordered)\n\n    @classmethod\n    def _from_values_or_dtype(\n        cls,\n        values=None,\n        categories=None,\n        ordered: Optional[bool] = None,\n        dtype: Optional[\"CategoricalDtype\"] = None,\n    ) -> \"CategoricalDtype\":\n        \"\"\"\n        Construct dtype from the input parameters used in :class:`Categorical`.\n\n        This constructor method specifically does not do the factorization\n        step, if that is needed to find the categories. This constructor may\n        therefore return ``CategoricalDtype(categories=None, ordered=None)``,\n        which may not be useful. Additional steps may therefore have to be\n        taken to create the final dtype.\n\n        The return dtype is specified from the inputs in this prioritized\n        order:\n        1. if dtype is a CategoricalDtype, return dtype\n        2. if dtype is the string 'category', create a CategoricalDtype from\n           the supplied categories and ordered parameters, and return that.\n        3. if values is a categorical, use value.dtype, but override it with\n           categories and ordered if either/both of those are not None.\n        4. if dtype is None and values is not a categorical, construct the\n           dtype from categories and ordered, even if either of those is None.\n\n        Parameters\n        ----------\n        values : list-like, optional\n            The list-like must be 1-dimensional.\n        categories : list-like, optional\n            Categories for the CategoricalDtype.\n        ordered : bool, optional\n            Designating if the categories are ordered.\n        dtype : CategoricalDtype or the string \"category\", optional\n            If ``CategoricalDtype``, cannot be used together with\n            `categories` or `ordered`.\n\n        Returns\n        -------\n        CategoricalDtype\n\n        Examples\n        --------\n        >>> pd.CategoricalDtype._from_values_or_dtype()\n        CategoricalDtype(categories=None, ordered=None)\n        >>> pd.CategoricalDtype._from_values_or_dtype(\n        ...     categories=['a', 'b'], ordered=True\n        ... )\n        CategoricalDtype(categories=['a', 'b'], ordered=True)\n        >>> dtype1 = pd.CategoricalDtype(['a', 'b'], ordered=True)\n        >>> dtype2 = pd.CategoricalDtype(['x', 'y'], ordered=False)\n        >>> c = pd.Categorical([0, 1], dtype=dtype1, fastpath=True)\n        >>> pd.CategoricalDtype._from_values_or_dtype(\n        ...     c, ['x', 'y'], ordered=True, dtype=dtype2\n        ... )\n        Traceback (most recent call last):\n            ...\n        ValueError: Cannot specify `categories` or `ordered` together with\n        `dtype`.\n\n        The supplied dtype takes precedence over values' dtype:\n\n        >>> pd.CategoricalDtype._from_values_or_dtype(c, dtype=dtype2)\n        CategoricalDtype(categories=['x', 'y'], ordered=False)\n        \"\"\"\n\n        if dtype is not None:\n            # The dtype argument takes precedence over values.dtype (if any)\n            if isinstance(dtype, str):\n                if dtype == \"category\":\n                    dtype = CategoricalDtype(categories, ordered)\n                else:\n                    raise ValueError(f\"Unknown dtype {repr(dtype)}\")\n            elif categories is not None or ordered is not None:\n                raise ValueError(\n                    \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                )\n            elif not isinstance(dtype, CategoricalDtype):\n                raise ValueError(f\"Cannot not construct CategoricalDtype from {dtype}\")\n        elif cls.is_dtype(values):\n            # If no \"dtype\" was passed, use the one from \"values\", but honor\n            # the \"ordered\" and \"categories\" arguments\n            dtype = values.dtype._from_categorical_dtype(\n                values.dtype, categories, ordered\n            )\n        else:\n            # If dtype=None and values is not categorical, create a new dtype.\n            # Note: This could potentially have categories=None and\n            # ordered=None.\n            dtype = CategoricalDtype(categories, ordered)\n\n        return dtype\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> \"CategoricalDtype\":\n        \"\"\"\n        Construct a CategoricalDtype from a string.\n\n        Parameters\n        ----------\n        string : str\n            Must be the string \"category\" in order to be successfully constructed.\n\n        Returns\n        -------\n        CategoricalDtype\n            Instance of the dtype.\n\n        Raises\n        ------\n        TypeError\n            If a CategoricalDtype cannot be constructed from the input.\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n        if string != cls.name:\n            raise TypeError(f\"Cannot construct a 'CategoricalDtype' from '{string}'\")\n\n        # need ordered=None to ensure that operations specifying dtype=\"category\" don't\n        # override the ordered value for existing categoricals\n        return cls(ordered=None)\n\n    def _finalize(self, categories, ordered: Ordered, fastpath: bool = False) -> None:\n\n        if ordered is not None:\n            self.validate_ordered(ordered)\n\n        if categories is not None:\n            categories = self.validate_categories(categories, fastpath=fastpath)\n\n        self._categories = categories\n        self._ordered = ordered\n\n    def __setstate__(self, state: MutableMapping[str_type, Any]) -> None:\n        # for pickle compat. __get_state__ is defined in the\n        # PandasExtensionDtype superclass and uses the public properties to\n        # pickle -> need to set the settable private ones here (see GH26067)\n        self._categories = state.pop(\"categories\", None)\n        self._ordered = state.pop(\"ordered\", False)\n\n    def __hash__(self) -> int:\n        # _hash_categories returns a uint64, so use the negative\n        # space for when we have unknown categories to avoid a conflict\n        if self.categories is None:\n            if self.ordered:\n                return -1\n            else:\n                return -2\n        # We *do* want to include the real self.ordered here\n        return int(self._hash_categories(self.categories, self.ordered))\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\"\n        Rules for CDT equality:\n        1) Any CDT is equal to the string 'category'\n        2) Any CDT is equal to itself\n        3) Any CDT is equal to a CDT with categories=None regardless of ordered\n        4) A CDT with ordered=True is only equal to another CDT with\n           ordered=True and identical categories in the same order\n        5) A CDT with ordered={False, None} is only equal to another CDT with\n           ordered={False, None} and identical categories, but same order is\n           not required. There is no distinction between False/None.\n        6) Any other comparison returns False\n        \"\"\"\n        if isinstance(other, str):\n            return other == self.name\n        elif other is self:\n            return True\n        elif not (hasattr(other, \"ordered\") and hasattr(other, \"categories\")):\n            return False\n        elif self.categories is None or other.categories is None:\n            # We're forced into a suboptimal corner thanks to math and\n            # backwards compatibility. We require that `CDT(...) == 'category'`\n            # for all CDTs **including** `CDT(None, ...)`. Therefore, *all*\n            # CDT(., .) = CDT(None, False) and *all*\n            # CDT(., .) = CDT(None, True).\n            return True\n        elif self.ordered or other.ordered:\n            # At least one has ordered=True; equal if both have ordered=True\n            # and the same values for categories in the same order.\n            return (self.ordered == other.ordered) and self.categories.equals(\n                other.categories\n            )\n        else:\n            # Neither has ordered=True; equal if both have the same categories,\n            # but same order is not necessary.  There is no distinction between\n            # ordered=False and ordered=None: CDT(., False) and CDT(., None)\n            # will be equal if they have the same categories.\n            left = self.categories\n            right = other.categories\n\n            # GH#36280 the ordering of checks here is for performance\n            if not left.dtype == right.dtype:\n                return False\n\n            if len(left) != len(right):\n                return False\n\n            if self.categories.equals(other.categories):\n                # Check and see if they happen to be identical categories\n                return True\n\n            if left.dtype != object:\n                # Faster than calculating hash\n                indexer = left.get_indexer(right)\n                # Because left and right have the same length and are unique,\n                #  `indexer` not having any -1s implies that there is a\n                #  bijection between `left` and `right`.\n                return (indexer != -1).all()\n\n            # With object-dtype we need a comparison that identifies\n            #  e.g. int(2) as distinct from float(2)\n            return hash(self) == hash(other)\n\n    def __repr__(self) -> str_type:\n        if self.categories is None:\n            data = \"None, \"\n        else:\n            data = self.categories._format_data(name=type(self).__name__)\n        return f\"CategoricalDtype(categories={data}ordered={self.ordered})\"\n\n    @staticmethod\n    def _hash_categories(categories, ordered: Ordered = True) -> int:\n        from pandas.core.util.hashing import (\n            combine_hash_arrays,\n            hash_array,\n            hash_tuples,\n        )\n\n        if len(categories) and isinstance(categories[0], tuple):\n            # assumes if any individual category is a tuple, then all our. ATM\n            # I don't really want to support just some of the categories being\n            # tuples.\n            categories = list(categories)  # breaks if a np.array of categories\n            cat_array = hash_tuples(categories)\n        else:\n            if categories.dtype == \"O\":\n                if len({type(x) for x in categories}) != 1:\n                    # TODO: hash_array doesn't handle mixed types. It casts\n                    # everything to a str first, which means we treat\n                    # {'1', '2'} the same as {'1', 2}\n                    # find a better solution\n                    hashed = hash((tuple(categories), ordered))\n                    return hashed\n\n            if DatetimeTZDtype.is_dtype(categories.dtype):\n                # Avoid future warning.\n                categories = categories.astype(\"datetime64[ns]\")\n\n            cat_array = hash_array(np.asarray(categories), categorize=False)\n        if ordered:\n            cat_array = np.vstack(\n                [cat_array, np.arange(len(cat_array), dtype=cat_array.dtype)]\n            )\n        else:\n            cat_array = [cat_array]\n        hashed = combine_hash_arrays(iter(cat_array), num_items=len(cat_array))\n        return np.bitwise_xor.reduce(hashed)\n\n    @classmethod\n    def construct_array_type(cls) -> Type[\"Categorical\"]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas import Categorical\n\n        return Categorical\n\n    @staticmethod\n    def validate_ordered(ordered: Ordered) -> None:\n        \"\"\"\n        Validates that we have a valid ordered parameter. If\n        it is not a boolean, a TypeError will be raised.\n\n        Parameters\n        ----------\n        ordered : object\n            The parameter to be verified.\n\n        Raises\n        ------\n        TypeError\n            If 'ordered' is not a boolean.\n        \"\"\"\n        if not is_bool(ordered):\n            raise TypeError(\"'ordered' must either be 'True' or 'False'\")\n\n    @staticmethod\n    def validate_categories(categories, fastpath: bool = False):\n        \"\"\"\n        Validates that we have good categories\n\n        Parameters\n        ----------\n        categories : array-like\n        fastpath : bool\n            Whether to skip nan and uniqueness checks\n\n        Returns\n        -------\n        categories : Index\n        \"\"\"\n        from pandas.core.indexes.base import Index\n\n        if not fastpath and not is_list_like(categories):\n            raise TypeError(\n                f\"Parameter 'categories' must be list-like, was {repr(categories)}\"\n            )\n        elif not isinstance(categories, ABCIndexClass):\n            categories = Index(categories, tupleize_cols=False)\n\n        if not fastpath:\n\n            if categories.hasnans:\n                raise ValueError(\"Categorical categories cannot be null\")\n\n            if not categories.is_unique:\n                raise ValueError(\"Categorical categories must be unique\")\n\n        if isinstance(categories, ABCCategoricalIndex):\n            categories = categories.categories\n\n        return categories\n\n    def update_dtype(\n        self, dtype: Union[str_type, \"CategoricalDtype\"]\n    ) -> \"CategoricalDtype\":\n        \"\"\"\n        Returns a CategoricalDtype with categories and ordered taken from dtype\n        if specified, otherwise falling back to self if unspecified\n\n        Parameters\n        ----------\n        dtype : CategoricalDtype\n\n        Returns\n        -------\n        new_dtype : CategoricalDtype\n        \"\"\"\n        if isinstance(dtype, str) and dtype == \"category\":\n            # dtype='category' should not change anything\n            return self\n        elif not self.is_dtype(dtype):\n            raise ValueError(\n                f\"a CategoricalDtype must be passed to perform an update, \"\n                f\"got {repr(dtype)}\"\n            )\n        else:\n            # from here on, dtype is a CategoricalDtype\n            dtype = cast(CategoricalDtype, dtype)\n\n        # update categories/ordered unless they've been explicitly passed as None\n        new_categories = (\n            dtype.categories if dtype.categories is not None else self.categories\n        )\n        new_ordered = dtype.ordered if dtype.ordered is not None else self.ordered\n\n        return CategoricalDtype(new_categories, new_ordered)\n\n    @property\n    def categories(self):\n        \"\"\"\n        An ``Index`` containing the unique categories allowed.\n        \"\"\"\n        return self._categories\n\n    @property\n    def ordered(self) -> Ordered:\n        \"\"\"\n        Whether the categories have an ordered relationship.\n        \"\"\"\n        return self._ordered\n\n    @property\n    def _is_boolean(self) -> bool:\n        from pandas.core.dtypes.common import is_bool_dtype\n\n        return is_bool_dtype(self.categories)\n\n    def _get_common_dtype(self, dtypes: List[DtypeObj]) -> Optional[DtypeObj]:\n        from pandas.core.arrays.sparse import SparseDtype\n\n        # check if we have all categorical dtype with identical categories\n        if all(isinstance(x, CategoricalDtype) for x in dtypes):\n            first = dtypes[0]\n            if all(first == other for other in dtypes[1:]):\n                return first\n\n        # special case non-initialized categorical\n        # TODO we should figure out the expected return value in general\n        non_init_cats = [\n            isinstance(x, CategoricalDtype) and x.categories is None for x in dtypes\n        ]\n        if all(non_init_cats):\n            return self\n        elif any(non_init_cats):\n            return None\n\n        # categorical is aware of Sparse -> extract sparse subdtypes\n        dtypes = [x.subtype if isinstance(x, SparseDtype) else x for x in dtypes]\n        # extract the categories' dtype\n        non_cat_dtypes = [\n            x.categories.dtype if isinstance(x, CategoricalDtype) else x for x in dtypes\n        ]\n        # TODO should categorical always give an answer?\n        from pandas.core.dtypes.cast import find_common_type\n\n        return find_common_type(non_cat_dtypes)\n\n\n@register_extension_dtype\nclass DatetimeTZDtype(PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for timezone-aware datetime data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    unit : str, default \"ns\"\n        The precision of the datetime data. Currently limited\n        to ``\"ns\"``.\n    tz : str, int, or datetime.tzinfo\n        The timezone.\n\n    Attributes\n    ----------\n    unit\n    tz\n\n    Methods\n    -------\n    None\n\n    Raises\n    ------\n    pytz.UnknownTimeZoneError\n        When the requested timezone cannot be found.\n\n    Examples\n    --------\n    >>> pd.DatetimeTZDtype(tz='UTC')\n    datetime64[ns, UTC]\n\n    >>> pd.DatetimeTZDtype(tz='dateutil/US/Central')\n    datetime64[ns, tzfile('/usr/share/zoneinfo/US/Central')]\n    \"\"\"\n\n    type: Type[Timestamp] = Timestamp\n    kind: str_type = \"M\"\n    str = \"|M8[ns]\"\n    num = 101\n    base = np.dtype(\"M8[ns]\")\n    na_value = NaT\n    _metadata = (\"unit\", \"tz\")\n    _match = re.compile(r\"(datetime64|M8)\\[(?P<unit>.+), (?P<tz>.+)\\]\")\n    _cache: Dict[str_type, PandasExtensionDtype] = {}\n\n    def __init__(self, unit: Union[str_type, \"DatetimeTZDtype\"] = \"ns\", tz=None):\n        if isinstance(unit, DatetimeTZDtype):\n            # error: \"str\" has no attribute \"tz\"\n            unit, tz = unit.unit, unit.tz  # type: ignore[attr-defined]\n\n        if unit != \"ns\":\n            if isinstance(unit, str) and tz is None:\n                # maybe a string like datetime64[ns, tz], which we support for\n                # now.\n                result = type(self).construct_from_string(unit)\n                unit = result.unit\n                tz = result.tz\n                msg = (\n                    f\"Passing a dtype alias like 'datetime64[ns, {tz}]' \"\n                    \"to DatetimeTZDtype is no longer supported. Use \"\n                    \"'DatetimeTZDtype.construct_from_string()' instead.\"\n                )\n                raise ValueError(msg)\n            else:\n                raise ValueError(\"DatetimeTZDtype only supports ns units\")\n\n        if tz:\n            tz = timezones.maybe_get_tz(tz)\n            tz = timezones.tz_standardize(tz)\n        elif tz is not None:\n            raise pytz.UnknownTimeZoneError(tz)\n        if tz is None:\n            raise TypeError(\"A 'tz' is required.\")\n\n        self._unit = unit\n        self._tz = tz\n\n    @property\n    def unit(self) -> str_type:\n        \"\"\"\n        The precision of the datetime data.\n        \"\"\"\n        return self._unit\n\n    @property\n    def tz(self):\n        \"\"\"\n        The timezone.\n        \"\"\"\n        return self._tz\n\n    @classmethod\n    def construct_array_type(cls) -> Type[\"DatetimeArray\"]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import DatetimeArray\n\n        return DatetimeArray\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> \"DatetimeTZDtype\":\n        \"\"\"\n        Construct a DatetimeTZDtype from a string.\n\n        Parameters\n        ----------\n        string : str\n            The string alias for this DatetimeTZDtype.\n            Should be formatted like ``datetime64[ns, <tz>]``,\n            where ``<tz>`` is the timezone name.\n\n        Examples\n        --------\n        >>> DatetimeTZDtype.construct_from_string('datetime64[ns, UTC]')\n        datetime64[ns, UTC]\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n\n        msg = f\"Cannot construct a 'DatetimeTZDtype' from '{string}'\"\n        match = cls._match.match(string)\n        if match:\n            d = match.groupdict()\n            try:\n                return cls(unit=d[\"unit\"], tz=d[\"tz\"])\n            except (KeyError, TypeError, ValueError) as err:\n                # KeyError if maybe_get_tz tries and fails to get a\n                #  pytz timezone (actually pytz.UnknownTimeZoneError).\n                # TypeError if we pass a nonsense tz;\n                # ValueError if we pass a unit other than \"ns\"\n                raise TypeError(msg) from err\n        raise TypeError(msg)\n\n    def __str__(self) -> str_type:\n        return f\"datetime64[{self.unit}, {self.tz}]\"\n\n    @property\n    def name(self) -> str_type:\n        \"\"\"A string representation of the dtype.\"\"\"\n        return str(self)\n\n    def __hash__(self) -> int:\n        # make myself hashable\n        # TODO: update this.\n        return hash(str(self))\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, str):\n            if other.startswith(\"M8[\"):\n                other = \"datetime64[\" + other[3:]\n            return other == self.name\n\n        return (\n            isinstance(other, DatetimeTZDtype)\n            and self.unit == other.unit\n            and str(self.tz) == str(other.tz)\n        )\n\n    def __setstate__(self, state) -> None:\n        # for pickle compat. __get_state__ is defined in the\n        # PandasExtensionDtype superclass and uses the public properties to\n        # pickle -> need to set the settable private ones here (see GH26067)\n        self._tz = state[\"tz\"]\n        self._unit = state[\"unit\"]\n\n\n@register_extension_dtype\nclass PeriodDtype(dtypes.PeriodDtypeBase, PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for Period data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    freq : str or DateOffset\n        The frequency of this PeriodDtype.\n\n    Attributes\n    ----------\n    freq\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> pd.PeriodDtype(freq='D')\n    period[D]\n\n    >>> pd.PeriodDtype(freq=pd.offsets.MonthEnd())\n    period[M]\n    \"\"\"\n\n    type: Type[Period] = Period\n    kind: str_type = \"O\"\n    str = \"|O08\"\n    base = np.dtype(\"O\")\n    num = 102\n    _metadata = (\"freq\",)\n    _match = re.compile(r\"(P|p)eriod\\[(?P<freq>.+)\\]\")\n    _cache: Dict[str_type, PandasExtensionDtype] = {}\n\n    def __new__(cls, freq=None):\n        \"\"\"\n        Parameters\n        ----------\n        freq : frequency\n        \"\"\"\n        if isinstance(freq, PeriodDtype):\n            return freq\n\n        elif freq is None:\n            # empty constructor for pickle compat\n            # -10_000 corresponds to PeriodDtypeCode.UNDEFINED\n            u = dtypes.PeriodDtypeBase.__new__(cls, -10_000)\n            u._freq = None\n            return u\n\n        if not isinstance(freq, BaseOffset):\n            freq = cls._parse_dtype_strict(freq)\n\n        try:\n            return cls._cache[freq.freqstr]\n        except KeyError:\n            dtype_code = freq._period_dtype_code\n            u = dtypes.PeriodDtypeBase.__new__(cls, dtype_code)\n            u._freq = freq\n            cls._cache[freq.freqstr] = u\n            return u\n\n    def __reduce__(self):\n        return type(self), (self.freq,)\n\n    @property\n    def freq(self):\n        \"\"\"\n        The frequency object of this PeriodDtype.\n        \"\"\"\n        return self._freq\n\n    @classmethod\n    def _parse_dtype_strict(cls, freq):\n        if isinstance(freq, str):\n            if freq.startswith(\"period[\") or freq.startswith(\"Period[\"):\n                m = cls._match.search(freq)\n                if m is not None:\n                    freq = m.group(\"freq\")\n\n            freq = to_offset(freq)\n            if freq is not None:\n                return freq\n\n        raise ValueError(\"could not construct PeriodDtype\")\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> \"PeriodDtype\":\n        \"\"\"\n        Strict construction from a string, raise a TypeError if not\n        possible\n        \"\"\"\n        if (\n            isinstance(string, str)\n            and (string.startswith(\"period[\") or string.startswith(\"Period[\"))\n            or isinstance(string, BaseOffset)\n        ):\n            # do not parse string like U as period[U]\n            # avoid tuple to be regarded as freq\n            try:\n                return cls(freq=string)\n            except ValueError:\n                pass\n        if isinstance(string, str):\n            msg = f\"Cannot construct a 'PeriodDtype' from '{string}'\"\n        else:\n            msg = f\"'construct_from_string' expects a string, got {type(string)}\"\n        raise TypeError(msg)\n\n    def __str__(self) -> str_type:\n        return self.name\n\n    @property\n    def name(self) -> str_type:\n        return f\"period[{self.freq.freqstr}]\"\n\n    @property\n    def na_value(self):\n        return NaT\n\n    def __hash__(self) -> int:\n        # make myself hashable\n        return hash(str(self))\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, str):\n            return other == self.name or other == self.name.title()\n\n        return isinstance(other, PeriodDtype) and self.freq == other.freq\n\n    def __ne__(self, other: Any) -> bool:\n        return not self.__eq__(other)\n\n    def __setstate__(self, state):\n        # for pickle compat. __getstate__ is defined in the\n        # PandasExtensionDtype superclass and uses the public properties to\n        # pickle -> need to set the settable private ones here (see GH26067)\n        self._freq = state[\"freq\"]\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        \"\"\"\n        Return a boolean if we if the passed type is an actual dtype that we\n        can match (via string or type)\n        \"\"\"\n        if isinstance(dtype, str):\n            # PeriodDtype can be instantiated from freq string like \"U\",\n            # but doesn't regard freq str like \"U\" as dtype.\n            if dtype.startswith(\"period[\") or dtype.startswith(\"Period[\"):\n                try:\n                    if cls._parse_dtype_strict(dtype) is not None:\n                        return True\n                    else:\n                        return False\n                except ValueError:\n                    return False\n            else:\n                return False\n        return super().is_dtype(dtype)\n\n    @classmethod\n    def construct_array_type(cls) -> Type[\"PeriodArray\"]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import PeriodArray\n\n        return PeriodArray\n\n    def __from_arrow__(\n        self, array: Union[\"pyarrow.Array\", \"pyarrow.ChunkedArray\"]\n    ) -> \"PeriodArray\":\n        \"\"\"\n        Construct PeriodArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays import PeriodArray\n        from pandas.core.arrays._arrow_utils import pyarrow_array_to_numpy_and_mask\n\n        if isinstance(array, pyarrow.Array):\n            chunks = [array]\n        else:\n            chunks = array.chunks\n\n        results = []\n        for arr in chunks:\n            data, mask = pyarrow_array_to_numpy_and_mask(arr, dtype=\"int64\")\n            parr = PeriodArray(data.copy(), freq=self.freq, copy=False)\n            parr[~mask] = NaT\n            results.append(parr)\n\n        return PeriodArray._concat_same_type(results)\n\n\n@register_extension_dtype\nclass IntervalDtype(PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for Interval data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    subtype : str, np.dtype\n        The dtype of the Interval bounds.\n\n    Attributes\n    ----------\n    subtype\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> pd.IntervalDtype(subtype='int64')\n    interval[int64]\n    \"\"\"\n\n    name = \"interval\"\n    kind: str_type = \"O\"\n    str = \"|O08\"\n    base = np.dtype(\"O\")\n    num = 103\n    _metadata = (\"subtype\",)\n    _match = re.compile(r\"(I|i)nterval\\[(?P<subtype>.+)\\]\")\n    _cache: Dict[str_type, PandasExtensionDtype] = {}\n\n    def __new__(cls, subtype=None):\n        from pandas.core.dtypes.common import is_string_dtype, pandas_dtype\n\n        if isinstance(subtype, IntervalDtype):\n            return subtype\n        elif subtype is None:\n            # we are called as an empty constructor\n            # generally for pickle compat\n            u = object.__new__(cls)\n            u._subtype = None\n            return u\n        elif isinstance(subtype, str) and subtype.lower() == \"interval\":\n            subtype = None\n        else:\n            if isinstance(subtype, str):\n                m = cls._match.search(subtype)\n                if m is not None:\n                    subtype = m.group(\"subtype\")\n\n            try:\n                subtype = pandas_dtype(subtype)\n            except TypeError as err:\n                raise TypeError(\"could not construct IntervalDtype\") from err\n\n        if CategoricalDtype.is_dtype(subtype) or is_string_dtype(subtype):\n            # GH 19016\n            msg = (\n                \"category, object, and string subtypes are not supported \"\n                \"for IntervalDtype\"\n            )\n            raise TypeError(msg)\n\n        try:\n            return cls._cache[str(subtype)]\n        except KeyError:\n            u = object.__new__(cls)\n            u._subtype = subtype\n            cls._cache[str(subtype)] = u\n            return u\n\n    @property\n    def subtype(self):\n        \"\"\"\n        The dtype of the Interval bounds.\n        \"\"\"\n        return self._subtype\n\n    @classmethod\n    def construct_array_type(cls) -> Type[\"IntervalArray\"]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import IntervalArray\n\n        return IntervalArray\n\n    @classmethod\n    def construct_from_string(cls, string):\n        \"\"\"\n        attempt to construct this type from a string, raise a TypeError\n        if its not possible\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n\n        if string.lower() == \"interval\" or cls._match.search(string) is not None:\n            return cls(string)\n\n        msg = (\n            f\"Cannot construct a 'IntervalDtype' from '{string}'.\\n\\n\"\n            \"Incorrectly formatted string passed to constructor. \"\n            \"Valid formats include Interval or Interval[dtype] \"\n            \"where dtype is numeric, datetime, or timedelta\"\n        )\n        raise TypeError(msg)\n\n    @property\n    def type(self):\n        return Interval\n\n    def __str__(self) -> str_type:\n        if self.subtype is None:\n            return \"interval\"\n        return f\"interval[{self.subtype}]\"\n\n    def __hash__(self) -> int:\n        # make myself hashable\n        return hash(str(self))\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, str):\n            return other.lower() in (self.name.lower(), str(self).lower())\n        elif not isinstance(other, IntervalDtype):\n            return False\n        elif self.subtype is None or other.subtype is None:\n            # None should match any subtype\n            return True\n        else:\n            from pandas.core.dtypes.common import is_dtype_equal\n\n            return is_dtype_equal(self.subtype, other.subtype)\n\n    def __setstate__(self, state):\n        # for pickle compat. __get_state__ is defined in the\n        # PandasExtensionDtype superclass and uses the public properties to\n        # pickle -> need to set the settable private ones here (see GH26067)\n        self._subtype = state[\"subtype\"]\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        \"\"\"\n        Return a boolean if we if the passed type is an actual dtype that we\n        can match (via string or type)\n        \"\"\"\n        if isinstance(dtype, str):\n            if dtype.lower().startswith(\"interval\"):\n                try:\n                    if cls.construct_from_string(dtype) is not None:\n                        return True\n                    else:\n                        return False\n                except (ValueError, TypeError):\n                    return False\n            else:\n                return False\n        return super().is_dtype(dtype)\n\n    def __from_arrow__(\n        self, array: Union[\"pyarrow.Array\", \"pyarrow.ChunkedArray\"]\n    ) -> \"IntervalArray\":\n        \"\"\"\n        Construct IntervalArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays import IntervalArray\n\n        if isinstance(array, pyarrow.Array):\n            chunks = [array]\n        else:\n            chunks = array.chunks\n\n        results = []\n        for arr in chunks:\n            left = np.asarray(arr.storage.field(\"left\"), dtype=self.subtype)\n            right = np.asarray(arr.storage.field(\"right\"), dtype=self.subtype)\n            iarr = IntervalArray.from_arrays(left, right, closed=array.type.closed)\n            results.append(iarr)\n\n        return IntervalArray._concat_same_type(results)\n"
    },
    {
      "filename": "pandas/core/indexes/base.py",
      "content": "from copy import copy as copy_func\nfrom datetime import datetime\nfrom itertools import zip_longest\nimport operator\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    FrozenSet,\n    Hashable,\n    List,\n    NewType,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import algos as libalgos, index as libindex, lib\nimport pandas._libs.join as libjoin\nfrom pandas._libs.lib import is_datetime_array, no_default\nfrom pandas._libs.tslibs import IncompatibleFrequency, OutOfBoundsDatetime, Timestamp\nfrom pandas._libs.tslibs.timezones import tz_compare\nfrom pandas._typing import AnyArrayLike, Dtype, DtypeObj, Label, Shape, final\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import DuplicateLabelError, InvalidIndexError\nfrom pandas.util._decorators import Appender, cache_readonly, doc\n\nfrom pandas.core.dtypes.cast import (\n    find_common_type,\n    maybe_cast_to_integer_array,\n    validate_numeric_casting,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_object,\n    ensure_platform_int,\n    is_bool_dtype,\n    is_categorical_dtype,\n    is_datetime64_any_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_hashable,\n    is_integer,\n    is_integer_dtype,\n    is_interval_dtype,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_period_dtype,\n    is_scalar,\n    is_signed_integer_dtype,\n    is_timedelta64_dtype,\n    is_unsigned_integer_dtype,\n    needs_i8_conversion,\n    pandas_dtype,\n    validate_all_hashable,\n)\nfrom pandas.core.dtypes.concat import concat_compat\nfrom pandas.core.dtypes.generic import (\n    ABCDatetimeIndex,\n    ABCMultiIndex,\n    ABCPandasArray,\n    ABCPeriodIndex,\n    ABCRangeIndex,\n    ABCSeries,\n    ABCTimedeltaIndex,\n)\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nfrom pandas.core import missing, ops\nfrom pandas.core.accessor import CachedAccessor\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import Categorical, ExtensionArray\nfrom pandas.core.arrays.datetimes import tz_to_dtype, validate_tz_from_dtype\nfrom pandas.core.base import IndexOpsMixin, PandasObject\nimport pandas.core.common as com\nfrom pandas.core.construction import extract_array\nfrom pandas.core.indexers import deprecate_ndim_indexing\nfrom pandas.core.indexes.frozen import FrozenList\nfrom pandas.core.ops import get_op_result_name\nfrom pandas.core.ops.invalid import make_invalid_op\nfrom pandas.core.sorting import ensure_key_mapped, nargsort\nfrom pandas.core.strings import StringMethods\n\nfrom pandas.io.formats.printing import (\n    PrettyDict,\n    default_pprint,\n    format_object_attrs,\n    format_object_summary,\n    pprint_thing,\n)\n\nif TYPE_CHECKING:\n    from pandas import MultiIndex, RangeIndex, Series\n\n\n__all__ = [\"Index\"]\n\n_unsortable_types = frozenset((\"mixed\", \"mixed-integer\"))\n\n_index_doc_kwargs = {\n    \"klass\": \"Index\",\n    \"inplace\": \"\",\n    \"target_klass\": \"Index\",\n    \"raises_section\": \"\",\n    \"unique\": \"Index\",\n    \"duplicated\": \"np.ndarray\",\n}\n_index_shared_docs = {}\nstr_t = str\n\n\n_o_dtype = np.dtype(object)\n\n\n_Identity = NewType(\"_Identity\", object)\n\n\ndef _new_Index(cls, d):\n    \"\"\"\n    This is called upon unpickling, rather than the default which doesn't\n    have arguments and breaks __new__.\n    \"\"\"\n    # required for backward compat, because PI can't be instantiated with\n    # ordinals through __new__ GH #13277\n    if issubclass(cls, ABCPeriodIndex):\n        from pandas.core.indexes.period import _new_PeriodIndex\n\n        return _new_PeriodIndex(cls, **d)\n\n    if issubclass(cls, ABCMultiIndex):\n        if \"labels\" in d and \"codes\" not in d:\n            # GH#23752 \"labels\" kwarg has been replaced with \"codes\"\n            d[\"codes\"] = d.pop(\"labels\")\n\n    return cls.__new__(cls, **d)\n\n\n_IndexT = TypeVar(\"_IndexT\", bound=\"Index\")\n\n\nclass Index(IndexOpsMixin, PandasObject):\n    \"\"\"\n    Immutable sequence used for indexing and alignment. The basic object\n    storing axis labels for all pandas objects.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray.\n    name : object\n        Name to be stored in the index.\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible.\n\n    See Also\n    --------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex : Index of datetime64 data.\n    TimedeltaIndex : Index of timedelta64 data.\n    PeriodIndex : Index of Period data.\n    Int64Index : A special case of :class:`Index` with purely integer labels.\n    UInt64Index : A special case of :class:`Index` with purely unsigned integer labels.\n    Float64Index : A special case of :class:`Index` with purely float labels.\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n    \"\"\"\n\n    # tolist is not actually deprecated, just suppressed in the __dir__\n    _hidden_attrs: FrozenSet[str] = (\n        PandasObject._hidden_attrs\n        | IndexOpsMixin._hidden_attrs\n        | frozenset([\"contains\", \"set_value\"])\n    )\n\n    # To hand over control to subclasses\n    _join_precedence = 1\n\n    # Cython methods; see github.com/cython/cython/issues/2647\n    #  for why we need to wrap these instead of making them class attributes\n    # Moreover, cython will choose the appropriate-dtyped sub-function\n    #  given the dtypes of the passed arguments\n    def _left_indexer_unique(self, left, right):\n        return libjoin.left_join_indexer_unique(left, right)\n\n    def _left_indexer(self, left, right):\n        return libjoin.left_join_indexer(left, right)\n\n    def _inner_indexer(self, left, right):\n        return libjoin.inner_join_indexer(left, right)\n\n    def _outer_indexer(self, left, right):\n        return libjoin.outer_join_indexer(left, right)\n\n    _typ = \"index\"\n    _data: Union[ExtensionArray, np.ndarray]\n    _id: Optional[_Identity] = None\n    _name: Label = None\n    # MultiIndex.levels previously allowed setting the index name. We\n    # don't allow this anymore, and raise if it happens rather than\n    # failing silently.\n    _no_setting_name: bool = False\n    _comparables = [\"name\"]\n    _attributes = [\"name\"]\n    _is_numeric_dtype = False\n    _can_hold_na = True\n    _can_hold_strings = True\n\n    # would we like our indexing holder to defer to us\n    _defer_to_indexing = False\n\n    _engine_type = libindex.ObjectEngine\n    # whether we support partial string indexing. Overridden\n    # in DatetimeIndex and PeriodIndex\n    _supports_partial_string_indexing = False\n\n    _accessors = {\"str\"}\n\n    str = CachedAccessor(\"str\", StringMethods)\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls, data=None, dtype=None, copy=False, name=None, tupleize_cols=True, **kwargs\n    ) -> \"Index\":\n\n        from pandas.core.indexes.range import RangeIndex\n\n        name = maybe_extract_name(name, data, cls)\n\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n        if \"tz\" in kwargs:\n            tz = kwargs.pop(\"tz\")\n            validate_tz_from_dtype(dtype, tz)\n            dtype = tz_to_dtype(tz)\n\n        if isinstance(data, ABCPandasArray):\n            # ensure users don't accidentally put a PandasArray in an index.\n            data = data.to_numpy()\n\n        data_dtype = getattr(data, \"dtype\", None)\n\n        # range\n        if isinstance(data, RangeIndex):\n            return RangeIndex(start=data, copy=copy, dtype=dtype, name=name)\n        elif isinstance(data, range):\n            return RangeIndex.from_range(data, dtype=dtype, name=name)\n\n        # categorical\n        elif is_categorical_dtype(data_dtype) or is_categorical_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas.core.indexes.category import CategoricalIndex\n\n            return _maybe_asobject(dtype, CategoricalIndex, data, copy, name, **kwargs)\n\n        # interval\n        elif is_interval_dtype(data_dtype) or is_interval_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas.core.indexes.interval import IntervalIndex\n\n            return _maybe_asobject(dtype, IntervalIndex, data, copy, name, **kwargs)\n\n        elif is_datetime64_any_dtype(data_dtype) or is_datetime64_any_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas import DatetimeIndex\n\n            return _maybe_asobject(dtype, DatetimeIndex, data, copy, name, **kwargs)\n\n        elif is_timedelta64_dtype(data_dtype) or is_timedelta64_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas import TimedeltaIndex\n\n            return _maybe_asobject(dtype, TimedeltaIndex, data, copy, name, **kwargs)\n\n        elif is_period_dtype(data_dtype) or is_period_dtype(dtype):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas import PeriodIndex\n\n            return _maybe_asobject(dtype, PeriodIndex, data, copy, name, **kwargs)\n\n        # extension dtype\n        elif is_extension_array_dtype(data_dtype) or is_extension_array_dtype(dtype):\n            if not (dtype is None or is_object_dtype(dtype)):\n                # coerce to the provided dtype\n                ea_cls = dtype.construct_array_type()\n                data = ea_cls._from_sequence(data, dtype=dtype, copy=False)\n            else:\n                data = np.asarray(data, dtype=object)\n\n            # coerce to the object dtype\n            data = data.astype(object)\n            return Index(data, dtype=object, copy=copy, name=name, **kwargs)\n\n        # index-like\n        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n            # Delay import for perf. https://github.com/pandas-dev/pandas/pull/31423\n            from pandas.core.indexes.numeric import (\n                Float64Index,\n                Int64Index,\n                UInt64Index,\n            )\n\n            if dtype is not None:\n                # we need to avoid having numpy coerce\n                # things that look like ints/floats to ints unless\n                # they are actually ints, e.g. '0' and 0.0\n                # should not be coerced\n                # GH 11836\n                data = _maybe_cast_with_dtype(data, dtype, copy)\n                dtype = data.dtype  # TODO: maybe not for object?\n\n            # maybe coerce to a sub-class\n            if is_signed_integer_dtype(data.dtype):\n                return Int64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_unsigned_integer_dtype(data.dtype):\n                return UInt64Index(data, copy=copy, dtype=dtype, name=name)\n            elif is_float_dtype(data.dtype):\n                return Float64Index(data, copy=copy, dtype=dtype, name=name)\n            elif issubclass(data.dtype.type, bool) or is_bool_dtype(data):\n                subarr = data.astype(\"object\")\n            else:\n                subarr = com.asarray_tuplesafe(data, dtype=object)\n\n            # asarray_tuplesafe does not always copy underlying data,\n            # so need to make sure that this happens\n            if copy:\n                subarr = subarr.copy()\n\n            if dtype is None:\n                new_data, new_dtype = _maybe_cast_data_without_dtype(subarr)\n                if new_dtype is not None:\n                    return cls(\n                        new_data, dtype=new_dtype, copy=False, name=name, **kwargs\n                    )\n\n            if kwargs:\n                raise TypeError(f\"Unexpected keyword arguments {repr(set(kwargs))}\")\n            if subarr.ndim > 1:\n                # GH#13601, GH#20285, GH#27125\n                raise ValueError(\"Index data must be 1-dimensional\")\n            return cls._simple_new(subarr, name)\n\n        elif data is None or is_scalar(data):\n            raise cls._scalar_data_error(data)\n        elif hasattr(data, \"__array__\"):\n            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name, **kwargs)\n        else:\n            if tupleize_cols and is_list_like(data):\n                # GH21470: convert iterable to list before determining if empty\n                if is_iterator(data):\n                    data = list(data)\n\n                if data and all(isinstance(e, tuple) for e in data):\n                    # we must be all tuples, otherwise don't construct\n                    # 10697\n                    from pandas.core.indexes.multi import MultiIndex\n\n                    return MultiIndex.from_tuples(\n                        data, names=name or kwargs.get(\"names\")\n                    )\n            # other iterable of some kind\n            subarr = com.asarray_tuplesafe(data, dtype=object)\n            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n\n    \"\"\"\n    NOTE for new Index creation:\n\n    - _simple_new: It returns new Index with the same type as the caller.\n      All metadata (such as name) must be provided by caller's responsibility.\n      Using _shallow_copy is recommended because it fills these metadata\n      otherwise specified.\n\n    - _shallow_copy: It returns new Index with the same type (using\n      _simple_new), but fills caller's metadata otherwise specified. Passed\n      kwargs will overwrite corresponding metadata.\n\n    See each method's docstring.\n    \"\"\"\n\n    @property\n    def asi8(self):\n        \"\"\"\n        Integer representation of the values.\n\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        warnings.warn(\n            \"Index.asi8 is deprecated and will be removed in a future version\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return None\n\n    @classmethod\n    def _simple_new(cls, values, name: Label = None):\n        \"\"\"\n        We require that we have a dtype compat for the values. If we are passed\n        a non-dtype compat, then coerce using the constructor.\n\n        Must be careful not to recurse.\n        \"\"\"\n        assert isinstance(values, np.ndarray), type(values)\n\n        result = object.__new__(cls)\n        result._data = values\n        # _index_data is a (temporary?) fix to ensure that the direct data\n        # manipulation we do in `_libs/reduction.pyx` continues to work.\n        # We need access to the actual ndarray, since we're messing with\n        # data buffers and strides.\n        result._index_data = values\n        result._name = name\n        result._cache = {}\n        result._reset_identity()\n\n        return result\n\n    @cache_readonly\n    def _constructor(self):\n        return type(self)\n\n    @final\n    def _maybe_check_unique(self):\n        \"\"\"\n        Check that an Index has no duplicates.\n\n        This is typically only called via\n        `NDFrame.flags.allows_duplicate_labels.setter` when it's set to\n        True (duplicates aren't allowed).\n\n        Raises\n        ------\n        DuplicateLabelError\n            When the index is not unique.\n        \"\"\"\n        if not self.is_unique:\n            msg = \"\"\"Index has duplicates.\"\"\"\n            duplicates = self._format_duplicate_message()\n            msg += f\"\\n{duplicates}\"\n\n            raise DuplicateLabelError(msg)\n\n    @final\n    def _format_duplicate_message(self):\n        \"\"\"\n        Construct the DataFrame for a DuplicateLabelError.\n\n        This returns a DataFrame indicating the labels and positions\n        of duplicates in an index. This should only be called when it's\n        already known that duplicates are present.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'a'])\n        >>> idx._format_duplicate_message()\n            positions\n        label\n        a        [0, 2]\n        \"\"\"\n        from pandas import Series\n\n        duplicates = self[self.duplicated(keep=\"first\")].unique()\n        assert len(duplicates)\n\n        out = Series(np.arange(len(self))).groupby(self).agg(list)[duplicates]\n        if self.nlevels == 1:\n            out = out.rename_axis(\"label\")\n        return out.to_frame(name=\"positions\")\n\n    # --------------------------------------------------------------------\n    # Index Internals Methods\n\n    @final\n    def _get_attributes_dict(self):\n        \"\"\"\n        Return an attributes dict for my class.\n        \"\"\"\n        return {k: getattr(self, k, None) for k in self._attributes}\n\n    def _shallow_copy(self, values=None, name: Label = no_default):\n        \"\"\"\n        Create a new Index with the same class as the caller, don't copy the\n        data, use the same object attributes with passed in attributes taking\n        precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        name : Label, defaults to self.name\n        \"\"\"\n        name = self.name if name is no_default else name\n\n        if values is not None:\n            return self._simple_new(values, name=name)\n\n        result = self._simple_new(self._values, name=name)\n        result._cache = self._cache\n        return result\n\n    @final\n    def is_(self, other) -> bool:\n        \"\"\"\n        More flexible, faster check like ``is`` but that works through views.\n\n        Note: this is *not* the same as ``Index.identical()``, which checks\n        that metadata is also the same.\n\n        Parameters\n        ----------\n        other : object\n            Other object to compare against.\n\n        Returns\n        -------\n        bool\n            True if both have same underlying data, False otherwise.\n\n        See Also\n        --------\n        Index.identical : Works like ``Index.is_`` but also checks metadata.\n        \"\"\"\n        if self is other:\n            return True\n        elif not hasattr(other, \"_id\"):\n            return False\n        elif self._id is None or other._id is None:\n            return False\n        else:\n            return self._id is other._id\n\n    @final\n    def _reset_identity(self) -> None:\n        \"\"\"\n        Initializes or resets ``_id`` attribute with new object.\n        \"\"\"\n        self._id = _Identity(object())\n\n    @final\n    def _cleanup(self):\n        self._engine.clear_mapping()\n\n    @cache_readonly\n    def _engine(self):\n        # property, for now, slow to look up\n\n        # to avoid a reference cycle, bind `target_values` to a local variable, so\n        # `self` is not passed into the lambda.\n        target_values = self._get_engine_target()\n        return self._engine_type(lambda: target_values, len(self))\n\n    @cache_readonly\n    def _dir_additions_for_owner(self) -> Set[str_t]:\n        \"\"\"\n        Add the string-like labels to the owner dataframe/series dir output.\n\n        If this is a MultiIndex, it's first level values are used.\n        \"\"\"\n        return {\n            c\n            for c in self.unique(level=0)[:100]\n            if isinstance(c, str) and c.isidentifier()\n        }\n\n    # --------------------------------------------------------------------\n    # Array-Like Methods\n\n    # ndarray compat\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the Index.\n        \"\"\"\n        return len(self._data)\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\"\n        The array interface, return my values.\n        \"\"\"\n        return np.asarray(self._data, dtype=dtype)\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc and other functions.\n        \"\"\"\n        result = lib.item_from_zerodim(result)\n        if is_bool_dtype(result) or lib.is_scalar(result) or np.ndim(result) > 1:\n            return result\n\n        attrs = self._get_attributes_dict()\n        return Index(result, **attrs)\n\n    @cache_readonly\n    def dtype(self):\n        \"\"\"\n        Return the dtype object of the underlying data.\n        \"\"\"\n        return self._data.dtype\n\n    @final\n    def ravel(self, order=\"C\"):\n        \"\"\"\n        Return an ndarray of the flattened values of the underlying data.\n\n        Returns\n        -------\n        numpy.ndarray\n            Flattened array.\n\n        See Also\n        --------\n        numpy.ndarray.ravel : Return a flattened array.\n        \"\"\"\n        warnings.warn(\n            \"Index.ravel returning ndarray is deprecated; in a future version \"\n            \"this will return a view on self.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        values = self._get_engine_target()\n        return values.ravel(order=order)\n\n    def view(self, cls=None):\n\n        # we need to see if we are subclassing an\n        # index type here\n        if cls is not None and not hasattr(cls, \"_typ\"):\n            result = self._data.view(cls)\n        else:\n            result = self._shallow_copy()\n        if isinstance(result, Index):\n            result._id = self._id\n        return result\n\n    def astype(self, dtype, copy=True):\n        \"\"\"\n        Create an Index with values cast to dtypes.\n\n        The class of a new Index is determined by dtype. When conversion is\n        impossible, a TypeError exception is raised.\n\n        Parameters\n        ----------\n        dtype : numpy dtype or pandas type\n            Note that any signed integer `dtype` is treated as ``'int64'``,\n            and any unsigned integer `dtype` is treated as ``'uint64'``,\n            regardless of the size.\n        copy : bool, default True\n            By default, astype always returns a newly allocated object.\n            If copy is set to False and internal requirements on dtype are\n            satisfied, the original data is used to create a new Index\n            or the original Index is returned.\n\n        Returns\n        -------\n        Index\n            Index with values cast to specified dtype.\n        \"\"\"\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        if is_dtype_equal(self.dtype, dtype):\n            return self.copy() if copy else self\n\n        elif is_categorical_dtype(dtype):\n            from pandas.core.indexes.category import CategoricalIndex\n\n            return CategoricalIndex(\n                self._values, name=self.name, dtype=dtype, copy=copy\n            )\n\n        elif is_extension_array_dtype(dtype):\n            return Index(np.asarray(self), name=self.name, dtype=dtype, copy=copy)\n\n        try:\n            casted = self._values.astype(dtype, copy=copy)\n        except (TypeError, ValueError) as err:\n            raise TypeError(\n                f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n            ) from err\n        return Index(casted, name=self.name, dtype=dtype)\n\n    _index_shared_docs[\n        \"take\"\n    ] = \"\"\"\n        Return a new %(klass)s of the values selected by the indices.\n\n        For internal compatibility with numpy arrays.\n\n        Parameters\n        ----------\n        indices : list\n            Indices to be taken.\n        axis : int, optional\n            The axis over which to select values, always 0.\n        allow_fill : bool, default True\n        fill_value : bool, default None\n            If allow_fill=True and fill_value is not None, indices specified by\n            -1 is regarded as NA. If Index doesn't hold NA, raise ValueError.\n\n        Returns\n        -------\n        numpy.ndarray\n            Elements of given indices.\n\n        See Also\n        --------\n        numpy.ndarray.take: Return an array formed from the\n            elements of a at the given indices.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):\n        if kwargs:\n            nv.validate_take((), kwargs)\n        indices = ensure_platform_int(indices)\n        allow_fill = self._maybe_disallow_fill(allow_fill, fill_value, indices)\n\n        # Note: we discard fill_value and use self._na_value, only relevant\n        #  in the case where allow_fill is True and fill_value is not None\n        taken = algos.take(\n            self._values, indices, allow_fill=allow_fill, fill_value=self._na_value\n        )\n        return self._shallow_copy(taken)\n\n    def _maybe_disallow_fill(self, allow_fill: bool, fill_value, indices) -> bool:\n        \"\"\"\n        We only use pandas-style take when allow_fill is True _and_\n        fill_value is not None.\n        \"\"\"\n        if allow_fill and fill_value is not None:\n            # only fill if we are passing a non-None fill_value\n            if self._can_hold_na:\n                if (indices < -1).any():\n                    raise ValueError(\n                        \"When allow_fill=True and fill_value is not None, \"\n                        \"all indices must be >= -1\"\n                    )\n            else:\n                cls_name = type(self).__name__\n                raise ValueError(\n                    f\"Unable to fill values because {cls_name} cannot contain NA\"\n                )\n        else:\n            allow_fill = False\n        return allow_fill\n\n    _index_shared_docs[\n        \"repeat\"\n    ] = \"\"\"\n        Repeat elements of a %(klass)s.\n\n        Returns a new %(klass)s where each element of the current %(klass)s\n        is repeated consecutively a given number of times.\n\n        Parameters\n        ----------\n        repeats : int or array of ints\n            The number of repetitions for each element. This should be a\n            non-negative integer. Repeating 0 times will return an empty\n            %(klass)s.\n        axis : None\n            Must be ``None``. Has no effect but is accepted for compatibility\n            with numpy.\n\n        Returns\n        -------\n        repeated_index : %(klass)s\n            Newly created %(klass)s with repeated elements.\n\n        See Also\n        --------\n        Series.repeat : Equivalent function for Series.\n        numpy.repeat : Similar method for :class:`numpy.ndarray`.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n        >>> idx.repeat(2)\n        Index(['a', 'a', 'b', 'b', 'c', 'c'], dtype='object')\n        >>> idx.repeat([1, 2, 3])\n        Index(['a', 'b', 'b', 'c', 'c', 'c'], dtype='object')\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n    def repeat(self, repeats, axis=None):\n        repeats = ensure_platform_int(repeats)\n        nv.validate_repeat((), {\"axis\": axis})\n        return self._shallow_copy(self._values.repeat(repeats))\n\n    # --------------------------------------------------------------------\n    # Copying Methods\n\n    def copy(\n        self: _IndexT,\n        name: Optional[Label] = None,\n        deep: bool = False,\n        dtype: Optional[Dtype] = None,\n        names: Optional[Sequence[Label]] = None,\n    ) -> _IndexT:\n        \"\"\"\n        Make a copy of this object.\n\n        Name and dtype sets those attributes on the new object.\n\n        Parameters\n        ----------\n        name : Label, optional\n            Set name for new object.\n        deep : bool, default False\n        dtype : numpy dtype or pandas type, optional\n            Set dtype for new object.\n\n            .. deprecated:: 1.2.0\n                use ``astype`` method instead.\n        names : list-like, optional\n            Kept for compatibility with MultiIndex. Should not be used.\n\n        Returns\n        -------\n        Index\n            Index refer to new object which is a copy of this object.\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        \"\"\"\n        name = self._validate_names(name=name, names=names, deep=deep)[0]\n        if deep:\n            new_index = self._shallow_copy(self._data.copy(), name=name)\n        else:\n            new_index = self._shallow_copy(name=name)\n\n        if dtype:\n            warnings.warn(\n                \"parameter dtype is deprecated and will be removed in a future \"\n                \"version. Use the astype method instead.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    @final\n    def __copy__(self, **kwargs):\n        return self.copy(**kwargs)\n\n    @final\n    def __deepcopy__(self, memo=None):\n        \"\"\"\n        Parameters\n        ----------\n        memo, default None\n            Standard signature. Unused\n        \"\"\"\n        return self.copy(deep=True)\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    def __repr__(self) -> str_t:\n        \"\"\"\n        Return a string representation for this object.\n        \"\"\"\n        klass_name = type(self).__name__\n        data = self._format_data()\n        attrs = self._format_attrs()\n        space = self._format_space()\n        attrs_str = [f\"{k}={v}\" for k, v in attrs]\n        prepr = f\",{space}\".join(attrs_str)\n\n        # no data provided, just attributes\n        if data is None:\n            data = \"\"\n\n        res = f\"{klass_name}({data}{prepr})\"\n\n        return res\n\n    def _format_space(self) -> str_t:\n\n        # using space here controls if the attributes\n        # are line separated or not (the default)\n\n        # max_seq_items = get_option('display.max_seq_items')\n        # if len(self) > max_seq_items:\n        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n        return \" \"\n\n    @property\n    def _formatter_func(self):\n        \"\"\"\n        Return the formatter function.\n        \"\"\"\n        return default_pprint\n\n    def _format_data(self, name=None) -> str_t:\n        \"\"\"\n        Return the formatted data as a unicode string.\n        \"\"\"\n        # do we want to justify (only do so for non-objects)\n        is_justify = True\n\n        if self.inferred_type == \"string\":\n            is_justify = False\n        elif self.inferred_type == \"categorical\":\n            # error: \"Index\" has no attribute \"categories\"\n            if is_object_dtype(self.categories):  # type: ignore[attr-defined]\n                is_justify = False\n\n        return format_object_summary(\n            self, self._formatter_func, is_justify=is_justify, name=name\n        )\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        return format_object_attrs(self)\n\n    def _mpl_repr(self):\n        # how to represent ourselves to matplotlib\n        return self.values\n\n    def format(\n        self,\n        name: bool = False,\n        formatter: Optional[Callable] = None,\n        na_rep: str_t = \"NaN\",\n    ) -> List[str_t]:\n        \"\"\"\n        Render a string representation of the Index.\n        \"\"\"\n        header = []\n        if name:\n            header.append(\n                pprint_thing(self.name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                if self.name is not None\n                else \"\"\n            )\n\n        if formatter is not None:\n            return header + list(self.map(formatter))\n\n        return self._format_with_header(header, na_rep=na_rep)\n\n    def _format_with_header(\n        self, header: List[str_t], na_rep: str_t = \"NaN\"\n    ) -> List[str_t]:\n        from pandas.io.formats.format import format_array\n\n        values = self._values\n\n        if is_object_dtype(values.dtype):\n            values = lib.maybe_convert_objects(values, safe=1)\n\n        if is_object_dtype(values.dtype):\n            result = [pprint_thing(x, escape_chars=(\"\\t\", \"\\r\", \"\\n\")) for x in values]\n\n            # could have nans\n            mask = isna(values)\n            if mask.any():\n                result_arr = np.array(result)\n                result_arr[mask] = na_rep\n                result = result_arr.tolist()\n        else:\n            result = trim_front(format_array(values, None, justify=\"left\"))\n        return header + result\n\n    def to_native_types(self, slicer=None, **kwargs):\n        \"\"\"\n        Format specified values of `self` and return them.\n\n        .. deprecated:: 1.2.0\n\n        Parameters\n        ----------\n        slicer : int, array-like\n            An indexer into `self` that specifies which values\n            are used in the formatting process.\n        kwargs : dict\n            Options for specifying how the values should be formatted.\n            These options include the following:\n\n            1) na_rep : str\n                The value that serves as a placeholder for NULL values\n            2) quoting : bool or None\n                Whether or not there are quoted values in `self`\n            3) date_format : str\n                The format used to represent date-like values.\n\n        Returns\n        -------\n        numpy.ndarray\n            Formatted values.\n        \"\"\"\n        warnings.warn(\n            \"The 'to_native_types' method is deprecated and will be removed in \"\n            \"a future version. Use 'astype(str)' instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        values = self\n        if slicer is not None:\n            values = values[slicer]\n        return values._format_native_types(**kwargs)\n\n    def _format_native_types(self, na_rep=\"\", quoting=None, **kwargs):\n        \"\"\"\n        Actually format specific types of the index.\n        \"\"\"\n        mask = isna(self)\n        if not self.is_object() and not quoting:\n            values = np.asarray(self).astype(str)\n        else:\n            values = np.array(self, dtype=object, copy=True)\n\n        values[mask] = na_rep\n        return values\n\n    def _summary(self, name=None) -> str_t:\n        \"\"\"\n        Return a summarized representation.\n\n        Parameters\n        ----------\n        name : str\n            name to use in the summary representation\n\n        Returns\n        -------\n        String with a summarized representation of the index\n        \"\"\"\n        if len(self) > 0:\n            head = self[0]\n            if hasattr(head, \"format\") and not isinstance(head, str):\n                head = head.format()\n            tail = self[-1]\n            if hasattr(tail, \"format\") and not isinstance(tail, str):\n                tail = tail.format()\n            index_summary = f\", {head} to {tail}\"\n        else:\n            index_summary = \"\"\n\n        if name is None:\n            name = type(self).__name__\n        return f\"{name}: {len(self)} entries{index_summary}\"\n\n    # --------------------------------------------------------------------\n    # Conversion Methods\n\n    def to_flat_index(self):\n        \"\"\"\n        Identity method.\n\n        .. versionadded:: 0.24.0\n\n        This is implemented for compatibility with subclass implementations\n        when chaining.\n\n        Returns\n        -------\n        pd.Index\n            Caller.\n\n        See Also\n        --------\n        MultiIndex.to_flat_index : Subclass implementation.\n        \"\"\"\n        return self\n\n    def to_series(self, index=None, name=None):\n        \"\"\"\n        Create a Series with both index and values equal to the index keys.\n\n        Useful with map for returning an indexer based on an index.\n\n        Parameters\n        ----------\n        index : Index, optional\n            Index of resulting Series. If None, defaults to original index.\n        name : str, optional\n            Name of resulting Series. If None, defaults to name of original\n            index.\n\n        Returns\n        -------\n        Series\n            The dtype will be based on the type of the Index values.\n\n        See Also\n        --------\n        Index.to_frame : Convert an Index to a DataFrame.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n\n        By default, the original Index and original name is reused.\n\n        >>> idx.to_series()\n        animal\n        Ant      Ant\n        Bear    Bear\n        Cow      Cow\n        Name: animal, dtype: object\n\n        To enforce a new Index, specify new labels to ``index``:\n\n        >>> idx.to_series(index=[0, 1, 2])\n        0     Ant\n        1    Bear\n        2     Cow\n        Name: animal, dtype: object\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_series(name='zoo')\n        animal\n        Ant      Ant\n        Bear    Bear\n        Cow      Cow\n        Name: zoo, dtype: object\n        \"\"\"\n        from pandas import Series\n\n        if index is None:\n            index = self._shallow_copy()\n        if name is None:\n            name = self.name\n\n        return Series(self.values.copy(), index=index, name=name)\n\n    def to_frame(self, index: bool = True, name=None):\n        \"\"\"\n        Create a DataFrame with a column containing the Index.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : bool, default True\n            Set the index of the returned DataFrame as the original Index.\n\n        name : object, default None\n            The passed name should substitute for the index name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame containing the original Index data.\n\n        See Also\n        --------\n        Index.to_series : Convert an Index to a Series.\n        Series.to_frame : Convert Series to DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n        >>> idx.to_frame()\n               animal\n        animal\n        Ant       Ant\n        Bear     Bear\n        Cow       Cow\n\n        By default, the original Index is reused. To enforce a new Index:\n\n        >>> idx.to_frame(index=False)\n            animal\n        0   Ant\n        1  Bear\n        2   Cow\n\n        To override the name of the resulting column, specify `name`:\n\n        >>> idx.to_frame(index=False, name='zoo')\n            zoo\n        0   Ant\n        1  Bear\n        2   Cow\n        \"\"\"\n        from pandas import DataFrame\n\n        if name is None:\n            name = self.name or 0\n        result = DataFrame({name: self._values.copy()})\n\n        if index:\n            result.index = self\n        return result\n\n    # --------------------------------------------------------------------\n    # Name-Centric Methods\n\n    @property\n    def name(self):\n        \"\"\"\n        Return Index or MultiIndex name.\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        if self._no_setting_name:\n            # Used in MultiIndex.levels to avoid silently ignoring name updates.\n            raise RuntimeError(\n                \"Cannot set name on a level of a MultiIndex. Use \"\n                \"'MultiIndex.set_names' instead.\"\n            )\n        maybe_extract_name(value, None, type(self))\n        self._name = value\n\n    @final\n    def _validate_names(self, name=None, names=None, deep: bool = False) -> List[Label]:\n        \"\"\"\n        Handles the quirks of having a singular 'name' parameter for general\n        Index and plural 'names' parameter for MultiIndex.\n        \"\"\"\n        from copy import deepcopy\n\n        if names is not None and name is not None:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        elif names is None and name is None:\n            new_names = deepcopy(self.names) if deep else self.names\n        elif names is not None:\n            if not is_list_like(names):\n                raise TypeError(\"Must pass list-like as `names`.\")\n            new_names = names\n        elif not is_list_like(name):\n            new_names = [name]\n        else:\n            new_names = name\n\n        if len(new_names) != len(self.names):\n            raise ValueError(\n                f\"Length of new names must be {len(self.names)}, got {len(new_names)}\"\n            )\n\n        # All items in 'new_names' need to be hashable\n        validate_all_hashable(*new_names, error_name=f\"{type(self).__name__}.name\")\n\n        return new_names\n\n    def _get_names(self):\n        return FrozenList((self.name,))\n\n    def _set_names(self, values, level=None):\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n        \"\"\"\n        if not is_list_like(values):\n            raise ValueError(\"Names must be a list-like\")\n        if len(values) != 1:\n            raise ValueError(f\"Length of new names must be 1, got {len(values)}\")\n\n        # GH 20527\n        # All items in 'name' need to be hashable:\n        validate_all_hashable(*values, error_name=f\"{type(self).__name__}.name\")\n\n        self._name = values[0]\n\n    names = property(fset=_set_names, fget=_get_names)\n\n    @final\n    def set_names(self, names, level=None, inplace: bool = False):\n        \"\"\"\n        Set Index or MultiIndex name.\n\n        Able to set new names partially and by level.\n\n        Parameters\n        ----------\n        names : label or list of label\n            Name(s) to set.\n        level : int, label or list of int or label, optional\n            If the index is a MultiIndex, level(s) to set (None for all\n            levels). Otherwise level must be None.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index or None\n            The same type as the caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.rename : Able to set new names without level.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n        >>> idx.set_names('quarter')\n        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   )\n        >>> idx.set_names(['kind', 'year'], inplace=True)\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.set_names('species', level=0)\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        \"\"\"\n        if level is not None and not isinstance(self, ABCMultiIndex):\n            raise ValueError(\"Level must be None for non-MultiIndex\")\n\n        if level is not None and not is_list_like(level) and is_list_like(names):\n            raise TypeError(\"Names must be a string when a single level is provided.\")\n\n        if not is_list_like(names) and level is None and self.nlevels > 1:\n            raise TypeError(\"Must pass list-like as `names`.\")\n\n        if not is_list_like(names):\n            names = [names]\n        if level is not None and not is_list_like(level):\n            level = [level]\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._set_names(names, level=level)\n        if not inplace:\n            return idx\n\n    def rename(self, name, inplace=False):\n        \"\"\"\n        Alter Index or MultiIndex name.\n\n        Able to set new names without level. Defaults to returning new index.\n        Length of names must match number of levels in MultiIndex.\n\n        Parameters\n        ----------\n        name : label or list of labels\n            Name(s) to set.\n        inplace : bool, default False\n            Modifies the object directly, instead of creating a new Index or\n            MultiIndex.\n\n        Returns\n        -------\n        Index or None\n            The same type as the caller or None if ``inplace=True``.\n\n        See Also\n        --------\n        Index.set_names : Able to set new names partially and by level.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n        >>> idx.rename('grade')\n        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n\n        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n        ...                                   [2018, 2019]],\n        ...                                   names=['kind', 'year'])\n        >>> idx\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['kind', 'year'])\n        >>> idx.rename(['species', 'year'])\n        MultiIndex([('python', 2018),\n                    ('python', 2019),\n                    ( 'cobra', 2018),\n                    ( 'cobra', 2019)],\n                   names=['species', 'year'])\n        >>> idx.rename('species')\n        Traceback (most recent call last):\n        TypeError: Must pass list-like as `names`.\n        \"\"\"\n        return self.set_names([name], inplace=inplace)\n\n    # --------------------------------------------------------------------\n    # Level-Centric Methods\n\n    @property\n    def nlevels(self) -> int:\n        \"\"\"\n        Number of levels.\n        \"\"\"\n        return 1\n\n    def _sort_levels_monotonic(self):\n        \"\"\"\n        Compat with MultiIndex.\n        \"\"\"\n        return self\n\n    @final\n    def _validate_index_level(self, level):\n        \"\"\"\n        Validate index level.\n\n        For single-level Index getting level number is a no-op, but some\n        verification must be done like in MultiIndex.\n\n        \"\"\"\n        if isinstance(level, int):\n            if level < 0 and level != -1:\n                raise IndexError(\n                    \"Too many levels: Index has only 1 level, \"\n                    f\"{level} is not a valid level number\"\n                )\n            elif level > 0:\n                raise IndexError(\n                    f\"Too many levels: Index has only 1 level, not {level + 1}\"\n                )\n        elif level != self.name:\n            raise KeyError(\n                f\"Requested level ({level}) does not match index name ({self.name})\"\n            )\n\n    def _get_level_number(self, level) -> int:\n        self._validate_index_level(level)\n        return 0\n\n    def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n        \"\"\"\n        For internal compatibility with the Index API.\n\n        Sort the Index. This is for compat with MultiIndex\n\n        Parameters\n        ----------\n        ascending : bool, default True\n            False to sort in descending order\n\n        level, sort_remaining are compat parameters\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        if not isinstance(ascending, (list, bool)):\n            raise TypeError(\n                \"ascending must be a single bool value or\"\n                \"a list of bool values of length 1\"\n            )\n\n        if isinstance(ascending, list):\n            if len(ascending) != 1:\n                raise TypeError(\"ascending must be a list of bool values of length 1\")\n            ascending = ascending[0]\n\n        if not isinstance(ascending, bool):\n            raise TypeError(\"ascending must be a bool value\")\n\n        return self.sort_values(return_indexer=True, ascending=ascending)\n\n    def _get_level_values(self, level):\n        \"\"\"\n        Return an Index of values for requested level.\n\n        This is primarily useful to get an individual level of values from a\n        MultiIndex, but is provided on Index as well for compatibility.\n\n        Parameters\n        ----------\n        level : int or str\n            It is either the integer position or the name of the level.\n\n        Returns\n        -------\n        Index\n            Calling object, as there is only one level in the Index.\n\n        See Also\n        --------\n        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n\n        Notes\n        -----\n        For Index, level should be 0, since there are no multiple levels.\n\n        Examples\n        --------\n        >>> idx = pd.Index(list('abc'))\n        >>> idx\n        Index(['a', 'b', 'c'], dtype='object')\n\n        Get level values by supplying `level` as integer:\n\n        >>> idx.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object')\n        \"\"\"\n        self._validate_index_level(level)\n        return self\n\n    get_level_values = _get_level_values\n\n    @final\n    def droplevel(self, level=0):\n        \"\"\"\n        Return index with requested level(s) removed.\n\n        If resulting index has only 1 level left, the result will be\n        of Index type, not MultiIndex.\n\n        Parameters\n        ----------\n        level : int, str, or list-like, default 0\n            If a string is given, must be the name of a level\n            If list-like, elements must be names or indexes of levels.\n\n        Returns\n        -------\n        Index or MultiIndex\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays(\n        ... [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z'])\n        >>> mi\n        MultiIndex([(1, 3, 5),\n                    (2, 4, 6)],\n                   names=['x', 'y', 'z'])\n\n        >>> mi.droplevel()\n        MultiIndex([(3, 5),\n                    (4, 6)],\n                   names=['y', 'z'])\n\n        >>> mi.droplevel(2)\n        MultiIndex([(1, 3),\n                    (2, 4)],\n                   names=['x', 'y'])\n\n        >>> mi.droplevel('z')\n        MultiIndex([(1, 3),\n                    (2, 4)],\n                   names=['x', 'y'])\n\n        >>> mi.droplevel(['x', 'y'])\n        Int64Index([5, 6], dtype='int64', name='z')\n        \"\"\"\n        if not isinstance(level, (tuple, list)):\n            level = [level]\n\n        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n\n        return self._drop_level_numbers(levnums)\n\n    def _drop_level_numbers(self, levnums: List[int]):\n        \"\"\"\n        Drop MultiIndex levels by level _number_, not name.\n        \"\"\"\n\n        if len(levnums) == 0:\n            return self\n        if len(levnums) >= self.nlevels:\n            raise ValueError(\n                f\"Cannot remove {len(levnums)} levels from an index with \"\n                f\"{self.nlevels} levels: at least one level must be left.\"\n            )\n        # The two checks above guarantee that here self is a MultiIndex\n        self = cast(\"MultiIndex\", self)\n\n        new_levels = list(self.levels)\n        new_codes = list(self.codes)\n        new_names = list(self.names)\n\n        for i in levnums:\n            new_levels.pop(i)\n            new_codes.pop(i)\n            new_names.pop(i)\n\n        if len(new_levels) == 1:\n\n            # set nan if needed\n            mask = new_codes[0] == -1\n            result = new_levels[0].take(new_codes[0])\n            if mask.any():\n                result = result.putmask(mask, np.nan)\n\n            result._name = new_names[0]\n            return result\n        else:\n            from pandas.core.indexes.multi import MultiIndex\n\n            return MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=new_names,\n                verify_integrity=False,\n            )\n\n    def _get_grouper_for_level(self, mapper, level=None):\n        \"\"\"\n        Get index grouper corresponding to an index level\n\n        Parameters\n        ----------\n        mapper: Group mapping function or None\n            Function mapping index values to groups\n        level : int or None\n            Index level\n\n        Returns\n        -------\n        grouper : Index\n            Index of values to group on.\n        labels : ndarray of int or None\n            Array of locations in level_index.\n        uniques : Index or None\n            Index of unique values for level.\n        \"\"\"\n        assert level is None or level == 0\n        if mapper is None:\n            grouper = self\n        else:\n            grouper = self.map(mapper)\n\n        return grouper, None, None\n\n    # --------------------------------------------------------------------\n    # Introspection Methods\n\n    @final\n    @property\n    def is_monotonic(self) -> bool:\n        \"\"\"\n        Alias for is_monotonic_increasing.\n        \"\"\"\n        return self.is_monotonic_increasing\n\n    @property\n    def is_monotonic_increasing(self) -> bool:\n        \"\"\"\n        Return if the index is monotonic increasing (only equal or\n        increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3]).is_monotonic_increasing\n        True\n        >>> Index([1, 2, 2]).is_monotonic_increasing\n        True\n        >>> Index([1, 3, 2]).is_monotonic_increasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_increasing\n\n    @property\n    def is_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1]).is_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2]).is_monotonic_decreasing\n        True\n        >>> Index([3, 1, 2]).is_monotonic_decreasing\n        False\n        \"\"\"\n        return self._engine.is_monotonic_decreasing\n\n    @property\n    def _is_strictly_monotonic_increasing(self) -> bool:\n        \"\"\"\n        Return if the index is strictly monotonic increasing\n        (only increasing) values.\n\n        Examples\n        --------\n        >>> Index([1, 2, 3])._is_strictly_monotonic_increasing\n        True\n        >>> Index([1, 2, 2])._is_strictly_monotonic_increasing\n        False\n        >>> Index([1, 3, 2])._is_strictly_monotonic_increasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_increasing\n\n    @property\n    def _is_strictly_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return if the index is strictly monotonic decreasing\n        (only decreasing) values.\n\n        Examples\n        --------\n        >>> Index([3, 2, 1])._is_strictly_monotonic_decreasing\n        True\n        >>> Index([3, 2, 2])._is_strictly_monotonic_decreasing\n        False\n        >>> Index([3, 1, 2])._is_strictly_monotonic_decreasing\n        False\n        \"\"\"\n        return self.is_unique and self.is_monotonic_decreasing\n\n    @cache_readonly\n    def is_unique(self) -> bool:\n        \"\"\"\n        Return if the index has unique values.\n        \"\"\"\n        return self._engine.is_unique\n\n    @property\n    def has_duplicates(self) -> bool:\n        \"\"\"\n        Check if the Index has duplicate values.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index has duplicate values.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 5, 7, 7])\n        >>> idx.has_duplicates\n        True\n\n        >>> idx = pd.Index([1, 5, 7])\n        >>> idx.has_duplicates\n        False\n\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.has_duplicates\n        True\n\n        >>> idx = pd.Index([\"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.has_duplicates\n        False\n        \"\"\"\n        return not self.is_unique\n\n    @final\n    def is_boolean(self) -> bool:\n        \"\"\"\n        Check if the Index only consists of booleans.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of booleans.\n\n        See Also\n        --------\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([True, False, True])\n        >>> idx.is_boolean()\n        True\n\n        >>> idx = pd.Index([\"True\", \"False\", \"True\"])\n        >>> idx.is_boolean()\n        False\n\n        >>> idx = pd.Index([True, False, \"True\"])\n        >>> idx.is_boolean()\n        False\n        \"\"\"\n        return self.inferred_type in [\"boolean\"]\n\n    @final\n    def is_integer(self) -> bool:\n        \"\"\"\n        Check if the Index only consists of integers.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of integers.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx.is_integer()\n        True\n\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_integer()\n        False\n\n        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n        >>> idx.is_integer()\n        False\n        \"\"\"\n        return self.inferred_type in [\"integer\"]\n\n    @final\n    def is_floating(self) -> bool:\n        \"\"\"\n        Check if the Index is a floating type.\n\n        The Index may consist of only floats, NaNs, or a mix of floats,\n        integers, or NaNs.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of only consists of floats, NaNs, or\n            a mix of floats, integers, or NaNs.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_floating()\n        True\n\n        >>> idx = pd.Index([1.0, 2.0, np.nan, 4.0])\n        >>> idx.is_floating()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4, np.nan])\n        >>> idx.is_floating()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx.is_floating()\n        False\n        \"\"\"\n        return self.inferred_type in [\"floating\", \"mixed-integer-float\", \"integer-na\"]\n\n    @final\n    def is_numeric(self) -> bool:\n        \"\"\"\n        Check if the Index only consists of numeric data.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index only consists of numeric data.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4.0])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan])\n        >>> idx.is_numeric()\n        True\n\n        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan, \"Apple\"])\n        >>> idx.is_numeric()\n        False\n        \"\"\"\n        return self.inferred_type in [\"integer\", \"floating\"]\n\n    @final\n    def is_object(self) -> bool:\n        \"\"\"\n        Check if the Index is of the object dtype.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index is of the object dtype.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n        >>> idx.is_object()\n        True\n\n        >>> idx = pd.Index([\"Apple\", \"Mango\", 2.0])\n        >>> idx.is_object()\n        True\n\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_object()\n        False\n\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n        >>> idx.is_object()\n        False\n        \"\"\"\n        return is_object_dtype(self.dtype)\n\n    @final\n    def is_categorical(self) -> bool:\n        \"\"\"\n        Check if the Index holds categorical data.\n\n        Returns\n        -------\n        bool\n            True if the Index is categorical.\n\n        See Also\n        --------\n        CategoricalIndex : Index for categorical data.\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_interval : Check if the Index holds Interval objects.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n        ...                 \"Watermelon\"]).astype(\"category\")\n        >>> idx.is_categorical()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_categorical()\n        False\n\n        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n        >>> s\n        0        Peter\n        1       Victor\n        2    Elisabeth\n        3          Mar\n        dtype: object\n        >>> s.index.is_categorical()\n        False\n        \"\"\"\n        return self.inferred_type in [\"categorical\"]\n\n    @final\n    def is_interval(self) -> bool:\n        \"\"\"\n        Check if the Index holds Interval objects.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index holds Interval objects.\n\n        See Also\n        --------\n        IntervalIndex : Index for Interval objects.\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_mixed : Check if the Index holds data with mixed data types.\n\n        Examples\n        --------\n        >>> idx = pd.Index([pd.Interval(left=0, right=5),\n        ...                 pd.Interval(left=5, right=10)])\n        >>> idx.is_interval()\n        True\n\n        >>> idx = pd.Index([1, 3, 5, 7])\n        >>> idx.is_interval()\n        False\n        \"\"\"\n        return self.inferred_type in [\"interval\"]\n\n    @final\n    def is_mixed(self) -> bool:\n        \"\"\"\n        Check if the Index holds data with mixed data types.\n\n        Returns\n        -------\n        bool\n            Whether or not the Index holds data with mixed data types.\n\n        See Also\n        --------\n        is_boolean : Check if the Index only consists of booleans.\n        is_integer : Check if the Index only consists of integers.\n        is_floating : Check if the Index is a floating type.\n        is_numeric : Check if the Index only consists of numeric data.\n        is_object : Check if the Index is of the object dtype.\n        is_categorical : Check if the Index holds categorical data.\n        is_interval : Check if the Index holds Interval objects.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', np.nan, 'b'])\n        >>> idx.is_mixed()\n        True\n\n        >>> idx = pd.Index([1.0, 2.0, 3.0, 5.0])\n        >>> idx.is_mixed()\n        False\n        \"\"\"\n        warnings.warn(\n            \"Index.is_mixed is deprecated and will be removed in a future version. \"\n            \"Check index.inferred_type directly instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self.inferred_type in [\"mixed\"]\n\n    @final\n    def holds_integer(self) -> bool:\n        \"\"\"\n        Whether the type is an integer type.\n        \"\"\"\n        return self.inferred_type in [\"integer\", \"mixed-integer\"]\n\n    @cache_readonly\n    def inferred_type(self) -> str_t:\n        \"\"\"\n        Return a string of the type inferred from the values.\n        \"\"\"\n        return lib.infer_dtype(self._values, skipna=False)\n\n    @cache_readonly\n    def _is_all_dates(self) -> bool:\n        \"\"\"\n        Whether or not the index values only consist of dates.\n        \"\"\"\n        return is_datetime_array(ensure_object(self._values))\n\n    @cache_readonly\n    def is_all_dates(self):\n        \"\"\"\n        Whether or not the index values only consist of dates.\n        \"\"\"\n        warnings.warn(\n            \"Index.is_all_dates is deprecated, will be removed in a future version.  \"\n            \"check index.inferred_type instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self._is_all_dates\n\n    # --------------------------------------------------------------------\n    # Pickle Methods\n\n    def __reduce__(self):\n        d = {\"data\": self._data}\n        d.update(self._get_attributes_dict())\n        return _new_Index, (type(self), d), None\n\n    # --------------------------------------------------------------------\n    # Null Handling Methods\n\n    _na_value = np.nan\n    \"\"\"The expected NA value to use with this index.\"\"\"\n\n    @cache_readonly\n    def _isnan(self):\n        \"\"\"\n        Return if each value is NaN.\n        \"\"\"\n        if self._can_hold_na:\n            return isna(self)\n        else:\n            # shouldn't reach to this condition by checking hasnans beforehand\n            values = np.empty(len(self), dtype=np.bool_)\n            values.fill(False)\n            return values\n\n    @cache_readonly\n    @final\n    def _nan_idxs(self):\n        if self._can_hold_na:\n            return self._isnan.nonzero()[0]\n        else:\n            return np.array([], dtype=np.int64)\n\n    @cache_readonly\n    def hasnans(self) -> bool:\n        \"\"\"\n        Return if I have any nans; enables various perf speedups.\n        \"\"\"\n        if self._can_hold_na:\n            return bool(self._isnan.any())\n        else:\n            return False\n\n    @final\n    def isna(self):\n        \"\"\"\n        Detect missing values.\n\n        Return a boolean same-sized object indicating if the values are NA.\n        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n        mapped to ``True`` values.\n        Everything else get mapped to ``False`` values. Characters such as\n        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\n        Returns\n        -------\n        numpy.ndarray\n            A boolean array of whether my values are NA.\n\n        See Also\n        --------\n        Index.notna : Boolean inverse of isna.\n        Index.dropna : Omit entries with missing values.\n        isna : Top-level isna.\n        Series.isna : Detect missing values in Series object.\n\n        Examples\n        --------\n        Show which entries in a pandas.Index are NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.isna()\n        array([False, False,  True])\n\n        Empty strings are not considered NA values. None is considered an NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.isna()\n        array([False, False, False,  True])\n\n        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n\n        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n        ...                         pd.Timestamp(''), None, pd.NaT])\n        >>> idx\n        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.isna()\n        array([False,  True,  True,  True])\n        \"\"\"\n        return self._isnan\n\n    isnull = isna\n\n    @final\n    def notna(self):\n        \"\"\"\n        Detect existing (non-missing) values.\n\n        Return a boolean same-sized object indicating if the values are not NA.\n        Non-missing values get mapped to ``True``. Characters such as empty\n        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n        values.\n\n        Returns\n        -------\n        numpy.ndarray\n            Boolean array to indicate which entries are not NA.\n\n        See Also\n        --------\n        Index.notnull : Alias of notna.\n        Index.isna: Inverse of notna.\n        notna : Top-level notna.\n\n        Examples\n        --------\n        Show which entries in an Index are not NA. The result is an\n        array.\n\n        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n        >>> idx\n        Float64Index([5.2, 6.0, nan], dtype='float64')\n        >>> idx.notna()\n        array([ True,  True, False])\n\n        Empty strings are not considered NA values. None is considered a NA\n        value.\n\n        >>> idx = pd.Index(['black', '', 'red', None])\n        >>> idx\n        Index(['black', '', 'red', None], dtype='object')\n        >>> idx.notna()\n        array([ True,  True,  True, False])\n        \"\"\"\n        return ~self.isna()\n\n    notnull = notna\n\n    def fillna(self, value=None, downcast=None):\n        \"\"\"\n        Fill NA/NaN values with the specified value.\n\n        Parameters\n        ----------\n        value : scalar\n            Scalar value to use to fill holes (e.g. 0).\n            This value cannot be a list-likes.\n        downcast : dict, default is None\n            A dict of item->dtype of what to downcast if possible,\n            or the string 'infer' which will try to downcast to an appropriate\n            equal type (e.g. float64 to int64 if possible).\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        DataFrame.fillna : Fill NaN values of a DataFrame.\n        Series.fillna : Fill NaN Values of a Series.\n        \"\"\"\n        value = self._require_scalar(value)\n        if self.hasnans:\n            result = self.putmask(self._isnan, value)\n            if downcast is None:\n                # no need to care metadata other than name\n                # because it can't have freq if\n                return Index(result, name=self.name)\n        return self._shallow_copy()\n\n    def dropna(self, how=\"any\"):\n        \"\"\"\n        Return Index without NA/NaN values.\n\n        Parameters\n        ----------\n        how : {'any', 'all'}, default 'any'\n            If the Index is a MultiIndex, drop the value when any or all levels\n            are NaN.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        if how not in (\"any\", \"all\"):\n            raise ValueError(f\"invalid how option: {how}\")\n\n        if self.hasnans:\n            return self._shallow_copy(self._values[~self._isnan])\n        return self._shallow_copy()\n\n    # --------------------------------------------------------------------\n    # Uniqueness Methods\n\n    def unique(self, level=None):\n        \"\"\"\n        Return unique values in the index.\n\n        Unique values are returned in order of appearance, this does NOT sort.\n\n        Parameters\n        ----------\n        level : int or str, optional, default None\n            Only return values from specified level (for MultiIndex).\n\n        Returns\n        -------\n        Index without duplicates\n\n        See Also\n        --------\n        unique : Numpy array of unique values in that column.\n        Series.unique : Return unique values of Series object.\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n\n        if self.is_unique:\n            return self._shallow_copy()\n\n        result = super().unique()\n        return self._shallow_copy(result)\n\n    @final\n    def drop_duplicates(self, keep=\"first\"):\n        \"\"\"\n        Return Index with duplicate values removed.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', ``False``}, default 'first'\n            - 'first' : Drop duplicates except for the first occurrence.\n            - 'last' : Drop duplicates except for the last occurrence.\n            - ``False`` : Drop all duplicates.\n\n        Returns\n        -------\n        deduplicated : Index\n\n        See Also\n        --------\n        Series.drop_duplicates : Equivalent method on Series.\n        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n        Index.duplicated : Related method on Index, indicating duplicate\n            Index values.\n\n        Examples\n        --------\n        Generate an pandas.Index with duplicate values.\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n\n        The `keep` parameter controls  which duplicate values are removed.\n        The value 'first' keeps the first occurrence for each\n        set of duplicated entries. The default value of keep is 'first'.\n\n        >>> idx.drop_duplicates(keep='first')\n        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n\n        The value 'last' keeps the last occurrence for each set of duplicated\n        entries.\n\n        >>> idx.drop_duplicates(keep='last')\n        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n\n        The value ``False`` discards all sets of duplicated entries.\n\n        >>> idx.drop_duplicates(keep=False)\n        Index(['cow', 'beetle', 'hippo'], dtype='object')\n        \"\"\"\n        if self.is_unique:\n            return self._shallow_copy()\n\n        return super().drop_duplicates(keep=keep)\n\n    def duplicated(self, keep=\"first\"):\n        \"\"\"\n        Indicate duplicate index values.\n\n        Duplicated values are indicated as ``True`` values in the resulting\n        array. Either all duplicates, all except the first, or all except the\n        last occurrence of duplicates can be indicated.\n\n        Parameters\n        ----------\n        keep : {'first', 'last', False}, default 'first'\n            The value or values in a set of duplicates to mark as missing.\n\n            - 'first' : Mark duplicates as ``True`` except for the first\n              occurrence.\n            - 'last' : Mark duplicates as ``True`` except for the last\n              occurrence.\n            - ``False`` : Mark all duplicates as ``True``.\n\n        Returns\n        -------\n        numpy.ndarray\n\n        See Also\n        --------\n        Series.duplicated : Equivalent method on pandas.Series.\n        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n        Index.drop_duplicates : Remove duplicate values from Index.\n\n        Examples\n        --------\n        By default, for each set of duplicated values, the first occurrence is\n        set to False and all others to True:\n\n        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n        >>> idx.duplicated()\n        array([False, False,  True, False,  True])\n\n        which is equivalent to\n\n        >>> idx.duplicated(keep='first')\n        array([False, False,  True, False,  True])\n\n        By using 'last', the last occurrence of each set of duplicated values\n        is set on False and all others on True:\n\n        >>> idx.duplicated(keep='last')\n        array([ True, False,  True, False, False])\n\n        By setting keep on ``False``, all duplicates are True:\n\n        >>> idx.duplicated(keep=False)\n        array([ True, False,  True, False,  True])\n        \"\"\"\n        if self.is_unique:\n            # fastpath available bc we are immutable\n            return np.zeros(len(self), dtype=bool)\n        return super().duplicated(keep=keep)\n\n    def _get_unique_index(self, dropna: bool = False):\n        \"\"\"\n        Returns an index containing unique values.\n\n        Parameters\n        ----------\n        dropna : bool, default False\n            If True, NaN values are dropped.\n\n        Returns\n        -------\n        uniques : index\n        \"\"\"\n        if self.is_unique and not dropna:\n            return self\n\n        if not self.is_unique:\n            values = self.unique()\n            if not isinstance(self, ABCMultiIndex):\n                # extract an array to pass to _shallow_copy\n                values = values._data\n        else:\n            values = self._values\n\n        if dropna and not isinstance(self, ABCMultiIndex):\n            # isna not defined for MultiIndex\n            if self.hasnans:\n                values = values[~isna(values)]\n\n        return self._shallow_copy(values)\n\n    # --------------------------------------------------------------------\n    # Arithmetic & Logical Methods\n\n    def __iadd__(self, other):\n        # alias for __add__\n        return self + other\n\n    @final\n    def __and__(self, other):\n        warnings.warn(\n            \"Index.__and__ operating as a set operation is deprecated, \"\n            \"in the future this will be a logical operation matching \"\n            \"Series.__and__.  Use index.intersection(other) instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self.intersection(other)\n\n    @final\n    def __or__(self, other):\n        warnings.warn(\n            \"Index.__or__ operating as a set operation is deprecated, \"\n            \"in the future this will be a logical operation matching \"\n            \"Series.__or__.  Use index.union(other) instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self.union(other)\n\n    @final\n    def __xor__(self, other):\n        warnings.warn(\n            \"Index.__xor__ operating as a set operation is deprecated, \"\n            \"in the future this will be a logical operation matching \"\n            \"Series.__xor__.  Use index.symmetric_difference(other) instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self.symmetric_difference(other)\n\n    @final\n    def __nonzero__(self):\n        raise ValueError(\n            f\"The truth value of a {type(self).__name__} is ambiguous. \"\n            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n        )\n\n    __bool__ = __nonzero__\n\n    # --------------------------------------------------------------------\n    # Set Operation Methods\n\n    @final\n    def _get_reconciled_name_object(self, other):\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.\n        \"\"\"\n        name = get_op_result_name(self, other)\n        if self.name != name:\n            return self.rename(name)\n        return self\n\n    @final\n    def _union_incompatible_dtypes(self, other, sort):\n        \"\"\"\n        Casts this and other index to object dtype to allow the formation\n        of a union between incompatible types.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        this = self.astype(object, copy=False)\n        # cast to Index for when `other` is list-like\n        other = Index(other).astype(object, copy=False)\n        return Index.union(this, other, sort=sort).astype(object, copy=False)\n\n    def _can_union_without_object_cast(self, other) -> bool:\n        \"\"\"\n        Check whether this and the other dtype are compatible with each other.\n        Meaning a union can be formed between them without needing to be cast\n        to dtype object.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return type(self) is type(other) and is_dtype_equal(self.dtype, other.dtype)\n\n    @final\n    def _validate_sort_keyword(self, sort):\n        if sort not in [None, False]:\n            raise ValueError(\n                \"The 'sort' keyword only takes the values of \"\n                f\"None or False; {sort} was passed.\"\n            )\n\n    def union(self, other, sort=None):\n        \"\"\"\n        Form the union of two Index objects.\n\n        If the Index objects are incompatible, both Index objects will be\n        cast to dtype('object') first.\n\n            .. versionchanged:: 0.25.0\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : bool or None, default None\n            Whether to sort the resulting Index.\n\n            * None : Sort the result, except when\n\n              1. `self` and `other` are equal.\n              2. `self` or `other` has length 0.\n              3. Some values in `self` or `other` cannot be compared.\n                 A RuntimeWarning is issued in this case.\n\n            * False : do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        union : Index\n\n        Examples\n        --------\n        Union matching dtypes\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.union(idx2)\n        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n\n        Union mismatched dtypes\n\n        >>> idx1 = pd.Index(['a', 'b', 'c', 'd'])\n        >>> idx2 = pd.Index([1, 2, 3, 4])\n        >>> idx1.union(idx2)\n        Index(['a', 'b', 'c', 'd', 1, 2, 3, 4], dtype='object')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other = ensure_index(other)\n\n        if not self._can_union_without_object_cast(other):\n            return self._union_incompatible_dtypes(other, sort=sort)\n\n        result = self._union(other, sort=sort)\n\n        return self._wrap_setop_result(other, result)\n\n    def _union(self, other, sort):\n        \"\"\"\n        Specific union logic should go here. In subclasses, union behavior\n        should be overwritten here rather than in `self.union`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        if not len(other) or self.equals(other):\n            return self\n\n        if not len(self):\n            return other\n\n        # TODO(EA): setops-refactor, clean all this up\n        lvals = self._values\n        rvals = other._values\n\n        if sort is None and self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._outer_indexer(lvals, rvals)[0]\n            except TypeError:\n                # incomparable objects\n                result = list(lvals)\n\n                # worth making this faster? a very unusual case\n                value_set = set(lvals)\n                result.extend([x for x in rvals if x not in value_set])\n                result = Index(result)._values  # do type inference here\n        else:\n            # find indexes of things in \"other\" that are not in \"self\"\n            if self.is_unique:\n                indexer = self.get_indexer(other)\n                indexer = (indexer == -1).nonzero()[0]\n            else:\n                indexer = algos.unique1d(self.get_indexer_non_unique(other)[1])\n\n            if len(indexer) > 0:\n                other_diff = algos.take_nd(rvals, indexer, allow_fill=False)\n                result = concat_compat((lvals, other_diff))\n\n            else:\n                result = lvals\n\n            if sort is None:\n                try:\n                    result = algos.safe_sort(result)\n                except TypeError as err:\n                    warnings.warn(\n                        f\"{err}, sort order is undefined for incomparable objects\",\n                        RuntimeWarning,\n                        stacklevel=3,\n                    )\n\n        return result\n\n    @final\n    def _wrap_setop_result(self, other, result):\n        if isinstance(self, (ABCDatetimeIndex, ABCTimedeltaIndex)) and isinstance(\n            result, np.ndarray\n        ):\n            result = type(self._data)._simple_new(result, dtype=self.dtype)\n        elif is_categorical_dtype(self.dtype) and isinstance(result, np.ndarray):\n            result = Categorical(result, dtype=self.dtype)\n\n        name = get_op_result_name(self, other)\n        if isinstance(result, Index):\n            if result.name != name:\n                return result.rename(name)\n            return result\n        else:\n            return self._shallow_copy(result, name=name)\n\n    # TODO: standardize return type of non-union setops type(self vs other)\n    def intersection(self, other, sort=False):\n        \"\"\"\n        Form the intersection of two Index objects.\n\n        This returns a new Index with elements common to the index and `other`.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default False\n            Whether to sort the resulting index.\n\n            * False : do not sort the result.\n            * None : sort the result, except when `self` and `other` are equal\n              or when the values cannot be compared.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default from ``True`` to ``False``, to match\n               the behaviour of 0.23.4 and earlier.\n\n        Returns\n        -------\n        intersection : Index\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.intersection(idx2)\n        Int64Index([3, 4], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, _ = self._convert_can_do_setop(other)\n\n        if self.equals(other) and not self.has_duplicates:\n            return self._get_reconciled_name_object(other)\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            dtype = find_common_type([self.dtype, other.dtype])\n            this = self.astype(dtype, copy=False)\n            other = other.astype(dtype, copy=False)\n            return this.intersection(other, sort=sort)\n\n        result = self._intersection(other, sort=sort)\n        return self._wrap_setop_result(other, result)\n\n    def _intersection(self, other, sort=False):\n        \"\"\"\n        intersection specialized to the case with matching dtypes.\n        \"\"\"\n        # TODO(EA): setops-refactor, clean all this up\n        lvals = self._values\n        rvals = other._values\n\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                result = self._inner_indexer(lvals, rvals)[0]\n            except TypeError:\n                pass\n            else:\n                return algos.unique1d(result)\n\n        try:\n            indexer = Index(rvals).get_indexer(lvals)\n            indexer = indexer.take((indexer != -1).nonzero()[0])\n        except (InvalidIndexError, IncompatibleFrequency):\n            # InvalidIndexError raised by get_indexer if non-unique\n            # IncompatibleFrequency raised by PeriodIndex.get_indexer\n            indexer = algos.unique1d(Index(rvals).get_indexer_non_unique(lvals)[0])\n            indexer = indexer[indexer != -1]\n\n        result = other.take(indexer).unique()._values\n\n        if sort is None:\n            result = algos.safe_sort(result)\n\n        # Intersection has to be unique\n        assert Index(result).is_unique\n\n        return result\n\n    def difference(self, other, sort=None):\n        \"\"\"\n        Return a new Index with elements of index not in `other`.\n\n        This is the set difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        difference : Index\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([2, 1, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.difference(idx2)\n        Int64Index([1, 2], dtype='int64')\n        >>> idx1.difference(idx2, sort=False)\n        Int64Index([2, 1], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n\n        if self.equals(other):\n            # pass an empty np.ndarray with the appropriate dtype\n            return self._shallow_copy(self._data[:0])\n\n        other, result_name = self._convert_can_do_setop(other)\n\n        this = self._get_unique_index()\n\n        indexer = this.get_indexer(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer, assume_unique=True)\n        the_diff = this._values.take(label_diff)\n        if sort is None:\n            try:\n                the_diff = algos.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        return this._shallow_copy(the_diff, name=result_name)\n\n    def symmetric_difference(self, other, result_name=None, sort=None):\n        \"\"\"\n        Compute the symmetric difference of two Index objects.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        result_name : str\n        sort : False or None, default None\n            Whether to sort the resulting index. By default, the\n            values are attempted to be sorted, but any TypeError from\n            incomparable elements is caught by pandas.\n\n            * None : Attempt to sort the result, but catch any TypeErrors\n              from comparing incomparable elements.\n            * False : Do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        symmetric_difference : Index\n\n        Notes\n        -----\n        ``symmetric_difference`` contains elements that appear in either\n        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n        dropped.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([2, 3, 4, 5])\n        >>> idx1.symmetric_difference(idx2)\n        Int64Index([1, 5], dtype='int64')\n\n        You can also use the ``^`` operator:\n\n        >>> idx1 ^ idx2\n        Int64Index([1, 5], dtype='int64')\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_name_update = self._convert_can_do_setop(other)\n        if result_name is None:\n            result_name = result_name_update\n\n        this = self._get_unique_index()\n        other = other._get_unique_index()\n        indexer = this.get_indexer(other)\n\n        # {this} minus {other}\n        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n        left_indexer = np.setdiff1d(\n            np.arange(this.size), common_indexer, assume_unique=True\n        )\n        left_diff = this._values.take(left_indexer)\n\n        # {other} minus {this}\n        right_indexer = (indexer == -1).nonzero()[0]\n        right_diff = other._values.take(right_indexer)\n\n        the_diff = concat_compat([left_diff, right_diff])\n        if sort is None:\n            try:\n                the_diff = algos.safe_sort(the_diff)\n            except TypeError:\n                pass\n\n        return Index(the_diff, dtype=self.dtype, name=result_name)\n\n    def _assert_can_do_setop(self, other):\n        if not is_list_like(other):\n            raise TypeError(\"Input must be Index or array-like\")\n        return True\n\n    def _convert_can_do_setop(self, other):\n        if not isinstance(other, Index):\n            other = Index(other, name=self.name)\n            result_name = self.name\n        else:\n            result_name = get_op_result_name(self, other)\n        return other, result_name\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    def get_loc(self, key, method=None, tolerance=None):\n        \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        tolerance : int or float, optional\n            Maximum distance from index value for inexact matches. The value of\n            the index at the matching location must satisfy the equation\n            ``abs(index[loc] - key) <= tolerance``.\n\n        Returns\n        -------\n        loc : int if unique index, slice if monotonic index, else mask\n\n        Examples\n        --------\n        >>> unique_index = pd.Index(list('abc'))\n        >>> unique_index.get_loc('b')\n        1\n\n        >>> monotonic_index = pd.Index(list('abbc'))\n        >>> monotonic_index.get_loc('b')\n        slice(1, 3, None)\n\n        >>> non_monotonic_index = pd.Index(list('abcb'))\n        >>> non_monotonic_index.get_loc('b')\n        array([False,  True, False,  True])\n        \"\"\"\n        if method is None:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if using pad, \"\n                    \"backfill or nearest lookups\"\n                )\n            casted_key = self._maybe_cast_indexer(key)\n            try:\n                return self._engine.get_loc(casted_key)\n            except KeyError as err:\n                raise KeyError(key) from err\n\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, np.asarray(key))\n\n        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n        if indexer.ndim > 1 or indexer.size > 1:\n            raise TypeError(\"get_loc requires scalar valued input\")\n        loc = indexer.item()\n        if loc == -1:\n            raise KeyError(key)\n        return loc\n\n    _index_shared_docs[\n        \"get_indexer\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n            * default: exact matches only.\n            * pad / ffill: find the PREVIOUS index value if no exact match.\n            * backfill / bfill: use NEXT index value if no exact match\n            * nearest: use the NEAREST index value if no exact match. Tied\n              distances are broken by preferring the larger index value.\n        limit : int, optional\n            Maximum number of consecutive labels in ``target`` to match for\n            inexact matches.\n        tolerance : optional\n            Maximum distance between original and new labels for inexact\n            matches. The values of the index at the matching locations must\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n            Tolerance may be a scalar value, which applies the same tolerance\n            to all values, or list-like, which applies variable tolerance per\n            element. List-like includes list, tuple, array, Series, and must be\n            the same size as the index and its dtype must exactly match the\n            index's type.\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        %(raises_section)s\n        Examples\n        --------\n        >>> index = pd.Index(['c', 'a', 'b'])\n        >>> index.get_indexer(['a', 'b', 'x'])\n        array([ 1,  2, -1])\n\n        Notice that the return value is an array of locations in ``index``\n        and ``x`` is marked by -1, as it is not in ``index``.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)\n    def get_indexer(\n        self, target, method=None, limit=None, tolerance=None\n    ) -> np.ndarray:\n        method = missing.clean_reindex_fill_method(method)\n        target = ensure_index(target)\n        if tolerance is not None:\n            tolerance = self._convert_tolerance(tolerance, target)\n\n        # Treat boolean labels passed to a numeric index as not found. Without\n        # this fix False and True would be treated as 0 and 1 respectively.\n        # (GH #16877)\n        if target.is_boolean() and self.is_numeric():\n            return ensure_platform_int(np.repeat(-1, target.size))\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer(\n                ptarget, method=method, limit=limit, tolerance=tolerance\n            )\n\n        if not is_dtype_equal(self.dtype, target.dtype):\n            this = self.astype(object)\n            target = target.astype(object)\n            return this.get_indexer(\n                target, method=method, limit=limit, tolerance=tolerance\n            )\n\n        if not self.is_unique:\n            raise InvalidIndexError(\n                \"Reindexing only valid with uniquely valued Index objects\"\n            )\n\n        if method == \"pad\" or method == \"backfill\":\n            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n        elif method == \"nearest\":\n            indexer = self._get_nearest_indexer(target, limit, tolerance)\n        else:\n            if tolerance is not None:\n                raise ValueError(\n                    \"tolerance argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n            if limit is not None:\n                raise ValueError(\n                    \"limit argument only valid if doing pad, \"\n                    \"backfill or nearest reindexing\"\n                )\n\n            indexer = self._engine.get_indexer(target._get_engine_target())\n\n        return ensure_platform_int(indexer)\n\n    def _convert_tolerance(self, tolerance, target):\n        # override this method on subclasses\n        tolerance = np.asarray(tolerance)\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        return tolerance\n\n    @final\n    def _get_fill_indexer(\n        self, target: \"Index\", method: str_t, limit=None, tolerance=None\n    ) -> np.ndarray:\n\n        target_values = target._get_engine_target()\n\n        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n            engine_method = (\n                self._engine.get_pad_indexer\n                if method == \"pad\"\n                else self._engine.get_backfill_indexer\n            )\n            indexer = engine_method(target_values, limit)\n        else:\n            indexer = self._get_fill_indexer_searchsorted(target, method, limit)\n        if tolerance is not None and len(self):\n            indexer = self._filter_indexer_tolerance(target_values, indexer, tolerance)\n        return indexer\n\n    @final\n    def _get_fill_indexer_searchsorted(\n        self, target: \"Index\", method: str_t, limit=None\n    ) -> np.ndarray:\n        \"\"\"\n        Fallback pad/backfill get_indexer that works for monotonic decreasing\n        indexes and non-monotonic targets.\n        \"\"\"\n        if limit is not None:\n            raise ValueError(\n                f\"limit argument for {repr(method)} method only well-defined \"\n                \"if index and target are monotonic\"\n            )\n\n        side = \"left\" if method == \"pad\" else \"right\"\n\n        # find exact matches first (this simplifies the algorithm)\n        indexer = self.get_indexer(target)\n        nonexact = indexer == -1\n        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact], side)\n        if side == \"left\":\n            # searchsorted returns \"indices into a sorted array such that,\n            # if the corresponding elements in v were inserted before the\n            # indices, the order of a would be preserved\".\n            # Thus, we need to subtract 1 to find values to the left.\n            indexer[nonexact] -= 1\n            # This also mapped not found values (values of 0 from\n            # np.searchsorted) to -1, which conveniently is also our\n            # sentinel for missing values\n        else:\n            # Mark indices to the right of the largest value as not found\n            indexer[indexer == len(self)] = -1\n        return indexer\n\n    @final\n    def _get_nearest_indexer(self, target: \"Index\", limit, tolerance) -> np.ndarray:\n        \"\"\"\n        Get the indexer for the nearest index labels; requires an index with\n        values that can be subtracted from each other (e.g., not strings or\n        tuples).\n        \"\"\"\n        if not len(self):\n            return self._get_fill_indexer(target, \"pad\")\n\n        left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n        right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n\n        target_values = target._values\n        # error: Unsupported left operand type for - (\"ExtensionArray\")\n        left_distances = np.abs(\n            self._values[left_indexer] - target_values  # type: ignore[operator]\n        )\n        # error: Unsupported left operand type for - (\"ExtensionArray\")\n        right_distances = np.abs(\n            self._values[right_indexer] - target_values  # type: ignore[operator]\n        )\n\n        op = operator.lt if self.is_monotonic_increasing else operator.le\n        indexer = np.where(\n            op(left_distances, right_distances) | (right_indexer == -1),\n            left_indexer,\n            right_indexer,\n        )\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target_values, indexer, tolerance)\n        return indexer\n\n    @final\n    def _filter_indexer_tolerance(\n        self,\n        target: Union[\"Index\", np.ndarray, ExtensionArray],\n        indexer: np.ndarray,\n        tolerance,\n    ) -> np.ndarray:\n        # error: Unsupported left operand type for - (\"ExtensionArray\")\n        distance = abs(self._values[indexer] - target)  # type: ignore[operator]\n        indexer = np.where(distance <= tolerance, indexer, -1)\n        return indexer\n\n    # --------------------------------------------------------------------\n    # Indexer Conversion Methods\n\n    def _get_partial_string_timestamp_match_key(self, key):\n        \"\"\"\n        Translate any partial string timestamp matches in key, returning the\n        new key.\n\n        Only relevant for MultiIndex.\n        \"\"\"\n        # GH#10331\n        return key\n\n    @final\n    def _validate_positional_slice(self, key: slice):\n        \"\"\"\n        For positional indexing, a slice must have either int or None\n        for each of start, stop, and step.\n        \"\"\"\n        self._validate_indexer(\"positional\", key.start, \"iloc\")\n        self._validate_indexer(\"positional\", key.stop, \"iloc\")\n        self._validate_indexer(\"positional\", key.step, \"iloc\")\n\n    def _convert_slice_indexer(self, key: slice, kind: str_t):\n        \"\"\"\n        Convert a slice indexer.\n\n        By definition, these are labels unless 'iloc' is passed in.\n        Floats are not allowed as the start, step, or stop of the slice.\n\n        Parameters\n        ----------\n        key : label of the slice bound\n        kind : {'loc', 'getitem'}\n        \"\"\"\n        assert kind in [\"loc\", \"getitem\"], kind\n\n        # potentially cast the bounds to integers\n        start, stop, step = key.start, key.stop, key.step\n\n        # figure out if this is a positional indexer\n        def is_int(v):\n            return v is None or is_integer(v)\n\n        is_index_slice = is_int(start) and is_int(stop) and is_int(step)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n\n        if kind == \"getitem\":\n            \"\"\"\n            called from the getitem slicers, validate that we are in fact\n            integers\n            \"\"\"\n            if self.is_integer() or is_index_slice:\n                self._validate_indexer(\"slice\", key.start, \"getitem\")\n                self._validate_indexer(\"slice\", key.stop, \"getitem\")\n                self._validate_indexer(\"slice\", key.step, \"getitem\")\n                return key\n\n        # convert the slice to an indexer here\n\n        # if we are mixed and have integers\n        if is_positional:\n            try:\n                # Validate start & stop\n                if start is not None:\n                    self.get_loc(start)\n                if stop is not None:\n                    self.get_loc(stop)\n                is_positional = False\n            except KeyError:\n                pass\n\n        if com.is_null_slice(key):\n            # It doesn't matter if we are positional or label based\n            indexer = key\n        elif is_positional:\n            if kind == \"loc\":\n                # GH#16121, GH#24612, GH#31810\n                warnings.warn(\n                    \"Slicing a positional slice with .loc is not supported, \"\n                    \"and will raise TypeError in a future version.  \"\n                    \"Use .loc with labels or .iloc with positions instead.\",\n                    FutureWarning,\n                    stacklevel=6,\n                )\n            indexer = key\n        else:\n            indexer = self.slice_indexer(start, stop, step, kind=kind)\n\n        return indexer\n\n    def _convert_listlike_indexer(self, keyarr):\n        \"\"\"\n        Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        indexer : numpy.ndarray or None\n            Return an ndarray or None if cannot convert.\n        keyarr : numpy.ndarray\n            Return tuple-safe keys.\n        \"\"\"\n        if isinstance(keyarr, Index):\n            pass\n        else:\n            keyarr = self._convert_arr_indexer(keyarr)\n\n        indexer = self._convert_list_indexer(keyarr)\n        return indexer, keyarr\n\n    def _convert_arr_indexer(self, keyarr):\n        \"\"\"\n        Convert an array-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : array-like\n            Indexer to convert.\n\n        Returns\n        -------\n        converted_keyarr : array-like\n        \"\"\"\n        keyarr = com.asarray_tuplesafe(keyarr)\n        return keyarr\n\n    def _convert_list_indexer(self, keyarr):\n        \"\"\"\n        Convert a list-like indexer to the appropriate dtype.\n\n        Parameters\n        ----------\n        keyarr : Index (or sub-class)\n            Indexer to convert.\n        kind : iloc, loc, optional\n\n        Returns\n        -------\n        positional indexer or None\n        \"\"\"\n        return None\n\n    @final\n    def _invalid_indexer(self, form: str_t, key) -> TypeError:\n        \"\"\"\n        Consistent invalid indexer message.\n        \"\"\"\n        return TypeError(\n            f\"cannot do {form} indexing on {type(self).__name__} with these \"\n            f\"indexers [{key}] of type {type(key).__name__}\"\n        )\n\n    # --------------------------------------------------------------------\n    # Reindex Methods\n\n    @final\n    def _can_reindex(self, indexer):\n        \"\"\"\n        Check if we are allowing reindexing with this particular indexer.\n\n        Parameters\n        ----------\n        indexer : an integer indexer\n\n        Raises\n        ------\n        ValueError if its a duplicate axis\n        \"\"\"\n        # trying to reindex on an axis with duplicates\n        if not self._index_as_unique and len(indexer):\n            raise ValueError(\"cannot reindex from a duplicate axis\")\n\n    def reindex(self, target, method=None, level=None, limit=None, tolerance=None):\n        \"\"\"\n        Create index with target's values.\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray or None\n            Indices of output values in original index.\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, \"name\")\n\n        # GH7774: preserve dtype/tz if target is empty and not an Index.\n        target = ensure_has_len(target)  # target may be an iterator\n\n        if not isinstance(target, Index) and len(target) == 0:\n            values: Union[range, ExtensionArray, np.ndarray]\n            if isinstance(self, ABCRangeIndex):\n                values = range(0)\n            else:\n                values = self._data[:0]  # appropriately-dtyped empty array\n            target = self._simple_new(values, name=self.name)\n        else:\n            target = ensure_index(target)\n\n        if level is not None:\n            if method is not None:\n                raise TypeError(\"Fill method not supported if level passed\")\n            _, indexer, _ = self._join_level(\n                target, level, how=\"right\", return_indexers=True\n            )\n        else:\n            if self.equals(target):\n                indexer = None\n            else:\n                if self._index_as_unique:\n                    indexer = self.get_indexer(\n                        target, method=method, limit=limit, tolerance=tolerance\n                    )\n                else:\n                    if method is not None or limit is not None:\n                        raise ValueError(\n                            \"cannot reindex a non-unique index \"\n                            \"with a method or limit\"\n                        )\n                    indexer, missing = self.get_indexer_non_unique(target)\n\n        if preserve_names and target.nlevels == 1 and target.name != self.name:\n            target = target.copy()\n            target.name = self.name\n\n        return target, indexer\n\n    def _reindex_non_unique(self, target):\n        \"\"\"\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target.\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index.\n        indexer : np.ndarray or None\n            Indices of output values in original index.\n\n        \"\"\"\n        target = ensure_index(target)\n        if len(target) == 0:\n            # GH#13691\n            return self[:0], np.array([], dtype=np.intp), None\n\n        indexer, missing = self.get_indexer_non_unique(target)\n        check = indexer != -1\n        new_labels = self.take(indexer[check])\n        new_indexer = None\n\n        if len(missing):\n            length = np.arange(len(indexer))\n\n            missing = ensure_platform_int(missing)\n            missing_labels = target.take(missing)\n            missing_indexer = ensure_int64(length[~check])\n            cur_labels = self.take(indexer[check]).values\n            cur_indexer = ensure_int64(length[check])\n\n            new_labels = np.empty((len(indexer),), dtype=object)\n            new_labels[cur_indexer] = cur_labels\n            new_labels[missing_indexer] = missing_labels\n\n            # a unique indexer\n            if target.is_unique:\n\n                # see GH5553, make sure we use the right indexer\n                new_indexer = np.arange(len(indexer))\n                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n                new_indexer[missing_indexer] = -1\n\n            # we have a non_unique selector, need to use the original\n            # indexer here\n            else:\n\n                # need to retake to have the same size as the indexer\n                indexer[~check] = -1\n\n                # reset the new indexer to account for the new size\n                new_indexer = np.arange(len(self.take(indexer)))\n                new_indexer[~check] = -1\n\n        if isinstance(self, ABCMultiIndex):\n            new_index = type(self).from_tuples(new_labels, names=self.names)\n        else:\n            new_index = Index(new_labels, name=self.name)\n        return new_index, indexer, new_indexer\n\n    # --------------------------------------------------------------------\n    # Join Methods\n\n    def join(self, other, how=\"left\", level=None, return_indexers=False, sort=False):\n        \"\"\"\n        Compute join_index and indexers to conform data\n        structures to the new index.\n\n        Parameters\n        ----------\n        other : Index\n        how : {'left', 'right', 'inner', 'outer'}\n        level : int or level name, default None\n        return_indexers : bool, default False\n        sort : bool, default False\n            Sort the join keys lexicographically in the result Index. If False,\n            the order of the join keys depends on the join type (how keyword).\n\n        Returns\n        -------\n        join_index, (left_indexer, right_indexer)\n        \"\"\"\n        other = ensure_index(other)\n        self_is_mi = isinstance(self, ABCMultiIndex)\n        other_is_mi = isinstance(other, ABCMultiIndex)\n\n        # try to figure out the join level\n        # GH3662\n        if level is None and (self_is_mi or other_is_mi):\n\n            # have the same levels/names so a simple join\n            if self.names == other.names:\n                pass\n            else:\n                return self._join_multi(other, how=how, return_indexers=return_indexers)\n\n        # join on the level\n        if level is not None and (self_is_mi or other_is_mi):\n            return self._join_level(\n                other, level, how=how, return_indexers=return_indexers\n            )\n\n        if len(other) == 0 and how in (\"left\", \"outer\"):\n            join_index = self._shallow_copy()\n            if return_indexers:\n                rindexer = np.repeat(-1, len(join_index))\n                return join_index, None, rindexer\n            else:\n                return join_index\n\n        if len(self) == 0 and how in (\"right\", \"outer\"):\n            join_index = other._shallow_copy()\n            if return_indexers:\n                lindexer = np.repeat(-1, len(join_index))\n                return join_index, lindexer, None\n            else:\n                return join_index\n\n        if self._join_precedence < other._join_precedence:\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n            result = other.join(\n                self, how=how, level=level, return_indexers=return_indexers\n            )\n            if return_indexers:\n                x, y, z = result\n                result = x, z, y\n            return result\n\n        if not is_dtype_equal(self.dtype, other.dtype):\n            this = self.astype(\"O\")\n            other = other.astype(\"O\")\n            return this.join(other, how=how, return_indexers=return_indexers)\n\n        _validate_join_method(how)\n\n        if not self.is_unique and not other.is_unique:\n            return self._join_non_unique(\n                other, how=how, return_indexers=return_indexers\n            )\n        elif not self.is_unique or not other.is_unique:\n            if self.is_monotonic and other.is_monotonic:\n                return self._join_monotonic(\n                    other, how=how, return_indexers=return_indexers\n                )\n            else:\n                return self._join_non_unique(\n                    other, how=how, return_indexers=return_indexers\n                )\n        elif self.is_monotonic and other.is_monotonic:\n            try:\n                return self._join_monotonic(\n                    other, how=how, return_indexers=return_indexers\n                )\n            except TypeError:\n                pass\n\n        if how == \"left\":\n            join_index = self\n        elif how == \"right\":\n            join_index = other\n        elif how == \"inner\":\n            # TODO: sort=False here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.intersection(other, sort=False)\n        elif how == \"outer\":\n            # TODO: sort=True here for backwards compat. It may\n            # be better to use the sort parameter passed into join\n            join_index = self.union(other)\n\n        if sort:\n            join_index = join_index.sort_values()\n\n        if return_indexers:\n            if join_index is self:\n                lindexer = None\n            else:\n                lindexer = self.get_indexer(join_index)\n            if join_index is other:\n                rindexer = None\n            else:\n                rindexer = other.get_indexer(join_index)\n            return join_index, lindexer, rindexer\n        else:\n            return join_index\n\n    @final\n    def _join_multi(self, other, how, return_indexers=True):\n        from pandas.core.indexes.multi import MultiIndex\n        from pandas.core.reshape.merge import restore_dropped_levels_multijoin\n\n        # figure out join names\n        self_names_list = list(com.not_none(*self.names))\n        other_names_list = list(com.not_none(*other.names))\n        self_names_order = self_names_list.index\n        other_names_order = other_names_list.index\n        self_names = set(self_names_list)\n        other_names = set(other_names_list)\n        overlap = self_names & other_names\n\n        # need at least 1 in common\n        if not overlap:\n            raise ValueError(\"cannot join with no overlapping index names\")\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n\n            # Drop the non-matching levels from left and right respectively\n            ldrop_names = sorted(self_names - overlap, key=self_names_order)\n            rdrop_names = sorted(other_names - overlap, key=other_names_order)\n\n            # if only the order differs\n            if not len(ldrop_names + rdrop_names):\n                self_jnlevels = self\n                other_jnlevels = other.reorder_levels(self.names)\n            else:\n                self_jnlevels = self.droplevel(ldrop_names)\n                other_jnlevels = other.droplevel(rdrop_names)\n\n            # Join left and right\n            # Join on same leveled multi-index frames is supported\n            join_idx, lidx, ridx = self_jnlevels.join(\n                other_jnlevels, how, return_indexers=True\n            )\n\n            # Restore the dropped levels\n            # Returned index level order is\n            # common levels, ldrop_names, rdrop_names\n            dropped_names = ldrop_names + rdrop_names\n\n            levels, codes, names = restore_dropped_levels_multijoin(\n                self, other, dropped_names, join_idx, lidx, ridx\n            )\n\n            # Re-create the multi-index\n            multi_join_idx = MultiIndex(\n                levels=levels, codes=codes, names=names, verify_integrity=False\n            )\n\n            multi_join_idx = multi_join_idx.remove_unused_levels()\n\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n\n        jl = list(overlap)[0]\n\n        # Case where only one index is multi\n        # make the indices into mi's that match\n        flip_order = False\n        if isinstance(self, MultiIndex):\n            self, other = other, self\n            flip_order = True\n            # flip if join method is right or left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        level = other.names.index(jl)\n        result = self._join_level(\n            other, level, how=how, return_indexers=return_indexers\n        )\n\n        if flip_order:\n            if isinstance(result, tuple):\n                return result[0], result[2], result[1]\n        return result\n\n    @final\n    def _join_non_unique(self, other, how=\"left\", return_indexers=False):\n        from pandas.core.reshape.merge import get_join_indexers\n\n        # We only get here if dtypes match\n        assert self.dtype == other.dtype\n\n        lvalues = self._get_engine_target()\n        rvalues = other._get_engine_target()\n\n        left_idx, right_idx = get_join_indexers(\n            [lvalues], [rvalues], how=how, sort=True\n        )\n\n        left_idx = ensure_platform_int(left_idx)\n        right_idx = ensure_platform_int(right_idx)\n\n        join_index = np.asarray(lvalues.take(left_idx))\n        mask = left_idx == -1\n        np.putmask(join_index, mask, rvalues.take(right_idx))\n\n        join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            return join_index, left_idx, right_idx\n        else:\n            return join_index\n\n    def _join_level(\n        self, other, level, how=\"left\", return_indexers=False, keep_order=True\n    ):\n        \"\"\"\n        The join method *only* affects the level of the resulting\n        MultiIndex. Otherwise it just exactly aligns the Index data to the\n        labels of the level in the MultiIndex.\n\n        If ```keep_order == True```, the order of the data indexed by the\n        MultiIndex will not be changed; otherwise, it will tie out\n        with `other`.\n        \"\"\"\n        from pandas.core.indexes.multi import MultiIndex\n\n        def _get_leaf_sorter(labels):\n            \"\"\"\n            Returns sorter for the inner most level while preserving the\n            order of higher levels.\n            \"\"\"\n            if labels[0].size == 0:\n                return np.empty(0, dtype=\"int64\")\n\n            if len(labels) == 1:\n                lab = ensure_int64(labels[0])\n                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())\n                return sorter\n\n            # find indexers of beginning of each set of\n            # same-key labels w.r.t all but last level\n            tic = labels[0][:-1] != labels[0][1:]\n            for lab in labels[1:-1]:\n                tic |= lab[:-1] != lab[1:]\n\n            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n            lab = ensure_int64(labels[-1])\n            return lib.get_level_sorter(lab, ensure_int64(starts))\n\n        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n            raise TypeError(\"Join on level between two MultiIndex objects is ambiguous\")\n\n        left, right = self, other\n\n        flip_order = not isinstance(self, MultiIndex)\n        if flip_order:\n            left, right = right, left\n            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n\n        assert isinstance(left, MultiIndex)\n\n        level = left._get_level_number(level)\n        old_level = left.levels[level]\n\n        if not right.is_unique:\n            raise NotImplementedError(\n                \"Index._join_level on non-unique index is not implemented\"\n            )\n\n        new_level, left_lev_indexer, right_lev_indexer = old_level.join(\n            right, how=how, return_indexers=True\n        )\n\n        if left_lev_indexer is None:\n            if keep_order or len(left) == 0:\n                left_indexer = None\n                join_index = left\n            else:  # sort the leaves\n                left_indexer = _get_leaf_sorter(left.codes[: level + 1])\n                join_index = left[left_indexer]\n\n        else:\n            left_lev_indexer = ensure_int64(left_lev_indexer)\n            rev_indexer = lib.get_reverse_indexer(left_lev_indexer, len(old_level))\n            old_codes = left.codes[level]\n            new_lev_codes = algos.take_nd(\n                rev_indexer, old_codes[old_codes != -1], allow_fill=False\n            )\n\n            new_codes = list(left.codes)\n            new_codes[level] = new_lev_codes\n\n            new_levels = list(left.levels)\n            new_levels[level] = new_level\n\n            if keep_order:  # just drop missing values. o.w. keep order\n                left_indexer = np.arange(len(left), dtype=np.intp)\n                mask = new_lev_codes != -1\n                if not mask.all():\n                    new_codes = [lab[mask] for lab in new_codes]\n                    left_indexer = left_indexer[mask]\n\n            else:  # tie out the order with other\n                if level == 0:  # outer most level, take the fast route\n                    ngroups = 1 + new_lev_codes.max()\n                    left_indexer, counts = libalgos.groupsort_indexer(\n                        new_lev_codes, ngroups\n                    )\n\n                    # missing values are placed first; drop them!\n                    left_indexer = left_indexer[counts[0] :]\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                else:  # sort the leaves\n                    mask = new_lev_codes != -1\n                    mask_all = mask.all()\n                    if not mask_all:\n                        new_codes = [lab[mask] for lab in new_codes]\n\n                    left_indexer = _get_leaf_sorter(new_codes[: level + 1])\n                    new_codes = [lab[left_indexer] for lab in new_codes]\n\n                    # left_indexers are w.r.t masked frame.\n                    # reverse to original frame!\n                    if not mask_all:\n                        left_indexer = mask.nonzero()[0][left_indexer]\n\n            join_index = MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=left.names,\n                verify_integrity=False,\n            )\n\n        if right_lev_indexer is not None:\n            right_indexer = algos.take_nd(\n                right_lev_indexer, join_index.codes[level], allow_fill=False\n            )\n        else:\n            right_indexer = join_index.codes[level]\n\n        if flip_order:\n            left_indexer, right_indexer = right_indexer, left_indexer\n\n        if return_indexers:\n            left_indexer = (\n                None if left_indexer is None else ensure_platform_int(left_indexer)\n            )\n            right_indexer = (\n                None if right_indexer is None else ensure_platform_int(right_indexer)\n            )\n            return join_index, left_indexer, right_indexer\n        else:\n            return join_index\n\n    def _join_monotonic(self, other, how=\"left\", return_indexers=False):\n        # We only get here with matching dtypes\n        assert other.dtype == self.dtype\n\n        if self.equals(other):\n            ret_index = other if how == \"right\" else self\n            if return_indexers:\n                return ret_index, None, None\n            else:\n                return ret_index\n\n        sv = self._get_engine_target()\n        ov = other._get_engine_target()\n\n        if self.is_unique and other.is_unique:\n            # We can perform much better than the general case\n            if how == \"left\":\n                join_index = self\n                lidx = None\n                ridx = self._left_indexer_unique(sv, ov)\n            elif how == \"right\":\n                join_index = other\n                lidx = self._left_indexer_unique(ov, sv)\n                ridx = None\n            elif how == \"inner\":\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n            elif how == \"outer\":\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n                join_index = self._wrap_joined_index(join_index, other)\n        else:\n            if how == \"left\":\n                join_index, lidx, ridx = self._left_indexer(sv, ov)\n            elif how == \"right\":\n                join_index, ridx, lidx = self._left_indexer(ov, sv)\n            elif how == \"inner\":\n                join_index, lidx, ridx = self._inner_indexer(sv, ov)\n            elif how == \"outer\":\n                join_index, lidx, ridx = self._outer_indexer(sv, ov)\n            join_index = self._wrap_joined_index(join_index, other)\n\n        if return_indexers:\n            lidx = None if lidx is None else ensure_platform_int(lidx)\n            ridx = None if ridx is None else ensure_platform_int(ridx)\n            return join_index, lidx, ridx\n        else:\n            return join_index\n\n    def _wrap_joined_index(\n        self: _IndexT, joined: np.ndarray, other: _IndexT\n    ) -> _IndexT:\n        assert other.dtype == self.dtype\n\n        if isinstance(self, ABCMultiIndex):\n            name = self.names if self.names == other.names else None\n        else:\n            name = get_op_result_name(self, other)\n        return self._constructor(joined, name=name)\n\n    # --------------------------------------------------------------------\n    # Uncategorized Methods\n\n    @property\n    def values(self) -> np.ndarray:\n        \"\"\"\n        Return an array representing the data in the Index.\n\n        .. warning::\n\n           We recommend using :attr:`Index.array` or\n           :meth:`Index.to_numpy`, depending on whether you need\n           a reference to the underlying data or a NumPy array.\n\n        Returns\n        -------\n        array: numpy.ndarray or ExtensionArray\n\n        See Also\n        --------\n        Index.array : Reference to the underlying data.\n        Index.to_numpy : A NumPy array representing the underlying data.\n        \"\"\"\n        return self._data.view(np.ndarray)\n\n    @cache_readonly\n    @doc(IndexOpsMixin.array)\n    def array(self) -> ExtensionArray:\n        array = self._data\n        if isinstance(array, np.ndarray):\n            from pandas.core.arrays.numpy_ import PandasArray\n\n            array = PandasArray(array)\n        return array\n\n    @property\n    def _values(self) -> Union[ExtensionArray, np.ndarray]:\n        \"\"\"\n        The best array representation.\n\n        This is an ndarray or ExtensionArray.\n\n        ``_values`` are consistent between ``Series`` and ``Index``.\n\n        It may differ from the public '.values' method.\n\n        index             | values          | _values       |\n        ----------------- | --------------- | ------------- |\n        Index             | ndarray         | ndarray       |\n        CategoricalIndex  | Categorical     | Categorical   |\n        DatetimeIndex     | ndarray[M8ns]   | DatetimeArray |\n        DatetimeIndex[tz] | ndarray[M8ns]   | DatetimeArray |\n        PeriodIndex       | ndarray[object] | PeriodArray   |\n        IntervalIndex     | IntervalArray   | IntervalArray |\n\n        See Also\n        --------\n        values : Values\n        \"\"\"\n        return self._data\n\n    def _get_engine_target(self) -> np.ndarray:\n        \"\"\"\n        Get the ndarray that we can pass to the IndexEngine constructor.\n        \"\"\"\n        return self._values\n\n    @doc(IndexOpsMixin.memory_usage)\n    def memory_usage(self, deep: bool = False) -> int:\n        result = super().memory_usage(deep=deep)\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    def where(self, cond, other=None):\n        \"\"\"\n        Replace values where the condition is False.\n\n        The replacement is taken from other.\n\n        Parameters\n        ----------\n        cond : bool array-like with the same length as self\n            Condition to select the values on.\n        other : scalar, or array-like, default None\n            Replacement if the condition is False.\n\n        Returns\n        -------\n        pandas.Index\n            A copy of self with values replaced from other\n            where the condition is False.\n\n        See Also\n        --------\n        Series.where : Same method for Series.\n        DataFrame.where : Same method for DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n        >>> idx\n        Index(['car', 'bike', 'train', 'tractor'], dtype='object')\n        >>> idx.where(idx.isin(['car', 'train']), 'other')\n        Index(['car', 'other', 'train', 'other'], dtype='object')\n        \"\"\"\n        if other is None:\n            other = self._na_value\n\n        values = self.values\n\n        try:\n            self._validate_fill_value(other)\n        except (ValueError, TypeError):\n            return self.astype(object).where(cond, other)\n\n        values = np.where(cond, values, other)\n\n        return Index(values, name=self.name)\n\n    # construction helpers\n    @final\n    @classmethod\n    def _scalar_data_error(cls, data):\n        # We return the TypeError so that we can raise it from the constructor\n        #  in order to keep mypy happy\n        return TypeError(\n            f\"{cls.__name__}(...) must be called with a collection of some \"\n            f\"kind, {repr(data)} was passed\"\n        )\n\n    @final\n    @classmethod\n    def _string_data_error(cls, data):\n        raise TypeError(\n            \"String dtype not supported, you may need \"\n            \"to explicitly cast to a numeric type\"\n        )\n\n    @final\n    def _coerce_scalar_to_index(self, item):\n        \"\"\"\n        We need to coerce a scalar to a compat for our index type.\n\n        Parameters\n        ----------\n        item : scalar item to coerce\n        \"\"\"\n        dtype = self.dtype\n\n        if self._is_numeric_dtype and isna(item):\n            # We can't coerce to the numeric dtype of \"self\" (unless\n            # it's float) if there are NaN values in our output.\n            dtype = None\n\n        return Index([item], dtype=dtype, **self._get_attributes_dict())\n\n    def _validate_fill_value(self, value):\n        \"\"\"\n        Check if the value can be inserted into our array, and convert\n        it to an appropriate native type if necessary.\n        \"\"\"\n        return value\n\n    @final\n    def _require_scalar(self, value):\n        \"\"\"\n        Check that this is a scalar value that we can use for setitem-like\n        operations without changing dtype.\n        \"\"\"\n        if not is_scalar(value):\n            raise TypeError(f\"'value' must be a scalar, passed: {type(value).__name__}\")\n        return value\n\n    @property\n    def _has_complex_internals(self) -> bool:\n        \"\"\"\n        Indicates if an index is not directly backed by a numpy array\n        \"\"\"\n        # used to avoid libreduction code paths, which raise or require conversion\n        return False\n\n    def _is_memory_usage_qualified(self) -> bool:\n        \"\"\"\n        Return a boolean if we need a qualified .info display.\n        \"\"\"\n        return self.is_object()\n\n    def is_type_compatible(self, kind: str_t) -> bool:\n        \"\"\"\n        Whether the index type is compatible with the provided type.\n        \"\"\"\n        return kind == self.inferred_type\n\n    def __contains__(self, key: Any) -> bool:\n        \"\"\"\n        Return a boolean indicating whether the provided key is in the index.\n\n        Parameters\n        ----------\n        key : label\n            The key to check if it is present in the index.\n\n        Returns\n        -------\n        bool\n            Whether the key search is in the index.\n\n        Raises\n        ------\n        TypeError\n            If the key is not hashable.\n\n        See Also\n        --------\n        Index.isin : Returns an ndarray of boolean dtype indicating whether the\n            list-like key is in the index.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1, 2, 3, 4])\n        >>> idx\n        Int64Index([1, 2, 3, 4], dtype='int64')\n\n        >>> 2 in idx\n        True\n        >>> 6 in idx\n        False\n        \"\"\"\n        hash(key)\n        try:\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    @final\n    def __hash__(self):\n        raise TypeError(f\"unhashable type: {repr(type(self).__name__)}\")\n\n    @final\n    def __setitem__(self, key, value):\n        raise TypeError(\"Index does not support mutable operations\")\n\n    def __getitem__(self, key):\n        \"\"\"\n        Override numpy.ndarray's __getitem__ method to work as desired.\n\n        This function adds lists and Series as valid boolean indexers\n        (ndarrays only supports ndarray with dtype=bool).\n\n        If resulting ndim != 1, plain ndarray is returned instead of\n        corresponding `Index` subclass.\n\n        \"\"\"\n        # There's no custom logic to be implemented in __getslice__, so it's\n        # not overloaded intentionally.\n        getitem = self._data.__getitem__\n        promote = self._shallow_copy\n\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key, warn_float=True)\n            return getitem(key)\n\n        if isinstance(key, slice):\n            # This case is separated from the conditional above to avoid\n            # pessimization of basic indexing.\n            return promote(getitem(key))\n\n        if com.is_bool_indexer(key):\n            key = np.asarray(key, dtype=bool)\n\n        result = getitem(key)\n        if not is_scalar(result):\n            if np.ndim(result) > 1:\n                deprecate_ndim_indexing(result)\n                return result\n            return promote(result)\n        else:\n            return result\n\n    @final\n    def _can_hold_identifiers_and_holds_name(self, name) -> bool:\n        \"\"\"\n        Faster check for ``name in self`` when we know `name` is a Python\n        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n        . key lookup). For indexes that can't hold identifiers (everything\n        but object & categorical) we just return False.\n\n        https://github.com/pandas-dev/pandas/issues/19764\n        \"\"\"\n        if self.is_object() or self.is_categorical():\n            return name in self\n        return False\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together.\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        appended : Index\n        \"\"\"\n        to_concat = [self]\n\n        if isinstance(other, (list, tuple)):\n            to_concat = to_concat + list(other)\n        else:\n            to_concat.append(other)\n\n        for obj in to_concat:\n            if not isinstance(obj, Index):\n                raise TypeError(\"all inputs must be Index\")\n\n        names = {obj.name for obj in to_concat}\n        name = None if len(names) > 1 else self.name\n\n        return self._concat(to_concat, name)\n\n    def _concat(self, to_concat: List[\"Index\"], name: Label) -> \"Index\":\n        \"\"\"\n        Concatenate multiple Index objects.\n        \"\"\"\n        to_concat_vals = [x._values for x in to_concat]\n\n        result = concat_compat(to_concat_vals)\n        return Index(result, name=name)\n\n    def putmask(self, mask, value):\n        \"\"\"\n        Return a new Index of the values set with the mask.\n\n        Returns\n        -------\n        Index\n\n        See Also\n        --------\n        numpy.ndarray.putmask : Changes elements of an array\n            based on conditional and input values.\n        \"\"\"\n        values = self._values.copy()\n        try:\n            converted = self._validate_fill_value(value)\n        except (ValueError, TypeError) as err:\n            if is_object_dtype(self):\n                raise err\n\n            # coerces to object\n            return self.astype(object).putmask(mask, value)\n\n        np.putmask(values, mask, converted)\n        return self._shallow_copy(values)\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Determine if two Index object are equal.\n\n        The things that are being compared are:\n\n        * The elements inside the Index object.\n        * The order of the elements inside the Index object.\n\n        Parameters\n        ----------\n        other : Any\n            The other object to compare against.\n\n        Returns\n        -------\n        bool\n            True if \"other\" is an Index and it has the same elements and order\n            as the calling index; False otherwise.\n\n        Examples\n        --------\n        >>> idx1 = pd.Index([1, 2, 3])\n        >>> idx1\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> idx1.equals(pd.Index([1, 2, 3]))\n        True\n\n        The elements inside are compared\n\n        >>> idx2 = pd.Index([\"1\", \"2\", \"3\"])\n        >>> idx2\n        Index(['1', '2', '3'], dtype='object')\n\n        >>> idx1.equals(idx2)\n        False\n\n        The order is compared\n\n        >>> ascending_idx = pd.Index([1, 2, 3])\n        >>> ascending_idx\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> descending_idx = pd.Index([3, 2, 1])\n        >>> descending_idx\n        Int64Index([3, 2, 1], dtype='int64')\n        >>> ascending_idx.equals(descending_idx)\n        False\n\n        The dtype is *not* compared\n\n        >>> int64_idx = pd.Int64Index([1, 2, 3])\n        >>> int64_idx\n        Int64Index([1, 2, 3], dtype='int64')\n        >>> uint64_idx = pd.UInt64Index([1, 2, 3])\n        >>> uint64_idx\n        UInt64Index([1, 2, 3], dtype='uint64')\n        >>> int64_idx.equals(uint64_idx)\n        True\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        # If other is a subclass of self and defines its own equals method, we\n        # dispatch to the subclass method. For instance for a MultiIndex,\n        # a d-level MultiIndex can equal d-tuple Index.\n        # Note: All EA-backed Index subclasses override equals\n        if (\n            isinstance(other, type(self))\n            and type(other) is not type(self)\n            and other.equals is not self.equals\n        ):\n            return other.equals(self)\n\n        return array_equivalent(self._values, other._values)\n\n    @final\n    def identical(self, other) -> bool:\n        \"\"\"\n        Similar to equals, but checks that object attributes and types are also equal.\n\n        Returns\n        -------\n        bool\n            If two Index objects have equal elements and same type True,\n            otherwise False.\n        \"\"\"\n        return (\n            self.equals(other)\n            and all(\n                getattr(self, c, None) == getattr(other, c, None)\n                for c in self._comparables\n            )\n            and type(self) == type(other)\n        )\n\n    @final\n    def asof(self, label):\n        \"\"\"\n        Return the label from the index, or, if not present, the previous one.\n\n        Assuming that the index is sorted, return the passed index label if it\n        is in the index, or return the previous index label if the passed one\n        is not in the index.\n\n        Parameters\n        ----------\n        label : object\n            The label up to which the method returns the latest index label.\n\n        Returns\n        -------\n        object\n            The passed label if it is in the index. The previous label if the\n            passed label is not in the sorted index or `NaN` if there is no\n            such label.\n\n        See Also\n        --------\n        Series.asof : Return the latest value in a Series up to the\n            passed index.\n        merge_asof : Perform an asof merge (similar to left join but it\n            matches on nearest key rather than equal key).\n        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n            with method='pad'.\n\n        Examples\n        --------\n        `Index.asof` returns the latest index label up to the passed label.\n\n        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n        >>> idx.asof('2014-01-01')\n        '2013-12-31'\n\n        If the label is in the index, the method returns the passed label.\n\n        >>> idx.asof('2014-01-02')\n        '2014-01-02'\n\n        If all of the labels in the index are later than the passed label,\n        NaN is returned.\n\n        >>> idx.asof('1999-01-02')\n        nan\n\n        If the index is not sorted, an error is raised.\n\n        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n        ...                            '2014-01-03'])\n        >>> idx_not_sorted.asof('2013-12-31')\n        Traceback (most recent call last):\n        ValueError: index must be monotonic increasing or decreasing\n        \"\"\"\n        try:\n            loc = self.get_loc(label, method=\"pad\")\n        except KeyError:\n            return self._na_value\n        else:\n            if isinstance(loc, slice):\n                loc = loc.indices(len(self))[-1]\n            return self[loc]\n\n    def asof_locs(self, where: \"Index\", mask) -> np.ndarray:\n        \"\"\"\n        Return the locations (indices) of labels in the index.\n\n        As in the `asof` function, if the label (a particular entry in\n        `where`) is not in the index, the latest index label up to the\n        passed label is chosen and its index returned.\n\n        If all of the labels in the index are later than a label in `where`,\n        -1 is returned.\n\n        `mask` is used to ignore NA values in the index during calculation.\n\n        Parameters\n        ----------\n        where : Index\n            An Index consisting of an array of timestamps.\n        mask : array-like\n            Array of booleans denoting where values in the original\n            data are not NA.\n\n        Returns\n        -------\n        numpy.ndarray\n            An array of locations (indices) of the labels from the Index\n            which correspond to the return values of the `asof` function\n            for every element in `where`.\n        \"\"\"\n        locs = self._values[mask].searchsorted(where._values, side=\"right\")\n        locs = np.where(locs > 0, locs - 1, 0)\n\n        result = np.arange(len(self))[mask].take(locs)\n\n        # TODO: overload return type of ExtensionArray.__getitem__\n        first_value = cast(Any, self._values[mask.argmax()])\n        result[(locs == 0) & (where._values < first_value)] = -1\n\n        return result\n\n    @final\n    def sort_values(\n        self,\n        return_indexer: bool = False,\n        ascending: bool = True,\n        na_position: str_t = \"last\",\n        key: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Return a sorted copy of the index.\n\n        Return a sorted copy of the index, and optionally return the indices\n        that sorted the index itself.\n\n        Parameters\n        ----------\n        return_indexer : bool, default False\n            Should the indices that would sort the index be returned.\n        ascending : bool, default True\n            Should the index values be sorted in an ascending order.\n        na_position : {'first' or 'last'}, default 'last'\n            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n            the end.\n\n            .. versionadded:: 1.2.0\n\n        key : callable, optional\n            If not None, apply the key function to the index values\n            before sorting. This is similar to the `key` argument in the\n            builtin :meth:`sorted` function, with the notable difference that\n            this `key` function should be *vectorized*. It should expect an\n            ``Index`` and return an ``Index`` of the same shape.\n\n            .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        sorted_index : pandas.Index\n            Sorted copy of the index.\n        indexer : numpy.ndarray, optional\n            The indices that the index itself was sorted by.\n\n        See Also\n        --------\n        Series.sort_values : Sort values of a Series.\n        DataFrame.sort_values : Sort values in a DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index([10, 100, 1, 1000])\n        >>> idx\n        Int64Index([10, 100, 1, 1000], dtype='int64')\n\n        Sort values in ascending order (default behavior).\n\n        >>> idx.sort_values()\n        Int64Index([1, 10, 100, 1000], dtype='int64')\n\n        Sort values in descending order, and also get the indices `idx` was\n        sorted by.\n\n        >>> idx.sort_values(ascending=False, return_indexer=True)\n        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n        \"\"\"\n        idx = ensure_key_mapped(self, key)\n\n        # GH 35584. Sort missing values according to na_position kwarg\n        # ignore na_position for MultiIndex\n        if not isinstance(self, ABCMultiIndex):\n            _as = nargsort(\n                items=idx, ascending=ascending, na_position=na_position, key=key\n            )\n        else:\n            _as = idx.argsort()\n            if not ascending:\n                _as = _as[::-1]\n\n        sorted_index = self.take(_as)\n\n        if return_indexer:\n            return sorted_index, _as\n        else:\n            return sorted_index\n\n    @final\n    def sort(self, *args, **kwargs):\n        \"\"\"\n        Use sort_values instead.\n        \"\"\"\n        raise TypeError(\"cannot sort an Index object in-place, use sort_values instead\")\n\n    def shift(self, periods=1, freq=None):\n        \"\"\"\n        Shift index by desired number of time frequency increments.\n\n        This method is for shifting the values of datetime-like indexes\n        by a specified time increment a given number of times.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Number of periods (or increments) to shift by,\n            can be positive or negative.\n        freq : pandas.DateOffset, pandas.Timedelta or str, optional\n            Frequency increment to shift by.\n            If None, the index is shifted by its own `freq` attribute.\n            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n\n        Returns\n        -------\n        pandas.Index\n            Shifted index.\n\n        See Also\n        --------\n        Series.shift : Shift values of Series.\n\n        Notes\n        -----\n        This method is only implemented for datetime-like index classes,\n        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n\n        Examples\n        --------\n        Put the first 5 month starts of 2011 into an index.\n\n        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n        >>> month_starts\n        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n                       '2011-05-01'],\n                      dtype='datetime64[ns]', freq='MS')\n\n        Shift the index by 10 days.\n\n        >>> month_starts.shift(10, freq='D')\n        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n                       '2011-05-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n        The default value of `freq` is the `freq` attribute of the index,\n        which is 'MS' (month start) in this example.\n\n        >>> month_starts.shift(10)\n        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n                       '2012-03-01'],\n                      dtype='datetime64[ns]', freq='MS')\n        \"\"\"\n        raise NotImplementedError(f\"Not supported for type {type(self).__name__}\")\n\n    def argsort(self, *args, **kwargs) -> np.ndarray:\n        \"\"\"\n        Return the integer indices that would sort the index.\n\n        Parameters\n        ----------\n        *args\n            Passed to `numpy.ndarray.argsort`.\n        **kwargs\n            Passed to `numpy.ndarray.argsort`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Integer indices that would sort the index if used as\n            an indexer.\n\n        See Also\n        --------\n        numpy.argsort : Similar method for NumPy arrays.\n        Index.sort_values : Return sorted copy of Index.\n\n        Examples\n        --------\n        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n        >>> idx\n        Index(['b', 'a', 'd', 'c'], dtype='object')\n\n        >>> order = idx.argsort()\n        >>> order\n        array([1, 0, 3, 2])\n\n        >>> idx[order]\n        Index(['a', 'b', 'c', 'd'], dtype='object')\n        \"\"\"\n        if needs_i8_conversion(self.dtype):\n            # TODO: these do not match the underlying EA argsort methods GH#37863\n            return self.asi8.argsort(*args, **kwargs)\n\n        # This works for either ndarray or EA, is overriden\n        #  by RangeIndex, MultIIndex\n        return self._data.argsort(*args, **kwargs)\n\n    @final\n    def get_value(self, series: \"Series\", key):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        Only use this if you know what you're doing.\n\n        Returns\n        -------\n        scalar or Series\n        \"\"\"\n        warnings.warn(\n            \"get_value is deprecated and will be removed in a future version. \"\n            \"Use Series[key] instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n        self._check_indexing_error(key)\n\n        try:\n            # GH 20882, 21257\n            # First try to convert the key to a location\n            # If that fails, raise a KeyError if an integer\n            # index, otherwise, see if key is an integer, and\n            # try that\n            loc = self.get_loc(key)\n        except KeyError:\n            if not self._should_fallback_to_positional():\n                raise\n            elif is_integer(key):\n                # If the Index cannot hold integer, then this is unambiguously\n                #  a locational lookup.\n                loc = key\n            else:\n                raise\n\n        return self._get_values_for_loc(series, loc, key)\n\n    def _check_indexing_error(self, key):\n        if not is_scalar(key):\n            # if key is not a scalar, directly raise an error (the code below\n            # would convert to numpy arrays and raise later any way) - GH29926\n            raise InvalidIndexError(key)\n\n    def _should_fallback_to_positional(self) -> bool:\n        \"\"\"\n        Should an integer key be treated as positional?\n        \"\"\"\n        if self.holds_integer() or self.is_boolean():\n            return False\n        return True\n\n    def _get_values_for_loc(self, series: \"Series\", loc, key):\n        \"\"\"\n        Do a positional lookup on the given Series, returning either a scalar\n        or a Series.\n\n        Assumes that `series.index is self`\n\n        key is included for MultiIndex compat.\n        \"\"\"\n        if is_integer(loc):\n            return series._values[loc]\n\n        return series.iloc[loc]\n\n    @final\n    def set_value(self, arr, key, value):\n        \"\"\"\n        Fast lookup of value from 1-dimensional ndarray.\n\n        .. deprecated:: 1.0\n\n        Notes\n        -----\n        Only use this if you know what you're doing.\n        \"\"\"\n        warnings.warn(\n            (\n                \"The 'set_value' method is deprecated, and \"\n                \"will be removed in a future version.\"\n            ),\n            FutureWarning,\n            stacklevel=2,\n        )\n        loc = self._engine.get_loc(key)\n        validate_numeric_casting(arr.dtype, value)\n        arr[loc] = value\n\n    _index_shared_docs[\n        \"get_indexer_non_unique\"\n    ] = \"\"\"\n        Compute indexer and mask for new index given the current index. The\n        indexer should be then used as an input to ndarray.take to align the\n        current data to the new index.\n\n        Parameters\n        ----------\n        target : %(target_klass)s\n\n        Returns\n        -------\n        indexer : ndarray of int\n            Integers from 0 to n - 1 indicating that the index at these\n            positions matches the corresponding target values. Missing values\n            in the target are marked by -1.\n        missing : ndarray of int\n            An indexer into the target of the values not found.\n            These correspond to the -1 in the indexer array.\n        \"\"\"\n\n    @Appender(_index_shared_docs[\"get_indexer_non_unique\"] % _index_doc_kwargs)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n\n        if target.is_boolean() and self.is_numeric():\n            # Treat boolean labels passed to a numeric index as not found. Without\n            # this fix False and True would be treated as 0 and 1 respectively.\n            # (GH #16877)\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n\n        if is_categorical_dtype(target.dtype):\n            tgt_values = np.asarray(target)\n        else:\n            tgt_values = target._get_engine_target()\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n\n    @final\n    def get_indexer_for(self, target, **kwargs):\n        \"\"\"\n        Guaranteed return of an indexer even when non-unique.\n\n        This dispatches to get_indexer or get_indexer_non_unique\n        as appropriate.\n\n        Returns\n        -------\n        numpy.ndarray\n            List of indices.\n        \"\"\"\n        if self._index_as_unique:\n            return self.get_indexer(target, **kwargs)\n        indexer, _ = self.get_indexer_non_unique(target)\n        return indexer\n\n    def _get_indexer_non_comparable(self, target: \"Index\", method, unique: bool = True):\n        \"\"\"\n        Called from get_indexer or get_indexer_non_unique when the target\n        is of a non-comparable dtype.\n\n        For get_indexer lookups with method=None, get_indexer is an _equality_\n        check, so non-comparable dtypes mean we will always have no matches.\n\n        For get_indexer lookups with a method, get_indexer is an _inequality_\n        check, so non-comparable dtypes mean we will always raise TypeError.\n\n        Parameters\n        ----------\n        target : Index\n        method : str or None\n        unique : bool, default True\n            * True if called from get_indexer.\n            * False if called from get_indexer_non_unique.\n\n        Raises\n        ------\n        TypeError\n            If doing an inequality check, i.e. method is not None.\n        \"\"\"\n        if method is not None:\n            other = _unpack_nested_dtype(target)\n            raise TypeError(f\"Cannot compare dtypes {self.dtype} and {other.dtype}\")\n\n        no_matches = -1 * np.ones(target.shape, dtype=np.intp)\n        if unique:\n            # This is for get_indexer\n            return no_matches\n        else:\n            # This is for get_indexer_non_unique\n            missing = np.arange(len(target), dtype=np.intp)\n            return no_matches, missing\n\n    @property\n    def _index_as_unique(self):\n        \"\"\"\n        Whether we should treat this as unique for the sake of\n        get_indexer vs get_indexer_non_unique.\n\n        For IntervalIndex compat.\n        \"\"\"\n        return self.is_unique\n\n    @final\n    def _maybe_promote(self, other: \"Index\"):\n        \"\"\"\n        When dealing with an object-dtype Index and a non-object Index, see\n        if we can upcast the object-dtype one to improve performance.\n        \"\"\"\n\n        if self.inferred_type == \"date\" and isinstance(other, ABCDatetimeIndex):\n            try:\n                return type(other)(self), other\n            except OutOfBoundsDatetime:\n                return self, other\n        elif self.inferred_type == \"timedelta\" and isinstance(other, ABCTimedeltaIndex):\n            # TODO: we dont have tests that get here\n            return type(other)(self), other\n        elif self.inferred_type == \"boolean\":\n            if not is_object_dtype(self.dtype):\n                return self.astype(\"object\"), other.astype(\"object\")\n\n        if not is_object_dtype(self.dtype) and is_object_dtype(other.dtype):\n            # Reverse op so we dont need to re-implement on the subclasses\n            other, self = other._maybe_promote(self)\n\n        return self, other\n\n    def _should_compare(self, other: \"Index\") -> bool:\n        \"\"\"\n        Check if `self == other` can ever have non-False entries.\n        \"\"\"\n        other = _unpack_nested_dtype(other)\n        dtype = other.dtype\n        return self._is_comparable_dtype(dtype) or is_object_dtype(dtype)\n\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        \"\"\"\n        Can we compare values of the given dtype to our own?\n        \"\"\"\n        return True\n\n    @final\n    def groupby(self, values) -> PrettyDict[Hashable, np.ndarray]:\n        \"\"\"\n        Group the index labels by a given array of values.\n\n        Parameters\n        ----------\n        values : array\n            Values used to determine the groups.\n\n        Returns\n        -------\n        dict\n            {group name -> group labels}\n        \"\"\"\n        # TODO: if we are a MultiIndex, we can do better\n        # that converting to tuples\n        if isinstance(values, ABCMultiIndex):\n            values = values._values\n        values = Categorical(values)\n        result = values._reverse_indexer()\n\n        # map to the label\n        result = {k: self.take(v) for k, v in result.items()}\n\n        return PrettyDict(result)\n\n    def map(self, mapper, na_action=None):\n        \"\"\"\n        Map values using input correspondence (a dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : function, dict, or Series\n            Mapping correspondence.\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping correspondence.\n\n        Returns\n        -------\n        applied : Union[Index, MultiIndex], inferred\n            The output of the mapping function applied to the index.\n            If the function returns a tuple with more than one element\n            a MultiIndex will be returned.\n        \"\"\"\n        from pandas.core.indexes.multi import MultiIndex\n\n        new_values = super()._map_values(mapper, na_action=na_action)\n\n        attributes = self._get_attributes_dict()\n\n        # we can return a MultiIndex\n        if new_values.size and isinstance(new_values[0], tuple):\n            if isinstance(self, MultiIndex):\n                names = self.names\n            elif attributes.get(\"name\"):\n                names = [attributes.get(\"name\")] * len(new_values[0])\n            else:\n                names = None\n            return MultiIndex.from_tuples(new_values, names=names)\n\n        attributes[\"copy\"] = False\n        if not new_values.size:\n            # empty\n            attributes[\"dtype\"] = self.dtype\n\n        return Index(new_values, **attributes)\n\n    # TODO: De-duplicate with map, xref GH#32349\n    @final\n    def _transform_index(self, func, level=None) -> \"Index\":\n        \"\"\"\n        Apply function to all values found in index.\n\n        This includes transforming multiindex entries separately.\n        Only apply function to one level of the MultiIndex if level is specified.\n        \"\"\"\n        if isinstance(self, ABCMultiIndex):\n            if level is not None:\n                items = [\n                    tuple(func(y) if i == level else y for i, y in enumerate(x))\n                    for x in self\n                ]\n            else:\n                items = [tuple(func(y) for y in x) for x in self]\n            return type(self).from_tuples(items, names=self.names)\n        else:\n            items = [func(x) for x in self]\n            return Index(items, name=self.name, tupleize_cols=False)\n\n    def isin(self, values, level=None):\n        \"\"\"\n        Return a boolean array where the index values are in `values`.\n\n        Compute boolean array of whether each index value is found in the\n        passed set of values. The length of the returned boolean array matches\n        the length of the index.\n\n        Parameters\n        ----------\n        values : set or list-like\n            Sought values.\n        level : str or int, optional\n            Name or position of the index level to use (if the index is a\n            `MultiIndex`).\n\n        Returns\n        -------\n        is_contained : ndarray\n            NumPy array of boolean values.\n\n        See Also\n        --------\n        Series.isin : Same for Series.\n        DataFrame.isin : Same method for DataFrames.\n\n        Notes\n        -----\n        In the case of `MultiIndex` you must either specify `values` as a\n        list-like object containing tuples that are the same length as the\n        number of levels, or specify `level`. Otherwise it will raise a\n        ``ValueError``.\n\n        If `level` is specified:\n\n        - if it is the name of one *and only one* index level, use that level;\n        - otherwise it should be a number indicating level position.\n\n        Examples\n        --------\n        >>> idx = pd.Index([1,2,3])\n        >>> idx\n        Int64Index([1, 2, 3], dtype='int64')\n\n        Check whether each index value in a list of values.\n\n        >>> idx.isin([1, 4])\n        array([ True, False, False])\n\n        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n        ...                                  ['red', 'blue', 'green']],\n        ...                                  names=('number', 'color'))\n        >>> midx\n        MultiIndex([(1,   'red'),\n                    (2,  'blue'),\n                    (3, 'green')],\n                   names=['number', 'color'])\n\n        Check whether the strings in the 'color' level of the MultiIndex\n        are in a list of colors.\n\n        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n        array([ True, False, False])\n\n        To check across the levels of a MultiIndex, pass a list of tuples:\n\n        >>> midx.isin([(1, 'red'), (3, 'red')])\n        array([ True, False, False])\n\n        For a DatetimeIndex, string values in `values` are converted to\n        Timestamps.\n\n        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n        >>> dti = pd.to_datetime(dates)\n        >>> dti\n        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n        dtype='datetime64[ns]', freq=None)\n\n        >>> dti.isin(['2000-03-11'])\n        array([ True, False, False])\n        \"\"\"\n        if level is not None:\n            self._validate_index_level(level)\n        return algos.isin(self._values, values)\n\n    def _get_string_slice(self, key: str_t):\n        # this is for partial string indexing,\n        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n        raise NotImplementedError\n\n    def slice_indexer(\n        self,\n        start: Optional[Label] = None,\n        end: Optional[Label] = None,\n        step: Optional[int] = None,\n        kind: Optional[str_t] = None,\n    ) -> slice:\n        \"\"\"\n        Compute the slice indexer for input labels and step.\n\n        Index needs to be ordered and unique.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning.\n        end : label, default None\n            If None, defaults to the end.\n        step : int, default None\n        kind : str, default None\n\n        Returns\n        -------\n        indexer : slice\n\n        Raises\n        ------\n        KeyError : If key does not exist, or key is not unique and index is\n            not ordered.\n\n        Notes\n        -----\n        This function assumes that the data is sorted, so use at your own peril\n\n        Examples\n        --------\n        This is a method on all index types. For example you can do:\n\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_indexer(start='b', end='c')\n        slice(1, 3, None)\n\n        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n        slice(1, 3, None)\n        \"\"\"\n        start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind)\n\n        # return a slice\n        if not is_scalar(start_slice):\n            raise AssertionError(\"Start slice bound is non-scalar\")\n        if not is_scalar(end_slice):\n            raise AssertionError(\"End slice bound is non-scalar\")\n\n        return slice(start_slice, end_slice, step)\n\n    def _maybe_cast_indexer(self, key):\n        \"\"\"\n        If we have a float key and are not a floating index, then try to cast\n        to an int if equivalent.\n        \"\"\"\n        if not self.is_floating():\n            return com.cast_scalar_indexer(key)\n        return key\n\n    @final\n    def _validate_indexer(self, form: str_t, key, kind: str_t):\n        \"\"\"\n        If we are positional indexer, validate that we have appropriate\n        typed bounds must be an integer.\n        \"\"\"\n        assert kind in [\"getitem\", \"iloc\"]\n\n        if key is None:\n            pass\n        elif is_integer(key):\n            pass\n        else:\n            raise self._invalid_indexer(form, key)\n\n    def _maybe_cast_slice_bound(self, label, side: str_t, kind):\n        \"\"\"\n        This function should be overloaded in subclasses that allow non-trivial\n        casting on label-slice bounds, e.g. datetime-like indices allowing\n        strings containing formatted datetimes.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'loc', 'getitem'} or None\n\n        Returns\n        -------\n        label : object\n\n        Notes\n        -----\n        Value of `side` parameter should be validated in caller.\n        \"\"\"\n        assert kind in [\"loc\", \"getitem\", None]\n\n        # We are a plain index here (sub-class override this method if they\n        # wish to have special treatment for floats/ints, e.g. Float64Index and\n        # datetimelike Indexes\n        # reject them, if index does not contain label\n        if (is_float(label) or is_integer(label)) and label not in self.values:\n            raise self._invalid_indexer(\"slice\", label)\n\n        return label\n\n    def _searchsorted_monotonic(self, label, side=\"left\"):\n        if self.is_monotonic_increasing:\n            return self.searchsorted(label, side=side)\n        elif self.is_monotonic_decreasing:\n            # np.searchsorted expects ascending sort order, have to reverse\n            # everything for it to work (element ordering, search side and\n            # resulting value).\n            pos = self[::-1].searchsorted(\n                label, side=\"right\" if side == \"left\" else \"left\"\n            )\n            return len(self) - pos\n\n        raise ValueError(\"index must be monotonic increasing or decreasing\")\n\n    def get_slice_bound(self, label, side: str_t, kind) -> int:\n        \"\"\"\n        Calculate slice bound that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n        of given label.\n\n        Parameters\n        ----------\n        label : object\n        side : {'left', 'right'}\n        kind : {'loc', 'getitem'} or None\n\n        Returns\n        -------\n        int\n            Index of label.\n        \"\"\"\n        assert kind in [\"loc\", \"getitem\", None]\n\n        if side not in (\"left\", \"right\"):\n            raise ValueError(\n                \"Invalid value for side kwarg, must be either \"\n                f\"'left' or 'right': {side}\"\n            )\n\n        original_label = label\n\n        # For datetime indices label may be a string that has to be converted\n        # to datetime boundary according to its resolution.\n        label = self._maybe_cast_slice_bound(label, side, kind)\n\n        # we need to look up the label\n        try:\n            slc = self.get_loc(label)\n        except KeyError as err:\n            try:\n                return self._searchsorted_monotonic(label, side)\n            except ValueError:\n                # raise the original KeyError\n                raise err\n\n        if isinstance(slc, np.ndarray):\n            # get_loc may return a boolean array or an array of indices, which\n            # is OK as long as they are representable by a slice.\n            if is_bool_dtype(slc):\n                slc = lib.maybe_booleans_to_slice(slc.view(\"u1\"))\n            else:\n                slc = lib.maybe_indices_to_slice(\n                    slc.astype(np.intp, copy=False), len(self)\n                )\n            if isinstance(slc, np.ndarray):\n                raise KeyError(\n                    f\"Cannot get {side} slice bound for non-unique \"\n                    f\"label: {repr(original_label)}\"\n                )\n\n        if isinstance(slc, slice):\n            if side == \"left\":\n                return slc.start\n            else:\n                return slc.stop\n        else:\n            if side == \"right\":\n                return slc + 1\n            else:\n                return slc\n\n    def slice_locs(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        Compute slice locations for input labels.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning.\n        end : label, default None\n            If None, defaults to the end.\n        step : int, defaults None\n            If None, defaults to 1.\n        kind : {'loc', 'getitem'} or None\n\n        Returns\n        -------\n        start, end : int\n\n        See Also\n        --------\n        Index.get_loc : Get location for a single label.\n\n        Notes\n        -----\n        This method only works if the index is monotonic or unique.\n\n        Examples\n        --------\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_locs(start='b', end='c')\n        (1, 3)\n        \"\"\"\n        inc = step is None or step >= 0\n\n        if not inc:\n            # If it's a reverse slice, temporarily swap bounds.\n            start, end = end, start\n\n        # GH 16785: If start and end happen to be date strings with UTC offsets\n        # attempt to parse and check that the offsets are the same\n        if isinstance(start, (str, datetime)) and isinstance(end, (str, datetime)):\n            try:\n                ts_start = Timestamp(start)\n                ts_end = Timestamp(end)\n            except (ValueError, TypeError):\n                pass\n            else:\n                if not tz_compare(ts_start.tzinfo, ts_end.tzinfo):\n                    raise ValueError(\"Both dates must have the same UTC offset\")\n\n        start_slice = None\n        if start is not None:\n            start_slice = self.get_slice_bound(start, \"left\", kind)\n        if start_slice is None:\n            start_slice = 0\n\n        end_slice = None\n        if end is not None:\n            end_slice = self.get_slice_bound(end, \"right\", kind)\n        if end_slice is None:\n            end_slice = len(self)\n\n        if not inc:\n            # Bounds at this moment are swapped, swap them back and shift by 1.\n            #\n            # slice_locs('B', 'A', step=-1): s='B', e='A'\n            #\n            #              s='A'                 e='B'\n            # AFTER SWAP:    |                     |\n            #                v ------------------> V\n            #           -----------------------------------\n            #           | | |A|A|A|A| | | | | |B|B| | | | |\n            #           -----------------------------------\n            #              ^ <------------------ ^\n            # SHOULD BE:   |                     |\n            #           end=s-1              start=e-1\n            #\n            end_slice, start_slice = start_slice - 1, end_slice - 1\n\n            # i == -1 triggers ``len(self) + i`` selection that points to the\n            # last element, not before-the-first one, subtracting len(self)\n            # compensates that.\n            if end_slice == -1:\n                end_slice -= len(self)\n            if start_slice == -1:\n                start_slice -= len(self)\n\n        return start_slice, end_slice\n\n    def delete(self, loc):\n        \"\"\"\n        Make new Index with passed location(-s) deleted.\n\n        Parameters\n        ----------\n        loc : int or list of int\n            Location of item(-s) which will be deleted.\n            Use a list of locations to delete more than one value at the same time.\n\n        Returns\n        -------\n        Index\n            New Index with passed location(-s) deleted.\n\n        See Also\n        --------\n        numpy.delete : Delete any rows and column from NumPy array (ndarray).\n\n        Examples\n        --------\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx.delete(1)\n        Index(['a', 'c'], dtype='object')\n\n        >>> idx = pd.Index(['a', 'b', 'c'])\n        >>> idx.delete([0, 2])\n        Index(['b'], dtype='object')\n        \"\"\"\n        return self._shallow_copy(np.delete(self._data, loc))\n\n    def insert(self, loc: int, item):\n        \"\"\"\n        Make new Index inserting new item at location.\n\n        Follows Python list.append semantics for negative values.\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        # Note: this method is overridden by all ExtensionIndex subclasses,\n        #  so self is never backed by an EA.\n        arr = np.asarray(self)\n        item = self._coerce_scalar_to_index(item)._values\n        idx = np.concatenate((arr[:loc], item, arr[loc:]))\n        return Index(idx, name=self.name)\n\n    def drop(self, labels, errors: str_t = \"raise\"):\n        \"\"\"\n        Make new Index with passed list of labels deleted.\n\n        Parameters\n        ----------\n        labels : array-like\n        errors : {'ignore', 'raise'}, default 'raise'\n            If 'ignore', suppress error and existing labels are dropped.\n\n        Returns\n        -------\n        dropped : Index\n\n        Raises\n        ------\n        KeyError\n            If not all of the labels are found in the selected axis\n        \"\"\"\n        arr_dtype = \"object\" if self.dtype == \"object\" else None\n        labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n        indexer = self.get_indexer(labels)\n        mask = indexer == -1\n        if mask.any():\n            if errors != \"ignore\":\n                raise KeyError(f\"{labels[mask]} not found in axis\")\n            indexer = indexer[~mask]\n        return self.delete(indexer)\n\n    # --------------------------------------------------------------------\n    # Generated Arithmetic, Comparison, and Unary Methods\n\n    def _cmp_method(self, other, op):\n        \"\"\"\n        Wrapper used to dispatch comparison operations.\n        \"\"\"\n        if self.is_(other):\n            # fastpath\n            if op in {operator.eq, operator.le, operator.ge}:\n                arr = np.ones(len(self), dtype=bool)\n                if self._can_hold_na and not isinstance(self, ABCMultiIndex):\n                    # TODO: should set MultiIndex._can_hold_na = False?\n                    arr[self.isna()] = False\n                return arr\n            elif op in {operator.ne, operator.lt, operator.gt}:\n                return np.zeros(len(self), dtype=bool)\n\n        if isinstance(other, (np.ndarray, Index, ABCSeries, ExtensionArray)):\n            if len(self) != len(other):\n                raise ValueError(\"Lengths must match to compare\")\n\n        if not isinstance(other, ABCMultiIndex):\n            other = extract_array(other, extract_numpy=True)\n        else:\n            other = np.asarray(other)\n\n        if is_object_dtype(self.dtype) and isinstance(other, ExtensionArray):\n            # e.g. PeriodArray, Categorical\n            with np.errstate(all=\"ignore\"):\n                result = op(self._values, other)\n\n        elif is_object_dtype(self.dtype) and not isinstance(self, ABCMultiIndex):\n            # don't pass MultiIndex\n            with np.errstate(all=\"ignore\"):\n                result = ops.comp_method_OBJECT_ARRAY(op, self._values, other)\n\n        elif is_interval_dtype(self.dtype):\n            with np.errstate(all=\"ignore\"):\n                result = op(self._values, np.asarray(other))\n\n        else:\n            with np.errstate(all=\"ignore\"):\n                result = ops.comparison_op(self._values, other, op)\n\n        return result\n\n    def _arith_method(self, other, op):\n        \"\"\"\n        Wrapper used to dispatch arithmetic operations.\n        \"\"\"\n\n        from pandas import Series\n\n        result = op(Series(self), other)\n        if isinstance(result, tuple):\n            return (Index(result[0]), Index(result[1]))\n        return Index(result)\n\n    def _unary_method(self, op):\n        result = op(self._values)\n        return Index(result, name=self.name)\n\n    def __abs__(self):\n        return self._unary_method(operator.abs)\n\n    def __neg__(self):\n        return self._unary_method(operator.neg)\n\n    def __pos__(self):\n        return self._unary_method(operator.pos)\n\n    def __inv__(self):\n        # TODO: why not operator.inv?\n        # TODO: __inv__ vs __invert__?\n        return self._unary_method(lambda x: -x)\n\n    def any(self, *args, **kwargs):\n        \"\"\"\n        Return whether any element is Truthy.\n\n        Parameters\n        ----------\n        *args\n            These parameters will be passed to numpy.any.\n        **kwargs\n            These parameters will be passed to numpy.any.\n\n        Returns\n        -------\n        any : bool or array_like (if axis is specified)\n            A single element array_like may be converted to bool.\n\n        See Also\n        --------\n        Index.all : Return whether all elements are True.\n        Series.all : Return whether all elements are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        >>> index = pd.Index([0, 1, 2])\n        >>> index.any()\n        True\n\n        >>> index = pd.Index([0, 0, 0])\n        >>> index.any()\n        False\n        \"\"\"\n        # FIXME: docstr inaccurate, args/kwargs not passed\n        self._maybe_disable_logical_methods(\"any\")\n        return np.any(self.values)\n\n    def all(self):\n        \"\"\"\n        Return whether all elements are Truthy.\n\n        Parameters\n        ----------\n        *args\n            These parameters will be passed to numpy.all.\n        **kwargs\n            These parameters will be passed to numpy.all.\n\n        Returns\n        -------\n        all : bool or array_like (if axis is specified)\n            A single element array_like may be converted to bool.\n\n        See Also\n        --------\n        Index.any : Return whether any element in an Index is True.\n        Series.any : Return whether any element in a Series is True.\n        Series.all : Return whether all elements in a Series are True.\n\n        Notes\n        -----\n        Not a Number (NaN), positive infinity and negative infinity\n        evaluate to True because these are not equal to zero.\n\n        Examples\n        --------\n        **all**\n\n        True, because nonzero integers are considered True.\n\n        >>> pd.Index([1, 2, 3]).all()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 1, 2]).all()\n        False\n\n        **any**\n\n        True, because ``1`` is considered True.\n\n        >>> pd.Index([0, 0, 1]).any()\n        True\n\n        False, because ``0`` is considered False.\n\n        >>> pd.Index([0, 0, 0]).any()\n        False\n        \"\"\"\n        # FIXME: docstr inaccurate, args/kwargs not passed\n\n        self._maybe_disable_logical_methods(\"all\")\n        return np.all(self.values)\n\n    @final\n    def _maybe_disable_logical_methods(self, opname: str_t):\n        \"\"\"\n        raise if this Index subclass does not support any or all.\n        \"\"\"\n        if (\n            isinstance(self, ABCMultiIndex)\n            or needs_i8_conversion(self.dtype)\n            or is_interval_dtype(self.dtype)\n            or is_categorical_dtype(self.dtype)\n            or is_float_dtype(self.dtype)\n        ):\n            # This call will raise\n            make_invalid_op(opname)(self)\n\n    @property\n    def shape(self) -> Shape:\n        \"\"\"\n        Return a tuple of the shape of the underlying data.\n        \"\"\"\n        # not using \"(len(self), )\" to return \"correct\" shape if the values\n        # consists of a >1 D array (see GH-27775)\n        # overridden in MultiIndex.shape to avoid materializing the values\n        return self._values.shape\n\n\ndef ensure_index_from_sequences(sequences, names=None):\n    \"\"\"\n    Construct an index from sequences of data.\n\n    A single sequence returns an Index. Many sequences returns a\n    MultiIndex.\n\n    Parameters\n    ----------\n    sequences : sequence of sequences\n    names : sequence of str\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index_from_sequences([[1, 2, 3]], names=[\"name\"])\n    Int64Index([1, 2, 3], dtype='int64', name='name')\n\n    >>> ensure_index_from_sequences([[\"a\", \"a\"], [\"a\", \"b\"]], names=[\"L1\", \"L2\"])\n    MultiIndex([('a', 'a'),\n                ('a', 'b')],\n               names=['L1', 'L2'])\n\n    See Also\n    --------\n    ensure_index\n    \"\"\"\n    from pandas.core.indexes.multi import MultiIndex\n\n    if len(sequences) == 1:\n        if names is not None:\n            names = names[0]\n        return Index(sequences[0], name=names)\n    else:\n        return MultiIndex.from_arrays(sequences, names=names)\n\n\ndef ensure_index(\n    index_like: Union[AnyArrayLike, Sequence], copy: bool = False\n) -> Index:\n    \"\"\"\n    Ensure that we have an index from some index-like object.\n\n    Parameters\n    ----------\n    index_like : sequence\n        An Index or other sequence\n    copy : bool, default False\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    See Also\n    --------\n    ensure_index_from_sequences\n\n    Examples\n    --------\n    >>> ensure_index(['a', 'b'])\n    Index(['a', 'b'], dtype='object')\n\n    >>> ensure_index([('a', 'a'),  ('b', 'c')])\n    Index([('a', 'a'), ('b', 'c')], dtype='object')\n\n    >>> ensure_index([['a', 'a'], ['b', 'c']])\n    MultiIndex([('a', 'b'),\n            ('a', 'c')],\n           )\n    \"\"\"\n    if isinstance(index_like, Index):\n        if copy:\n            index_like = index_like.copy()\n        return index_like\n    if hasattr(index_like, \"name\"):\n        # https://github.com/python/mypy/issues/1424\n        # error: Item \"ExtensionArray\" of \"Union[ExtensionArray,\n        # Sequence[Any]]\" has no attribute \"name\"  [union-attr]\n        # error: Item \"Sequence[Any]\" of \"Union[ExtensionArray, Sequence[Any]]\"\n        # has no attribute \"name\"  [union-attr]\n        # error: \"Sequence[Any]\" has no attribute \"name\"  [attr-defined]\n        # error: Item \"Sequence[Any]\" of \"Union[Series, Sequence[Any]]\" has no\n        # attribute \"name\"  [union-attr]\n        # error: Item \"Sequence[Any]\" of \"Union[Any, Sequence[Any]]\" has no\n        # attribute \"name\"  [union-attr]\n        name = index_like.name  # type: ignore[union-attr, attr-defined]\n        return Index(index_like, name=name, copy=copy)\n\n    if is_iterator(index_like):\n        index_like = list(index_like)\n\n    # must check for exactly list here because of strict type\n    # check in clean_index_list\n    if isinstance(index_like, list):\n        if type(index_like) != list:\n            index_like = list(index_like)\n\n        converted, all_arrays = lib.clean_index_list(index_like)\n\n        if len(converted) > 0 and all_arrays:\n            from pandas.core.indexes.multi import MultiIndex\n\n            return MultiIndex.from_arrays(converted)\n        else:\n            if isinstance(converted, np.ndarray) and converted.dtype == np.int64:\n                # Check for overflows if we should actually be uint64\n                # xref GH#35481\n                alt = np.asarray(index_like)\n                if alt.dtype == np.uint64:\n                    converted = alt\n\n            index_like = converted\n    else:\n        # clean_index_list does the equivalent of copying\n        # so only need to do this if not list instance\n        if copy:\n            index_like = copy_func(index_like)\n\n    return Index(index_like)\n\n\ndef ensure_has_len(seq):\n    \"\"\"\n    If seq is an iterator, put its values into a list.\n    \"\"\"\n    try:\n        len(seq)\n    except TypeError:\n        return list(seq)\n    else:\n        return seq\n\n\ndef trim_front(strings: List[str]) -> List[str]:\n    \"\"\"\n    Trims zeros and decimal points.\n    \"\"\"\n    trimmed = strings\n    while len(strings) > 0 and all(x[0] == \" \" for x in trimmed):\n        trimmed = [x[1:] for x in trimmed]\n    return trimmed\n\n\ndef _validate_join_method(method: str):\n    if method not in [\"left\", \"right\", \"inner\", \"outer\"]:\n        raise ValueError(f\"do not recognize join method {method}\")\n\n\ndef default_index(n: int) -> \"RangeIndex\":\n    from pandas.core.indexes.range import RangeIndex\n\n    return RangeIndex(0, n, name=None)\n\n\ndef maybe_extract_name(name, obj, cls) -> Label:\n    \"\"\"\n    If no name is passed, then extract it from data, validating hashability.\n    \"\"\"\n    if name is None and isinstance(obj, (Index, ABCSeries)):\n        # Note we don't just check for \"name\" attribute since that would\n        #  pick up e.g. dtype.name\n        name = obj.name\n\n    # GH#29069\n    if not is_hashable(name):\n        raise TypeError(f\"{cls.__name__}.name must be a hashable type\")\n\n    return name\n\n\ndef _maybe_cast_with_dtype(data: np.ndarray, dtype: np.dtype, copy: bool) -> np.ndarray:\n    \"\"\"\n    If a dtype is passed, cast to the closest matching dtype that is supported\n    by Index.\n\n    Parameters\n    ----------\n    data : np.ndarray\n    dtype : np.dtype\n    copy : bool\n\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    # we need to avoid having numpy coerce\n    # things that look like ints/floats to ints unless\n    # they are actually ints, e.g. '0' and 0.0\n    # should not be coerced\n    # GH 11836\n    if is_integer_dtype(dtype):\n        inferred = lib.infer_dtype(data, skipna=False)\n        if inferred == \"integer\":\n            data = maybe_cast_to_integer_array(data, dtype, copy=copy)\n        elif inferred in [\"floating\", \"mixed-integer-float\"]:\n            if isna(data).any():\n                raise ValueError(\"cannot convert float NaN to integer\")\n\n            if inferred == \"mixed-integer-float\":\n                data = maybe_cast_to_integer_array(data, dtype)\n\n            # If we are actually all equal to integers,\n            # then coerce to integer.\n            try:\n                data = _try_convert_to_int_array(data, copy, dtype)\n            except ValueError:\n                data = np.array(data, dtype=np.float64, copy=copy)\n\n        elif inferred == \"string\":\n            pass\n        else:\n            data = data.astype(dtype)\n    elif is_float_dtype(dtype):\n        inferred = lib.infer_dtype(data, skipna=False)\n        if inferred == \"string\":\n            pass\n        else:\n            data = data.astype(dtype)\n    else:\n        data = np.array(data, dtype=dtype, copy=copy)\n\n    return data\n\n\ndef _maybe_cast_data_without_dtype(subarr):\n    \"\"\"\n    If we have an arraylike input but no passed dtype, try to infer\n    a supported dtype.\n\n    Parameters\n    ----------\n    subarr : np.ndarray, Index, or Series\n\n    Returns\n    -------\n    converted : np.ndarray or ExtensionArray\n    dtype : np.dtype or ExtensionDtype\n    \"\"\"\n    # Runtime import needed bc IntervalArray imports Index\n    from pandas.core.arrays import (\n        DatetimeArray,\n        IntervalArray,\n        PeriodArray,\n        TimedeltaArray,\n    )\n\n    inferred = lib.infer_dtype(subarr, skipna=False)\n\n    if inferred == \"integer\":\n        try:\n            data = _try_convert_to_int_array(subarr, False, None)\n            return data, data.dtype\n        except ValueError:\n            pass\n\n        return subarr, object\n\n    elif inferred in [\"floating\", \"mixed-integer-float\", \"integer-na\"]:\n        # TODO: Returns IntegerArray for integer-na case in the future\n        return subarr, np.float64\n\n    elif inferred == \"interval\":\n        try:\n            data = IntervalArray._from_sequence(subarr, copy=False)\n            return data, data.dtype\n        except ValueError:\n            # GH27172: mixed closed Intervals --> object dtype\n            pass\n    elif inferred == \"boolean\":\n        # don't support boolean explicitly ATM\n        pass\n    elif inferred != \"string\":\n        if inferred.startswith(\"datetime\"):\n            try:\n                data = DatetimeArray._from_sequence(subarr, copy=False)\n                return data, data.dtype\n            except (ValueError, OutOfBoundsDatetime):\n                # GH 27011\n                # If we have mixed timezones, just send it\n                # down the base constructor\n                pass\n\n        elif inferred.startswith(\"timedelta\"):\n            data = TimedeltaArray._from_sequence(subarr, copy=False)\n            return data, data.dtype\n        elif inferred == \"period\":\n            try:\n                data = PeriodArray._from_sequence(subarr)\n                return data, data.dtype\n            except IncompatibleFrequency:\n                pass\n\n    return subarr, subarr.dtype\n\n\ndef _try_convert_to_int_array(\n    data: np.ndarray, copy: bool, dtype: np.dtype\n) -> np.ndarray:\n    \"\"\"\n    Attempt to convert an array of data into an integer array.\n\n    Parameters\n    ----------\n    data : The data to convert.\n    copy : bool\n        Whether to copy the data or not.\n    dtype : np.dtype\n\n    Returns\n    -------\n    int_array : data converted to either an ndarray[int64] or ndarray[uint64]\n\n    Raises\n    ------\n    ValueError if the conversion was not successful.\n    \"\"\"\n    if not is_unsigned_integer_dtype(dtype):\n        # skip int64 conversion attempt if uint-like dtype is passed, as\n        # this could return Int64Index when UInt64Index is what's desired\n        try:\n            res = data.astype(\"i8\", copy=False)\n            if (res == data).all():\n                return res  # TODO: might still need to copy\n        except (OverflowError, TypeError, ValueError):\n            pass\n\n    # Conversion to int64 failed (possibly due to overflow) or was skipped,\n    # so let's try now with uint64.\n    try:\n        res = data.astype(\"u8\", copy=False)\n        if (res == data).all():\n            return res  # TODO: might still need to copy\n    except (OverflowError, TypeError, ValueError):\n        pass\n\n    raise ValueError\n\n\ndef _maybe_asobject(dtype, klass, data, copy: bool, name: Label, **kwargs):\n    \"\"\"\n    If an object dtype was specified, create the non-object Index\n    and then convert it to object.\n\n    Parameters\n    ----------\n    dtype : np.dtype, ExtensionDtype, str\n    klass : Index subclass\n    data : list-like\n    copy : bool\n    name : hashable\n    **kwargs\n\n    Returns\n    -------\n    Index\n\n    Notes\n    -----\n    We assume that calling .astype(object) on this klass will make a copy.\n    \"\"\"\n\n    # GH#23524 passing `dtype=object` to DatetimeIndex is invalid,\n    #  will raise in the where `data` is already tz-aware.  So\n    #  we leave it out of this step and cast to object-dtype after\n    #  the DatetimeIndex construction.\n\n    if is_dtype_equal(_o_dtype, dtype):\n        # Note we can pass copy=False because the .astype below\n        #  will always make a copy\n        index = klass(data, copy=False, name=name, **kwargs)\n        return index.astype(object)\n\n    return klass(data, dtype=dtype, copy=copy, name=name, **kwargs)\n\n\ndef get_unanimous_names(*indexes: Index) -> Tuple[Label, ...]:\n    \"\"\"\n    Return common name if all indices agree, otherwise None (level-by-level).\n\n    Parameters\n    ----------\n    indexes : list of Index objects\n\n    Returns\n    -------\n    list\n        A list representing the unanimous 'names' found.\n    \"\"\"\n    name_tups = [tuple(i.names) for i in indexes]\n    name_sets = [{*ns} for ns in zip_longest(*name_tups)]\n    names = tuple(ns.pop() if len(ns) == 1 else None for ns in name_sets)\n    return names\n\n\ndef _unpack_nested_dtype(other: Index) -> Index:\n    \"\"\"\n    When checking if our dtype is comparable with another, we need\n    to unpack CategoricalDtype to look at its categories.dtype.\n\n    Parameters\n    ----------\n    other : Index\n\n    Returns\n    -------\n    Index\n    \"\"\"\n    dtype = other.dtype\n    if is_categorical_dtype(dtype):\n        # If there is ever a SparseIndex, this could get dispatched\n        #  here too.\n        return dtype.categories\n    return other\n"
    },
    {
      "filename": "pandas/core/indexes/interval.py",
      "content": "\"\"\" define the IntervalIndex \"\"\"\nfrom functools import wraps\nfrom operator import le, lt\nimport textwrap\nfrom typing import TYPE_CHECKING, Any, List, Optional, Tuple, Union, cast\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import lib\nfrom pandas._libs.interval import Interval, IntervalMixin, IntervalTree\nfrom pandas._libs.tslibs import BaseOffset, Timedelta, Timestamp, to_offset\nfrom pandas._typing import AnyArrayLike, Label\nfrom pandas.errors import InvalidIndexError\nfrom pandas.util._decorators import Appender, Substitution, cache_readonly\nfrom pandas.util._exceptions import rewrite_exception\n\nfrom pandas.core.dtypes.cast import (\n    find_common_type,\n    infer_dtype_from_scalar,\n    maybe_box_datetimelike,\n    maybe_downcast_to_dtype,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_platform_int,\n    is_categorical_dtype,\n    is_datetime64tz_dtype,\n    is_datetime_or_timedelta_dtype,\n    is_dtype_equal,\n    is_float,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_interval_dtype,\n    is_list_like,\n    is_number,\n    is_object_dtype,\n    is_scalar,\n)\n\nfrom pandas.core.algorithms import take_1d\nfrom pandas.core.arrays.interval import IntervalArray, _interval_shared_docs\nimport pandas.core.common as com\nfrom pandas.core.indexers import is_valid_positional_slice\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import (\n    Index,\n    _index_shared_docs,\n    default_pprint,\n    ensure_index,\n    maybe_extract_name,\n)\nfrom pandas.core.indexes.datetimes import DatetimeIndex, date_range\nfrom pandas.core.indexes.extension import ExtensionIndex, inherit_names\nfrom pandas.core.indexes.multi import MultiIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex, timedelta_range\nfrom pandas.core.ops import get_op_result_name\n\nif TYPE_CHECKING:\n    from pandas import CategoricalIndex\n\n_index_doc_kwargs = dict(ibase._index_doc_kwargs)\n\n_index_doc_kwargs.update(\n    {\n        \"klass\": \"IntervalIndex\",\n        \"qualname\": \"IntervalIndex\",\n        \"target_klass\": \"IntervalIndex or list of Intervals\",\n        \"name\": textwrap.dedent(\n            \"\"\"\\\n         name : object, optional\n              Name to be stored in the index.\n         \"\"\"\n        ),\n    }\n)\n\n\ndef _get_next_label(label):\n    dtype = getattr(label, \"dtype\", type(label))\n    if isinstance(label, (Timestamp, Timedelta)):\n        dtype = \"datetime64\"\n    if is_datetime_or_timedelta_dtype(dtype) or is_datetime64tz_dtype(dtype):\n        return label + np.timedelta64(1, \"ns\")\n    elif is_integer_dtype(dtype):\n        return label + 1\n    elif is_float_dtype(dtype):\n        return np.nextafter(label, np.infty)\n    else:\n        raise TypeError(f\"cannot determine next label for type {repr(type(label))}\")\n\n\ndef _get_prev_label(label):\n    dtype = getattr(label, \"dtype\", type(label))\n    if isinstance(label, (Timestamp, Timedelta)):\n        dtype = \"datetime64\"\n    if is_datetime_or_timedelta_dtype(dtype) or is_datetime64tz_dtype(dtype):\n        return label - np.timedelta64(1, \"ns\")\n    elif is_integer_dtype(dtype):\n        return label - 1\n    elif is_float_dtype(dtype):\n        return np.nextafter(label, -np.infty)\n    else:\n        raise TypeError(f\"cannot determine next label for type {repr(type(label))}\")\n\n\ndef _new_IntervalIndex(cls, d):\n    \"\"\"\n    This is called upon unpickling, rather than the default which doesn't have\n    arguments and breaks __new__.\n    \"\"\"\n    return cls.from_arrays(**d)\n\n\ndef setop_check(method):\n    \"\"\"\n    This is called to decorate the set operations of IntervalIndex\n    to perform the type check in advance.\n    \"\"\"\n    op_name = method.__name__\n\n    @wraps(method)\n    def wrapped(self, other, sort=False):\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, _ = self._convert_can_do_setop(other)\n\n        if op_name == \"intersection\":\n            if self.equals(other):\n                return self._get_reconciled_name_object(other)\n\n        if not isinstance(other, IntervalIndex):\n            result = getattr(self.astype(object), op_name)(other)\n            if op_name in (\"difference\",):\n                result = result.astype(self.dtype)\n            return result\n\n        return method(self, other, sort)\n\n    return wrapped\n\n\n@Appender(\n    _interval_shared_docs[\"class\"]\n    % {\n        \"klass\": \"IntervalIndex\",\n        \"summary\": \"Immutable index of intervals that are closed on the same side.\",\n        \"name\": _index_doc_kwargs[\"name\"],\n        \"versionadded\": \"0.20.0\",\n        \"extra_attributes\": \"is_overlapping\\nvalues\\n\",\n        \"extra_methods\": \"\",\n        \"examples\": textwrap.dedent(\n            \"\"\"\\\n    Examples\n    --------\n    A new ``IntervalIndex`` is typically constructed using\n    :func:`interval_range`:\n\n    >>> pd.interval_range(start=0, end=5)\n    IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]],\n                  closed='right',\n                  dtype='interval[int64]')\n\n    It may also be constructed using one of the constructor\n    methods: :meth:`IntervalIndex.from_arrays`,\n    :meth:`IntervalIndex.from_breaks`, and :meth:`IntervalIndex.from_tuples`.\n\n    See further examples in the doc strings of ``interval_range`` and the\n    mentioned constructor methods.\n    \"\"\"\n        ),\n    }\n)\n@inherit_names([\"set_closed\", \"to_tuples\"], IntervalArray, wrap=True)\n@inherit_names([\"__array__\", \"overlaps\", \"contains\"], IntervalArray)\n@inherit_names([\"is_non_overlapping_monotonic\", \"closed\"], IntervalArray, cache=True)\nclass IntervalIndex(IntervalMixin, ExtensionIndex):\n    _typ = \"intervalindex\"\n    _comparables = [\"name\"]\n    _attributes = [\"name\", \"closed\"]\n\n    # we would like our indexing holder to defer to us\n    _defer_to_indexing = True\n\n    _data: IntervalArray\n    _values: IntervalArray\n    _can_hold_strings = False\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls,\n        data,\n        closed=None,\n        dtype=None,\n        copy: bool = False,\n        name=None,\n        verify_integrity: bool = True,\n    ):\n\n        name = maybe_extract_name(name, data, cls)\n\n        with rewrite_exception(\"IntervalArray\", cls.__name__):\n            array = IntervalArray(\n                data,\n                closed=closed,\n                copy=copy,\n                dtype=dtype,\n                verify_integrity=verify_integrity,\n            )\n\n        return cls._simple_new(array, name)\n\n    @classmethod\n    def _simple_new(cls, array: IntervalArray, name: Label = None):\n        \"\"\"\n        Construct from an IntervalArray\n\n        Parameters\n        ----------\n        array : IntervalArray\n        name : Label, default None\n            Attached as result.name\n        \"\"\"\n        assert isinstance(array, IntervalArray), type(array)\n\n        result = IntervalMixin.__new__(cls)\n        result._data = array\n        result.name = name\n        result._cache = {}\n        result._reset_identity()\n        return result\n\n    @classmethod\n    @Appender(\n        _interval_shared_docs[\"from_breaks\"]\n        % {\n            \"klass\": \"IntervalIndex\",\n            \"examples\": textwrap.dedent(\n                \"\"\"\\\n        Examples\n        --------\n        >>> pd.IntervalIndex.from_breaks([0, 1, 2, 3])\n        IntervalIndex([(0, 1], (1, 2], (2, 3]],\n                      closed='right',\n                      dtype='interval[int64]')\n        \"\"\"\n            ),\n        }\n    )\n    def from_breaks(\n        cls, breaks, closed: str = \"right\", name=None, copy: bool = False, dtype=None\n    ):\n        with rewrite_exception(\"IntervalArray\", cls.__name__):\n            array = IntervalArray.from_breaks(\n                breaks, closed=closed, copy=copy, dtype=dtype\n            )\n        return cls._simple_new(array, name=name)\n\n    @classmethod\n    @Appender(\n        _interval_shared_docs[\"from_arrays\"]\n        % {\n            \"klass\": \"IntervalIndex\",\n            \"examples\": textwrap.dedent(\n                \"\"\"\\\n        Examples\n        --------\n        >>> pd.IntervalIndex.from_arrays([0, 1, 2], [1, 2, 3])\n        IntervalIndex([(0, 1], (1, 2], (2, 3]],\n                      closed='right',\n                      dtype='interval[int64]')\n        \"\"\"\n            ),\n        }\n    )\n    def from_arrays(\n        cls,\n        left,\n        right,\n        closed: str = \"right\",\n        name=None,\n        copy: bool = False,\n        dtype=None,\n    ):\n        with rewrite_exception(\"IntervalArray\", cls.__name__):\n            array = IntervalArray.from_arrays(\n                left, right, closed, copy=copy, dtype=dtype\n            )\n        return cls._simple_new(array, name=name)\n\n    @classmethod\n    @Appender(\n        _interval_shared_docs[\"from_tuples\"]\n        % {\n            \"klass\": \"IntervalIndex\",\n            \"examples\": textwrap.dedent(\n                \"\"\"\\\n        Examples\n        --------\n        >>> pd.IntervalIndex.from_tuples([(0, 1), (1, 2)])\n        IntervalIndex([(0, 1], (1, 2]],\n                       closed='right',\n                       dtype='interval[int64]')\n        \"\"\"\n            ),\n        }\n    )\n    def from_tuples(\n        cls, data, closed: str = \"right\", name=None, copy: bool = False, dtype=None\n    ):\n        with rewrite_exception(\"IntervalArray\", cls.__name__):\n            arr = IntervalArray.from_tuples(data, closed=closed, copy=copy, dtype=dtype)\n        return cls._simple_new(arr, name=name)\n\n    # --------------------------------------------------------------------\n\n    @cache_readonly\n    def _engine(self):\n        left = self._maybe_convert_i8(self.left)\n        right = self._maybe_convert_i8(self.right)\n        return IntervalTree(left, right, closed=self.closed)\n\n    def __contains__(self, key: Any) -> bool:\n        \"\"\"\n        return a boolean if this key is IN the index\n        We *only* accept an Interval\n\n        Parameters\n        ----------\n        key : Interval\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        hash(key)\n        if not isinstance(key, Interval):\n            return False\n\n        try:\n            self.get_loc(key)\n            return True\n        except KeyError:\n            return False\n\n    @cache_readonly\n    def _multiindex(self) -> MultiIndex:\n        return MultiIndex.from_arrays([self.left, self.right], names=[\"left\", \"right\"])\n\n    @cache_readonly\n    def values(self) -> IntervalArray:\n        \"\"\"\n        Return the IntervalIndex's data as an IntervalArray.\n        \"\"\"\n        return self._data\n\n    def __array_wrap__(self, result, context=None):\n        # we don't want the superclass implementation\n        return result\n\n    def __reduce__(self):\n        d = {\"left\": self.left, \"right\": self.right}\n        d.update(self._get_attributes_dict())\n        return _new_IntervalIndex, (type(self), d), None\n\n    @Appender(Index.astype.__doc__)\n    def astype(self, dtype, copy: bool = True):\n        with rewrite_exception(\"IntervalArray\", type(self).__name__):\n            new_values = self._values.astype(dtype, copy=copy)\n        return Index(new_values, dtype=new_values.dtype, name=self.name)\n\n    @property\n    def inferred_type(self) -> str:\n        \"\"\"Return a string of the type inferred from the values\"\"\"\n        return \"interval\"\n\n    @Appender(Index.memory_usage.__doc__)\n    def memory_usage(self, deep: bool = False) -> int:\n        # we don't use an explicit engine\n        # so return the bytes here\n        return self.left.memory_usage(deep=deep) + self.right.memory_usage(deep=deep)\n\n    # IntervalTree doesn't have a is_monotonic_decreasing, so have to override\n    #  the Index implementation\n    @cache_readonly\n    def is_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        Return True if the IntervalIndex is monotonic decreasing (only equal or\n        decreasing values), else False\n        \"\"\"\n        return self[::-1].is_monotonic_increasing\n\n    @cache_readonly\n    def is_unique(self) -> bool:\n        \"\"\"\n        Return True if the IntervalIndex contains unique elements, else False.\n        \"\"\"\n        left = self.left\n        right = self.right\n\n        if self.isna().sum() > 1:\n            return False\n\n        if left.is_unique or right.is_unique:\n            return True\n\n        seen_pairs = set()\n        check_idx = np.where(left.duplicated(keep=False))[0]\n        for idx in check_idx:\n            pair = (left[idx], right[idx])\n            if pair in seen_pairs:\n                return False\n            seen_pairs.add(pair)\n\n        return True\n\n    @property\n    def is_overlapping(self) -> bool:\n        \"\"\"\n        Return True if the IntervalIndex has overlapping intervals, else False.\n\n        Two intervals overlap if they share a common point, including closed\n        endpoints. Intervals that only have an open endpoint in common do not\n        overlap.\n\n        .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        bool\n            Boolean indicating if the IntervalIndex has overlapping intervals.\n\n        See Also\n        --------\n        Interval.overlaps : Check whether two Interval objects overlap.\n        IntervalIndex.overlaps : Check an IntervalIndex elementwise for\n            overlaps.\n\n        Examples\n        --------\n        >>> index = pd.IntervalIndex.from_tuples([(0, 2), (1, 3), (4, 5)])\n        >>> index\n        IntervalIndex([(0, 2], (1, 3], (4, 5]],\n              closed='right',\n              dtype='interval[int64]')\n        >>> index.is_overlapping\n        True\n\n        Intervals that share closed endpoints overlap:\n\n        >>> index = pd.interval_range(0, 3, closed='both')\n        >>> index\n        IntervalIndex([[0, 1], [1, 2], [2, 3]],\n              closed='both',\n              dtype='interval[int64]')\n        >>> index.is_overlapping\n        True\n\n        Intervals that only have an open endpoint in common do not overlap:\n\n        >>> index = pd.interval_range(0, 3, closed='left')\n        >>> index\n        IntervalIndex([[0, 1), [1, 2), [2, 3)],\n              closed='left',\n              dtype='interval[int64]')\n        >>> index.is_overlapping\n        False\n        \"\"\"\n        # GH 23309\n        return self._engine.is_overlapping\n\n    def _needs_i8_conversion(self, key) -> bool:\n        \"\"\"\n        Check if a given key needs i8 conversion. Conversion is necessary for\n        Timestamp, Timedelta, DatetimeIndex, and TimedeltaIndex keys. An\n        Interval-like requires conversion if its endpoints are one of the\n        aforementioned types.\n\n        Assumes that any list-like data has already been cast to an Index.\n\n        Parameters\n        ----------\n        key : scalar or Index-like\n            The key that should be checked for i8 conversion\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        if is_interval_dtype(key) or isinstance(key, Interval):\n            return self._needs_i8_conversion(key.left)\n\n        i8_types = (Timestamp, Timedelta, DatetimeIndex, TimedeltaIndex)\n        return isinstance(key, i8_types)\n\n    def _maybe_convert_i8(self, key):\n        \"\"\"\n        Maybe convert a given key to its equivalent i8 value(s). Used as a\n        preprocessing step prior to IntervalTree queries (self._engine), which\n        expects numeric data.\n\n        Parameters\n        ----------\n        key : scalar or list-like\n            The key that should maybe be converted to i8.\n\n        Returns\n        -------\n        scalar or list-like\n            The original key if no conversion occurred, int if converted scalar,\n            Int64Index if converted list-like.\n        \"\"\"\n        original = key\n        if is_list_like(key):\n            key = ensure_index(key)\n\n        if not self._needs_i8_conversion(key):\n            return original\n\n        scalar = is_scalar(key)\n        if is_interval_dtype(key) or isinstance(key, Interval):\n            # convert left/right and reconstruct\n            left = self._maybe_convert_i8(key.left)\n            right = self._maybe_convert_i8(key.right)\n            constructor = Interval if scalar else IntervalIndex.from_arrays\n            return constructor(left, right, closed=self.closed)\n\n        if scalar:\n            # Timestamp/Timedelta\n            key_dtype, key_i8 = infer_dtype_from_scalar(key, pandas_dtype=True)\n            if lib.is_period(key):\n                key_i8 = key.ordinal\n        else:\n            # DatetimeIndex/TimedeltaIndex\n            key_dtype, key_i8 = key.dtype, Index(key.asi8)\n            if key.hasnans:\n                # convert NaT from its i8 value to np.nan so it's not viewed\n                # as a valid value, maybe causing errors (e.g. is_overlapping)\n                key_i8 = key_i8.where(~key._isnan)\n\n        # ensure consistency with IntervalIndex subtype\n        subtype = self.dtype.subtype\n\n        if not is_dtype_equal(subtype, key_dtype):\n            raise ValueError(\n                f\"Cannot index an IntervalIndex of subtype {subtype} with \"\n                f\"values of dtype {key_dtype}\"\n            )\n\n        return key_i8\n\n    def _searchsorted_monotonic(self, label, side, exclude_label=False):\n        if not self.is_non_overlapping_monotonic:\n            raise KeyError(\n                \"can only get slices from an IntervalIndex if bounds are \"\n                \"non-overlapping and all monotonic increasing or decreasing\"\n            )\n\n        if isinstance(label, IntervalMixin):\n            raise NotImplementedError(\"Interval objects are not currently supported\")\n\n        # GH 20921: \"not is_monotonic_increasing\" for the second condition\n        # instead of \"is_monotonic_decreasing\" to account for single element\n        # indexes being both increasing and decreasing\n        if (side == \"left\" and self.left.is_monotonic_increasing) or (\n            side == \"right\" and not self.left.is_monotonic_increasing\n        ):\n            sub_idx = self.right\n            if self.open_right or exclude_label:\n                label = _get_next_label(label)\n        else:\n            sub_idx = self.left\n            if self.open_left or exclude_label:\n                label = _get_prev_label(label)\n\n        return sub_idx._searchsorted_monotonic(label, side)\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    def get_loc(\n        self, key, method: Optional[str] = None, tolerance=None\n    ) -> Union[int, slice, np.ndarray]:\n        \"\"\"\n        Get integer location, slice or boolean mask for requested label.\n\n        Parameters\n        ----------\n        key : label\n        method : {None}, optional\n            * default: matches where the label is within an interval only.\n\n        Returns\n        -------\n        int if unique index, slice if monotonic index, else mask\n\n        Examples\n        --------\n        >>> i1, i2 = pd.Interval(0, 1), pd.Interval(1, 2)\n        >>> index = pd.IntervalIndex([i1, i2])\n        >>> index.get_loc(1)\n        0\n\n        You can also supply a point inside an interval.\n\n        >>> index.get_loc(1.5)\n        1\n\n        If a label is in several intervals, you get the locations of all the\n        relevant intervals.\n\n        >>> i3 = pd.Interval(0, 2)\n        >>> overlapping_index = pd.IntervalIndex([i1, i2, i3])\n        >>> overlapping_index.get_loc(0.5)\n        array([ True, False,  True])\n\n        Only exact matches will be returned if an interval is provided.\n\n        >>> index.get_loc(pd.Interval(0, 1))\n        0\n        \"\"\"\n        self._check_indexing_method(method)\n\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n\n        if isinstance(key, Interval):\n            if self.closed != key.closed:\n                raise KeyError(key)\n            mask = (self.left == key.left) & (self.right == key.right)\n        else:\n            # assume scalar\n            op_left = le if self.closed_left else lt\n            op_right = le if self.closed_right else lt\n            try:\n                mask = op_left(self.left, key) & op_right(key, self.right)\n            except TypeError as err:\n                # scalar is not comparable to II subtype --> invalid label\n                raise KeyError(key) from err\n\n        matches = mask.sum()\n        if matches == 0:\n            raise KeyError(key)\n        elif matches == 1:\n            return mask.argmax()\n        return lib.maybe_booleans_to_slice(mask.view(\"u1\"))\n\n    @Substitution(\n        **dict(\n            _index_doc_kwargs,\n            **{\n                \"raises_section\": textwrap.dedent(\n                    \"\"\"\n        Raises\n        ------\n        NotImplementedError\n            If any method argument other than the default of\n            None is specified as these are not yet implemented.\n        \"\"\"\n                )\n            },\n        )\n    )\n    @Appender(_index_shared_docs[\"get_indexer\"])\n    def get_indexer(\n        self,\n        target: AnyArrayLike,\n        method: Optional[str] = None,\n        limit: Optional[int] = None,\n        tolerance: Optional[Any] = None,\n    ) -> np.ndarray:\n\n        self._check_indexing_method(method)\n\n        if self.is_overlapping:\n            raise InvalidIndexError(\n                \"cannot handle overlapping indices; \"\n                \"use IntervalIndex.get_indexer_non_unique\"\n            )\n\n        target_as_index = ensure_index(target)\n\n        if isinstance(target_as_index, IntervalIndex):\n            # equal indexes -> 1:1 positional match\n            if self.equals(target_as_index):\n                return np.arange(len(self), dtype=\"intp\")\n\n            if self._is_non_comparable_own_type(target_as_index):\n                # different closed or incompatible subtype -> no matches\n                return np.repeat(np.intp(-1), len(target_as_index))\n\n            # non-overlapping -> at most one match per interval in target_as_index\n            # want exact matches -> need both left/right to match, so defer to\n            # left/right get_indexer, compare elementwise, equality -> match\n            left_indexer = self.left.get_indexer(target_as_index.left)\n            right_indexer = self.right.get_indexer(target_as_index.right)\n            indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n        elif is_categorical_dtype(target_as_index.dtype):\n            target_as_index = cast(\"CategoricalIndex\", target_as_index)\n            # get an indexer for unique categories then propagate to codes via take_1d\n            categories_indexer = self.get_indexer(target_as_index.categories)\n            indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)\n        elif not is_object_dtype(target_as_index):\n            # homogeneous scalar index: use IntervalTree\n            target_as_index = self._maybe_convert_i8(target_as_index)\n            indexer = self._engine.get_indexer(target_as_index.values)\n        else:\n            # heterogeneous scalar index: defer elementwise to get_loc\n            return self._get_indexer_pointwise(target_as_index)[0]\n\n        return ensure_platform_int(indexer)\n\n    @Appender(_index_shared_docs[\"get_indexer_non_unique\"] % _index_doc_kwargs)\n    def get_indexer_non_unique(\n        self, target: AnyArrayLike\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        target_as_index = ensure_index(target)\n\n        # check that target_as_index IntervalIndex is compatible\n        if isinstance(target_as_index, IntervalIndex):\n\n            if self._is_non_comparable_own_type(target_as_index):\n                # different closed or incompatible subtype -> no matches\n                return (\n                    np.repeat(-1, len(target_as_index)),\n                    np.arange(len(target_as_index)),\n                )\n\n        if is_object_dtype(target_as_index) or isinstance(\n            target_as_index, IntervalIndex\n        ):\n            # target_as_index might contain intervals: defer elementwise to get_loc\n            return self._get_indexer_pointwise(target_as_index)\n\n        else:\n            target_as_index = self._maybe_convert_i8(target_as_index)\n            indexer, missing = self._engine.get_indexer_non_unique(\n                target_as_index.values\n            )\n\n        return ensure_platform_int(indexer), ensure_platform_int(missing)\n\n    def _get_indexer_pointwise(self, target: Index) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        pointwise implementation for get_indexer and get_indexer_non_unique.\n        \"\"\"\n        indexer, missing = [], []\n        for i, key in enumerate(target):\n            try:\n                locs = self.get_loc(key)\n                if isinstance(locs, slice):\n                    # Only needed for get_indexer_non_unique\n                    locs = np.arange(locs.start, locs.stop, locs.step, dtype=\"intp\")\n                locs = np.array(locs, ndmin=1)\n            except KeyError:\n                missing.append(i)\n                locs = np.array([-1])\n            except InvalidIndexError as err:\n                # i.e. non-scalar key\n                raise TypeError(key) from err\n\n            indexer.append(locs)\n\n        indexer = np.concatenate(indexer)\n        return ensure_platform_int(indexer), ensure_platform_int(missing)\n\n    @property\n    def _index_as_unique(self):\n        return not self.is_overlapping\n\n    def _convert_slice_indexer(self, key: slice, kind: str):\n        if not (key.step is None or key.step == 1):\n            # GH#31658 if label-based, we require step == 1,\n            #  if positional, we disallow float start/stop\n            msg = \"label-based slicing with step!=1 is not supported for IntervalIndex\"\n            if kind == \"loc\":\n                raise ValueError(msg)\n            elif kind == \"getitem\":\n                if not is_valid_positional_slice(key):\n                    # i.e. this cannot be interpreted as a positional slice\n                    raise ValueError(msg)\n\n        return super()._convert_slice_indexer(key, kind)\n\n    def _should_fallback_to_positional(self) -> bool:\n        # integer lookups in Series.__getitem__ are unambiguously\n        #  positional in this case\n        return self.dtype.subtype.kind in [\"m\", \"M\"]\n\n    def _maybe_cast_slice_bound(self, label, side: str, kind):\n        return getattr(self, side)._maybe_cast_slice_bound(label, side, kind)\n\n    @Appender(Index._convert_list_indexer.__doc__)\n    def _convert_list_indexer(self, keyarr):\n        \"\"\"\n        we are passed a list-like indexer. Return the\n        indexer for matching intervals.\n        \"\"\"\n        locs = self.get_indexer_for(keyarr)\n\n        # we have missing values\n        if (locs == -1).any():\n            raise KeyError(keyarr[locs == -1].tolist())\n\n        return locs\n\n    def _is_non_comparable_own_type(self, other: \"IntervalIndex\") -> bool:\n        # different closed or incompatible subtype -> no matches\n\n        # TODO: once closed is part of IntervalDtype, we can just define\n        #  is_comparable_dtype GH#19371\n        if self.closed != other.closed:\n            return True\n        common_subtype = find_common_type([self.dtype.subtype, other.dtype.subtype])\n        return is_object_dtype(common_subtype)\n\n    # --------------------------------------------------------------------\n\n    @cache_readonly\n    def left(self) -> Index:\n        return Index(self._data.left, copy=False)\n\n    @cache_readonly\n    def right(self) -> Index:\n        return Index(self._data.right, copy=False)\n\n    @cache_readonly\n    def mid(self):\n        return Index(self._data.mid, copy=False)\n\n    @property\n    def length(self):\n        return Index(self._data.length, copy=False)\n\n    def putmask(self, mask, value):\n        arr = self._data.copy()\n        try:\n            value_left, value_right = arr._validate_setitem_value(value)\n        except (ValueError, TypeError):\n            return self.astype(object).putmask(mask, value)\n\n        if isinstance(self._data._left, np.ndarray):\n            np.putmask(arr._left, mask, value_left)\n            np.putmask(arr._right, mask, value_right)\n        else:\n            # TODO: special case not needed with __array_function__\n            arr._left.putmask(mask, value_left)\n            arr._right.putmask(mask, value_right)\n        return type(self)._simple_new(arr, name=self.name)\n\n    @Appender(Index.where.__doc__)\n    def where(self, cond, other=None):\n        if other is None:\n            other = self._na_value\n        values = np.where(cond, self._values, other)\n        result = IntervalArray(values)\n        return type(self)._simple_new(result, name=self.name)\n\n    def delete(self, loc):\n        \"\"\"\n        Return a new IntervalIndex with passed location(-s) deleted\n\n        Returns\n        -------\n        IntervalIndex\n        \"\"\"\n        new_left = self.left.delete(loc)\n        new_right = self.right.delete(loc)\n        result = self._data._shallow_copy(new_left, new_right)\n        return type(self)._simple_new(result, name=self.name)\n\n    def insert(self, loc, item):\n        \"\"\"\n        Return a new IntervalIndex inserting new item at location. Follows\n        Python list.append semantics for negative values.  Only Interval\n        objects and NA can be inserted into an IntervalIndex\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        IntervalIndex\n        \"\"\"\n        left_insert, right_insert = self._data._validate_scalar(item)\n\n        new_left = self.left.insert(loc, left_insert)\n        new_right = self.right.insert(loc, right_insert)\n        result = self._data._shallow_copy(new_left, new_right)\n        return type(self)._simple_new(result, name=self.name)\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n    # __repr__ associated methods are based on MultiIndex\n\n    def _format_with_header(self, header: List[str], na_rep: str = \"NaN\") -> List[str]:\n        return header + list(self._format_native_types(na_rep=na_rep))\n\n    def _format_native_types(self, na_rep=\"NaN\", quoting=None, **kwargs):\n        # GH 28210: use base method but with different default na_rep\n        return super()._format_native_types(na_rep=na_rep, quoting=quoting, **kwargs)\n\n    def _format_data(self, name=None):\n\n        # TODO: integrate with categorical and make generic\n        # name argument is unused here; just for compat with base / categorical\n        n = len(self)\n        max_seq_items = min((get_option(\"display.max_seq_items\") or n) // 10, 10)\n\n        formatter = str\n\n        if n == 0:\n            summary = \"[]\"\n        elif n == 1:\n            first = formatter(self[0])\n            summary = f\"[{first}]\"\n        elif n == 2:\n            first = formatter(self[0])\n            last = formatter(self[-1])\n            summary = f\"[{first}, {last}]\"\n        else:\n\n            if n > max_seq_items:\n                n = min(max_seq_items // 2, 10)\n                head = [formatter(x) for x in self[:n]]\n                tail = [formatter(x) for x in self[-n:]]\n                head_joined = \", \".join(head)\n                tail_joined = \", \".join(tail)\n                summary = f\"[{head_joined} ... {tail_joined}]\"\n            else:\n                tail = [formatter(x) for x in self]\n                joined = \", \".join(tail)\n                summary = f\"[{joined}]\"\n\n        return summary + \",\" + self._format_space()\n\n    def _format_attrs(self):\n        attrs = [(\"closed\", repr(self.closed))]\n        if self.name is not None:\n            attrs.append((\"name\", default_pprint(self.name)))\n        attrs.append((\"dtype\", f\"'{self.dtype}'\"))\n        return attrs\n\n    def _format_space(self) -> str:\n        space = \" \" * (len(type(self).__name__) + 1)\n        return f\"\\n{space}\"\n\n    # --------------------------------------------------------------------\n    # Set Operations\n\n    def _assert_can_do_setop(self, other):\n        super()._assert_can_do_setop(other)\n\n        if isinstance(other, IntervalIndex) and self._is_non_comparable_own_type(other):\n            # GH#19016: ensure set op will not return a prohibited dtype\n            raise TypeError(\n                \"can only do set operations between two IntervalIndex \"\n                \"objects that are closed on the same side \"\n                \"and have compatible dtypes\"\n            )\n\n    @Appender(Index.intersection.__doc__)\n    @setop_check\n    def intersection(self, other, sort=False) -> Index:\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, _ = self._convert_can_do_setop(other)\n\n        if self.equals(other) and not self.has_duplicates:\n            return self._get_reconciled_name_object(other)\n\n        if not isinstance(other, IntervalIndex):\n            return self.astype(object).intersection(other)\n\n        result = self._intersection(other, sort=sort)\n        return self._wrap_setop_result(other, result)\n\n    def _intersection(self, other, sort):\n        \"\"\"\n        intersection specialized to the case with matching dtypes.\n        \"\"\"\n        # For IntervalIndex we also know other.closed == self.closed\n        if self.left.is_unique and self.right.is_unique:\n            taken = self._intersection_unique(other)\n        elif other.left.is_unique and other.right.is_unique and self.isna().sum() <= 1:\n            # Swap other/self if other is unique and self does not have\n            # multiple NaNs\n            taken = other._intersection_unique(self)\n        else:\n            # duplicates\n            taken = self._intersection_non_unique(other)\n\n        if sort is None:\n            taken = taken.sort_values()\n\n        return taken\n\n    def _intersection_unique(self, other: \"IntervalIndex\") -> \"IntervalIndex\":\n        \"\"\"\n        Used when the IntervalIndex does not have any common endpoint,\n        no matter left or right.\n        Return the intersection with another IntervalIndex.\n\n        Parameters\n        ----------\n        other : IntervalIndex\n\n        Returns\n        -------\n        IntervalIndex\n        \"\"\"\n        lindexer = self.left.get_indexer(other.left)\n        rindexer = self.right.get_indexer(other.right)\n\n        match = (lindexer == rindexer) & (lindexer != -1)\n        indexer = lindexer.take(match.nonzero()[0])\n\n        return self.take(indexer)\n\n    def _intersection_non_unique(self, other: \"IntervalIndex\") -> \"IntervalIndex\":\n        \"\"\"\n        Used when the IntervalIndex does have some common endpoints,\n        on either sides.\n        Return the intersection with another IntervalIndex.\n\n        Parameters\n        ----------\n        other : IntervalIndex\n\n        Returns\n        -------\n        IntervalIndex\n        \"\"\"\n        mask = np.zeros(len(self), dtype=bool)\n\n        if self.hasnans and other.hasnans:\n            first_nan_loc = np.arange(len(self))[self.isna()][0]\n            mask[first_nan_loc] = True\n\n        other_tups = set(zip(other.left, other.right))\n        for i, tup in enumerate(zip(self.left, self.right)):\n            if tup in other_tups:\n                mask[i] = True\n\n        return self[mask]\n\n    def _setop(op_name: str, sort=None):\n        def func(self, other, sort=sort):\n            result = getattr(self._multiindex, op_name)(other._multiindex, sort=sort)\n            result_name = get_op_result_name(self, other)\n\n            # GH 19101: ensure empty results have correct dtype\n            if result.empty:\n                result = result._values.astype(self.dtype.subtype)\n            else:\n                result = result._values\n\n            return type(self).from_tuples(result, closed=self.closed, name=result_name)\n\n        func.__name__ = op_name\n        return setop_check(func)\n\n    union = _setop(\"union\")\n    difference = _setop(\"difference\")\n    symmetric_difference = _setop(\"symmetric_difference\")\n\n    # --------------------------------------------------------------------\n\n    @property\n    def _is_all_dates(self) -> bool:\n        \"\"\"\n        This is False even when left/right contain datetime-like objects,\n        as the check is done on the Interval itself\n        \"\"\"\n        return False\n\n    # TODO: arithmetic operations\n\n\ndef _is_valid_endpoint(endpoint) -> bool:\n    \"\"\"\n    Helper for interval_range to check if start/end are valid types.\n    \"\"\"\n    return any(\n        [\n            is_number(endpoint),\n            isinstance(endpoint, Timestamp),\n            isinstance(endpoint, Timedelta),\n            endpoint is None,\n        ]\n    )\n\n\ndef _is_type_compatible(a, b) -> bool:\n    \"\"\"\n    Helper for interval_range to check type compat of start/end/freq.\n    \"\"\"\n    is_ts_compat = lambda x: isinstance(x, (Timestamp, BaseOffset))\n    is_td_compat = lambda x: isinstance(x, (Timedelta, BaseOffset))\n    return (\n        (is_number(a) and is_number(b))\n        or (is_ts_compat(a) and is_ts_compat(b))\n        or (is_td_compat(a) and is_td_compat(b))\n        or com.any_none(a, b)\n    )\n\n\ndef interval_range(\n    start=None, end=None, periods=None, freq=None, name=None, closed=\"right\"\n):\n    \"\"\"\n    Return a fixed frequency IntervalIndex.\n\n    Parameters\n    ----------\n    start : numeric or datetime-like, default None\n        Left bound for generating intervals.\n    end : numeric or datetime-like, default None\n        Right bound for generating intervals.\n    periods : int, default None\n        Number of periods to generate.\n    freq : numeric, str, or DateOffset, default None\n        The length of each interval. Must be consistent with the type of start\n        and end, e.g. 2 for numeric, or '5H' for datetime-like.  Default is 1\n        for numeric and 'D' for datetime-like.\n    name : str, default None\n        Name of the resulting IntervalIndex.\n    closed : {'left', 'right', 'both', 'neither'}, default 'right'\n        Whether the intervals are closed on the left-side, right-side, both\n        or neither.\n\n    Returns\n    -------\n    IntervalIndex\n\n    See Also\n    --------\n    IntervalIndex : An Index of intervals that are all closed on the same side.\n\n    Notes\n    -----\n    Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,\n    exactly three must be specified. If ``freq`` is omitted, the resulting\n    ``IntervalIndex`` will have ``periods`` linearly spaced elements between\n    ``start`` and ``end``, inclusively.\n\n    To learn more about datetime-like frequency strings, please see `this link\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n    Examples\n    --------\n    Numeric ``start`` and  ``end`` is supported.\n\n    >>> pd.interval_range(start=0, end=5)\n    IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]],\n                  closed='right', dtype='interval[int64]')\n\n    Additionally, datetime-like input is also supported.\n\n    >>> pd.interval_range(start=pd.Timestamp('2017-01-01'),\n    ...                   end=pd.Timestamp('2017-01-04'))\n    IntervalIndex([(2017-01-01, 2017-01-02], (2017-01-02, 2017-01-03],\n                   (2017-01-03, 2017-01-04]],\n                  closed='right', dtype='interval[datetime64[ns]]')\n\n    The ``freq`` parameter specifies the frequency between the left and right.\n    endpoints of the individual intervals within the ``IntervalIndex``.  For\n    numeric ``start`` and ``end``, the frequency must also be numeric.\n\n    >>> pd.interval_range(start=0, periods=4, freq=1.5)\n    IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]],\n                  closed='right', dtype='interval[float64]')\n\n    Similarly, for datetime-like ``start`` and ``end``, the frequency must be\n    convertible to a DateOffset.\n\n    >>> pd.interval_range(start=pd.Timestamp('2017-01-01'),\n    ...                   periods=3, freq='MS')\n    IntervalIndex([(2017-01-01, 2017-02-01], (2017-02-01, 2017-03-01],\n                   (2017-03-01, 2017-04-01]],\n                  closed='right', dtype='interval[datetime64[ns]]')\n\n    Specify ``start``, ``end``, and ``periods``; the frequency is generated\n    automatically (linearly spaced).\n\n    >>> pd.interval_range(start=0, end=6, periods=4)\n    IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]],\n              closed='right',\n              dtype='interval[float64]')\n\n    The ``closed`` parameter specifies which endpoints of the individual\n    intervals within the ``IntervalIndex`` are closed.\n\n    >>> pd.interval_range(end=5, periods=4, closed='both')\n    IntervalIndex([[1, 2], [2, 3], [3, 4], [4, 5]],\n                  closed='both', dtype='interval[int64]')\n    \"\"\"\n    start = maybe_box_datetimelike(start)\n    end = maybe_box_datetimelike(end)\n    endpoint = start if start is not None else end\n\n    if freq is None and com.any_none(periods, start, end):\n        freq = 1 if is_number(endpoint) else \"D\"\n\n    if com.count_not_none(start, end, periods, freq) != 3:\n        raise ValueError(\n            \"Of the four parameters: start, end, periods, and \"\n            \"freq, exactly three must be specified\"\n        )\n\n    if not _is_valid_endpoint(start):\n        raise ValueError(f\"start must be numeric or datetime-like, got {start}\")\n    elif not _is_valid_endpoint(end):\n        raise ValueError(f\"end must be numeric or datetime-like, got {end}\")\n\n    if is_float(periods):\n        periods = int(periods)\n    elif not is_integer(periods) and periods is not None:\n        raise TypeError(f\"periods must be a number, got {periods}\")\n\n    if freq is not None and not is_number(freq):\n        try:\n            freq = to_offset(freq)\n        except ValueError as err:\n            raise ValueError(\n                f\"freq must be numeric or convertible to DateOffset, got {freq}\"\n            ) from err\n\n    # verify type compatibility\n    if not all(\n        [\n            _is_type_compatible(start, end),\n            _is_type_compatible(start, freq),\n            _is_type_compatible(end, freq),\n        ]\n    ):\n        raise TypeError(\"start, end, freq need to be type compatible\")\n\n    # +1 to convert interval count to breaks count (n breaks = n-1 intervals)\n    if periods is not None:\n        periods += 1\n\n    if is_number(endpoint):\n        # force consistency between start/end/freq (lower end if freq skips it)\n        if com.all_not_none(start, end, freq):\n            end -= (end - start) % freq\n\n        # compute the period/start/end if unspecified (at most one)\n        if periods is None:\n            periods = int((end - start) // freq) + 1\n        elif start is None:\n            start = end - (periods - 1) * freq\n        elif end is None:\n            end = start + (periods - 1) * freq\n\n        breaks = np.linspace(start, end, periods)\n        if all(is_integer(x) for x in com.not_none(start, end, freq)):\n            # np.linspace always produces float output\n            breaks = maybe_downcast_to_dtype(breaks, \"int64\")\n    else:\n        # delegate to the appropriate range function\n        if isinstance(endpoint, Timestamp):\n            breaks = date_range(start=start, end=end, periods=periods, freq=freq)\n        else:\n            breaks = timedelta_range(start=start, end=end, periods=periods, freq=freq)\n\n    return IntervalIndex.from_breaks(breaks, name=name, closed=closed)\n"
    },
    {
      "filename": "pandas/core/indexes/multi.py",
      "content": "from functools import wraps\nfrom sys import getsizeof\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import get_option\n\nfrom pandas._libs import algos as libalgos, index as libindex, lib\nfrom pandas._libs.hashtable import duplicated_int64\nfrom pandas._typing import AnyArrayLike, Label, Scalar, Shape\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import InvalidIndexError, PerformanceWarning, UnsortedIndexError\nfrom pandas.util._decorators import Appender, cache_readonly, doc\n\nfrom pandas.core.dtypes.cast import coerce_indexer_dtype\nfrom pandas.core.dtypes.common import (\n    ensure_int64,\n    ensure_platform_int,\n    is_categorical_dtype,\n    is_hashable,\n    is_integer,\n    is_iterator,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCDatetimeIndex, ABCTimedeltaIndex\nfrom pandas.core.dtypes.missing import array_equivalent, isna\n\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import Categorical\nfrom pandas.core.arrays.categorical import factorize_from_iterables\nimport pandas.core.common as com\nimport pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import (\n    Index,\n    _index_shared_docs,\n    ensure_index,\n    get_unanimous_names,\n)\nfrom pandas.core.indexes.frozen import FrozenList\nfrom pandas.core.indexes.numeric import Int64Index\nimport pandas.core.missing as missing\nfrom pandas.core.ops.invalid import make_invalid_op\nfrom pandas.core.sorting import (\n    get_group_index,\n    indexer_from_factorized,\n    lexsort_indexer,\n)\n\nfrom pandas.io.formats.printing import (\n    format_object_attrs,\n    format_object_summary,\n    pprint_thing,\n)\n\nif TYPE_CHECKING:\n    from pandas import Series\n\n_index_doc_kwargs = dict(ibase._index_doc_kwargs)\n_index_doc_kwargs.update(\n    {\"klass\": \"MultiIndex\", \"target_klass\": \"MultiIndex or list of tuples\"}\n)\n\n\nclass MultiIndexUIntEngine(libindex.BaseMultiIndexCodesEngine, libindex.UInt64Engine):\n    \"\"\"\n    This class manages a MultiIndex by mapping label combinations to positive\n    integers.\n    \"\"\"\n\n    _base = libindex.UInt64Engine\n\n    def _codes_to_ints(self, codes):\n        \"\"\"\n        Transform combination(s) of uint64 in one uint64 (each), in a strictly\n        monotonic way (i.e. respecting the lexicographic order of integer\n        combinations): see BaseMultiIndexCodesEngine documentation.\n\n        Parameters\n        ----------\n        codes : 1- or 2-dimensional array of dtype uint64\n            Combinations of integers (one per row)\n\n        Returns\n        -------\n        scalar or 1-dimensional array, of dtype uint64\n            Integer(s) representing one combination (each).\n        \"\"\"\n        # Shift the representation of each level by the pre-calculated number\n        # of bits:\n        codes <<= self.offsets\n\n        # Now sum and OR are in fact interchangeable. This is a simple\n        # composition of the (disjunct) significant bits of each level (i.e.\n        # each column in \"codes\") in a single positive integer:\n        if codes.ndim == 1:\n            # Single key\n            return np.bitwise_or.reduce(codes)\n\n        # Multiple keys\n        return np.bitwise_or.reduce(codes, axis=1)\n\n\nclass MultiIndexPyIntEngine(libindex.BaseMultiIndexCodesEngine, libindex.ObjectEngine):\n    \"\"\"\n    This class manages those (extreme) cases in which the number of possible\n    label combinations overflows the 64 bits integers, and uses an ObjectEngine\n    containing Python integers.\n    \"\"\"\n\n    _base = libindex.ObjectEngine\n\n    def _codes_to_ints(self, codes):\n        \"\"\"\n        Transform combination(s) of uint64 in one Python integer (each), in a\n        strictly monotonic way (i.e. respecting the lexicographic order of\n        integer combinations): see BaseMultiIndexCodesEngine documentation.\n\n        Parameters\n        ----------\n        codes : 1- or 2-dimensional array of dtype uint64\n            Combinations of integers (one per row)\n\n        Returns\n        -------\n        int, or 1-dimensional array of dtype object\n            Integer(s) representing one combination (each).\n        \"\"\"\n        # Shift the representation of each level by the pre-calculated number\n        # of bits. Since this can overflow uint64, first make sure we are\n        # working with Python integers:\n        codes = codes.astype(\"object\") << self.offsets\n\n        # Now sum and OR are in fact interchangeable. This is a simple\n        # composition of the (disjunct) significant bits of each level (i.e.\n        # each column in \"codes\") in a single positive integer (per row):\n        if codes.ndim == 1:\n            # Single key\n            return np.bitwise_or.reduce(codes)\n\n        # Multiple keys\n        return np.bitwise_or.reduce(codes, axis=1)\n\n\ndef names_compat(meth):\n    \"\"\"\n    A decorator to allow either `name` or `names` keyword but not both.\n\n    This makes it easier to share code with base class.\n    \"\"\"\n\n    @wraps(meth)\n    def new_meth(self_or_cls, *args, **kwargs):\n        if \"name\" in kwargs and \"names\" in kwargs:\n            raise TypeError(\"Can only provide one of `names` and `name`\")\n        elif \"name\" in kwargs:\n            kwargs[\"names\"] = kwargs.pop(\"name\")\n\n        return meth(self_or_cls, *args, **kwargs)\n\n    return new_meth\n\n\nclass MultiIndex(Index):\n    \"\"\"\n    A multi-level, or hierarchical, index object for pandas objects.\n\n    Parameters\n    ----------\n    levels : sequence of arrays\n        The unique labels for each level.\n    codes : sequence of arrays\n        Integers for each level designating which label at each location.\n\n        .. versionadded:: 0.24.0\n    sortorder : optional int\n        Level of sortedness (must be lexicographically sorted by that\n        level).\n    names : optional sequence of objects\n        Names for each of the index levels. (name is accepted for compat).\n    copy : bool, default False\n        Copy the meta-data.\n    verify_integrity : bool, default True\n        Check that the levels/codes are consistent and valid.\n\n    Attributes\n    ----------\n    names\n    levels\n    codes\n    nlevels\n    levshape\n\n    Methods\n    -------\n    from_arrays\n    from_tuples\n    from_product\n    from_frame\n    set_levels\n    set_codes\n    to_frame\n    to_flat_index\n    is_lexsorted\n    sortlevel\n    droplevel\n    swaplevel\n    reorder_levels\n    remove_unused_levels\n    get_locs\n\n    See Also\n    --------\n    MultiIndex.from_arrays  : Convert list of arrays to MultiIndex.\n    MultiIndex.from_product : Create a MultiIndex from the cartesian product\n                              of iterables.\n    MultiIndex.from_tuples  : Convert list of tuples to a MultiIndex.\n    MultiIndex.from_frame   : Make a MultiIndex from a DataFrame.\n    Index : The base pandas Index type.\n\n    Notes\n    -----\n    See the `user guide\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html>`_\n    for more.\n\n    Examples\n    --------\n    A new ``MultiIndex`` is typically constructed using one of the helper\n    methods :meth:`MultiIndex.from_arrays`, :meth:`MultiIndex.from_product`\n    and :meth:`MultiIndex.from_tuples`. For example (using ``.from_arrays``):\n\n    >>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n    >>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\n    MultiIndex([(1,  'red'),\n                (1, 'blue'),\n                (2,  'red'),\n                (2, 'blue')],\n               names=['number', 'color'])\n\n    See further examples for how to construct a MultiIndex in the doc strings\n    of the mentioned helper methods.\n    \"\"\"\n\n    _hidden_attrs = Index._hidden_attrs | frozenset()\n\n    # initialize to zero-length tuples to make everything work\n    _typ = \"multiindex\"\n    _names = FrozenList()\n    _levels = FrozenList()\n    _codes = FrozenList()\n    _comparables = [\"names\"]\n    rename = Index.set_names\n\n    sortorder: Optional[int]\n\n    # --------------------------------------------------------------------\n    # Constructors\n\n    def __new__(\n        cls,\n        levels=None,\n        codes=None,\n        sortorder=None,\n        names=None,\n        dtype=None,\n        copy=False,\n        name=None,\n        verify_integrity: bool = True,\n    ):\n\n        # compat with Index\n        if name is not None:\n            names = name\n        if levels is None or codes is None:\n            raise TypeError(\"Must pass both levels and codes\")\n        if len(levels) != len(codes):\n            raise ValueError(\"Length of levels and codes must be the same.\")\n        if len(levels) == 0:\n            raise ValueError(\"Must pass non-zero number of levels/codes\")\n\n        result = object.__new__(MultiIndex)\n        result._cache = {}\n\n        # we've already validated levels and codes, so shortcut here\n        result._set_levels(levels, copy=copy, validate=False)\n        result._set_codes(codes, copy=copy, validate=False)\n\n        result._names = [None] * len(levels)\n        if names is not None:\n            # handles name validation\n            result._set_names(names)\n\n        if sortorder is not None:\n            result.sortorder = int(sortorder)\n        else:\n            result.sortorder = sortorder\n\n        if verify_integrity:\n            new_codes = result._verify_integrity()\n            result._codes = new_codes\n\n        result._reset_identity()\n\n        return result\n\n    def _validate_codes(self, level: List, code: List):\n        \"\"\"\n        Reassign code values as -1 if their corresponding levels are NaN.\n\n        Parameters\n        ----------\n        code : list\n            Code to reassign.\n        level : list\n            Level to check for missing values (NaN, NaT, None).\n\n        Returns\n        -------\n        new code where code value = -1 if it corresponds\n        to a level with missing values (NaN, NaT, None).\n        \"\"\"\n        null_mask = isna(level)\n        if np.any(null_mask):\n            code = np.where(null_mask[code], -1, code)\n        return code\n\n    def _verify_integrity(\n        self, codes: Optional[List] = None, levels: Optional[List] = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        codes : optional list\n            Codes to check for validity. Defaults to current codes.\n        levels : optional list\n            Levels to check for validity. Defaults to current levels.\n\n        Raises\n        ------\n        ValueError\n            If length of levels and codes don't match, if the codes for any\n            level would exceed level bounds, or there are any duplicate levels.\n\n        Returns\n        -------\n        new codes where code value = -1 if it corresponds to a\n        NaN level.\n        \"\"\"\n        # NOTE: Currently does not check, among other things, that cached\n        # nlevels matches nor that sortorder matches actually sortorder.\n        codes = codes or self.codes\n        levels = levels or self.levels\n\n        if len(levels) != len(codes):\n            raise ValueError(\n                \"Length of levels and codes must match. NOTE: \"\n                \"this index is in an inconsistent state.\"\n            )\n        codes_length = len(codes[0])\n        for i, (level, level_codes) in enumerate(zip(levels, codes)):\n            if len(level_codes) != codes_length:\n                raise ValueError(\n                    f\"Unequal code lengths: {[len(code_) for code_ in codes]}\"\n                )\n            if len(level_codes) and level_codes.max() >= len(level):\n                raise ValueError(\n                    f\"On level {i}, code max ({level_codes.max()}) >= length of \"\n                    f\"level ({len(level)}). NOTE: this index is in an \"\n                    \"inconsistent state\"\n                )\n            if len(level_codes) and level_codes.min() < -1:\n                raise ValueError(f\"On level {i}, code value ({level_codes.min()}) < -1\")\n            if not level.is_unique:\n                raise ValueError(\n                    f\"Level values must be unique: {list(level)} on level {i}\"\n                )\n        if self.sortorder is not None:\n            if self.sortorder > self._lexsort_depth():\n                raise ValueError(\n                    \"Value for sortorder must be inferior or equal to actual \"\n                    f\"lexsort_depth: sortorder {self.sortorder} \"\n                    f\"with lexsort_depth {self._lexsort_depth()}\"\n                )\n\n        codes = [\n            self._validate_codes(level, code) for level, code in zip(levels, codes)\n        ]\n        new_codes = FrozenList(codes)\n        return new_codes\n\n    @classmethod\n    def from_arrays(cls, arrays, sortorder=None, names=lib.no_default) -> \"MultiIndex\":\n        \"\"\"\n        Convert arrays to MultiIndex.\n\n        Parameters\n        ----------\n        arrays : list / sequence of array-likes\n            Each array-like gives one level's value for each data point.\n            len(arrays) is the number of levels.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list / sequence of str, optional\n            Names for the levels in the index.\n\n        Returns\n        -------\n        MultiIndex\n\n        See Also\n        --------\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_product : Make a MultiIndex from cartesian product\n                                  of iterables.\n        MultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\n        Examples\n        --------\n        >>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n        >>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\n        MultiIndex([(1,  'red'),\n                    (1, 'blue'),\n                    (2,  'red'),\n                    (2, 'blue')],\n                   names=['number', 'color'])\n        \"\"\"\n        error_msg = \"Input must be a list / sequence of array-likes.\"\n        if not is_list_like(arrays):\n            raise TypeError(error_msg)\n        elif is_iterator(arrays):\n            arrays = list(arrays)\n\n        # Check if elements of array are list-like\n        for array in arrays:\n            if not is_list_like(array):\n                raise TypeError(error_msg)\n\n        # Check if lengths of all arrays are equal or not,\n        # raise ValueError, if not\n        for i in range(1, len(arrays)):\n            if len(arrays[i]) != len(arrays[i - 1]):\n                raise ValueError(\"all arrays must be same length\")\n\n        codes, levels = factorize_from_iterables(arrays)\n        if names is lib.no_default:\n            names = [getattr(arr, \"name\", None) for arr in arrays]\n\n        return cls(\n            levels=levels,\n            codes=codes,\n            sortorder=sortorder,\n            names=names,\n            verify_integrity=False,\n        )\n\n    @classmethod\n    @names_compat\n    def from_tuples(\n        cls,\n        tuples,\n        sortorder: Optional[int] = None,\n        names: Optional[Sequence[Label]] = None,\n    ):\n        \"\"\"\n        Convert list of tuples to MultiIndex.\n\n        Parameters\n        ----------\n        tuples : list / sequence of tuple-likes\n            Each tuple is the index of one row/column.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list / sequence of str, optional\n            Names for the levels in the index.\n\n        Returns\n        -------\n        MultiIndex\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert list of arrays to MultiIndex.\n        MultiIndex.from_product : Make a MultiIndex from cartesian product\n                                  of iterables.\n        MultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\n        Examples\n        --------\n        >>> tuples = [(1, 'red'), (1, 'blue'),\n        ...           (2, 'red'), (2, 'blue')]\n        >>> pd.MultiIndex.from_tuples(tuples, names=('number', 'color'))\n        MultiIndex([(1,  'red'),\n                    (1, 'blue'),\n                    (2,  'red'),\n                    (2, 'blue')],\n                   names=['number', 'color'])\n        \"\"\"\n        if not is_list_like(tuples):\n            raise TypeError(\"Input must be a list / sequence of tuple-likes.\")\n        elif is_iterator(tuples):\n            tuples = list(tuples)\n\n        arrays: List[Sequence[Label]]\n        if len(tuples) == 0:\n            if names is None:\n                raise TypeError(\"Cannot infer number of levels from empty list\")\n            arrays = [[]] * len(names)\n        elif isinstance(tuples, (np.ndarray, Index)):\n            if isinstance(tuples, Index):\n                tuples = tuples._values\n\n            arrays = list(lib.tuples_to_object_array(tuples).T)\n        elif isinstance(tuples, list):\n            arrays = list(lib.to_object_array_tuples(tuples).T)\n        else:\n            arrays = zip(*tuples)\n\n        return cls.from_arrays(arrays, sortorder=sortorder, names=names)\n\n    @classmethod\n    def from_product(cls, iterables, sortorder=None, names=lib.no_default):\n        \"\"\"\n        Make a MultiIndex from the cartesian product of multiple iterables.\n\n        Parameters\n        ----------\n        iterables : list / sequence of iterables\n            Each iterable has unique labels for each level of the index.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list / sequence of str, optional\n            Names for the levels in the index.\n\n            .. versionchanged:: 1.0.0\n\n               If not explicitly provided, names will be inferred from the\n               elements of iterables if an element has a name attribute\n\n        Returns\n        -------\n        MultiIndex\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert list of arrays to MultiIndex.\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\n        Examples\n        --------\n        >>> numbers = [0, 1, 2]\n        >>> colors = ['green', 'purple']\n        >>> pd.MultiIndex.from_product([numbers, colors],\n        ...                            names=['number', 'color'])\n        MultiIndex([(0,  'green'),\n                    (0, 'purple'),\n                    (1,  'green'),\n                    (1, 'purple'),\n                    (2,  'green'),\n                    (2, 'purple')],\n                   names=['number', 'color'])\n        \"\"\"\n        from pandas.core.reshape.util import cartesian_product\n\n        if not is_list_like(iterables):\n            raise TypeError(\"Input must be a list / sequence of iterables.\")\n        elif is_iterator(iterables):\n            iterables = list(iterables)\n\n        codes, levels = factorize_from_iterables(iterables)\n        if names is lib.no_default:\n            names = [getattr(it, \"name\", None) for it in iterables]\n\n        # codes are all ndarrays, so cartesian_product is lossless\n        codes = cartesian_product(codes)\n        return cls(levels, codes, sortorder=sortorder, names=names)\n\n    @classmethod\n    def from_frame(cls, df, sortorder=None, names=None):\n        \"\"\"\n        Make a MultiIndex from a DataFrame.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        df : DataFrame\n            DataFrame to be converted to MultiIndex.\n        sortorder : int, optional\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list-like, optional\n            If no names are provided, use the column names, or tuple of column\n            names if the columns is a MultiIndex. If a sequence, overwrite\n            names with the given sequence.\n\n        Returns\n        -------\n        MultiIndex\n            The MultiIndex representation of the given DataFrame.\n\n        See Also\n        --------\n        MultiIndex.from_arrays : Convert list of arrays to MultiIndex.\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_product : Make a MultiIndex from cartesian product\n                                  of iterables.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame([['HI', 'Temp'], ['HI', 'Precip'],\n        ...                    ['NJ', 'Temp'], ['NJ', 'Precip']],\n        ...                   columns=['a', 'b'])\n        >>> df\n              a       b\n        0    HI    Temp\n        1    HI  Precip\n        2    NJ    Temp\n        3    NJ  Precip\n\n        >>> pd.MultiIndex.from_frame(df)\n        MultiIndex([('HI',   'Temp'),\n                    ('HI', 'Precip'),\n                    ('NJ',   'Temp'),\n                    ('NJ', 'Precip')],\n                   names=['a', 'b'])\n\n        Using explicit names, instead of the column names\n\n        >>> pd.MultiIndex.from_frame(df, names=['state', 'observation'])\n        MultiIndex([('HI',   'Temp'),\n                    ('HI', 'Precip'),\n                    ('NJ',   'Temp'),\n                    ('NJ', 'Precip')],\n                   names=['state', 'observation'])\n        \"\"\"\n        if not isinstance(df, ABCDataFrame):\n            raise TypeError(\"Input must be a DataFrame\")\n\n        column_names, columns = zip(*df.items())\n        names = column_names if names is None else names\n        return cls.from_arrays(columns, sortorder=sortorder, names=names)\n\n    # --------------------------------------------------------------------\n\n    @cache_readonly\n    def _values(self):\n        # We override here, since our parent uses _data, which we don't use.\n        values = []\n\n        for i in range(self.nlevels):\n            vals = self._get_level_values(i)\n            if is_categorical_dtype(vals.dtype):\n                vals = vals._internal_get_values()\n            if isinstance(vals.dtype, ExtensionDtype) or isinstance(\n                vals, (ABCDatetimeIndex, ABCTimedeltaIndex)\n            ):\n                vals = vals.astype(object)\n            vals = np.array(vals, copy=False)\n            values.append(vals)\n\n        arr = lib.fast_zip(values)\n        return arr\n\n    @property\n    def values(self):\n        return self._values\n\n    @property\n    def array(self):\n        \"\"\"\n        Raises a ValueError for `MultiIndex` because there's no single\n        array backing a MultiIndex.\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n        raise ValueError(\n            \"MultiIndex has no single backing array. Use \"\n            \"'MultiIndex.to_numpy()' to get a NumPy array of tuples.\"\n        )\n\n    @property\n    def shape(self) -> Shape:\n        \"\"\"\n        Return a tuple of the shape of the underlying data.\n        \"\"\"\n        # overriding the base Index.shape definition to avoid materializing\n        # the values (GH-27384, GH-27775)\n        return (len(self),)\n\n    def __len__(self) -> int:\n        return len(self.codes[0])\n\n    # --------------------------------------------------------------------\n    # Levels Methods\n\n    @cache_readonly\n    def levels(self):\n        # Use cache_readonly to ensure that self.get_locs doesn't repeatedly\n        # create new IndexEngine\n        # https://github.com/pandas-dev/pandas/issues/31648\n        result = [\n            x._shallow_copy(name=name) for x, name in zip(self._levels, self._names)\n        ]\n        for level in result:\n            # disallow midx.levels[0].name = \"foo\"\n            level._no_setting_name = True\n        return FrozenList(result)\n\n    def _set_levels(\n        self,\n        levels,\n        level=None,\n        copy: bool = False,\n        validate: bool = True,\n        verify_integrity: bool = False,\n    ) -> None:\n        # This is NOT part of the levels property because it should be\n        # externally not allowed to set levels. User beware if you change\n        # _levels directly\n        if validate:\n            if len(levels) == 0:\n                raise ValueError(\"Must set non-zero number of levels.\")\n            if level is None and len(levels) != self.nlevels:\n                raise ValueError(\"Length of levels must match number of levels.\")\n            if level is not None and len(levels) != len(level):\n                raise ValueError(\"Length of levels must match length of level.\")\n\n        if level is None:\n            new_levels = FrozenList(\n                ensure_index(lev, copy=copy)._shallow_copy() for lev in levels\n            )\n        else:\n            level_numbers = [self._get_level_number(lev) for lev in level]\n            new_levels_list = list(self._levels)\n            for lev_num, lev in zip(level_numbers, levels):\n                new_levels_list[lev_num] = ensure_index(lev, copy=copy)._shallow_copy()\n            new_levels = FrozenList(new_levels_list)\n\n        if verify_integrity:\n            new_codes = self._verify_integrity(levels=new_levels)\n            self._codes = new_codes\n\n        names = self.names\n        self._levels = new_levels\n        if any(names):\n            self._set_names(names)\n\n        self._reset_cache()\n\n    def set_levels(self, levels, level=None, inplace=None, verify_integrity=True):\n        \"\"\"\n        Set new levels on MultiIndex. Defaults to returning new index.\n\n        Parameters\n        ----------\n        levels : sequence or list of sequence\n            New level(s) to apply.\n        level : int, level name, or sequence of int/level names (default None)\n            Level(s) to set (None for all levels).\n        inplace : bool\n            If True, mutates in place.\n\n            .. deprecated:: 1.2.0\n        verify_integrity : bool, default True\n            If True, checks that levels and codes are compatible.\n\n        Returns\n        -------\n        new index (of same type and class...etc) or None\n            The same type as the caller or None if ``inplace=True``.\n\n        Examples\n        --------\n        >>> idx = pd.MultiIndex.from_tuples(\n        ...     [\n        ...         (1, \"one\"),\n        ...         (1, \"two\"),\n        ...         (2, \"one\"),\n        ...         (2, \"two\"),\n        ...         (3, \"one\"),\n        ...         (3, \"two\")\n        ...     ],\n        ...     names=[\"foo\", \"bar\"]\n        ... )\n        >>> idx\n        MultiIndex([(1, 'one'),\n            (1, 'two'),\n            (2, 'one'),\n            (2, 'two'),\n            (3, 'one'),\n            (3, 'two')],\n           names=['foo', 'bar'])\n\n        >>> idx.set_levels([['a', 'b', 'c'], [1, 2]])\n        MultiIndex([('a', 1),\n                    ('a', 2),\n                    ('b', 1),\n                    ('b', 2),\n                    ('c', 1),\n                    ('c', 2)],\n                   names=['foo', 'bar'])\n        >>> idx.set_levels(['a', 'b', 'c'], level=0)\n        MultiIndex([('a', 'one'),\n                    ('a', 'two'),\n                    ('b', 'one'),\n                    ('b', 'two'),\n                    ('c', 'one'),\n                    ('c', 'two')],\n                   names=['foo', 'bar'])\n        >>> idx.set_levels(['a', 'b'], level='bar')\n        MultiIndex([(1, 'a'),\n                    (1, 'b'),\n                    (2, 'a'),\n                    (2, 'b'),\n                    (3, 'a'),\n                    (3, 'b')],\n                   names=['foo', 'bar'])\n\n        If any of the levels passed to ``set_levels()`` exceeds the\n        existing length, all of the values from that argument will\n        be stored in the MultiIndex levels, though the values will\n        be truncated in the MultiIndex output.\n\n        >>> idx.set_levels([['a', 'b', 'c'], [1, 2, 3, 4]], level=[0, 1])\n        MultiIndex([('a', 1),\n            ('a', 2),\n            ('b', 1),\n            ('b', 2),\n            ('c', 1),\n            ('c', 2)],\n           names=['foo', 'bar'])\n        >>> idx.set_levels([['a', 'b', 'c'], [1, 2, 3, 4]], level=[0, 1]).levels\n        FrozenList([['a', 'b', 'c'], [1, 2, 3, 4]])\n        \"\"\"\n        if inplace is not None:\n            warnings.warn(\n                \"inplace is deprecated and will be removed in a future version.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        else:\n            inplace = False\n\n        if is_list_like(levels) and not isinstance(levels, Index):\n            levels = list(levels)\n\n        if level is not None and not is_list_like(level):\n            if not is_list_like(levels):\n                raise TypeError(\"Levels must be list-like\")\n            if is_list_like(levels[0]):\n                raise TypeError(\"Levels must be list-like\")\n            level = [level]\n            levels = [levels]\n        elif level is None or is_list_like(level):\n            if not is_list_like(levels) or not is_list_like(levels[0]):\n                raise TypeError(\"Levels must be list of lists-like\")\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._reset_identity()\n        idx._set_levels(\n            levels, level=level, validate=True, verify_integrity=verify_integrity\n        )\n        if not inplace:\n            return idx\n\n    @property\n    def nlevels(self) -> int:\n        \"\"\"\n        Integer number of levels in this MultiIndex.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([['a'], ['b'], ['c']])\n        >>> mi\n        MultiIndex([('a', 'b', 'c')],\n                   )\n        >>> mi.nlevels\n        3\n        \"\"\"\n        return len(self._levels)\n\n    @property\n    def levshape(self):\n        \"\"\"\n        A tuple with the length of each level.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([['a'], ['b'], ['c']])\n        >>> mi\n        MultiIndex([('a', 'b', 'c')],\n                   )\n        >>> mi.levshape\n        (1, 1, 1)\n        \"\"\"\n        return tuple(len(x) for x in self.levels)\n\n    # --------------------------------------------------------------------\n    # Codes Methods\n\n    @property\n    def codes(self):\n        return self._codes\n\n    def _set_codes(\n        self,\n        codes,\n        level=None,\n        copy: bool = False,\n        validate: bool = True,\n        verify_integrity: bool = False,\n    ) -> None:\n        if validate:\n            if level is None and len(codes) != self.nlevels:\n                raise ValueError(\"Length of codes must match number of levels\")\n            if level is not None and len(codes) != len(level):\n                raise ValueError(\"Length of codes must match length of levels.\")\n\n        if level is None:\n            new_codes = FrozenList(\n                _coerce_indexer_frozen(level_codes, lev, copy=copy).view()\n                for lev, level_codes in zip(self._levels, codes)\n            )\n        else:\n            level_numbers = [self._get_level_number(lev) for lev in level]\n            new_codes_list = list(self._codes)\n            for lev_num, level_codes in zip(level_numbers, codes):\n                lev = self.levels[lev_num]\n                new_codes_list[lev_num] = _coerce_indexer_frozen(\n                    level_codes, lev, copy=copy\n                )\n            new_codes = FrozenList(new_codes_list)\n\n        if verify_integrity:\n            new_codes = self._verify_integrity(codes=new_codes)\n\n        self._codes = new_codes\n\n        self._reset_cache()\n\n    def set_codes(self, codes, level=None, inplace=None, verify_integrity=True):\n        \"\"\"\n        Set new codes on MultiIndex. Defaults to returning new index.\n\n        .. versionadded:: 0.24.0\n\n           New name for deprecated method `set_labels`.\n\n        Parameters\n        ----------\n        codes : sequence or list of sequence\n            New codes to apply.\n        level : int, level name, or sequence of int/level names (default None)\n            Level(s) to set (None for all levels).\n        inplace : bool\n            If True, mutates in place.\n\n            .. deprecated:: 1.2.0\n        verify_integrity : bool (default True)\n            If True, checks that levels and codes are compatible.\n\n        Returns\n        -------\n        new index (of same type and class...etc) or None\n            The same type as the caller or None if ``inplace=True``.\n\n        Examples\n        --------\n        >>> idx = pd.MultiIndex.from_tuples(\n        ...     [(1, \"one\"), (1, \"two\"), (2, \"one\"), (2, \"two\")], names=[\"foo\", \"bar\"]\n        ... )\n        >>> idx\n        MultiIndex([(1, 'one'),\n            (1, 'two'),\n            (2, 'one'),\n            (2, 'two')],\n           names=['foo', 'bar'])\n\n        >>> idx.set_codes([[1, 0, 1, 0], [0, 0, 1, 1]])\n        MultiIndex([(2, 'one'),\n                    (1, 'one'),\n                    (2, 'two'),\n                    (1, 'two')],\n                   names=['foo', 'bar'])\n        >>> idx.set_codes([1, 0, 1, 0], level=0)\n        MultiIndex([(2, 'one'),\n                    (1, 'two'),\n                    (2, 'one'),\n                    (1, 'two')],\n                   names=['foo', 'bar'])\n        >>> idx.set_codes([0, 0, 1, 1], level='bar')\n        MultiIndex([(1, 'one'),\n                    (1, 'one'),\n                    (2, 'two'),\n                    (2, 'two')],\n                   names=['foo', 'bar'])\n        >>> idx.set_codes([[1, 0, 1, 0], [0, 0, 1, 1]], level=[0, 1])\n        MultiIndex([(2, 'one'),\n                    (1, 'one'),\n                    (2, 'two'),\n                    (1, 'two')],\n                   names=['foo', 'bar'])\n        \"\"\"\n        if inplace is not None:\n            warnings.warn(\n                \"inplace is deprecated and will be removed in a future version.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        else:\n            inplace = False\n\n        if level is not None and not is_list_like(level):\n            if not is_list_like(codes):\n                raise TypeError(\"Codes must be list-like\")\n            if is_list_like(codes[0]):\n                raise TypeError(\"Codes must be list-like\")\n            level = [level]\n            codes = [codes]\n        elif level is None or is_list_like(level):\n            if not is_list_like(codes) or not is_list_like(codes[0]):\n                raise TypeError(\"Codes must be list of lists-like\")\n\n        if inplace:\n            idx = self\n        else:\n            idx = self._shallow_copy()\n        idx._reset_identity()\n        idx._set_codes(codes, level=level, verify_integrity=verify_integrity)\n        if not inplace:\n            return idx\n\n    # --------------------------------------------------------------------\n    # Index Internals\n\n    @cache_readonly\n    def _engine(self):\n        # Calculate the number of bits needed to represent labels in each\n        # level, as log2 of their sizes (including -1 for NaN):\n        sizes = np.ceil(np.log2([len(level) + 1 for level in self.levels]))\n\n        # Sum bit counts, starting from the _right_....\n        lev_bits = np.cumsum(sizes[::-1])[::-1]\n\n        # ... in order to obtain offsets such that sorting the combination of\n        # shifted codes (one for each level, resulting in a unique integer) is\n        # equivalent to sorting lexicographically the codes themselves. Notice\n        # that each level needs to be shifted by the number of bits needed to\n        # represent the _previous_ ones:\n        offsets = np.concatenate([lev_bits[1:], [0]]).astype(\"uint64\")\n\n        # Check the total number of bits needed for our representation:\n        if lev_bits[0] > 64:\n            # The levels would overflow a 64 bit uint - use Python integers:\n            return MultiIndexPyIntEngine(self.levels, self.codes, offsets)\n        return MultiIndexUIntEngine(self.levels, self.codes, offsets)\n\n    @property\n    def _constructor(self):\n        return type(self).from_tuples\n\n    @doc(Index._shallow_copy)\n    def _shallow_copy(self, values=None, name=lib.no_default):\n        names = name if name is not lib.no_default else self.names\n\n        if values is not None:\n            return type(self).from_tuples(values, sortorder=None, names=names)\n\n        result = type(self)(\n            levels=self.levels,\n            codes=self.codes,\n            sortorder=None,\n            names=names,\n            verify_integrity=False,\n        )\n        result._cache = self._cache.copy()\n        result._cache.pop(\"levels\", None)  # GH32669\n        return result\n\n    # --------------------------------------------------------------------\n\n    def copy(\n        self,\n        names=None,\n        dtype=None,\n        levels=None,\n        codes=None,\n        deep=False,\n        name=None,\n    ):\n        \"\"\"\n        Make a copy of this object. Names, dtype, levels and codes can be\n        passed and will be set on new copy.\n\n        Parameters\n        ----------\n        names : sequence, optional\n        dtype : numpy dtype or pandas type, optional\n\n            .. deprecated:: 1.2.0\n        levels : sequence, optional\n\n            .. deprecated:: 1.2.0\n        codes : sequence, optional\n\n            .. deprecated:: 1.2.0\n        deep : bool, default False\n        name : Label\n            Kept for compatibility with 1-dimensional Index. Should not be used.\n\n        Returns\n        -------\n        MultiIndex\n\n        Notes\n        -----\n        In most cases, there should be no functional difference from using\n        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n        This could be potentially expensive on large MultiIndex objects.\n        \"\"\"\n        names = self._validate_names(name=name, names=names, deep=deep)\n        if levels is not None:\n            warnings.warn(\n                \"parameter levels is deprecated and will be removed in a future \"\n                \"version. Use the set_levels method instead.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n        if codes is not None:\n            warnings.warn(\n                \"parameter codes is deprecated and will be removed in a future \"\n                \"version. Use the set_codes method instead.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n\n        if deep:\n            from copy import deepcopy\n\n            if levels is None:\n                levels = deepcopy(self.levels)\n            if codes is None:\n                codes = deepcopy(self.codes)\n\n        levels = levels if levels is not None else self.levels\n        codes = codes if codes is not None else self.codes\n\n        new_index = type(self)(\n            levels=levels,\n            codes=codes,\n            sortorder=self.sortorder,\n            names=names,\n            verify_integrity=False,\n        )\n        new_index._cache = self._cache.copy()\n        new_index._cache.pop(\"levels\", None)  # GH32669\n\n        if dtype:\n            warnings.warn(\n                \"parameter dtype is deprecated and will be removed in a future \"\n                \"version. Use the astype method instead.\",\n                FutureWarning,\n                stacklevel=2,\n            )\n            new_index = new_index.astype(dtype)\n        return new_index\n\n    def __array__(self, dtype=None) -> np.ndarray:\n        \"\"\" the array interface, return my values \"\"\"\n        return self.values\n\n    def view(self, cls=None):\n        \"\"\" this is defined as a copy with the same identity \"\"\"\n        result = self.copy()\n        result._id = self._id\n        return result\n\n    @doc(Index.__contains__)\n    def __contains__(self, key: Any) -> bool:\n        hash(key)\n        try:\n            self.get_loc(key)\n            return True\n        except (LookupError, TypeError, ValueError):\n            return False\n\n    @cache_readonly\n    def dtype(self) -> np.dtype:\n        return np.dtype(\"O\")\n\n    def _is_memory_usage_qualified(self) -> bool:\n        \"\"\" return a boolean if we need a qualified .info display \"\"\"\n\n        def f(level):\n            return \"mixed\" in level or \"string\" in level or \"unicode\" in level\n\n        return any(f(level) for level in self._inferred_type_levels)\n\n    @doc(Index.memory_usage)\n    def memory_usage(self, deep: bool = False) -> int:\n        # we are overwriting our base class to avoid\n        # computing .values here which could materialize\n        # a tuple representation unnecessarily\n        return self._nbytes(deep)\n\n    @cache_readonly\n    def nbytes(self) -> int:\n        \"\"\" return the number of bytes in the underlying data \"\"\"\n        return self._nbytes(False)\n\n    def _nbytes(self, deep: bool = False) -> int:\n        \"\"\"\n        return the number of bytes in the underlying data\n        deeply introspect the level data if deep=True\n\n        include the engine hashtable\n\n        *this is in internal routine*\n\n        \"\"\"\n        # for implementations with no useful getsizeof (PyPy)\n        objsize = 24\n\n        level_nbytes = sum(i.memory_usage(deep=deep) for i in self.levels)\n        label_nbytes = sum(i.nbytes for i in self.codes)\n        names_nbytes = sum(getsizeof(i, objsize) for i in self.names)\n        result = level_nbytes + label_nbytes + names_nbytes\n\n        # include our engine hashtable\n        result += self._engine.sizeof(deep=deep)\n        return result\n\n    # --------------------------------------------------------------------\n    # Rendering Methods\n\n    def _formatter_func(self, tup):\n        \"\"\"\n        Formats each item in tup according to its level's formatter function.\n        \"\"\"\n        formatter_funcs = [level._formatter_func for level in self.levels]\n        return tuple(func(val) for func, val in zip(formatter_funcs, tup))\n\n    def _format_data(self, name=None):\n        \"\"\"\n        Return the formatted data as a unicode string\n        \"\"\"\n        return format_object_summary(\n            self, self._formatter_func, name=name, line_break_each_value=True\n        )\n\n    def _format_attrs(self):\n        \"\"\"\n        Return a list of tuples of the (attr,formatted_value).\n        \"\"\"\n        return format_object_attrs(self, include_dtype=False)\n\n    def _format_native_types(self, na_rep=\"nan\", **kwargs):\n        new_levels = []\n        new_codes = []\n\n        # go through the levels and format them\n        for level, level_codes in zip(self.levels, self.codes):\n            level = level._format_native_types(na_rep=na_rep, **kwargs)\n            # add nan values, if there are any\n            mask = level_codes == -1\n            if mask.any():\n                nan_index = len(level)\n                level = np.append(level, na_rep)\n                assert not level_codes.flags.writeable  # i.e. copy is needed\n                level_codes = level_codes.copy()  # make writeable\n                level_codes[mask] = nan_index\n            new_levels.append(level)\n            new_codes.append(level_codes)\n\n        if len(new_levels) == 1:\n            # a single-level multi-index\n            return Index(new_levels[0].take(new_codes[0]))._format_native_types()\n        else:\n            # reconstruct the multi-index\n            mi = MultiIndex(\n                levels=new_levels,\n                codes=new_codes,\n                names=self.names,\n                sortorder=self.sortorder,\n                verify_integrity=False,\n            )\n            return mi._values\n\n    def format(\n        self,\n        name: Optional[bool] = None,\n        formatter: Optional[Callable] = None,\n        na_rep: Optional[str] = None,\n        names: bool = False,\n        space: int = 2,\n        sparsify=None,\n        adjoin: bool = True,\n    ) -> List:\n        if name is not None:\n            names = name\n\n        if len(self) == 0:\n            return []\n\n        stringified_levels = []\n        for lev, level_codes in zip(self.levels, self.codes):\n            na = na_rep if na_rep is not None else _get_na_rep(lev.dtype.type)\n\n            if len(lev) > 0:\n\n                formatted = lev.take(level_codes).format(formatter=formatter)\n\n                # we have some NA\n                mask = level_codes == -1\n                if mask.any():\n                    formatted = np.array(formatted, dtype=object)\n                    formatted[mask] = na\n                    formatted = formatted.tolist()\n\n            else:\n                # weird all NA case\n                formatted = [\n                    pprint_thing(na if isna(x) else x, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                    for x in algos.take_1d(lev._values, level_codes)\n                ]\n            stringified_levels.append(formatted)\n\n        result_levels = []\n        for lev, lev_name in zip(stringified_levels, self.names):\n            level = []\n\n            if names:\n                level.append(\n                    pprint_thing(lev_name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n                    if lev_name is not None\n                    else \"\"\n                )\n\n            level.extend(np.array(lev, dtype=object))\n            result_levels.append(level)\n\n        if sparsify is None:\n            sparsify = get_option(\"display.multi_sparse\")\n\n        if sparsify:\n            sentinel = \"\"\n            # GH3547 use value of sparsify as sentinel if it's \"Falsey\"\n            assert isinstance(sparsify, bool) or sparsify is lib.no_default\n            if sparsify in [False, lib.no_default]:\n                sentinel = sparsify\n            # little bit of a kludge job for #1217\n            result_levels = sparsify_labels(\n                result_levels, start=int(names), sentinel=sentinel\n            )\n\n        if adjoin:\n            from pandas.io.formats.format import get_adjustment\n\n            adj = get_adjustment()\n            return adj.adjoin(space, *result_levels).split(\"\\n\")\n        else:\n            return result_levels\n\n    # --------------------------------------------------------------------\n    # Names Methods\n\n    def _get_names(self):\n        return FrozenList(self._names)\n\n    def _set_names(self, names, level=None, validate=True):\n        \"\"\"\n        Set new names on index. Each name has to be a hashable type.\n\n        Parameters\n        ----------\n        values : str or sequence\n            name(s) to set\n        level : int, level name, or sequence of int/level names (default None)\n            If the index is a MultiIndex (hierarchical), level(s) to set (None\n            for all levels).  Otherwise level must be None\n        validate : boolean, default True\n            validate that the names match level lengths\n\n        Raises\n        ------\n        TypeError if each name is not hashable.\n\n        Notes\n        -----\n        sets names on levels. WARNING: mutates!\n\n        Note that you generally want to set this *after* changing levels, so\n        that it only acts on copies\n        \"\"\"\n        # GH 15110\n        # Don't allow a single string for names in a MultiIndex\n        if names is not None and not is_list_like(names):\n            raise ValueError(\"Names should be list-like for a MultiIndex\")\n        names = list(names)\n\n        if validate:\n            if level is not None and len(names) != len(level):\n                raise ValueError(\"Length of names must match length of level.\")\n            if level is None and len(names) != self.nlevels:\n                raise ValueError(\n                    \"Length of names must match number of levels in MultiIndex.\"\n                )\n\n        if level is None:\n            level = range(self.nlevels)\n        else:\n            level = [self._get_level_number(lev) for lev in level]\n\n        # set the name\n        for lev, name in zip(level, names):\n            if name is not None:\n                # GH 20527\n                # All items in 'names' need to be hashable:\n                if not is_hashable(name):\n                    raise TypeError(\n                        f\"{type(self).__name__}.name must be a hashable type\"\n                    )\n            # pandas\\core\\indexes\\multi.py:1448: error: Cannot determine type\n            # of '__setitem__'  [has-type]\n            self._names[lev] = name  # type: ignore[has-type]\n\n        # If .levels has been accessed, the names in our cache will be stale.\n        self._reset_cache()\n\n    names = property(\n        fset=_set_names,\n        fget=_get_names,\n        doc=\"\"\"\n        Names of levels in MultiIndex.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays(\n        ... [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z'])\n        >>> mi\n        MultiIndex([(1, 3, 5),\n                    (2, 4, 6)],\n                   names=['x', 'y', 'z'])\n        >>> mi.names\n        FrozenList(['x', 'y', 'z'])\n        \"\"\",\n    )\n\n    # --------------------------------------------------------------------\n\n    @doc(Index._get_grouper_for_level)\n    def _get_grouper_for_level(self, mapper, level):\n        indexer = self.codes[level]\n        level_index = self.levels[level]\n\n        if mapper is not None:\n            # Handle group mapping function and return\n            level_values = self.levels[level].take(indexer)\n            grouper = level_values.map(mapper)\n            return grouper, None, None\n\n        codes, uniques = algos.factorize(indexer, sort=True)\n\n        if len(uniques) > 0 and uniques[0] == -1:\n            # Handle NAs\n            mask = indexer != -1\n            ok_codes, uniques = algos.factorize(indexer[mask], sort=True)\n\n            codes = np.empty(len(indexer), dtype=indexer.dtype)\n            codes[mask] = ok_codes\n            codes[~mask] = -1\n\n        if len(uniques) < len(level_index):\n            # Remove unobserved levels from level_index\n            level_index = level_index.take(uniques)\n        else:\n            # break references back to us so that setting the name\n            # on the output of a groupby doesn't reflect back here.\n            level_index = level_index.copy()\n\n        if level_index._can_hold_na:\n            grouper = level_index.take(codes, fill_value=True)\n        else:\n            grouper = level_index.take(codes)\n\n        return grouper, codes, level_index\n\n    @cache_readonly\n    def inferred_type(self) -> str:\n        return \"mixed\"\n\n    def _get_level_number(self, level) -> int:\n        count = self.names.count(level)\n        if (count > 1) and not is_integer(level):\n            raise ValueError(\n                f\"The name {level} occurs multiple times, use a level number\"\n            )\n        try:\n            level = self.names.index(level)\n        except ValueError as err:\n            if not is_integer(level):\n                raise KeyError(f\"Level {level} not found\") from err\n            elif level < 0:\n                level += self.nlevels\n                if level < 0:\n                    orig_level = level - self.nlevels\n                    raise IndexError(\n                        f\"Too many levels: Index has only {self.nlevels} levels, \"\n                        f\"{orig_level} is not a valid level number\"\n                    ) from err\n            # Note: levels are zero-based\n            elif level >= self.nlevels:\n                raise IndexError(\n                    f\"Too many levels: Index has only {self.nlevels} levels, \"\n                    f\"not {level + 1}\"\n                ) from err\n        return level\n\n    @property\n    def _has_complex_internals(self) -> bool:\n        # used to avoid libreduction code paths, which raise or require conversion\n        return True\n\n    @cache_readonly\n    def is_monotonic_increasing(self) -> bool:\n        \"\"\"\n        return if the index is monotonic increasing (only equal or\n        increasing) values.\n        \"\"\"\n        if any(-1 in code for code in self.codes):\n            return False\n\n        if all(level.is_monotonic for level in self.levels):\n            # If each level is sorted, we can operate on the codes directly. GH27495\n            return libalgos.is_lexsorted(\n                [x.astype(\"int64\", copy=False) for x in self.codes]\n            )\n\n        # reversed() because lexsort() wants the most significant key last.\n        values = [\n            self._get_level_values(i)._values for i in reversed(range(len(self.levels)))\n        ]\n        try:\n            sort_order = np.lexsort(values)\n            return Index(sort_order).is_monotonic\n        except TypeError:\n\n            # we have mixed types and np.lexsort is not happy\n            return Index(self._values).is_monotonic\n\n    @cache_readonly\n    def is_monotonic_decreasing(self) -> bool:\n        \"\"\"\n        return if the index is monotonic decreasing (only equal or\n        decreasing) values.\n        \"\"\"\n        # monotonic decreasing if and only if reverse is monotonic increasing\n        return self[::-1].is_monotonic_increasing\n\n    @cache_readonly\n    def _inferred_type_levels(self):\n        \"\"\" return a list of the inferred types, one for each level \"\"\"\n        return [i.inferred_type for i in self.levels]\n\n    @doc(Index.duplicated)\n    def duplicated(self, keep=\"first\"):\n        shape = map(len, self.levels)\n        ids = get_group_index(self.codes, shape, sort=False, xnull=False)\n\n        return duplicated_int64(ids, keep)\n\n    def fillna(self, value=None, downcast=None):\n        \"\"\"\n        fillna is not implemented for MultiIndex\n        \"\"\"\n        raise NotImplementedError(\"isna is not defined for MultiIndex\")\n\n    @doc(Index.dropna)\n    def dropna(self, how=\"any\"):\n        nans = [level_codes == -1 for level_codes in self.codes]\n        if how == \"any\":\n            indexer = np.any(nans, axis=0)\n        elif how == \"all\":\n            indexer = np.all(nans, axis=0)\n        else:\n            raise ValueError(f\"invalid how option: {how}\")\n\n        new_codes = [level_codes[~indexer] for level_codes in self.codes]\n        return self.set_codes(codes=new_codes)\n\n    def _get_level_values(self, level, unique=False):\n        \"\"\"\n        Return vector of label values for requested level,\n        equal to the length of the index\n\n        **this is an internal method**\n\n        Parameters\n        ----------\n        level : int level\n        unique : bool, default False\n            if True, drop duplicated values\n\n        Returns\n        -------\n        values : ndarray\n        \"\"\"\n        lev = self.levels[level]\n        level_codes = self.codes[level]\n        name = self._names[level]\n        if unique:\n            level_codes = algos.unique(level_codes)\n        filled = algos.take_1d(lev._values, level_codes, fill_value=lev._na_value)\n        return lev._shallow_copy(filled, name=name)\n\n    def get_level_values(self, level):\n        \"\"\"\n        Return vector of label values for requested level.\n\n        Length of returned vector is equal to the length of the index.\n\n        Parameters\n        ----------\n        level : int or str\n            ``level`` is either the integer position of the level in the\n            MultiIndex, or the name of the level.\n\n        Returns\n        -------\n        values : Index\n            Values is a level of this MultiIndex converted to\n            a single :class:`Index` (or subclass thereof).\n\n        Examples\n        --------\n        Create a MultiIndex:\n\n        >>> mi = pd.MultiIndex.from_arrays((list('abc'), list('def')))\n        >>> mi.names = ['level_1', 'level_2']\n\n        Get level values by supplying level as either integer or name:\n\n        >>> mi.get_level_values(0)\n        Index(['a', 'b', 'c'], dtype='object', name='level_1')\n        >>> mi.get_level_values('level_2')\n        Index(['d', 'e', 'f'], dtype='object', name='level_2')\n        \"\"\"\n        level = self._get_level_number(level)\n        values = self._get_level_values(level)\n        return values\n\n    @doc(Index.unique)\n    def unique(self, level=None):\n\n        if level is None:\n            return super().unique()\n        else:\n            level = self._get_level_number(level)\n            return self._get_level_values(level=level, unique=True)\n\n    def to_frame(self, index=True, name=None):\n        \"\"\"\n        Create a DataFrame with the levels of the MultiIndex as columns.\n\n        Column ordering is determined by the DataFrame constructor with data as\n        a dict.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : bool, default True\n            Set the index of the returned DataFrame as the original MultiIndex.\n\n        name : list / sequence of str, optional\n            The passed names should substitute index level names.\n\n        Returns\n        -------\n        DataFrame : a DataFrame containing the original MultiIndex data.\n\n        See Also\n        --------\n        DataFrame : Two-dimensional, size-mutable, potentially heterogeneous\n            tabular data.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([['a', 'b'], ['c', 'd']])\n        >>> mi\n        MultiIndex([('a', 'c'),\n                    ('b', 'd')],\n                   )\n\n        >>> df = mi.to_frame()\n        >>> df\n             0  1\n        a c  a  c\n        b d  b  d\n\n        >>> df = mi.to_frame(index=False)\n        >>> df\n           0  1\n        0  a  c\n        1  b  d\n\n        >>> df = mi.to_frame(name=['x', 'y'])\n        >>> df\n             x  y\n        a c  a  c\n        b d  b  d\n        \"\"\"\n        from pandas import DataFrame\n\n        if name is not None:\n            if not is_list_like(name):\n                raise TypeError(\"'name' must be a list / sequence of column names.\")\n\n            if len(name) != len(self.levels):\n                raise ValueError(\n                    \"'name' should have same length as number of levels on index.\"\n                )\n            idx_names = name\n        else:\n            idx_names = self.names\n\n        # Guarantee resulting column order - PY36+ dict maintains insertion order\n        result = DataFrame(\n            {\n                (level if lvlname is None else lvlname): self._get_level_values(level)\n                for lvlname, level in zip(idx_names, range(len(self.levels)))\n            },\n            copy=False,\n        )\n\n        if index:\n            result.index = self\n        return result\n\n    def to_flat_index(self):\n        \"\"\"\n        Convert a MultiIndex to an Index of Tuples containing the level values.\n\n        .. versionadded:: 0.24.0\n\n        Returns\n        -------\n        pd.Index\n            Index with the MultiIndex data represented in Tuples.\n\n        Notes\n        -----\n        This method will simply return the caller if called by anything other\n        than a MultiIndex.\n\n        Examples\n        --------\n        >>> index = pd.MultiIndex.from_product(\n        ...     [['foo', 'bar'], ['baz', 'qux']],\n        ...     names=['a', 'b'])\n        >>> index.to_flat_index()\n        Index([('foo', 'baz'), ('foo', 'qux'),\n               ('bar', 'baz'), ('bar', 'qux')],\n              dtype='object')\n        \"\"\"\n        return Index(self._values, tupleize_cols=False)\n\n    @property\n    def _is_all_dates(self) -> bool:\n        return False\n\n    def is_lexsorted(self) -> bool:\n        \"\"\"\n        Return True if the codes are lexicographically sorted.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        In the below examples, the first level of the MultiIndex is sorted because\n        a<b<c, so there is no need to look at the next level.\n\n        >>> pd.MultiIndex.from_arrays([['a', 'b', 'c'], ['d', 'e', 'f']]).is_lexsorted()\n        True\n        >>> pd.MultiIndex.from_arrays([['a', 'b', 'c'], ['d', 'f', 'e']]).is_lexsorted()\n        True\n\n        In case there is a tie, the lexicographical sorting looks\n        at the next level of the MultiIndex.\n\n        >>> pd.MultiIndex.from_arrays([[0, 1, 1], ['a', 'b', 'c']]).is_lexsorted()\n        True\n        >>> pd.MultiIndex.from_arrays([[0, 1, 1], ['a', 'c', 'b']]).is_lexsorted()\n        False\n        >>> pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'],\n        ...                            ['aa', 'bb', 'aa', 'bb']]).is_lexsorted()\n        True\n        >>> pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'],\n        ...                            ['bb', 'aa', 'aa', 'bb']]).is_lexsorted()\n        False\n        \"\"\"\n        return self.lexsort_depth == self.nlevels\n\n    @cache_readonly\n    def lexsort_depth(self):\n        if self.sortorder is not None:\n            return self.sortorder\n\n        return self._lexsort_depth()\n\n    def _lexsort_depth(self) -> int:\n        \"\"\"\n        Compute and return the lexsort_depth, the number of levels of the\n        MultiIndex that are sorted lexically\n\n        Returns\n        -------\n        int\n        \"\"\"\n        int64_codes = [ensure_int64(level_codes) for level_codes in self.codes]\n        for k in range(self.nlevels, 0, -1):\n            if libalgos.is_lexsorted(int64_codes[:k]):\n                return k\n        return 0\n\n    def _sort_levels_monotonic(self):\n        \"\"\"\n        This is an *internal* function.\n\n        Create a new MultiIndex from the current to monotonically sorted\n        items IN the levels. This does not actually make the entire MultiIndex\n        monotonic, JUST the levels.\n\n        The resulting MultiIndex will have the same outward\n        appearance, meaning the same .values and ordering. It will also\n        be .equals() to the original.\n\n        Returns\n        -------\n        MultiIndex\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex(levels=[['a', 'b'], ['bb', 'aa']],\n        ...                    codes=[[0, 0, 1, 1], [0, 1, 0, 1]])\n        >>> mi\n        MultiIndex([('a', 'bb'),\n                    ('a', 'aa'),\n                    ('b', 'bb'),\n                    ('b', 'aa')],\n                   )\n\n        >>> mi.sort_values()\n        MultiIndex([('a', 'aa'),\n                    ('a', 'bb'),\n                    ('b', 'aa'),\n                    ('b', 'bb')],\n                   )\n        \"\"\"\n        if self.is_lexsorted() and self.is_monotonic:\n            return self\n\n        new_levels = []\n        new_codes = []\n\n        for lev, level_codes in zip(self.levels, self.codes):\n\n            if not lev.is_monotonic:\n                try:\n                    # indexer to reorder the levels\n                    indexer = lev.argsort()\n                except TypeError:\n                    pass\n                else:\n                    lev = lev.take(indexer)\n\n                    # indexer to reorder the level codes\n                    indexer = ensure_int64(indexer)\n                    ri = lib.get_reverse_indexer(indexer, len(indexer))\n                    level_codes = algos.take_1d(ri, level_codes)\n\n            new_levels.append(lev)\n            new_codes.append(level_codes)\n\n        return MultiIndex(\n            new_levels,\n            new_codes,\n            names=self.names,\n            sortorder=self.sortorder,\n            verify_integrity=False,\n        )\n\n    def remove_unused_levels(self):\n        \"\"\"\n        Create new MultiIndex from current that removes unused levels.\n\n        Unused level(s) means levels that are not expressed in the\n        labels. The resulting MultiIndex will have the same outward\n        appearance, meaning the same .values and ordering. It will\n        also be .equals() to the original.\n\n        Returns\n        -------\n        MultiIndex\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_product([range(2), list('ab')])\n        >>> mi\n        MultiIndex([(0, 'a'),\n                    (0, 'b'),\n                    (1, 'a'),\n                    (1, 'b')],\n                   )\n\n        >>> mi[2:]\n        MultiIndex([(1, 'a'),\n                    (1, 'b')],\n                   )\n\n        The 0 from the first level is not represented\n        and can be removed\n\n        >>> mi2 = mi[2:].remove_unused_levels()\n        >>> mi2.levels\n        FrozenList([[1], ['a', 'b']])\n        \"\"\"\n        new_levels = []\n        new_codes = []\n\n        changed = False\n        for lev, level_codes in zip(self.levels, self.codes):\n\n            # Since few levels are typically unused, bincount() is more\n            # efficient than unique() - however it only accepts positive values\n            # (and drops order):\n            uniques = np.where(np.bincount(level_codes + 1) > 0)[0] - 1\n            has_na = int(len(uniques) and (uniques[0] == -1))\n\n            if len(uniques) != len(lev) + has_na:\n                # We have unused levels\n                changed = True\n\n                # Recalculate uniques, now preserving order.\n                # Can easily be cythonized by exploiting the already existing\n                # \"uniques\" and stop parsing \"level_codes\" when all items\n                # are found:\n                uniques = algos.unique(level_codes)\n                if has_na:\n                    na_idx = np.where(uniques == -1)[0]\n                    # Just ensure that -1 is in first position:\n                    uniques[[0, na_idx[0]]] = uniques[[na_idx[0], 0]]\n\n                # codes get mapped from uniques to 0:len(uniques)\n                # -1 (if present) is mapped to last position\n                code_mapping = np.zeros(len(lev) + has_na)\n                # ... and reassigned value -1:\n                code_mapping[uniques] = np.arange(len(uniques)) - has_na\n\n                level_codes = code_mapping[level_codes]\n\n                # new levels are simple\n                lev = lev.take(uniques[has_na:])\n\n            new_levels.append(lev)\n            new_codes.append(level_codes)\n\n        result = self.view()\n\n        if changed:\n            result._reset_identity()\n            result._set_levels(new_levels, validate=False)\n            result._set_codes(new_codes, validate=False)\n\n        return result\n\n    # --------------------------------------------------------------------\n    # Pickling Methods\n\n    def __reduce__(self):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        d = {\n            \"levels\": list(self.levels),\n            \"codes\": list(self.codes),\n            \"sortorder\": self.sortorder,\n            \"names\": list(self.names),\n        }\n        return ibase._new_Index, (type(self), d), None\n\n    # --------------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if is_scalar(key):\n            key = com.cast_scalar_indexer(key, warn_float=True)\n\n            retval = []\n            for lev, level_codes in zip(self.levels, self.codes):\n                if level_codes[key] == -1:\n                    retval.append(np.nan)\n                else:\n                    retval.append(lev[level_codes[key]])\n\n            return tuple(retval)\n        else:\n            if com.is_bool_indexer(key):\n                key = np.asarray(key, dtype=bool)\n                sortorder = self.sortorder\n            else:\n                # cannot be sure whether the result will be sorted\n                sortorder = None\n\n                if isinstance(key, Index):\n                    key = np.asarray(key)\n\n            new_codes = [level_codes[key] for level_codes in self.codes]\n\n            return MultiIndex(\n                levels=self.levels,\n                codes=new_codes,\n                names=self.names,\n                sortorder=sortorder,\n                verify_integrity=False,\n            )\n\n    @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):\n        nv.validate_take((), kwargs)\n        indices = ensure_platform_int(indices)\n\n        # only fill if we are passing a non-None fill_value\n        allow_fill = self._maybe_disallow_fill(allow_fill, fill_value, indices)\n\n        na_value = -1\n\n        if allow_fill:\n            taken = [lab.take(indices) for lab in self.codes]\n            mask = indices == -1\n            if mask.any():\n                masked = []\n                for new_label in taken:\n                    label_values = new_label\n                    label_values[mask] = na_value\n                    masked.append(np.asarray(label_values))\n                taken = masked\n        else:\n            taken = [lab.take(indices) for lab in self.codes]\n\n        return MultiIndex(\n            levels=self.levels, codes=taken, names=self.names, verify_integrity=False\n        )\n\n    def append(self, other):\n        \"\"\"\n        Append a collection of Index options together\n\n        Parameters\n        ----------\n        other : Index or list/tuple of indices\n\n        Returns\n        -------\n        appended : Index\n        \"\"\"\n        if not isinstance(other, (list, tuple)):\n            other = [other]\n\n        if all(\n            (isinstance(o, MultiIndex) and o.nlevels >= self.nlevels) for o in other\n        ):\n            arrays = []\n            for i in range(self.nlevels):\n                label = self._get_level_values(i)\n                appended = [o._get_level_values(i) for o in other]\n                arrays.append(label.append(appended))\n            return MultiIndex.from_arrays(arrays, names=self.names)\n\n        to_concat = (self._values,) + tuple(k._values for k in other)\n        new_tuples = np.concatenate(to_concat)\n\n        # if all(isinstance(x, MultiIndex) for x in other):\n        try:\n            return MultiIndex.from_tuples(new_tuples, names=self.names)\n        except (TypeError, IndexError):\n            return Index(new_tuples)\n\n    def argsort(self, *args, **kwargs) -> np.ndarray:\n        return self._values.argsort(*args, **kwargs)\n\n    @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n    def repeat(self, repeats, axis=None):\n        nv.validate_repeat((), {\"axis\": axis})\n        repeats = ensure_platform_int(repeats)\n        return MultiIndex(\n            levels=self.levels,\n            codes=[\n                level_codes.view(np.ndarray).astype(np.intp).repeat(repeats)\n                for level_codes in self.codes\n            ],\n            names=self.names,\n            sortorder=self.sortorder,\n            verify_integrity=False,\n        )\n\n    def where(self, cond, other=None):\n        raise NotImplementedError(\".where is not supported for MultiIndex operations\")\n\n    def drop(self, codes, level=None, errors=\"raise\"):\n        \"\"\"\n        Make new MultiIndex with passed list of codes deleted\n\n        Parameters\n        ----------\n        codes : array-like\n            Must be a list of tuples when level is not specified\n        level : int or level name, default None\n        errors : str, default 'raise'\n\n        Returns\n        -------\n        dropped : MultiIndex\n        \"\"\"\n        if level is not None:\n            return self._drop_from_level(codes, level, errors)\n\n        if not isinstance(codes, (np.ndarray, Index)):\n            try:\n                codes = com.index_labels_to_array(codes, dtype=object)\n            except ValueError:\n                pass\n\n        inds = []\n        for level_codes in codes:\n            try:\n                loc = self.get_loc(level_codes)\n                # get_loc returns either an integer, a slice, or a boolean\n                # mask\n                if isinstance(loc, int):\n                    inds.append(loc)\n                elif isinstance(loc, slice):\n                    inds.extend(range(loc.start, loc.stop))\n                elif com.is_bool_indexer(loc):\n                    if self.lexsort_depth == 0:\n                        warnings.warn(\n                            \"dropping on a non-lexsorted multi-index \"\n                            \"without a level parameter may impact performance.\",\n                            PerformanceWarning,\n                            stacklevel=3,\n                        )\n                    loc = loc.nonzero()[0]\n                    inds.extend(loc)\n                else:\n                    msg = f\"unsupported indexer of type {type(loc)}\"\n                    raise AssertionError(msg)\n            except KeyError:\n                if errors != \"ignore\":\n                    raise\n\n        return self.delete(inds)\n\n    def _drop_from_level(self, codes, level, errors=\"raise\"):\n        codes = com.index_labels_to_array(codes)\n        i = self._get_level_number(level)\n        index = self.levels[i]\n        values = index.get_indexer(codes)\n        # If nan should be dropped it will equal -1 here. We have to check which values\n        # are not nan and equal -1, this means they are missing in the index\n        nan_codes = isna(codes)\n        values[(np.equal(nan_codes, False)) & (values == -1)] = -2\n        if index.shape[0] == self.shape[0]:\n            values[np.equal(nan_codes, True)] = -2\n\n        not_found = codes[values == -2]\n        if len(not_found) != 0 and errors != \"ignore\":\n            raise KeyError(f\"labels {not_found} not found in level\")\n        mask = ~algos.isin(self.codes[i], values)\n\n        return self[mask]\n\n    def swaplevel(self, i=-2, j=-1):\n        \"\"\"\n        Swap level i with level j.\n\n        Calling this method does not change the ordering of the values.\n\n        Parameters\n        ----------\n        i : int, str, default -2\n            First level of index to be swapped. Can pass level name as string.\n            Type of parameters can be mixed.\n        j : int, str, default -1\n            Second level of index to be swapped. Can pass level name as string.\n            Type of parameters can be mixed.\n\n        Returns\n        -------\n        MultiIndex\n            A new MultiIndex.\n\n        See Also\n        --------\n        Series.swaplevel : Swap levels i and j in a MultiIndex.\n        Dataframe.swaplevel : Swap levels i and j in a MultiIndex on a\n            particular axis.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex(levels=[['a', 'b'], ['bb', 'aa']],\n        ...                    codes=[[0, 0, 1, 1], [0, 1, 0, 1]])\n        >>> mi\n        MultiIndex([('a', 'bb'),\n                    ('a', 'aa'),\n                    ('b', 'bb'),\n                    ('b', 'aa')],\n                   )\n        >>> mi.swaplevel(0, 1)\n        MultiIndex([('bb', 'a'),\n                    ('aa', 'a'),\n                    ('bb', 'b'),\n                    ('aa', 'b')],\n                   )\n        \"\"\"\n        new_levels = list(self.levels)\n        new_codes = list(self.codes)\n        new_names = list(self.names)\n\n        i = self._get_level_number(i)\n        j = self._get_level_number(j)\n\n        new_levels[i], new_levels[j] = new_levels[j], new_levels[i]\n        new_codes[i], new_codes[j] = new_codes[j], new_codes[i]\n        new_names[i], new_names[j] = new_names[j], new_names[i]\n\n        return MultiIndex(\n            levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\n        )\n\n    def reorder_levels(self, order):\n        \"\"\"\n        Rearrange levels using input order. May not drop or duplicate levels.\n\n        Parameters\n        ----------\n        order : list of int or list of str\n            List representing new level order. Reference level by number\n            (position) or by key (label).\n\n        Returns\n        -------\n        MultiIndex\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[1, 2], [3, 4]], names=['x', 'y'])\n        >>> mi\n        MultiIndex([(1, 3),\n                    (2, 4)],\n                   names=['x', 'y'])\n\n        >>> mi.reorder_levels(order=[1, 0])\n        MultiIndex([(3, 1),\n                    (4, 2)],\n                   names=['y', 'x'])\n\n        >>> mi.reorder_levels(order=['y', 'x'])\n        MultiIndex([(3, 1),\n                    (4, 2)],\n                   names=['y', 'x'])\n        \"\"\"\n        order = [self._get_level_number(i) for i in order]\n        if len(order) != self.nlevels:\n            raise AssertionError(\n                f\"Length of order must be same as number of levels ({self.nlevels}), \"\n                f\"got {len(order)}\"\n            )\n        new_levels = [self.levels[i] for i in order]\n        new_codes = [self.codes[i] for i in order]\n        new_names = [self.names[i] for i in order]\n\n        return MultiIndex(\n            levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False\n        )\n\n    def _get_codes_for_sorting(self):\n        \"\"\"\n        we are categorizing our codes by using the\n        available categories (all, not just observed)\n        excluding any missing ones (-1); this is in preparation\n        for sorting, where we need to disambiguate that -1 is not\n        a valid valid\n        \"\"\"\n\n        def cats(level_codes):\n            return np.arange(\n                np.array(level_codes).max() + 1 if len(level_codes) else 0,\n                dtype=level_codes.dtype,\n            )\n\n        return [\n            Categorical.from_codes(level_codes, cats(level_codes), ordered=True)\n            for level_codes in self.codes\n        ]\n\n    def sortlevel(self, level=0, ascending=True, sort_remaining=True):\n        \"\"\"\n        Sort MultiIndex at the requested level.\n\n        The result will respect the original ordering of the associated\n        factor at that level.\n\n        Parameters\n        ----------\n        level : list-like, int or str, default 0\n            If a string is given, must be a name of the level.\n            If list-like must be names or ints of levels.\n        ascending : bool, default True\n            False to sort in descending order.\n            Can also be a list to specify a directed ordering.\n        sort_remaining : sort by the remaining levels after level\n\n        Returns\n        -------\n        sorted_index : pd.MultiIndex\n            Resulting index.\n        indexer : np.ndarray\n            Indices of output values in original index.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([[0, 0], [2, 1]])\n        >>> mi\n        MultiIndex([(0, 2),\n                    (0, 1)],\n                   )\n\n        >>> mi.sortlevel()\n        (MultiIndex([(0, 1),\n                    (0, 2)],\n                   ), array([1, 0]))\n\n        >>> mi.sortlevel(sort_remaining=False)\n        (MultiIndex([(0, 2),\n                    (0, 1)],\n                   ), array([0, 1]))\n\n        >>> mi.sortlevel(1)\n        (MultiIndex([(0, 1),\n                    (0, 2)],\n                   ), array([1, 0]))\n\n        >>> mi.sortlevel(1, ascending=False)\n        (MultiIndex([(0, 2),\n                    (0, 1)],\n                   ), array([0, 1]))\n        \"\"\"\n        if isinstance(level, (str, int)):\n            level = [level]\n        level = [self._get_level_number(lev) for lev in level]\n        sortorder = None\n\n        # we have a directed ordering via ascending\n        if isinstance(ascending, list):\n            if not len(level) == len(ascending):\n                raise ValueError(\"level must have same length as ascending\")\n\n            indexer = lexsort_indexer(\n                [self.codes[lev] for lev in level], orders=ascending\n            )\n\n        # level ordering\n        else:\n\n            codes = list(self.codes)\n            shape = list(self.levshape)\n\n            # partition codes and shape\n            primary = tuple(codes[lev] for lev in level)\n            primshp = tuple(shape[lev] for lev in level)\n\n            # Reverse sorted to retain the order of\n            # smaller indices that needs to be removed\n            for lev in sorted(level, reverse=True):\n                codes.pop(lev)\n                shape.pop(lev)\n\n            if sort_remaining:\n                primary += primary + tuple(codes)\n                primshp += primshp + tuple(shape)\n            else:\n                sortorder = level[0]\n\n            indexer = indexer_from_factorized(primary, primshp, compress=False)\n\n            if not ascending:\n                indexer = indexer[::-1]\n\n        indexer = ensure_platform_int(indexer)\n        new_codes = [level_codes.take(indexer) for level_codes in self.codes]\n\n        new_index = MultiIndex(\n            codes=new_codes,\n            levels=self.levels,\n            names=self.names,\n            sortorder=sortorder,\n            verify_integrity=False,\n        )\n\n        return new_index, indexer\n\n    def reindex(self, target, method=None, level=None, limit=None, tolerance=None):\n        \"\"\"\n        Create index with target's values (move/add/delete values as necessary)\n\n        Returns\n        -------\n        new_index : pd.MultiIndex\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index.\n\n        \"\"\"\n        # GH6552: preserve names when reindexing to non-named target\n        # (i.e. neither Index nor Series).\n        preserve_names = not hasattr(target, \"names\")\n\n        if level is not None:\n            if method is not None:\n                raise TypeError(\"Fill method not supported if level passed\")\n\n            # GH7774: preserve dtype/tz if target is empty and not an Index.\n            # target may be an iterator\n            target = ibase.ensure_has_len(target)\n            if len(target) == 0 and not isinstance(target, Index):\n                idx = self.levels[level]\n                attrs = idx._get_attributes_dict()\n                attrs.pop(\"freq\", None)  # don't preserve freq\n                target = type(idx)._simple_new(np.empty(0, dtype=idx.dtype), **attrs)\n            else:\n                target = ensure_index(target)\n            target, indexer, _ = self._join_level(\n                target, level, how=\"right\", return_indexers=True, keep_order=False\n            )\n        else:\n            target = ensure_index(target)\n            if self.equals(target):\n                indexer = None\n            else:\n                if self.is_unique:\n                    indexer = self.get_indexer(\n                        target, method=method, limit=limit, tolerance=tolerance\n                    )\n                else:\n                    raise ValueError(\"cannot handle a non-unique multi-index!\")\n\n        if not isinstance(target, MultiIndex):\n            if indexer is None:\n                target = self\n            elif (indexer >= 0).all():\n                target = self.take(indexer)\n            else:\n                # hopefully?\n                target = MultiIndex.from_tuples(target)\n\n        if (\n            preserve_names\n            and target.nlevels == self.nlevels\n            and target.names != self.names\n        ):\n            target = target.copy(deep=False)\n            target.names = self.names\n\n        return target, indexer\n\n    # --------------------------------------------------------------------\n    # Indexing Methods\n\n    def _check_indexing_error(self, key):\n        if not is_hashable(key) or is_iterator(key):\n            # We allow tuples if they are hashable, whereas other Index\n            #  subclasses require scalar.\n            # We have to explicitly exclude generators, as these are hashable.\n            raise InvalidIndexError(key)\n\n    def _should_fallback_to_positional(self) -> bool:\n        \"\"\"\n        Should integer key(s) be treated as positional?\n        \"\"\"\n        # GH#33355\n        return self.levels[0]._should_fallback_to_positional()\n\n    def _get_values_for_loc(self, series: \"Series\", loc, key):\n        \"\"\"\n        Do a positional lookup on the given Series, returning either a scalar\n        or a Series.\n\n        Assumes that `series.index is self`\n        \"\"\"\n        new_values = series._values[loc]\n        if is_scalar(loc):\n            return new_values\n\n        if len(new_values) == 1 and not self.nlevels > 1:\n            # If more than one level left, we can not return a scalar\n            return new_values[0]\n\n        new_index = self[loc]\n        new_index = maybe_droplevels(new_index, key)\n        new_ser = series._constructor(new_values, index=new_index, name=series.name)\n        return new_ser.__finalize__(series)\n\n    def _convert_listlike_indexer(self, keyarr):\n        \"\"\"\n        Parameters\n        ----------\n        keyarr : list-like\n            Indexer to convert.\n\n        Returns\n        -------\n        tuple (indexer, keyarr)\n            indexer is an ndarray or None if cannot convert\n            keyarr are tuple-safe keys\n        \"\"\"\n        indexer, keyarr = super()._convert_listlike_indexer(keyarr)\n\n        # are we indexing a specific level\n        if indexer is None and len(keyarr) and not isinstance(keyarr[0], tuple):\n            level = 0\n            _, indexer = self.reindex(keyarr, level=level)\n\n            # take all\n            if indexer is None:\n                indexer = np.arange(len(self))\n\n            check = self.levels[0].get_indexer(keyarr)\n            mask = check == -1\n            if mask.any():\n                raise KeyError(f\"{keyarr[mask]} not in index\")\n\n        return indexer, keyarr\n\n    def _get_partial_string_timestamp_match_key(self, key):\n        \"\"\"\n        Translate any partial string timestamp matches in key, returning the\n        new key.\n\n        Only relevant for MultiIndex.\n        \"\"\"\n        # GH#10331\n        if isinstance(key, str) and self.levels[0]._supports_partial_string_indexing:\n            # Convert key '2016-01-01' to\n            # ('2016-01-01'[, slice(None, None, None)]+)\n            key = (key,) + (slice(None),) * (len(self.levels) - 1)\n\n        if isinstance(key, tuple):\n            # Convert (..., '2016-01-01', ...) in tuple to\n            # (..., slice('2016-01-01', '2016-01-01', None), ...)\n            new_key = []\n            for i, component in enumerate(key):\n                if (\n                    isinstance(component, str)\n                    and self.levels[i]._supports_partial_string_indexing\n                ):\n                    new_key.append(slice(component, component, None))\n                else:\n                    new_key.append(component)\n            key = tuple(new_key)\n\n        return key\n\n    @Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)\n    def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        method = missing.clean_reindex_fill_method(method)\n        target = ensure_index(target)\n\n        # empty indexer\n        if is_list_like(target) and not len(target):\n            return ensure_platform_int(np.array([]))\n\n        if not isinstance(target, MultiIndex):\n            try:\n                target = MultiIndex.from_tuples(target)\n            except (TypeError, ValueError):\n\n                # let's instead try with a straight Index\n                if method is None:\n                    return Index(self._values).get_indexer(\n                        target, method=method, limit=limit, tolerance=tolerance\n                    )\n\n        if not self.is_unique:\n            raise ValueError(\"Reindexing only valid with uniquely valued Index objects\")\n\n        if method == \"pad\" or method == \"backfill\":\n            if tolerance is not None:\n                raise NotImplementedError(\n                    \"tolerance not implemented yet for MultiIndex\"\n                )\n            indexer = self._engine.get_indexer(\n                values=self._values, target=target, method=method, limit=limit\n            )\n        elif method == \"nearest\":\n            raise NotImplementedError(\n                \"method='nearest' not implemented yet \"\n                \"for MultiIndex; see GitHub issue 9365\"\n            )\n        else:\n            indexer = self._engine.get_indexer(target)\n\n        return ensure_platform_int(indexer)\n\n    def get_slice_bound(\n        self, label: Union[Hashable, Sequence[Hashable]], side: str, kind: str\n    ) -> int:\n        \"\"\"\n        For an ordered MultiIndex, compute slice bound\n        that corresponds to given label.\n\n        Returns leftmost (one-past-the-rightmost if `side=='right') position\n        of given label.\n\n        Parameters\n        ----------\n        label : object or tuple of objects\n        side : {'left', 'right'}\n        kind : {'loc', 'getitem'}\n\n        Returns\n        -------\n        int\n            Index of label.\n\n        Notes\n        -----\n        This method only works if level 0 index of the MultiIndex is lexsorted.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list('abbc'), list('gefd')])\n\n        Get the locations from the leftmost 'b' in the first level\n        until the end of the multiindex:\n\n        >>> mi.get_slice_bound('b', side=\"left\", kind=\"loc\")\n        1\n\n        Like above, but if you get the locations from the rightmost\n        'b' in the first level and 'f' in the second level:\n\n        >>> mi.get_slice_bound(('b','f'), side=\"right\", kind=\"loc\")\n        3\n\n        See Also\n        --------\n        MultiIndex.get_loc : Get location for a label or a tuple of labels.\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n        \"\"\"\n        if not isinstance(label, tuple):\n            label = (label,)\n        return self._partial_tup_index(label, side=side)\n\n    def slice_locs(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        For an ordered MultiIndex, compute the slice locations for input\n        labels.\n\n        The input labels can be tuples representing partial levels, e.g. for a\n        MultiIndex with 3 levels, you can pass a single value (corresponding to\n        the first level), or a 1-, 2-, or 3-tuple.\n\n        Parameters\n        ----------\n        start : label or tuple, default None\n            If None, defaults to the beginning\n        end : label or tuple\n            If None, defaults to the end\n        step : int or None\n            Slice step\n        kind : string, optional, defaults None\n\n        Returns\n        -------\n        (start, end) : (int, int)\n\n        Notes\n        -----\n        This method only works if the MultiIndex is properly lexsorted. So,\n        if only the first 2 levels of a 3-level MultiIndex are lexsorted,\n        you can only pass two levels to ``.slice_locs``.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list('abbd'), list('deff')],\n        ...                                names=['A', 'B'])\n\n        Get the slice locations from the beginning of 'b' in the first level\n        until the end of the multiindex:\n\n        >>> mi.slice_locs(start='b')\n        (1, 4)\n\n        Like above, but stop at the end of 'b' in the first level and 'f' in\n        the second level:\n\n        >>> mi.slice_locs(start='b', end=('b', 'f'))\n        (1, 3)\n\n        See Also\n        --------\n        MultiIndex.get_loc : Get location for a label or a tuple of labels.\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n        \"\"\"\n        # This function adds nothing to its parent implementation (the magic\n        # happens in get_slice_bound method), but it adds meaningful doc.\n        return super().slice_locs(start, end, step, kind=kind)\n\n    def _partial_tup_index(self, tup, side=\"left\"):\n        if len(tup) > self.lexsort_depth:\n            raise UnsortedIndexError(\n                f\"Key length ({len(tup)}) was greater than MultiIndex lexsort depth \"\n                f\"({self.lexsort_depth})\"\n            )\n\n        n = len(tup)\n        start, end = 0, len(self)\n        zipped = zip(tup, self.levels, self.codes)\n        for k, (lab, lev, labs) in enumerate(zipped):\n            section = labs[start:end]\n\n            if lab not in lev and not isna(lab):\n                if not lev.is_type_compatible(lib.infer_dtype([lab], skipna=False)):\n                    raise TypeError(f\"Level type mismatch: {lab}\")\n\n                # short circuit\n                loc = lev.searchsorted(lab, side=side)\n                if side == \"right\" and loc >= 0:\n                    loc -= 1\n                return start + section.searchsorted(loc, side=side)\n\n            idx = self._get_loc_single_level_index(lev, lab)\n            if isinstance(idx, slice) and k < n - 1:\n                # Get start and end value from slice, necessary when a non-integer\n                # interval is given as input GH#37707\n                start = idx.start\n                end = idx.stop\n            elif k < n - 1:\n                end = start + section.searchsorted(idx, side=\"right\")\n                start = start + section.searchsorted(idx, side=\"left\")\n            elif isinstance(idx, slice):\n                idx = idx.start\n                return start + section.searchsorted(idx, side=side)\n            else:\n                return start + section.searchsorted(idx, side=side)\n\n    def _get_loc_single_level_index(self, level_index: Index, key: Hashable) -> int:\n        \"\"\"\n        If key is NA value, location of index unify as -1.\n\n        Parameters\n        ----------\n        level_index: Index\n        key : label\n\n        Returns\n        -------\n        loc : int\n            If key is NA value, loc is -1\n            Else, location of key in index.\n\n        See Also\n        --------\n        Index.get_loc : The get_loc method for (single-level) index.\n        \"\"\"\n        if is_scalar(key) and isna(key):\n            return -1\n        else:\n            return level_index.get_loc(key)\n\n    def get_loc(self, key, method=None):\n        \"\"\"\n        Get location for a label or a tuple of labels.\n\n        The location is returned as an integer/slice or boolean\n        mask.\n\n        Parameters\n        ----------\n        key : label or tuple of labels (one for each level)\n        method : None\n\n        Returns\n        -------\n        loc : int, slice object or boolean mask\n            If the key is past the lexsort depth, the return may be a\n            boolean mask array, otherwise it is always a slice or int.\n\n        See Also\n        --------\n        Index.get_loc : The get_loc method for (single-level) index.\n        MultiIndex.slice_locs : Get slice location given start label(s) and\n                                end label(s).\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n\n        Notes\n        -----\n        The key cannot be a slice, list of same-level labels, a boolean mask,\n        or a sequence of such. If you want to use those, use\n        :meth:`MultiIndex.get_locs` instead.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')])\n\n        >>> mi.get_loc('b')\n        slice(1, 3, None)\n\n        >>> mi.get_loc(('b', 'e'))\n        1\n        \"\"\"\n        if method is not None:\n            raise NotImplementedError(\n                \"only the default get_loc method is \"\n                \"currently supported for MultiIndex\"\n            )\n\n        hash(key)\n\n        def _maybe_to_slice(loc):\n            \"\"\"convert integer indexer to boolean mask or slice if possible\"\"\"\n            if not isinstance(loc, np.ndarray) or loc.dtype != np.intp:\n                return loc\n\n            loc = lib.maybe_indices_to_slice(loc, len(self))\n            if isinstance(loc, slice):\n                return loc\n\n            mask = np.empty(len(self), dtype=\"bool\")\n            mask.fill(False)\n            mask[loc] = True\n            return mask\n\n        if not isinstance(key, tuple):\n            loc = self._get_level_indexer(key, level=0)\n            return _maybe_to_slice(loc)\n\n        keylen = len(key)\n        if self.nlevels < keylen:\n            raise KeyError(\n                f\"Key length ({keylen}) exceeds index depth ({self.nlevels})\"\n            )\n\n        if keylen == self.nlevels and self.is_unique:\n            return self._engine.get_loc(key)\n\n        # -- partial selection or non-unique index\n        # break the key into 2 parts based on the lexsort_depth of the index;\n        # the first part returns a continuous slice of the index; the 2nd part\n        # needs linear search within the slice\n        i = self.lexsort_depth\n        lead_key, follow_key = key[:i], key[i:]\n        start, stop = (\n            self.slice_locs(lead_key, lead_key) if lead_key else (0, len(self))\n        )\n\n        if start == stop:\n            raise KeyError(key)\n\n        if not follow_key:\n            return slice(start, stop)\n\n        warnings.warn(\n            \"indexing past lexsort depth may impact performance.\",\n            PerformanceWarning,\n            stacklevel=10,\n        )\n\n        loc = np.arange(start, stop, dtype=np.intp)\n\n        for i, k in enumerate(follow_key, len(lead_key)):\n            mask = self.codes[i][loc] == self._get_loc_single_level_index(\n                self.levels[i], k\n            )\n            if not mask.all():\n                loc = loc[mask]\n            if not len(loc):\n                raise KeyError(key)\n\n        return _maybe_to_slice(loc) if len(loc) != stop - start else slice(start, stop)\n\n    def get_loc_level(self, key, level=0, drop_level: bool = True):\n        \"\"\"\n        Get location and sliced index for requested label(s)/level(s).\n\n        Parameters\n        ----------\n        key : label or sequence of labels\n        level : int/level name or list thereof, optional\n        drop_level : bool, default True\n            If ``False``, the resulting index will not drop any level.\n\n        Returns\n        -------\n        loc : A 2-tuple where the elements are:\n              Element 0: int, slice object or boolean array\n              Element 1: The resulting sliced multiindex/index. If the key\n              contains all levels, this will be ``None``.\n\n        See Also\n        --------\n        MultiIndex.get_loc  : Get location for a label or a tuple of labels.\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')],\n        ...                                names=['A', 'B'])\n\n        >>> mi.get_loc_level('b')\n        (slice(1, 3, None), Index(['e', 'f'], dtype='object', name='B'))\n\n        >>> mi.get_loc_level('e', level='B')\n        (array([False,  True, False]), Index(['b'], dtype='object', name='A'))\n\n        >>> mi.get_loc_level(['b', 'e'])\n        (1, None)\n        \"\"\"\n        if not isinstance(level, (list, tuple)):\n            level = self._get_level_number(level)\n        else:\n            level = [self._get_level_number(lev) for lev in level]\n        return self._get_loc_level(key, level=level, drop_level=drop_level)\n\n    def _get_loc_level(\n        self, key, level: Union[int, List[int]] = 0, drop_level: bool = True\n    ):\n        \"\"\"\n        get_loc_level but with `level` known to be positional, not name-based.\n        \"\"\"\n\n        # different name to distinguish from maybe_droplevels\n        def maybe_mi_droplevels(indexer, levels, drop_level: bool):\n            if not drop_level:\n                return self[indexer]\n            # kludge around\n            orig_index = new_index = self[indexer]\n\n            for i in sorted(levels, reverse=True):\n                try:\n                    new_index = new_index._drop_level_numbers([i])\n                except ValueError:\n\n                    # no dropping here\n                    return orig_index\n            return new_index\n\n        if isinstance(level, (tuple, list)):\n            if len(key) != len(level):\n                raise AssertionError(\n                    \"Key for location must have same length as number of levels\"\n                )\n            result = None\n            for lev, k in zip(level, key):\n                loc, new_index = self._get_loc_level(k, level=lev)\n                if isinstance(loc, slice):\n                    mask = np.zeros(len(self), dtype=bool)\n                    mask[loc] = True\n                    loc = mask\n\n                result = loc if result is None else result & loc\n\n            return result, maybe_mi_droplevels(result, level, drop_level)\n\n        # kludge for #1796\n        if isinstance(key, list):\n            key = tuple(key)\n\n        if isinstance(key, tuple) and level == 0:\n\n            try:\n                if key in self.levels[0]:\n                    indexer = self._get_level_indexer(key, level=level)\n                    new_index = maybe_mi_droplevels(indexer, [0], drop_level)\n                    return indexer, new_index\n            except (TypeError, InvalidIndexError):\n                pass\n\n            if not any(isinstance(k, slice) for k in key):\n\n                # partial selection\n                # optionally get indexer to avoid re-calculation\n                def partial_selection(key, indexer=None):\n                    if indexer is None:\n                        indexer = self.get_loc(key)\n                    ilevels = [\n                        i for i in range(len(key)) if key[i] != slice(None, None)\n                    ]\n                    return indexer, maybe_mi_droplevels(indexer, ilevels, drop_level)\n\n                if len(key) == self.nlevels and self.is_unique:\n                    # Complete key in unique index -> standard get_loc\n                    try:\n                        return (self._engine.get_loc(key), None)\n                    except KeyError as e:\n                        raise KeyError(key) from e\n                else:\n                    return partial_selection(key)\n            else:\n                indexer = None\n                for i, k in enumerate(key):\n                    if not isinstance(k, slice):\n                        k = self._get_level_indexer(k, level=i)\n                        if isinstance(k, slice):\n                            # everything\n                            if k.start == 0 and k.stop == len(self):\n                                k = slice(None, None)\n                        else:\n                            k_index = k\n\n                    if isinstance(k, slice):\n                        if k == slice(None, None):\n                            continue\n                        else:\n                            raise TypeError(key)\n\n                    if indexer is None:\n                        indexer = k_index\n                    else:  # pragma: no cover\n                        indexer &= k_index\n                if indexer is None:\n                    indexer = slice(None, None)\n                ilevels = [i for i in range(len(key)) if key[i] != slice(None, None)]\n                return indexer, maybe_mi_droplevels(indexer, ilevels, drop_level)\n        else:\n            indexer = self._get_level_indexer(key, level=level)\n            return indexer, maybe_mi_droplevels(indexer, [level], drop_level)\n\n    def _get_level_indexer(self, key, level: int = 0, indexer=None):\n        # `level` kwarg is _always_ positional, never name\n        # return an indexer, boolean array or a slice showing where the key is\n        # in the totality of values\n        # if the indexer is provided, then use this\n\n        level_index = self.levels[level]\n        level_codes = self.codes[level]\n\n        def convert_indexer(start, stop, step, indexer=indexer, codes=level_codes):\n            # given the inputs and the codes/indexer, compute an indexer set\n            # if we have a provided indexer, then this need not consider\n            # the entire labels set\n\n            r = np.arange(start, stop, step)\n            if indexer is not None and len(indexer) != len(codes):\n\n                # we have an indexer which maps the locations in the labels\n                # that we have already selected (and is not an indexer for the\n                # entire set) otherwise this is wasteful so we only need to\n                # examine locations that are in this set the only magic here is\n                # that the result are the mappings to the set that we have\n                # selected\n                from pandas import Series\n\n                mapper = Series(indexer)\n                indexer = codes.take(ensure_platform_int(indexer))\n                result = Series(Index(indexer).isin(r).nonzero()[0])\n                m = result.map(mapper)\n                m = np.asarray(m)\n\n            else:\n                m = np.zeros(len(codes), dtype=bool)\n                m[np.in1d(codes, r, assume_unique=Index(codes).is_unique)] = True\n\n            return m\n\n        if isinstance(key, slice):\n            # handle a slice, returning a slice if we can\n            # otherwise a boolean indexer\n\n            try:\n                if key.start is not None:\n                    start = level_index.get_loc(key.start)\n                else:\n                    start = 0\n                if key.stop is not None:\n                    stop = level_index.get_loc(key.stop)\n                elif isinstance(start, slice):\n                    stop = len(level_index)\n                else:\n                    stop = len(level_index) - 1\n                step = key.step\n            except KeyError:\n\n                # we have a partial slice (like looking up a partial date\n                # string)\n                start = stop = level_index.slice_indexer(\n                    key.start, key.stop, key.step, kind=\"loc\"\n                )\n                step = start.step\n\n            if isinstance(start, slice) or isinstance(stop, slice):\n                # we have a slice for start and/or stop\n                # a partial date slicer on a DatetimeIndex generates a slice\n                # note that the stop ALREADY includes the stopped point (if\n                # it was a string sliced)\n                start = getattr(start, \"start\", start)\n                stop = getattr(stop, \"stop\", stop)\n                return convert_indexer(start, stop, step)\n\n            elif level > 0 or self.lexsort_depth == 0 or step is not None:\n                # need to have like semantics here to right\n                # searching as when we are using a slice\n                # so include the stop+1 (so we include stop)\n                return convert_indexer(start, stop + 1, step)\n            else:\n                # sorted, so can return slice object -> view\n                i = level_codes.searchsorted(start, side=\"left\")\n                j = level_codes.searchsorted(stop, side=\"right\")\n                return slice(i, j, step)\n\n        else:\n\n            idx = self._get_loc_single_level_index(level_index, key)\n\n            if level > 0 or self.lexsort_depth == 0:\n                # Desired level is not sorted\n                locs = np.array(level_codes == idx, dtype=bool, copy=False)\n                if not locs.any():\n                    # The label is present in self.levels[level] but unused:\n                    raise KeyError(key)\n                return locs\n\n            if isinstance(idx, slice):\n                start = idx.start\n                end = idx.stop\n            else:\n                start = level_codes.searchsorted(idx, side=\"left\")\n                end = level_codes.searchsorted(idx, side=\"right\")\n\n            if start == end:\n                # The label is present in self.levels[level] but unused:\n                raise KeyError(key)\n            return slice(start, end)\n\n    def get_locs(self, seq):\n        \"\"\"\n        Get location for a sequence of labels.\n\n        Parameters\n        ----------\n        seq : label, slice, list, mask or a sequence of such\n           You should use one of the above for each level.\n           If a level should not be used, set it to ``slice(None)``.\n\n        Returns\n        -------\n        numpy.ndarray\n            NumPy array of integers suitable for passing to iloc.\n\n        See Also\n        --------\n        MultiIndex.get_loc : Get location for a label or a tuple of labels.\n        MultiIndex.slice_locs : Get slice location given start label(s) and\n                                end label(s).\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')])\n\n        >>> mi.get_locs('b')  # doctest: +SKIP\n        array([1, 2], dtype=int64)\n\n        >>> mi.get_locs([slice(None), ['e', 'f']])  # doctest: +SKIP\n        array([1, 2], dtype=int64)\n\n        >>> mi.get_locs([[True, False, True], slice('e', 'f')])  # doctest: +SKIP\n        array([2], dtype=int64)\n        \"\"\"\n\n        # must be lexsorted to at least as many levels\n        true_slices = [i for (i, s) in enumerate(com.is_true_slices(seq)) if s]\n        if true_slices and true_slices[-1] >= self.lexsort_depth:\n            raise UnsortedIndexError(\n                \"MultiIndex slicing requires the index to be lexsorted: slicing \"\n                f\"on levels {true_slices}, lexsort depth {self.lexsort_depth}\"\n            )\n        # indexer\n        # this is the list of all values that we want to select\n        n = len(self)\n        indexer = None\n\n        def _convert_to_indexer(r) -> Int64Index:\n            # return an indexer\n            if isinstance(r, slice):\n                m = np.zeros(n, dtype=bool)\n                m[r] = True\n                r = m.nonzero()[0]\n            elif com.is_bool_indexer(r):\n                if len(r) != n:\n                    raise ValueError(\n                        \"cannot index with a boolean indexer \"\n                        \"that is not the same length as the \"\n                        \"index\"\n                    )\n                r = r.nonzero()[0]\n            return Int64Index(r)\n\n        def _update_indexer(\n            idxr: Optional[Index], indexer: Optional[Index], key\n        ) -> Index:\n            if indexer is None:\n                indexer = Index(np.arange(n))\n            if idxr is None:\n                return indexer\n            indexer_intersection = indexer.intersection(idxr)\n            if indexer_intersection.empty and not idxr.empty and not indexer.empty:\n                raise KeyError(key)\n            return indexer_intersection\n\n        for i, k in enumerate(seq):\n\n            if com.is_bool_indexer(k):\n                # a boolean indexer, must be the same length!\n                k = np.asarray(k)\n                indexer = _update_indexer(\n                    _convert_to_indexer(k), indexer=indexer, key=seq\n                )\n\n            elif is_list_like(k):\n                # a collection of labels to include from this level (these\n                # are or'd)\n                indexers: Optional[Int64Index] = None\n                for x in k:\n                    try:\n                        idxrs = _convert_to_indexer(\n                            self._get_level_indexer(x, level=i, indexer=indexer)\n                        )\n                        indexers = (idxrs if indexers is None else indexers).union(\n                            idxrs, sort=False\n                        )\n                    except KeyError:\n\n                        # ignore not founds\n                        continue\n\n                if indexers is not None:\n                    indexer = _update_indexer(indexers, indexer=indexer, key=seq)\n                else:\n                    # no matches we are done\n                    return np.array([], dtype=np.int64)\n\n            elif com.is_null_slice(k):\n                # empty slice\n                indexer = _update_indexer(None, indexer=indexer, key=seq)\n\n            elif isinstance(k, slice):\n\n                # a slice, include BOTH of the labels\n                indexer = _update_indexer(\n                    _convert_to_indexer(\n                        self._get_level_indexer(k, level=i, indexer=indexer)\n                    ),\n                    indexer=indexer,\n                    key=seq,\n                )\n            else:\n                # a single label\n                indexer = _update_indexer(\n                    _convert_to_indexer(\n                        self.get_loc_level(k, level=i, drop_level=False)[0]\n                    ),\n                    indexer=indexer,\n                    key=seq,\n                )\n\n        # empty indexer\n        if indexer is None:\n            return np.array([], dtype=np.int64)\n\n        assert isinstance(indexer, Int64Index), type(indexer)\n        indexer = self._reorder_indexer(seq, indexer)\n\n        return indexer._values\n\n    # --------------------------------------------------------------------\n\n    def _reorder_indexer(\n        self,\n        seq: Tuple[Union[Scalar, Iterable, AnyArrayLike], ...],\n        indexer: Int64Index,\n    ) -> Int64Index:\n        \"\"\"\n        Reorder an indexer of a MultiIndex (self) so that the label are in the\n        same order as given in seq\n\n        Parameters\n        ----------\n        seq : label/slice/list/mask or a sequence of such\n        indexer: an Int64Index indexer of self\n\n        Returns\n        -------\n        indexer : a sorted Int64Index indexer of self ordered as seq\n        \"\"\"\n        # If the index is lexsorted and the list_like label in seq are sorted\n        # then we do not need to sort\n        if self.is_lexsorted():\n            need_sort = False\n            for i, k in enumerate(seq):\n                if is_list_like(k):\n                    if not need_sort:\n                        k_codes = self.levels[i].get_indexer(k)\n                        k_codes = k_codes[k_codes >= 0]  # Filter absent keys\n                        # True if the given codes are not ordered\n                        need_sort = (k_codes[:-1] > k_codes[1:]).any()\n            # Bail out if both index and seq are sorted\n            if not need_sort:\n                return indexer\n\n        n = len(self)\n        keys: Tuple[np.ndarray, ...] = ()\n        # For each level of the sequence in seq, map the level codes with the\n        # order they appears in a list-like sequence\n        # This mapping is then use to reorder the indexer\n        for i, k in enumerate(seq):\n            if is_scalar(k):\n                # GH#34603 we want to treat a scalar the same as an all equal list\n                k = [k]\n            if com.is_bool_indexer(k):\n                new_order = np.arange(n)[indexer]\n            elif is_list_like(k):\n                # Generate a map with all level codes as sorted initially\n                key_order_map = np.ones(len(self.levels[i]), dtype=np.uint64) * len(\n                    self.levels[i]\n                )\n                # Set order as given in the indexer list\n                level_indexer = self.levels[i].get_indexer(k)\n                level_indexer = level_indexer[level_indexer >= 0]  # Filter absent keys\n                key_order_map[level_indexer] = np.arange(len(level_indexer))\n\n                new_order = key_order_map[self.codes[i][indexer]]\n            elif isinstance(k, slice) and k.start is None and k.stop is None:\n                # slice(None) should not determine order GH#31330\n                new_order = np.ones((n,))[indexer]\n            else:\n                # For all other case, use the same order as the level\n                new_order = np.arange(n)[indexer]\n            keys = (new_order,) + keys\n\n        # Find the reordering using lexsort on the keys mapping\n        ind = np.lexsort(keys)\n        return indexer[ind]\n\n    def truncate(self, before=None, after=None):\n        \"\"\"\n        Slice index between two labels / tuples, return new MultiIndex\n\n        Parameters\n        ----------\n        before : label or tuple, can be partial. Default None\n            None defaults to start\n        after : label or tuple, can be partial. Default None\n            None defaults to end\n\n        Returns\n        -------\n        truncated : MultiIndex\n        \"\"\"\n        if after and before and after < before:\n            raise ValueError(\"after < before\")\n\n        i, j = self.levels[0].slice_locs(before, after)\n        left, right = self.slice_locs(before, after)\n\n        new_levels = list(self.levels)\n        new_levels[0] = new_levels[0][i:j]\n\n        new_codes = [level_codes[left:right] for level_codes in self.codes]\n        new_codes[0] = new_codes[0] - i\n\n        return MultiIndex(\n            levels=new_levels,\n            codes=new_codes,\n            names=self._names,\n            verify_integrity=False,\n        )\n\n    def equals(self, other: object) -> bool:\n        \"\"\"\n        Determines if two MultiIndex objects have the same labeling information\n        (the levels themselves do not necessarily have to be the same)\n\n        See Also\n        --------\n        equal_levels\n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if len(self) != len(other):\n            return False\n\n        if not isinstance(other, MultiIndex):\n            # d-level MultiIndex can equal d-tuple Index\n            if not is_object_dtype(other.dtype):\n                # other cannot contain tuples, so cannot match self\n                return False\n            return array_equivalent(self._values, other._values)\n\n        if self.nlevels != other.nlevels:\n            return False\n\n        for i in range(self.nlevels):\n            self_codes = self.codes[i]\n            self_codes = self_codes[self_codes != -1]\n            self_values = algos.take_nd(\n                np.asarray(self.levels[i]._values), self_codes, allow_fill=False\n            )\n\n            other_codes = other.codes[i]\n            other_codes = other_codes[other_codes != -1]\n            other_values = algos.take_nd(\n                np.asarray(other.levels[i]._values), other_codes, allow_fill=False\n            )\n\n            # since we use NaT both datetime64 and timedelta64 we can have a\n            # situation where a level is typed say timedelta64 in self (IOW it\n            # has other values than NaT) but types datetime64 in other (where\n            # its all NaT) but these are equivalent\n            if len(self_values) == 0 and len(other_values) == 0:\n                continue\n\n            if not array_equivalent(self_values, other_values):\n                return False\n\n        return True\n\n    def equal_levels(self, other) -> bool:\n        \"\"\"\n        Return True if the levels of both MultiIndex objects are the same\n\n        \"\"\"\n        if self.nlevels != other.nlevels:\n            return False\n\n        for i in range(self.nlevels):\n            if not self.levels[i].equals(other.levels[i]):\n                return False\n        return True\n\n    # --------------------------------------------------------------------\n    # Set Methods\n\n    def union(self, other, sort=None):\n        \"\"\"\n        Form the union of two MultiIndex objects\n\n        Parameters\n        ----------\n        other : MultiIndex or array / Index of tuples\n        sort : False or None, default None\n            Whether to sort the resulting Index.\n\n            * None : Sort the result, except when\n\n              1. `self` and `other` are equal.\n              2. `self` has length 0.\n              3. Some values in `self` or `other` cannot be compared.\n                 A RuntimeWarning is issued in this case.\n\n            * False : do not sort the result.\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        Index\n\n        Examples\n        --------\n        >>> idx1 = pd.MultiIndex.from_arrays(\n        ...     [[1, 1, 2, 2], [\"Red\", \"Blue\", \"Red\", \"Blue\"]]\n        ... )\n        >>> idx1\n        MultiIndex([(1,  'Red'),\n            (1, 'Blue'),\n            (2,  'Red'),\n            (2, 'Blue')],\n           )\n        >>> idx2 = pd.MultiIndex.from_arrays(\n        ...     [[3, 3, 2, 2], [\"Red\", \"Green\", \"Red\", \"Green\"]]\n        ... )\n        >>> idx2\n        MultiIndex([(3,   'Red'),\n            (3, 'Green'),\n            (2,   'Red'),\n            (2, 'Green')],\n           )\n\n        >>> idx1.union(idx2)\n        MultiIndex([(1,  'Blue'),\n            (1,   'Red'),\n            (2,  'Blue'),\n            (2, 'Green'),\n            (2,   'Red'),\n            (3, 'Green'),\n            (3,   'Red')],\n           )\n\n        >>> idx1.union(idx2, sort=False)\n        MultiIndex([(1,   'Red'),\n            (1,  'Blue'),\n            (2,   'Red'),\n            (2,  'Blue'),\n            (3,   'Red'),\n            (3, 'Green'),\n            (2, 'Green')],\n           )\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_names = self._convert_can_do_setop(other)\n\n        if len(other) == 0 or self.equals(other):\n            return self.rename(result_names)\n\n        # TODO: Index.union returns other when `len(self)` is 0.\n\n        if not is_object_dtype(other.dtype):\n            raise NotImplementedError(\n                \"Can only union MultiIndex with MultiIndex or Index of tuples, \"\n                \"try mi.to_flat_index().union(other) instead.\"\n            )\n\n        uniq_tuples = lib.fast_unique_multiple([self._values, other._values], sort=sort)\n\n        return MultiIndex.from_arrays(\n            zip(*uniq_tuples), sortorder=0, names=result_names\n        )\n\n    def intersection(self, other, sort=False):\n        \"\"\"\n        Form the intersection of two MultiIndex objects.\n\n        Parameters\n        ----------\n        other : MultiIndex or array / Index of tuples\n        sort : False or None, default False\n            Sort the resulting MultiIndex if possible\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default from ``True`` to ``False``, to match\n               behaviour from before 0.24.0\n\n        Returns\n        -------\n        Index\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_names = self._convert_can_do_setop(other)\n\n        if self.equals(other):\n            if self.has_duplicates:\n                return self.unique().rename(result_names)\n            return self._get_reconciled_name_object(other)\n\n        return self._intersection(other, sort=sort)\n\n    def _intersection(self, other, sort=False):\n        other, result_names = self._convert_can_do_setop(other)\n\n        if not is_object_dtype(other.dtype):\n            # The intersection is empty\n            # TODO: we have no tests that get here\n            return MultiIndex(\n                levels=self.levels,\n                codes=[[]] * self.nlevels,\n                names=result_names,\n                verify_integrity=False,\n            )\n\n        lvals = self._values\n        rvals = other._values\n\n        uniq_tuples = None  # flag whether _inner_indexer was successful\n        if self.is_monotonic and other.is_monotonic:\n            try:\n                inner_tuples = self._inner_indexer(lvals, rvals)[0]\n                sort = False  # inner_tuples is already sorted\n            except TypeError:\n                pass\n            else:\n                uniq_tuples = algos.unique(inner_tuples)\n\n        if uniq_tuples is None:\n            other_uniq = set(rvals)\n            seen = set()\n            # pandas\\core\\indexes\\multi.py:3503: error: \"add\" of \"set\" does not\n            # return a value  [func-returns-value]\n            uniq_tuples = [\n                x\n                for x in lvals\n                if x in other_uniq\n                and not (x in seen or seen.add(x))  # type: ignore[func-returns-value]\n            ]\n\n        if sort is None:\n            uniq_tuples = sorted(uniq_tuples)\n\n        if len(uniq_tuples) == 0:\n            return MultiIndex(\n                levels=self.levels,\n                codes=[[]] * self.nlevels,\n                names=result_names,\n                verify_integrity=False,\n            )\n        else:\n            return MultiIndex.from_arrays(\n                zip(*uniq_tuples), sortorder=0, names=result_names\n            )\n\n    def difference(self, other, sort=None):\n        \"\"\"\n        Compute set difference of two MultiIndex objects\n\n        Parameters\n        ----------\n        other : MultiIndex\n        sort : False or None, default None\n            Sort the resulting MultiIndex if possible\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default value from ``True`` to ``None``\n               (without change in behaviour).\n\n        Returns\n        -------\n        diff : MultiIndex\n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_names = self._convert_can_do_setop(other)\n\n        if len(other) == 0:\n            return self.rename(result_names)\n\n        if self.equals(other):\n            return MultiIndex(\n                levels=self.levels,\n                codes=[[]] * self.nlevels,\n                names=result_names,\n                verify_integrity=False,\n            )\n\n        this = self._get_unique_index()\n\n        indexer = this.get_indexer(other)\n        indexer = indexer.take((indexer != -1).nonzero()[0])\n\n        label_diff = np.setdiff1d(np.arange(this.size), indexer, assume_unique=True)\n        difference = this._values.take(label_diff)\n        if sort is None:\n            difference = sorted(difference)\n\n        if len(difference) == 0:\n            return MultiIndex(\n                levels=[[]] * self.nlevels,\n                codes=[[]] * self.nlevels,\n                names=result_names,\n                verify_integrity=False,\n            )\n        else:\n            return MultiIndex.from_tuples(difference, sortorder=0, names=result_names)\n\n    def _convert_can_do_setop(self, other):\n        result_names = self.names\n\n        if not isinstance(other, Index):\n\n            if len(other) == 0:\n                return self[:0], self.names\n            else:\n                msg = \"other must be a MultiIndex or a list of tuples\"\n                try:\n                    other = MultiIndex.from_tuples(other, names=self.names)\n                except (ValueError, TypeError) as err:\n                    # ValueError raised by tuples_to_object_array if we\n                    #  have non-object dtype\n                    raise TypeError(msg) from err\n        else:\n            result_names = get_unanimous_names(self, other)\n\n        return other, result_names\n\n    def symmetric_difference(self, other, result_name=None, sort=None):\n        # On equal symmetric_difference MultiIndexes the difference is empty.\n        # Therefore, an empty MultiIndex is returned GH13490\n        tups = Index.symmetric_difference(self, other, result_name, sort)\n        if len(tups) == 0:\n            return type(self)(\n                levels=[[] for _ in range(self.nlevels)],\n                codes=[[] for _ in range(self.nlevels)],\n                names=tups.name,\n            )\n        return type(self).from_tuples(tups, names=tups.name)\n\n    # --------------------------------------------------------------------\n\n    @doc(Index.astype)\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if is_categorical_dtype(dtype):\n            msg = \"> 1 ndim Categorical are not supported at this time\"\n            raise NotImplementedError(msg)\n        elif not is_object_dtype(dtype):\n            raise TypeError(\n                \"Setting a MultiIndex dtype to anything other than object \"\n                \"is not supported\"\n            )\n        elif copy is True:\n            return self._shallow_copy()\n        return self\n\n    def _validate_fill_value(self, item):\n        if not isinstance(item, tuple):\n            # Pad the key with empty strings if lower levels of the key\n            # aren't specified:\n            item = (item,) + (\"\",) * (self.nlevels - 1)\n        elif len(item) != self.nlevels:\n            raise ValueError(\"Item must have length equal to number of levels.\")\n        return item\n\n    def insert(self, loc: int, item):\n        \"\"\"\n        Make new MultiIndex inserting new item at location\n\n        Parameters\n        ----------\n        loc : int\n        item : tuple\n            Must be same length as number of levels in the MultiIndex\n\n        Returns\n        -------\n        new_index : Index\n        \"\"\"\n        item = self._validate_fill_value(item)\n\n        new_levels = []\n        new_codes = []\n        for k, level, level_codes in zip(item, self.levels, self.codes):\n            if k not in level:\n                # have to insert into level\n                # must insert at end otherwise you have to recompute all the\n                # other codes\n                lev_loc = len(level)\n                try:\n                    level = level.insert(lev_loc, k)\n                except TypeError:\n                    # TODO: Should this be done inside insert?\n                    # TODO: smarter casting rules?\n                    level = level.astype(object).insert(lev_loc, k)\n            else:\n                lev_loc = level.get_loc(k)\n\n            new_levels.append(level)\n            new_codes.append(np.insert(ensure_int64(level_codes), loc, lev_loc))\n\n        return MultiIndex(\n            levels=new_levels, codes=new_codes, names=self.names, verify_integrity=False\n        )\n\n    def delete(self, loc):\n        \"\"\"\n        Make new index with passed location deleted\n\n        Returns\n        -------\n        new_index : MultiIndex\n        \"\"\"\n        new_codes = [np.delete(level_codes, loc) for level_codes in self.codes]\n        return MultiIndex(\n            levels=self.levels,\n            codes=new_codes,\n            names=self.names,\n            verify_integrity=False,\n        )\n\n    @doc(Index.isin)\n    def isin(self, values, level=None):\n        if level is None:\n            values = MultiIndex.from_tuples(values, names=self.names)._values\n            return algos.isin(self._values, values)\n        else:\n            num = self._get_level_number(level)\n            levs = self.get_level_values(num)\n\n            if levs.size == 0:\n                return np.zeros(len(levs), dtype=np.bool_)\n            return levs.isin(values)\n\n    # ---------------------------------------------------------------\n    # Arithmetic/Numeric Methods - Disabled\n\n    __add__ = make_invalid_op(\"__add__\")\n    __radd__ = make_invalid_op(\"__radd__\")\n    __iadd__ = make_invalid_op(\"__iadd__\")\n    __sub__ = make_invalid_op(\"__sub__\")\n    __rsub__ = make_invalid_op(\"__rsub__\")\n    __isub__ = make_invalid_op(\"__isub__\")\n    __pow__ = make_invalid_op(\"__pow__\")\n    __rpow__ = make_invalid_op(\"__rpow__\")\n    __mul__ = make_invalid_op(\"__mul__\")\n    __rmul__ = make_invalid_op(\"__rmul__\")\n    __floordiv__ = make_invalid_op(\"__floordiv__\")\n    __rfloordiv__ = make_invalid_op(\"__rfloordiv__\")\n    __truediv__ = make_invalid_op(\"__truediv__\")\n    __rtruediv__ = make_invalid_op(\"__rtruediv__\")\n    __mod__ = make_invalid_op(\"__mod__\")\n    __rmod__ = make_invalid_op(\"__rmod__\")\n    __divmod__ = make_invalid_op(\"__divmod__\")\n    __rdivmod__ = make_invalid_op(\"__rdivmod__\")\n    # Unary methods disabled\n    __neg__ = make_invalid_op(\"__neg__\")\n    __pos__ = make_invalid_op(\"__pos__\")\n    __abs__ = make_invalid_op(\"__abs__\")\n    __inv__ = make_invalid_op(\"__inv__\")\n\n\ndef sparsify_labels(label_list, start: int = 0, sentinel=\"\"):\n    pivoted = list(zip(*label_list))\n    k = len(label_list)\n\n    result = pivoted[: start + 1]\n    prev = pivoted[start]\n\n    for cur in pivoted[start + 1 :]:\n        sparse_cur = []\n\n        for i, (p, t) in enumerate(zip(prev, cur)):\n            if i == k - 1:\n                sparse_cur.append(t)\n                result.append(sparse_cur)\n                break\n\n            if p == t:\n                sparse_cur.append(sentinel)\n            else:\n                sparse_cur.extend(cur[i:])\n                result.append(sparse_cur)\n                break\n\n        prev = cur\n\n    return list(zip(*result))\n\n\ndef _get_na_rep(dtype) -> str:\n    return {np.datetime64: \"NaT\", np.timedelta64: \"NaT\"}.get(dtype, \"NaN\")\n\n\ndef maybe_droplevels(index, key):\n    \"\"\"\n    Attempt to drop level or levels from the given index.\n\n    Parameters\n    ----------\n    index: Index\n    key : scalar or tuple\n\n    Returns\n    -------\n    Index\n    \"\"\"\n    # drop levels\n    original_index = index\n    if isinstance(key, tuple):\n        for _ in key:\n            try:\n                index = index._drop_level_numbers([0])\n            except ValueError:\n                # we have dropped too much, so back out\n                return original_index\n    else:\n        try:\n            index = index._drop_level_numbers([0])\n        except ValueError:\n            pass\n\n    return index\n\n\ndef _coerce_indexer_frozen(array_like, categories, copy: bool = False) -> np.ndarray:\n    \"\"\"\n    Coerce the array_like indexer to the smallest integer dtype that can encode all\n    of the given categories.\n\n    Parameters\n    ----------\n    array_like : array-like\n    categories : array-like\n    copy : bool\n\n    Returns\n    -------\n    np.ndarray\n        Non-writeable.\n    \"\"\"\n    array_like = coerce_indexer_dtype(array_like, categories)\n    if copy:\n        array_like = array_like.copy()\n    array_like.flags.writeable = False\n    return array_like\n"
    },
    {
      "filename": "pandas/core/indexes/numeric.py",
      "content": "from typing import Any\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import index as libindex, lib\nfrom pandas._typing import Dtype, Label\nfrom pandas.util._decorators import doc\n\nfrom pandas.core.dtypes.cast import astype_nansafe\nfrom pandas.core.dtypes.common import (\n    is_bool,\n    is_bool_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float,\n    is_float_dtype,\n    is_integer_dtype,\n    is_numeric_dtype,\n    is_scalar,\n    is_signed_integer_dtype,\n    is_unsigned_integer_dtype,\n    needs_i8_conversion,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.generic import ABCSeries\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n\nimport pandas.core.common as com\nfrom pandas.core.indexes.base import Index, maybe_extract_name\n\n_num_index_shared_docs = {}\n\n\nclass NumericIndex(Index):\n    \"\"\"\n    Provide numeric type operations.\n\n    This is an abstract class.\n    \"\"\"\n\n    _default_dtype: np.dtype\n\n    _is_numeric_dtype = True\n    _can_hold_strings = False\n\n    def __new__(cls, data=None, dtype=None, copy=False, name=None):\n        cls._validate_dtype(dtype)\n        name = maybe_extract_name(name, data, cls)\n\n        # Coerce to ndarray if not already ndarray or Index\n        if not isinstance(data, (np.ndarray, Index)):\n            if is_scalar(data):\n                raise cls._scalar_data_error(data)\n\n            # other iterable of some kind\n            if not isinstance(data, (ABCSeries, list, tuple)):\n                data = list(data)\n\n            data = np.asarray(data, dtype=dtype)\n\n        if issubclass(data.dtype.type, str):\n            cls._string_data_error(data)\n\n        if copy or not is_dtype_equal(data.dtype, cls._default_dtype):\n            subarr = np.array(data, dtype=cls._default_dtype, copy=copy)\n            cls._assert_safe_casting(data, subarr)\n        else:\n            subarr = data\n\n        if subarr.ndim > 1:\n            # GH#13601, GH#20285, GH#27125\n            raise ValueError(\"Index data must be 1-dimensional\")\n\n        subarr = np.asarray(subarr)\n        return cls._simple_new(subarr, name=name)\n\n    @classmethod\n    def _validate_dtype(cls, dtype: Dtype) -> None:\n        if dtype is None:\n            return\n        validation_metadata = {\n            \"int64index\": (is_signed_integer_dtype, \"signed integer\"),\n            \"uint64index\": (is_unsigned_integer_dtype, \"unsigned integer\"),\n            \"float64index\": (is_float_dtype, \"float\"),\n            \"rangeindex\": (is_signed_integer_dtype, \"signed integer\"),\n        }\n\n        validation_func, expected = validation_metadata[cls._typ]\n        if not validation_func(dtype):\n            raise ValueError(\n                f\"Incorrect `dtype` passed: expected {expected}, received {dtype}\"\n            )\n\n    # ----------------------------------------------------------------\n    # Indexing Methods\n\n    @doc(Index._maybe_cast_slice_bound)\n    def _maybe_cast_slice_bound(self, label, side: str, kind):\n        assert kind in [\"loc\", \"getitem\", None]\n\n        # we will try to coerce to integers\n        return self._maybe_cast_indexer(label)\n\n    # ----------------------------------------------------------------\n\n    @doc(Index._shallow_copy)\n    def _shallow_copy(self, values=None, name: Label = lib.no_default):\n        if values is not None and not self._can_hold_na and values.dtype.kind == \"f\":\n            name = self.name if name is lib.no_default else name\n            # Ensure we are not returning an Int64Index with float data:\n            return Float64Index._simple_new(values, name=name)\n        return super()._shallow_copy(values=values, name=name)\n\n    def _validate_fill_value(self, value):\n        \"\"\"\n        Convert value to be insertable to ndarray.\n        \"\"\"\n        if is_bool(value) or is_bool_dtype(value):\n            # force conversion to object\n            # so we don't lose the bools\n            raise TypeError\n        elif isinstance(value, str) or lib.is_complex(value):\n            raise TypeError\n        elif is_scalar(value) and isna(value):\n            if is_valid_nat_for_dtype(value, self.dtype):\n                value = self._na_value\n            else:\n                # NaT, np.datetime64(\"NaT\"), np.timedelta64(\"NaT\")\n                raise TypeError\n\n        return value\n\n    def _convert_tolerance(self, tolerance, target):\n        tolerance = np.asarray(tolerance)\n        if target.size != tolerance.size and tolerance.size > 1:\n            raise ValueError(\"list-like tolerance size must match target index size\")\n        if not np.issubdtype(tolerance.dtype, np.number):\n            if tolerance.ndim > 0:\n                raise ValueError(\n                    f\"tolerance argument for {type(self).__name__} must contain \"\n                    \"numeric elements if it is list type\"\n                )\n            else:\n                raise ValueError(\n                    f\"tolerance argument for {type(self).__name__} must be numeric \"\n                    f\"if it is a scalar: {repr(tolerance)}\"\n                )\n        return tolerance\n\n    @classmethod\n    def _assert_safe_casting(cls, data, subarr):\n        \"\"\"\n        Subclasses need to override this only if the process of casting data\n        from some accepted dtype to the internal dtype(s) bears the risk of\n        truncation (e.g. float to int).\n        \"\"\"\n        pass\n\n    @property\n    def _is_all_dates(self) -> bool:\n        \"\"\"\n        Checks that all the labels are datetime objects.\n        \"\"\"\n        return False\n\n    @doc(Index.insert)\n    def insert(self, loc: int, item):\n        try:\n            item = self._validate_fill_value(item)\n        except TypeError:\n            return self.astype(object).insert(loc, item)\n\n        return super().insert(loc, item)\n\n    def _union(self, other, sort):\n        # Right now, we treat union(int, float) a bit special.\n        # See https://github.com/pandas-dev/pandas/issues/26778 for discussion\n        # We may change union(int, float) to go to object.\n        # float | [u]int -> float  (the special case)\n        # <T>   | <T>    -> T\n        # <T>   | <U>    -> object\n        needs_cast = (is_integer_dtype(self.dtype) and is_float_dtype(other.dtype)) or (\n            is_integer_dtype(other.dtype) and is_float_dtype(self.dtype)\n        )\n        if needs_cast:\n            first = self.astype(\"float\")\n            second = other.astype(\"float\")\n            return first._union(second, sort)\n        else:\n            return super()._union(other, sort)\n\n\n_num_index_shared_docs[\n    \"class_descr\"\n] = \"\"\"\n    Immutable sequence used for indexing and alignment. The basic object\n    storing axis labels for all pandas objects. %(klass)s is a special case\n    of `Index` with purely %(ltype)s labels. %(extra)s.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: %(dtype)s)\n    copy : bool\n        Make a copy of input ndarray.\n    name : object\n        Name to be stored in the index.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects.\n\"\"\"\n\n_int64_descr_args = {\n    \"klass\": \"Int64Index\",\n    \"ltype\": \"integer\",\n    \"dtype\": \"int64\",\n    \"extra\": \"\",\n}\n\n\nclass IntegerIndex(NumericIndex):\n    \"\"\"\n    This is an abstract class for Int64Index, UInt64Index.\n    \"\"\"\n\n    _default_dtype: np.dtype\n    _can_hold_na = False\n\n    @classmethod\n    def _assert_safe_casting(cls, data, subarr):\n        \"\"\"\n        Ensure incoming data can be represented with matching signed-ness.\n        \"\"\"\n        if data.dtype.kind != cls._default_dtype.kind:\n            if not np.array_equal(data, subarr):\n                raise TypeError(\"Unsafe NumPy casting, you must explicitly cast\")\n\n    def _can_union_without_object_cast(self, other) -> bool:\n        # See GH#26778, further casting may occur in NumericIndex._union\n        return other.dtype == \"f8\" or other.dtype == self.dtype\n\n    def __contains__(self, key) -> bool:\n        \"\"\"\n        Check if key is a float and has a decimal. If it has, return False.\n        \"\"\"\n        hash(key)\n        try:\n            if is_float(key) and int(key) != key:\n                return False\n            return key in self._engine\n        except (OverflowError, TypeError, ValueError):\n            return False\n\n    @property\n    def inferred_type(self) -> str:\n        \"\"\"\n        Always 'integer' for ``Int64Index`` and ``UInt64Index``\n        \"\"\"\n        return \"integer\"\n\n    @property\n    def asi8(self) -> np.ndarray:\n        # do not cache or you'll create a memory leak\n        warnings.warn(\n            \"Index.asi8 is deprecated and will be removed in a future version\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return self._values.view(self._default_dtype)\n\n\nclass Int64Index(IntegerIndex):\n    __doc__ = _num_index_shared_docs[\"class_descr\"] % _int64_descr_args\n\n    _typ = \"int64index\"\n    _engine_type = libindex.Int64Engine\n    _default_dtype = np.dtype(np.int64)\n\n\n_uint64_descr_args = {\n    \"klass\": \"UInt64Index\",\n    \"ltype\": \"unsigned integer\",\n    \"dtype\": \"uint64\",\n    \"extra\": \"\",\n}\n\n\nclass UInt64Index(IntegerIndex):\n    __doc__ = _num_index_shared_docs[\"class_descr\"] % _uint64_descr_args\n\n    _typ = \"uint64index\"\n    _engine_type = libindex.UInt64Engine\n    _default_dtype = np.dtype(np.uint64)\n\n    # ----------------------------------------------------------------\n    # Indexing Methods\n\n    @doc(Index._convert_arr_indexer)\n    def _convert_arr_indexer(self, keyarr):\n        # Cast the indexer to uint64 if possible so that the values returned\n        # from indexing are also uint64.\n        dtype = None\n        if is_integer_dtype(keyarr) or (\n            lib.infer_dtype(keyarr, skipna=False) == \"integer\"\n        ):\n            dtype = np.uint64\n\n        return com.asarray_tuplesafe(keyarr, dtype=dtype)\n\n\n_float64_descr_args = {\n    \"klass\": \"Float64Index\",\n    \"dtype\": \"float64\",\n    \"ltype\": \"float\",\n    \"extra\": \"\",\n}\n\n\nclass Float64Index(NumericIndex):\n    __doc__ = _num_index_shared_docs[\"class_descr\"] % _float64_descr_args\n\n    _typ = \"float64index\"\n    _engine_type = libindex.Float64Engine\n    _default_dtype = np.dtype(np.float64)\n\n    @property\n    def inferred_type(self) -> str:\n        \"\"\"\n        Always 'floating' for ``Float64Index``\n        \"\"\"\n        return \"floating\"\n\n    @doc(Index.astype)\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if needs_i8_conversion(dtype):\n            raise TypeError(\n                f\"Cannot convert Float64Index to dtype {dtype}; integer \"\n                \"values are required for conversion\"\n            )\n        elif is_integer_dtype(dtype) and not is_extension_array_dtype(dtype):\n            # TODO(jreback); this can change once we have an EA Index type\n            # GH 13149\n            arr = astype_nansafe(self._values, dtype=dtype)\n            return Int64Index(arr, name=self.name)\n        return super().astype(dtype, copy=copy)\n\n    # ----------------------------------------------------------------\n    # Indexing Methods\n\n    @doc(Index._should_fallback_to_positional)\n    def _should_fallback_to_positional(self) -> bool:\n        return False\n\n    @doc(Index._convert_slice_indexer)\n    def _convert_slice_indexer(self, key: slice, kind: str):\n        assert kind in [\"loc\", \"getitem\"]\n\n        # We always treat __getitem__ slicing as label-based\n        # translate to locations\n        return self.slice_indexer(key.start, key.stop, key.step, kind=kind)\n\n    @doc(Index.get_loc)\n    def get_loc(self, key, method=None, tolerance=None):\n        if is_bool(key):\n            # Catch this to avoid accidentally casting to 1.0\n            raise KeyError(key)\n\n        if is_float(key) and np.isnan(key):\n            nan_idxs = self._nan_idxs\n            if not len(nan_idxs):\n                raise KeyError(key)\n            elif len(nan_idxs) == 1:\n                return nan_idxs[0]\n            return nan_idxs\n\n        return super().get_loc(key, method=method, tolerance=tolerance)\n\n    # ----------------------------------------------------------------\n\n    def _format_native_types(\n        self, na_rep=\"\", float_format=None, decimal=\".\", quoting=None, **kwargs\n    ):\n        from pandas.io.formats.format import FloatArrayFormatter\n\n        formatter = FloatArrayFormatter(\n            self._values,\n            na_rep=na_rep,\n            float_format=float_format,\n            decimal=decimal,\n            quoting=quoting,\n            fixed_width=False,\n        )\n        return formatter.get_result_as_array()\n\n    def __contains__(self, other: Any) -> bool:\n        hash(other)\n        if super().__contains__(other):\n            return True\n\n        return is_float(other) and np.isnan(other) and self.hasnans\n\n    def _can_union_without_object_cast(self, other) -> bool:\n        # See GH#26778, further casting may occur in NumericIndex._union\n        return is_numeric_dtype(other.dtype)\n"
    },
    {
      "filename": "pandas/core/ops/methods.py",
      "content": "\"\"\"\nFunctions to generate methods and pin them to the appropriate classes.\n\"\"\"\nimport operator\n\nfrom pandas.core.dtypes.generic import ABCDataFrame, ABCSeries\n\nfrom pandas.core.ops.roperator import (\n    radd,\n    rdivmod,\n    rfloordiv,\n    rmod,\n    rmul,\n    rpow,\n    rsub,\n    rtruediv,\n)\n\n\ndef _get_method_wrappers(cls):\n    \"\"\"\n    Find the appropriate operation-wrappers to use when defining flex/special\n    arithmetic, boolean, and comparison operations with the given class.\n\n    Parameters\n    ----------\n    cls : class\n\n    Returns\n    -------\n    arith_flex : function or None\n    comp_flex : function or None\n    \"\"\"\n    # TODO: make these non-runtime imports once the relevant functions\n    #  are no longer in __init__\n    from pandas.core.ops import (\n        flex_arith_method_FRAME,\n        flex_comp_method_FRAME,\n        flex_method_SERIES,\n    )\n\n    if issubclass(cls, ABCSeries):\n        # Just Series\n        arith_flex = flex_method_SERIES\n        comp_flex = flex_method_SERIES\n    elif issubclass(cls, ABCDataFrame):\n        arith_flex = flex_arith_method_FRAME\n        comp_flex = flex_comp_method_FRAME\n    return arith_flex, comp_flex\n\n\ndef add_flex_arithmetic_methods(cls):\n    \"\"\"\n    Adds the full suite of flex arithmetic methods (``pow``, ``mul``, ``add``)\n    to the class.\n\n    Parameters\n    ----------\n    cls : class\n        flex methods will be defined and pinned to this class\n    \"\"\"\n    flex_arith_method, flex_comp_method = _get_method_wrappers(cls)\n    new_methods = _create_methods(cls, flex_arith_method, flex_comp_method)\n    new_methods.update(\n        {\n            \"multiply\": new_methods[\"mul\"],\n            \"subtract\": new_methods[\"sub\"],\n            \"divide\": new_methods[\"div\"],\n        }\n    )\n    # opt out of bool flex methods for now\n    assert not any(kname in new_methods for kname in (\"ror_\", \"rxor\", \"rand_\"))\n\n    _add_methods(cls, new_methods=new_methods)\n\n\ndef _create_methods(cls, arith_method, comp_method):\n    # creates actual flex methods based upon arithmetic, and comp method\n    # constructors.\n\n    have_divmod = issubclass(cls, ABCSeries)\n    # divmod is available for Series\n\n    new_methods = {}\n\n    new_methods.update(\n        {\n            \"add\": arith_method(operator.add),\n            \"radd\": arith_method(radd),\n            \"sub\": arith_method(operator.sub),\n            \"mul\": arith_method(operator.mul),\n            \"truediv\": arith_method(operator.truediv),\n            \"floordiv\": arith_method(operator.floordiv),\n            \"mod\": arith_method(operator.mod),\n            \"pow\": arith_method(operator.pow),\n            \"rmul\": arith_method(rmul),\n            \"rsub\": arith_method(rsub),\n            \"rtruediv\": arith_method(rtruediv),\n            \"rfloordiv\": arith_method(rfloordiv),\n            \"rpow\": arith_method(rpow),\n            \"rmod\": arith_method(rmod),\n        }\n    )\n    new_methods[\"div\"] = new_methods[\"truediv\"]\n    new_methods[\"rdiv\"] = new_methods[\"rtruediv\"]\n    if have_divmod:\n        # divmod doesn't have an op that is supported by numexpr\n        new_methods[\"divmod\"] = arith_method(divmod)\n        new_methods[\"rdivmod\"] = arith_method(rdivmod)\n\n    new_methods.update(\n        {\n            \"eq\": comp_method(operator.eq),\n            \"ne\": comp_method(operator.ne),\n            \"lt\": comp_method(operator.lt),\n            \"gt\": comp_method(operator.gt),\n            \"le\": comp_method(operator.le),\n            \"ge\": comp_method(operator.ge),\n        }\n    )\n\n    new_methods = {k.strip(\"_\"): v for k, v in new_methods.items()}\n    return new_methods\n\n\ndef _add_methods(cls, new_methods):\n    for name, method in new_methods.items():\n        setattr(cls, name, method)\n"
    },
    {
      "filename": "pandas/core/strings/accessor.py",
      "content": "import codecs\nfrom functools import wraps\nimport re\nfrom typing import Dict, List, Optional\nimport warnings\n\nimport numpy as np\n\nimport pandas._libs.lib as lib\nfrom pandas.util._decorators import Appender\n\nfrom pandas.core.dtypes.common import (\n    ensure_object,\n    is_bool_dtype,\n    is_categorical_dtype,\n    is_integer,\n    is_list_like,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCIndexClass,\n    ABCMultiIndex,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core.base import NoNewAttributesMixin\n\n_shared_docs: Dict[str, str] = {}\n_cpython_optimized_encoders = (\n    \"utf-8\",\n    \"utf8\",\n    \"latin-1\",\n    \"latin1\",\n    \"iso-8859-1\",\n    \"mbcs\",\n    \"ascii\",\n)\n_cpython_optimized_decoders = _cpython_optimized_encoders + (\"utf-16\", \"utf-32\")\n\n\ndef forbid_nonstring_types(forbidden, name=None):\n    \"\"\"\n    Decorator to forbid specific types for a method of StringMethods.\n\n    For calling `.str.{method}` on a Series or Index, it is necessary to first\n    initialize the :class:`StringMethods` object, and then call the method.\n    However, different methods allow different input types, and so this can not\n    be checked during :meth:`StringMethods.__init__`, but must be done on a\n    per-method basis. This decorator exists to facilitate this process, and\n    make it explicit which (inferred) types are disallowed by the method.\n\n    :meth:`StringMethods.__init__` allows the *union* of types its different\n    methods allow (after skipping NaNs; see :meth:`StringMethods._validate`),\n    namely: ['string', 'empty', 'bytes', 'mixed', 'mixed-integer'].\n\n    The default string types ['string', 'empty'] are allowed for all methods.\n    For the additional types ['bytes', 'mixed', 'mixed-integer'], each method\n    then needs to forbid the types it is not intended for.\n\n    Parameters\n    ----------\n    forbidden : list-of-str or None\n        List of forbidden non-string types, may be one or more of\n        `['bytes', 'mixed', 'mixed-integer']`.\n    name : str, default None\n        Name of the method to use in the error message. By default, this is\n        None, in which case the name from the method being wrapped will be\n        copied. However, for working with further wrappers (like _pat_wrapper\n        and _noarg_wrapper), it is necessary to specify the name.\n\n    Returns\n    -------\n    func : wrapper\n        The method to which the decorator is applied, with an added check that\n        enforces the inferred type to not be in the list of forbidden types.\n\n    Raises\n    ------\n    TypeError\n        If the inferred type of the underlying data is in `forbidden`.\n    \"\"\"\n    # deal with None\n    forbidden = [] if forbidden is None else forbidden\n\n    allowed_types = {\"string\", \"empty\", \"bytes\", \"mixed\", \"mixed-integer\"} - set(\n        forbidden\n    )\n\n    def _forbid_nonstring_types(func):\n        func_name = func.__name__ if name is None else name\n\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            if self._inferred_dtype not in allowed_types:\n                msg = (\n                    f\"Cannot use .str.{func_name} with values of \"\n                    f\"inferred dtype '{self._inferred_dtype}'.\"\n                )\n                raise TypeError(msg)\n            return func(self, *args, **kwargs)\n\n        wrapper.__name__ = func_name\n        return wrapper\n\n    return _forbid_nonstring_types\n\n\ndef _map_and_wrap(name, docstring):\n    @forbid_nonstring_types([\"bytes\"], name=name)\n    def wrapper(self):\n        result = getattr(self._array, f\"_str_{name}\")()\n        return self._wrap_result(result)\n\n    wrapper.__doc__ = docstring\n    return wrapper\n\n\nclass StringMethods(NoNewAttributesMixin):\n    \"\"\"\n    Vectorized string functions for Series and Index.\n\n    NAs stay NA unless handled otherwise by a particular method.\n    Patterned after Python's string methods, with some inspiration from\n    R's stringr package.\n\n    Examples\n    --------\n    >>> s = pd.Series([\"A_Str_Series\"])\n    >>> s\n    0    A_Str_Series\n    dtype: object\n\n    >>> s.str.split(\"_\")\n    0    [A, Str, Series]\n    dtype: object\n\n    >>> s.str.replace(\"_\", \"\")\n    0    AStrSeries\n    dtype: object\n    \"\"\"\n\n    # Note: see the docstring in pandas.core.strings.__init__\n    # for an explanation of the implementation.\n    # TODO: Dispatch all the methods\n    # Currently the following are not dispatched to the array\n    # * cat\n    # * extract\n    # * extractall\n\n    def __init__(self, data):\n        from pandas.core.arrays.string_ import StringDtype\n\n        self._inferred_dtype = self._validate(data)\n        self._is_categorical = is_categorical_dtype(data.dtype)\n        self._is_string = isinstance(data.dtype, StringDtype)\n        array = data.array\n        self._array = array\n\n        self._index = self._name = None\n        if isinstance(data, ABCSeries):\n            self._index = data.index\n            self._name = data.name\n\n        # ._values.categories works for both Series/Index\n        self._parent = data._values.categories if self._is_categorical else data\n        # save orig to blow up categoricals to the right type\n        self._orig = data\n        self._freeze()\n\n    @staticmethod\n    def _validate(data):\n        \"\"\"\n        Auxiliary function for StringMethods, infers and checks dtype of data.\n\n        This is a \"first line of defence\" at the creation of the StringMethods-\n        object, and just checks that the dtype is in the\n        *union* of the allowed types over all string methods below; this\n        restriction is then refined on a per-method basis using the decorator\n        @forbid_nonstring_types (more info in the corresponding docstring).\n\n        This really should exclude all series/index with any non-string values,\n        but that isn't practical for performance reasons until we have a str\n        dtype (GH 9343 / 13877)\n\n        Parameters\n        ----------\n        data : The content of the Series\n\n        Returns\n        -------\n        dtype : inferred dtype of data\n        \"\"\"\n        from pandas import StringDtype\n\n        if isinstance(data, ABCMultiIndex):\n            raise AttributeError(\n                \"Can only use .str accessor with Index, not MultiIndex\"\n            )\n\n        # see _libs/lib.pyx for list of inferred types\n        allowed_types = [\"string\", \"empty\", \"bytes\", \"mixed\", \"mixed-integer\"]\n\n        values = getattr(data, \"values\", data)  # Series / Index\n        values = getattr(values, \"categories\", values)  # categorical / normal\n\n        # explicitly allow StringDtype\n        if isinstance(values.dtype, StringDtype):\n            return \"string\"\n\n        try:\n            inferred_dtype = lib.infer_dtype(values, skipna=True)\n        except ValueError:\n            # GH#27571 mostly occurs with ExtensionArray\n            inferred_dtype = None\n\n        if inferred_dtype not in allowed_types:\n            raise AttributeError(\"Can only use .str accessor with string values!\")\n        return inferred_dtype\n\n    def __getitem__(self, key):\n        result = self._array._str_getitem(key)\n        return self._wrap_result(result)\n\n    def __iter__(self):\n        warnings.warn(\n            \"Columnar iteration over characters will be deprecated in future releases.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        i = 0\n        g = self.get(i)\n        while g.notna().any():\n            yield g\n            i += 1\n            g = self.get(i)\n\n    def _wrap_result(\n        self,\n        result,\n        name=None,\n        expand=None,\n        fill_value=np.nan,\n        returns_string=True,\n    ):\n        from pandas import Index, MultiIndex\n\n        if not hasattr(result, \"ndim\") or not hasattr(result, \"dtype\"):\n            if isinstance(result, ABCDataFrame):\n                result = result.__finalize__(self._orig, name=\"str\")\n            return result\n        assert result.ndim < 3\n\n        # We can be wrapping a string / object / categorical result, in which\n        # case we'll want to return the same dtype as the input.\n        # Or we can be wrapping a numeric output, in which case we don't want\n        # to return a StringArray.\n        # Ideally the array method returns the right array type.\n        if expand is None:\n            # infer from ndim if expand is not specified\n            expand = result.ndim != 1\n\n        elif expand is True and not isinstance(self._orig, ABCIndexClass):\n            # required when expand=True is explicitly specified\n            # not needed when inferred\n\n            def cons_row(x):\n                if is_list_like(x):\n                    return x\n                else:\n                    return [x]\n\n            result = [cons_row(x) for x in result]\n            if result:\n                # propagate nan values to match longest sequence (GH 18450)\n                max_len = max(len(x) for x in result)\n                result = [\n                    x * max_len if len(x) == 0 or x[0] is np.nan else x for x in result\n                ]\n\n        if not isinstance(expand, bool):\n            raise ValueError(\"expand must be True or False\")\n\n        if expand is False:\n            # if expand is False, result should have the same name\n            # as the original otherwise specified\n            if name is None:\n                name = getattr(result, \"name\", None)\n            if name is None:\n                # do not use logical or, _orig may be a DataFrame\n                # which has \"name\" column\n                name = self._orig.name\n\n        # Wait until we are sure result is a Series or Index before\n        # checking attributes (GH 12180)\n        if isinstance(self._orig, ABCIndexClass):\n            # if result is a boolean np.array, return the np.array\n            # instead of wrapping it into a boolean Index (GH 8875)\n            if is_bool_dtype(result):\n                return result\n\n            if expand:\n                result = list(result)\n                out = MultiIndex.from_tuples(result, names=name)\n                if out.nlevels == 1:\n                    # We had all tuples of length-one, which are\n                    # better represented as a regular Index.\n                    out = out.get_level_values(0)\n                return out\n            else:\n                return Index(result, name=name)\n        else:\n            index = self._orig.index\n            # This is a mess.\n            dtype: Optional[str]\n            if self._is_string and returns_string:\n                dtype = \"string\"\n            else:\n                dtype = None\n\n            if expand:\n                cons = self._orig._constructor_expanddim\n                result = cons(result, columns=name, index=index, dtype=dtype)\n            else:\n                # Must be a Series\n                cons = self._orig._constructor\n                result = cons(result, name=name, index=index)\n            result = result.__finalize__(self._orig, method=\"str\")\n            if name is not None and result.ndim == 1:\n                # __finalize__ might copy over the original name, but we may\n                # want the new name (e.g. str.extract).\n                result.name = name\n            return result\n\n    def _get_series_list(self, others):\n        \"\"\"\n        Auxiliary function for :meth:`str.cat`. Turn potentially mixed input\n        into a list of Series (elements without an index must match the length\n        of the calling Series/Index).\n\n        Parameters\n        ----------\n        others : Series, DataFrame, np.ndarray, list-like or list-like of\n            Objects that are either Series, Index or np.ndarray (1-dim).\n\n        Returns\n        -------\n        list of Series\n            Others transformed into list of Series.\n        \"\"\"\n        from pandas import DataFrame, Series\n\n        # self._orig is either Series or Index\n        idx = self._orig if isinstance(self._orig, ABCIndexClass) else self._orig.index\n\n        # Generally speaking, all objects without an index inherit the index\n        # `idx` of the calling Series/Index - i.e. must have matching length.\n        # Objects with an index (i.e. Series/Index/DataFrame) keep their own.\n        if isinstance(others, ABCSeries):\n            return [others]\n        elif isinstance(others, ABCIndexClass):\n            return [Series(others._values, index=idx)]\n        elif isinstance(others, ABCDataFrame):\n            return [others[x] for x in others]\n        elif isinstance(others, np.ndarray) and others.ndim == 2:\n            others = DataFrame(others, index=idx)\n            return [others[x] for x in others]\n        elif is_list_like(others, allow_sets=False):\n            others = list(others)  # ensure iterators do not get read twice etc\n\n            # in case of list-like `others`, all elements must be\n            # either Series/Index/np.ndarray (1-dim)...\n            if all(\n                isinstance(x, (ABCSeries, ABCIndexClass))\n                or (isinstance(x, np.ndarray) and x.ndim == 1)\n                for x in others\n            ):\n                los: List[Series] = []\n                while others:  # iterate through list and append each element\n                    los = los + self._get_series_list(others.pop(0))\n                return los\n            # ... or just strings\n            elif all(not is_list_like(x) for x in others):\n                return [Series(others, index=idx)]\n        raise TypeError(\n            \"others must be Series, Index, DataFrame, np.ndarray \"\n            \"or list-like (either containing only strings or \"\n            \"containing only objects of type Series/Index/\"\n            \"np.ndarray[1-dim])\"\n        )\n\n    @forbid_nonstring_types([\"bytes\", \"mixed\", \"mixed-integer\"])\n    def cat(self, others=None, sep=None, na_rep=None, join=\"left\"):\n        \"\"\"\n        Concatenate strings in the Series/Index with given separator.\n\n        If `others` is specified, this function concatenates the Series/Index\n        and elements of `others` element-wise.\n        If `others` is not passed, then all values in the Series/Index are\n        concatenated into a single string with a given `sep`.\n\n        Parameters\n        ----------\n        others : Series, Index, DataFrame, np.ndarray or list-like\n            Series, Index, DataFrame, np.ndarray (one- or two-dimensional) and\n            other list-likes of strings must have the same length as the\n            calling Series/Index, with the exception of indexed objects (i.e.\n            Series/Index/DataFrame) if `join` is not None.\n\n            If others is a list-like that contains a combination of Series,\n            Index or np.ndarray (1-dim), then all elements will be unpacked and\n            must satisfy the above criteria individually.\n\n            If others is None, the method returns the concatenation of all\n            strings in the calling Series/Index.\n        sep : str, default ''\n            The separator between the different elements/columns. By default\n            the empty string `''` is used.\n        na_rep : str or None, default None\n            Representation that is inserted for all missing values:\n\n            - If `na_rep` is None, and `others` is None, missing values in the\n              Series/Index are omitted from the result.\n            - If `na_rep` is None, and `others` is not None, a row containing a\n              missing value in any of the columns (before concatenation) will\n              have a missing value in the result.\n        join : {'left', 'right', 'outer', 'inner'}, default 'left'\n            Determines the join-style between the calling Series/Index and any\n            Series/Index/DataFrame in `others` (objects without an index need\n            to match the length of the calling Series/Index). To disable\n            alignment, use `.values` on any Series/Index/DataFrame in `others`.\n\n            .. versionadded:: 0.23.0\n            .. versionchanged:: 1.0.0\n                Changed default of `join` from None to `'left'`.\n\n        Returns\n        -------\n        str, Series or Index\n            If `others` is None, `str` is returned, otherwise a `Series/Index`\n            (same type as caller) of objects is returned.\n\n        See Also\n        --------\n        split : Split each string in the Series/Index.\n        join : Join lists contained as elements in the Series/Index.\n\n        Examples\n        --------\n        When not passing `others`, all values are concatenated into a single\n        string:\n\n        >>> s = pd.Series(['a', 'b', np.nan, 'd'])\n        >>> s.str.cat(sep=' ')\n        'a b d'\n\n        By default, NA values in the Series are ignored. Using `na_rep`, they\n        can be given a representation:\n\n        >>> s.str.cat(sep=' ', na_rep='?')\n        'a b ? d'\n\n        If `others` is specified, corresponding values are concatenated with\n        the separator. Result will be a Series of strings.\n\n        >>> s.str.cat(['A', 'B', 'C', 'D'], sep=',')\n        0    a,A\n        1    b,B\n        2    NaN\n        3    d,D\n        dtype: object\n\n        Missing values will remain missing in the result, but can again be\n        represented using `na_rep`\n\n        >>> s.str.cat(['A', 'B', 'C', 'D'], sep=',', na_rep='-')\n        0    a,A\n        1    b,B\n        2    -,C\n        3    d,D\n        dtype: object\n\n        If `sep` is not specified, the values are concatenated without\n        separation.\n\n        >>> s.str.cat(['A', 'B', 'C', 'D'], na_rep='-')\n        0    aA\n        1    bB\n        2    -C\n        3    dD\n        dtype: object\n\n        Series with different indexes can be aligned before concatenation. The\n        `join`-keyword works as in other methods.\n\n        >>> t = pd.Series(['d', 'a', 'e', 'c'], index=[3, 0, 4, 2])\n        >>> s.str.cat(t, join='left', na_rep='-')\n        0    aa\n        1    b-\n        2    -c\n        3    dd\n        dtype: object\n        >>>\n        >>> s.str.cat(t, join='outer', na_rep='-')\n        0    aa\n        1    b-\n        2    -c\n        3    dd\n        4    -e\n        dtype: object\n        >>>\n        >>> s.str.cat(t, join='inner', na_rep='-')\n        0    aa\n        2    -c\n        3    dd\n        dtype: object\n        >>>\n        >>> s.str.cat(t, join='right', na_rep='-')\n        3    dd\n        0    aa\n        4    -e\n        2    -c\n        dtype: object\n\n        For more examples, see :ref:`here <text.concatenate>`.\n        \"\"\"\n        # TODO: dispatch\n        from pandas import Index, Series, concat\n\n        if isinstance(others, str):\n            raise ValueError(\"Did you mean to supply a `sep` keyword?\")\n        if sep is None:\n            sep = \"\"\n\n        if isinstance(self._orig, ABCIndexClass):\n            data = Series(self._orig, index=self._orig)\n        else:  # Series\n            data = self._orig\n\n        # concatenate Series/Index with itself if no \"others\"\n        if others is None:\n            data = ensure_object(data)\n            na_mask = isna(data)\n            if na_rep is None and na_mask.any():\n                data = data[~na_mask]\n            elif na_rep is not None and na_mask.any():\n                data = np.where(na_mask, na_rep, data)\n            return sep.join(data)\n\n        try:\n            # turn anything in \"others\" into lists of Series\n            others = self._get_series_list(others)\n        except ValueError as err:  # do not catch TypeError raised by _get_series_list\n            raise ValueError(\n                \"If `others` contains arrays or lists (or other \"\n                \"list-likes without an index), these must all be \"\n                \"of the same length as the calling Series/Index.\"\n            ) from err\n\n        # align if required\n        if any(not data.index.equals(x.index) for x in others):\n            # Need to add keys for uniqueness in case of duplicate columns\n            others = concat(\n                others,\n                axis=1,\n                join=(join if join == \"inner\" else \"outer\"),\n                keys=range(len(others)),\n                sort=False,\n                copy=False,\n            )\n            data, others = data.align(others, join=join)\n            others = [others[x] for x in others]  # again list of Series\n\n        all_cols = [ensure_object(x) for x in [data] + others]\n        na_masks = np.array([isna(x) for x in all_cols])\n        union_mask = np.logical_or.reduce(na_masks, axis=0)\n\n        if na_rep is None and union_mask.any():\n            # no na_rep means NaNs for all rows where any column has a NaN\n            # only necessary if there are actually any NaNs\n            result = np.empty(len(data), dtype=object)\n            np.putmask(result, union_mask, np.nan)\n\n            not_masked = ~union_mask\n            result[not_masked] = cat_safe([x[not_masked] for x in all_cols], sep)\n        elif na_rep is not None and union_mask.any():\n            # fill NaNs with na_rep in case there are actually any NaNs\n            all_cols = [\n                np.where(nm, na_rep, col) for nm, col in zip(na_masks, all_cols)\n            ]\n            result = cat_safe(all_cols, sep)\n        else:\n            # no NaNs - can just concatenate\n            result = cat_safe(all_cols, sep)\n\n        if isinstance(self._orig, ABCIndexClass):\n            # add dtype for case that result is all-NA\n            result = Index(result, dtype=object, name=self._orig.name)\n        else:  # Series\n            if is_categorical_dtype(self._orig.dtype):\n                # We need to infer the new categories.\n                dtype = None\n            else:\n                dtype = self._orig.dtype\n            result = Series(result, dtype=dtype, index=data.index, name=self._orig.name)\n            result = result.__finalize__(self._orig, method=\"str_cat\")\n        return result\n\n    _shared_docs[\n        \"str_split\"\n    ] = r\"\"\"\n    Split strings around given separator/delimiter.\n\n    Splits the string in the Series/Index from the %(side)s,\n    at the specified delimiter string. Equivalent to :meth:`str.%(method)s`.\n\n    Parameters\n    ----------\n    pat : str, optional\n        String or regular expression to split on.\n        If not specified, split on whitespace.\n    n : int, default -1 (all)\n        Limit number of splits in output.\n        ``None``, 0 and -1 will be interpreted as return all splits.\n    expand : bool, default False\n        Expand the split strings into separate columns.\n\n        * If ``True``, return DataFrame/MultiIndex expanding dimensionality.\n        * If ``False``, return Series/Index, containing lists of strings.\n\n    Returns\n    -------\n    Series, Index, DataFrame or MultiIndex\n        Type matches caller unless ``expand=True`` (see Notes).\n\n    See Also\n    --------\n    Series.str.split : Split strings around given separator/delimiter.\n    Series.str.rsplit : Splits string around given separator/delimiter,\n        starting from the right.\n    Series.str.join : Join lists contained as elements in the Series/Index\n        with passed delimiter.\n    str.split : Standard library version for split.\n    str.rsplit : Standard library version for rsplit.\n\n    Notes\n    -----\n    The handling of the `n` keyword depends on the number of found splits:\n\n    - If found splits > `n`,  make first `n` splits only\n    - If found splits <= `n`, make all splits\n    - If for a certain row the number of found splits < `n`,\n      append `None` for padding up to `n` if ``expand=True``\n\n    If using ``expand=True``, Series and Index callers return DataFrame and\n    MultiIndex objects, respectively.\n\n    Examples\n    --------\n    >>> s = pd.Series(\n    ...     [\n    ...         \"this is a regular sentence\",\n    ...         \"https://docs.python.org/3/tutorial/index.html\",\n    ...         np.nan\n    ...     ]\n    ... )\n    >>> s\n    0                       this is a regular sentence\n    1    https://docs.python.org/3/tutorial/index.html\n    2                                              NaN\n    dtype: object\n\n    In the default setting, the string is split by whitespace.\n\n    >>> s.str.split()\n    0                   [this, is, a, regular, sentence]\n    1    [https://docs.python.org/3/tutorial/index.html]\n    2                                                NaN\n    dtype: object\n\n    Without the `n` parameter, the outputs of `rsplit` and `split`\n    are identical.\n\n    >>> s.str.rsplit()\n    0                   [this, is, a, regular, sentence]\n    1    [https://docs.python.org/3/tutorial/index.html]\n    2                                                NaN\n    dtype: object\n\n    The `n` parameter can be used to limit the number of splits on the\n    delimiter. The outputs of `split` and `rsplit` are different.\n\n    >>> s.str.split(n=2)\n    0                     [this, is, a regular sentence]\n    1    [https://docs.python.org/3/tutorial/index.html]\n    2                                                NaN\n    dtype: object\n\n    >>> s.str.rsplit(n=2)\n    0                     [this is a, regular, sentence]\n    1    [https://docs.python.org/3/tutorial/index.html]\n    2                                                NaN\n    dtype: object\n\n    The `pat` parameter can be used to split by other characters.\n\n    >>> s.str.split(pat=\"/\")\n    0                         [this is a regular sentence]\n    1    [https:, , docs.python.org, 3, tutorial, index...\n    2                                                  NaN\n    dtype: object\n\n    When using ``expand=True``, the split elements will expand out into\n    separate columns. If NaN is present, it is propagated throughout\n    the columns during the split.\n\n    >>> s.str.split(expand=True)\n                                                   0     1     2        3         4\n    0                                           this    is     a  regular  sentence\n    1  https://docs.python.org/3/tutorial/index.html  None  None     None      None\n    2                                            NaN   NaN   NaN      NaN       NaN\n\n    For slightly more complex use cases like splitting the html document name\n    from a url, a combination of parameter settings can be used.\n\n    >>> s.str.rsplit(\"/\", n=1, expand=True)\n                                        0           1\n    0          this is a regular sentence        None\n    1  https://docs.python.org/3/tutorial  index.html\n    2                                 NaN         NaN\n\n    Remember to escape special characters when explicitly using regular\n    expressions.\n\n    >>> s = pd.Series([\"1+1=2\"])\n    >>> s\n    0    1+1=2\n    dtype: object\n    >>> s.str.split(r\"\\+|=\", expand=True)\n         0    1    2\n    0    1    1    2\n    \"\"\"\n\n    @Appender(_shared_docs[\"str_split\"] % {\"side\": \"beginning\", \"method\": \"split\"})\n    @forbid_nonstring_types([\"bytes\"])\n    def split(self, pat=None, n=-1, expand=False):\n        result = self._array._str_split(pat, n, expand)\n        return self._wrap_result(result, returns_string=expand, expand=expand)\n\n    @Appender(_shared_docs[\"str_split\"] % {\"side\": \"end\", \"method\": \"rsplit\"})\n    @forbid_nonstring_types([\"bytes\"])\n    def rsplit(self, pat=None, n=-1, expand=False):\n        result = self._array._str_rsplit(pat, n=n)\n        return self._wrap_result(result, expand=expand, returns_string=expand)\n\n    _shared_docs[\n        \"str_partition\"\n    ] = \"\"\"\n    Split the string at the %(side)s occurrence of `sep`.\n\n    This method splits the string at the %(side)s occurrence of `sep`,\n    and returns 3 elements containing the part before the separator,\n    the separator itself, and the part after the separator.\n    If the separator is not found, return %(return)s.\n\n    Parameters\n    ----------\n    sep : str, default whitespace\n        String to split on.\n    expand : bool, default True\n        If True, return DataFrame/MultiIndex expanding dimensionality.\n        If False, return Series/Index.\n\n    Returns\n    -------\n    DataFrame/MultiIndex or Series/Index of objects\n\n    See Also\n    --------\n    %(also)s\n    Series.str.split : Split strings around given separators.\n    str.partition : Standard library version.\n\n    Examples\n    --------\n\n    >>> s = pd.Series(['Linda van der Berg', 'George Pitt-Rivers'])\n    >>> s\n    0    Linda van der Berg\n    1    George Pitt-Rivers\n    dtype: object\n\n    >>> s.str.partition()\n            0  1             2\n    0   Linda     van der Berg\n    1  George      Pitt-Rivers\n\n    To partition by the last space instead of the first one:\n\n    >>> s.str.rpartition()\n                   0  1            2\n    0  Linda van der            Berg\n    1         George     Pitt-Rivers\n\n    To partition by something different than a space:\n\n    >>> s.str.partition('-')\n                        0  1       2\n    0  Linda van der Berg\n    1         George Pitt  -  Rivers\n\n    To return a Series containing tuples instead of a DataFrame:\n\n    >>> s.str.partition('-', expand=False)\n    0    (Linda van der Berg, , )\n    1    (George Pitt, -, Rivers)\n    dtype: object\n\n    Also available on indices:\n\n    >>> idx = pd.Index(['X 123', 'Y 999'])\n    >>> idx\n    Index(['X 123', 'Y 999'], dtype='object')\n\n    Which will create a MultiIndex:\n\n    >>> idx.str.partition()\n    MultiIndex([('X', ' ', '123'),\n                ('Y', ' ', '999')],\n               )\n\n    Or an index with tuples with ``expand=False``:\n\n    >>> idx.str.partition(expand=False)\n    Index([('X', ' ', '123'), ('Y', ' ', '999')], dtype='object')\n    \"\"\"\n\n    @Appender(\n        _shared_docs[\"str_partition\"]\n        % {\n            \"side\": \"first\",\n            \"return\": \"3 elements containing the string itself, followed by two \"\n            \"empty strings\",\n            \"also\": \"rpartition : Split the string at the last occurrence of `sep`.\",\n        }\n    )\n    @forbid_nonstring_types([\"bytes\"])\n    def partition(self, sep=\" \", expand=True):\n        result = self._array._str_partition(sep, expand)\n        return self._wrap_result(result, expand=expand, returns_string=expand)\n\n    @Appender(\n        _shared_docs[\"str_partition\"]\n        % {\n            \"side\": \"last\",\n            \"return\": \"3 elements containing two empty strings, followed by the \"\n            \"string itself\",\n            \"also\": \"partition : Split the string at the first occurrence of `sep`.\",\n        }\n    )\n    @forbid_nonstring_types([\"bytes\"])\n    def rpartition(self, sep=\" \", expand=True):\n        result = self._array._str_rpartition(sep, expand)\n        return self._wrap_result(result, expand=expand, returns_string=expand)\n\n    def get(self, i):\n        \"\"\"\n        Extract element from each component at specified position.\n\n        Extract element from lists, tuples, or strings in each element in the\n        Series/Index.\n\n        Parameters\n        ----------\n        i : int\n            Position of element to extract.\n\n        Returns\n        -------\n        Series or Index\n\n        Examples\n        --------\n        >>> s = pd.Series([\"String\",\n        ...               (1, 2, 3),\n        ...               [\"a\", \"b\", \"c\"],\n        ...               123,\n        ...               -456,\n        ...               {1: \"Hello\", \"2\": \"World\"}])\n        >>> s\n        0                        String\n        1                     (1, 2, 3)\n        2                     [a, b, c]\n        3                           123\n        4                          -456\n        5    {1: 'Hello', '2': 'World'}\n        dtype: object\n\n        >>> s.str.get(1)\n        0        t\n        1        2\n        2        b\n        3      NaN\n        4      NaN\n        5    Hello\n        dtype: object\n\n        >>> s.str.get(-1)\n        0      g\n        1      3\n        2      c\n        3    NaN\n        4    NaN\n        5    None\n        dtype: object\n        \"\"\"\n        result = self._array._str_get(i)\n        return self._wrap_result(result)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def join(self, sep):\n        \"\"\"\n        Join lists contained as elements in the Series/Index with passed delimiter.\n\n        If the elements of a Series are lists themselves, join the content of these\n        lists using the delimiter passed to the function.\n        This function is an equivalent to :meth:`str.join`.\n\n        Parameters\n        ----------\n        sep : str\n            Delimiter to use between list entries.\n\n        Returns\n        -------\n        Series/Index: object\n            The list entries concatenated by intervening occurrences of the\n            delimiter.\n\n        Raises\n        ------\n        AttributeError\n            If the supplied Series contains neither strings nor lists.\n\n        See Also\n        --------\n        str.join : Standard library version of this method.\n        Series.str.split : Split strings around given separator/delimiter.\n\n        Notes\n        -----\n        If any of the list items is not a string object, the result of the join\n        will be `NaN`.\n\n        Examples\n        --------\n        Example with a list that contains non-string elements.\n\n        >>> s = pd.Series([['lion', 'elephant', 'zebra'],\n        ...                [1.1, 2.2, 3.3],\n        ...                ['cat', np.nan, 'dog'],\n        ...                ['cow', 4.5, 'goat'],\n        ...                ['duck', ['swan', 'fish'], 'guppy']])\n        >>> s\n        0        [lion, elephant, zebra]\n        1                [1.1, 2.2, 3.3]\n        2                [cat, nan, dog]\n        3               [cow, 4.5, goat]\n        4    [duck, [swan, fish], guppy]\n        dtype: object\n\n        Join all lists using a '-'. The lists containing object(s) of types other\n        than str will produce a NaN.\n\n        >>> s.str.join('-')\n        0    lion-elephant-zebra\n        1                    NaN\n        2                    NaN\n        3                    NaN\n        4                    NaN\n        dtype: object\n        \"\"\"\n        result = self._array._str_join(sep)\n        return self._wrap_result(result)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def contains(self, pat, case=True, flags=0, na=None, regex=True):\n        r\"\"\"\n        Test if pattern or regex is contained within a string of a Series or Index.\n\n        Return boolean Series or Index based on whether a given pattern or regex is\n        contained within a string of a Series or Index.\n\n        Parameters\n        ----------\n        pat : str\n            Character sequence or regular expression.\n        case : bool, default True\n            If True, case sensitive.\n        flags : int, default 0 (no flags)\n            Flags to pass through to the re module, e.g. re.IGNORECASE.\n        na : scalar, optional\n            Fill value for missing values. The default depends on dtype of the\n            array. For object-dtype, ``numpy.nan`` is used. For ``StringDtype``,\n            ``pandas.NA`` is used.\n        regex : bool, default True\n            If True, assumes the pat is a regular expression.\n\n            If False, treats the pat as a literal string.\n\n        Returns\n        -------\n        Series or Index of boolean values\n            A Series or Index of boolean values indicating whether the\n            given pattern is contained within the string of each element\n            of the Series or Index.\n\n        See Also\n        --------\n        match : Analogous, but stricter, relying on re.match instead of re.search.\n        Series.str.startswith : Test if the start of each string element matches a\n            pattern.\n        Series.str.endswith : Same as startswith, but tests the end of string.\n\n        Examples\n        --------\n        Returning a Series of booleans using only a literal pattern.\n\n        >>> s1 = pd.Series(['Mouse', 'dog', 'house and parrot', '23', np.NaN])\n        >>> s1.str.contains('og', regex=False)\n        0    False\n        1     True\n        2    False\n        3    False\n        4      NaN\n        dtype: object\n\n        Returning an Index of booleans using only a literal pattern.\n\n        >>> ind = pd.Index(['Mouse', 'dog', 'house and parrot', '23.0', np.NaN])\n        >>> ind.str.contains('23', regex=False)\n        Index([False, False, False, True, nan], dtype='object')\n\n        Specifying case sensitivity using `case`.\n\n        >>> s1.str.contains('oG', case=True, regex=True)\n        0    False\n        1    False\n        2    False\n        3    False\n        4      NaN\n        dtype: object\n\n        Specifying `na` to be `False` instead of `NaN` replaces NaN values\n        with `False`. If Series or Index does not contain NaN values\n        the resultant dtype will be `bool`, otherwise, an `object` dtype.\n\n        >>> s1.str.contains('og', na=False, regex=True)\n        0    False\n        1     True\n        2    False\n        3    False\n        4    False\n        dtype: bool\n\n        Returning 'house' or 'dog' when either expression occurs in a string.\n\n        >>> s1.str.contains('house|dog', regex=True)\n        0    False\n        1     True\n        2     True\n        3    False\n        4      NaN\n        dtype: object\n\n        Ignoring case sensitivity using `flags` with regex.\n\n        >>> import re\n        >>> s1.str.contains('PARROT', flags=re.IGNORECASE, regex=True)\n        0    False\n        1    False\n        2     True\n        3    False\n        4      NaN\n        dtype: object\n\n        Returning any digit using regular expression.\n\n        >>> s1.str.contains('\\\\d', regex=True)\n        0    False\n        1    False\n        2    False\n        3     True\n        4      NaN\n        dtype: object\n\n        Ensure `pat` is a not a literal pattern when `regex` is set to True.\n        Note in the following example one might expect only `s2[1]` and `s2[3]` to\n        return `True`. However, '.0' as a regex matches any character\n        followed by a 0.\n\n        >>> s2 = pd.Series(['40', '40.0', '41', '41.0', '35'])\n        >>> s2.str.contains('.0', regex=True)\n        0     True\n        1     True\n        2    False\n        3     True\n        4    False\n        dtype: bool\n        \"\"\"\n        result = self._array._str_contains(pat, case, flags, na, regex)\n        return self._wrap_result(result, fill_value=na, returns_string=False)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def match(self, pat, case=True, flags=0, na=None):\n        \"\"\"\n        Determine if each string starts with a match of a regular expression.\n\n        Parameters\n        ----------\n        pat : str\n            Character sequence or regular expression.\n        case : bool, default True\n            If True, case sensitive.\n        flags : int, default 0 (no flags)\n            Regex module flags, e.g. re.IGNORECASE.\n        na : scalar, optional\n            Fill value for missing values. The default depends on dtype of the\n            array. For object-dtype, ``numpy.nan`` is used. For ``StringDtype``,\n            ``pandas.NA`` is used.\n\n        Returns\n        -------\n        Series/array of boolean values\n\n        See Also\n        --------\n        fullmatch : Stricter matching that requires the entire string to match.\n        contains : Analogous, but less strict, relying on re.search instead of\n            re.match.\n        extract : Extract matched groups.\n        \"\"\"\n        result = self._array._str_match(pat, case=case, flags=flags, na=na)\n        return self._wrap_result(result, fill_value=na, returns_string=False)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def fullmatch(self, pat, case=True, flags=0, na=None):\n        \"\"\"\n        Determine if each string entirely matches a regular expression.\n\n        .. versionadded:: 1.1.0\n\n        Parameters\n        ----------\n        pat : str\n            Character sequence or regular expression.\n        case : bool, default True\n            If True, case sensitive.\n        flags : int, default 0 (no flags)\n            Regex module flags, e.g. re.IGNORECASE.\n        na : scalar, optional.\n            Fill value for missing values. The default depends on dtype of the\n            array. For object-dtype, ``numpy.nan`` is used. For ``StringDtype``,\n            ``pandas.NA`` is used.\n\n        Returns\n        -------\n        Series/array of boolean values\n\n        See Also\n        --------\n        match : Similar, but also returns `True` when only a *prefix* of the string\n            matches the regular expression.\n        extract : Extract matched groups.\n        \"\"\"\n        result = self._array._str_fullmatch(pat, case=case, flags=flags, na=na)\n        return self._wrap_result(result, fill_value=na, returns_string=False)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def replace(self, pat, repl, n=-1, case=None, flags=0, regex=None):\n        r\"\"\"\n        Replace each occurrence of pattern/regex in the Series/Index.\n\n        Equivalent to :meth:`str.replace` or :func:`re.sub`, depending on\n        the regex value.\n\n        Parameters\n        ----------\n        pat : str or compiled regex\n            String can be a character sequence or regular expression.\n        repl : str or callable\n            Replacement string or a callable. The callable is passed the regex\n            match object and must return a replacement string to be used.\n            See :func:`re.sub`.\n        n : int, default -1 (all)\n            Number of replacements to make from start.\n        case : bool, default None\n            Determines if replace is case sensitive:\n\n            - If True, case sensitive (the default if `pat` is a string)\n            - Set to False for case insensitive\n            - Cannot be set if `pat` is a compiled regex.\n\n        flags : int, default 0 (no flags)\n            Regex module flags, e.g. re.IGNORECASE. Cannot be set if `pat` is a compiled\n            regex.\n        regex : bool, default True\n            Determines if assumes the passed-in pattern is a regular expression:\n\n            - If True, assumes the passed-in pattern is a regular expression.\n            - If False, treats the pattern as a literal string\n            - Cannot be set to False if `pat` is a compiled regex or `repl` is\n              a callable.\n\n            .. versionadded:: 0.23.0\n\n        Returns\n        -------\n        Series or Index of object\n            A copy of the object with all matching occurrences of `pat` replaced by\n            `repl`.\n\n        Raises\n        ------\n        ValueError\n            * if `regex` is False and `repl` is a callable or `pat` is a compiled\n              regex\n            * if `pat` is a compiled regex and `case` or `flags` is set\n\n        Notes\n        -----\n        When `pat` is a compiled regex, all flags should be included in the\n        compiled regex. Use of `case`, `flags`, or `regex=False` with a compiled\n        regex will raise an error.\n\n        Examples\n        --------\n        When `pat` is a string and `regex` is True (the default), the given `pat`\n        is compiled as a regex. When `repl` is a string, it replaces matching\n        regex patterns as with :meth:`re.sub`. NaN value(s) in the Series are\n        left as is:\n\n        >>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f.', 'ba', regex=True)\n        0    bao\n        1    baz\n        2    NaN\n        dtype: object\n\n        When `pat` is a string and `regex` is False, every `pat` is replaced with\n        `repl` as with :meth:`str.replace`:\n\n        >>> pd.Series(['f.o', 'fuz', np.nan]).str.replace('f.', 'ba', regex=False)\n        0    bao\n        1    fuz\n        2    NaN\n        dtype: object\n\n        When `repl` is a callable, it is called on every `pat` using\n        :func:`re.sub`. The callable should expect one positional argument\n        (a regex object) and return a string.\n\n        To get the idea:\n\n        >>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f', repr)\n        0    <re.Match object; span=(0, 1), match='f'>oo\n        1    <re.Match object; span=(0, 1), match='f'>uz\n        2                                            NaN\n        dtype: object\n\n        Reverse every lowercase alphabetic word:\n\n        >>> repl = lambda m: m.group(0)[::-1]\n        >>> pd.Series(['foo 123', 'bar baz', np.nan]).str.replace(r'[a-z]+', repl)\n        0    oof 123\n        1    rab zab\n        2        NaN\n        dtype: object\n\n        Using regex groups (extract second group and swap case):\n\n        >>> pat = r\"(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)\"\n        >>> repl = lambda m: m.group('two').swapcase()\n        >>> pd.Series(['One Two Three', 'Foo Bar Baz']).str.replace(pat, repl)\n        0    tWO\n        1    bAR\n        dtype: object\n\n        Using a compiled regex with flags\n\n        >>> import re\n        >>> regex_pat = re.compile(r'FUZ', flags=re.IGNORECASE)\n        >>> pd.Series(['foo', 'fuz', np.nan]).str.replace(regex_pat, 'bar')\n        0    foo\n        1    bar\n        2    NaN\n        dtype: object\n        \"\"\"\n        if regex is None:\n            if isinstance(pat, str) and any(c in pat for c in \".+*|^$?[](){}\\\\\"):\n                # warn only in cases where regex behavior would differ from literal\n                msg = (\n                    \"The default value of regex will change from True to False \"\n                    \"in a future version.\"\n                )\n                if len(pat) == 1:\n                    msg += (\n                        \" In addition, single character regular expressions will\"\n                        \"*not* be treated as literal strings when regex=True.\"\n                    )\n                warnings.warn(msg, FutureWarning, stacklevel=3)\n            regex = True\n        result = self._array._str_replace(\n            pat, repl, n=n, case=case, flags=flags, regex=regex\n        )\n        return self._wrap_result(result)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def repeat(self, repeats):\n        \"\"\"\n        Duplicate each string in the Series or Index.\n\n        Parameters\n        ----------\n        repeats : int or sequence of int\n            Same value for all (int) or different value per (sequence).\n\n        Returns\n        -------\n        Series or Index of object\n            Series or Index of repeated string objects specified by\n            input parameter repeats.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'b', 'c'])\n        >>> s\n        0    a\n        1    b\n        2    c\n        dtype: object\n\n        Single int repeats string in Series\n\n        >>> s.str.repeat(repeats=2)\n        0    aa\n        1    bb\n        2    cc\n        dtype: object\n\n        Sequence of int repeats corresponding string in Series\n\n        >>> s.str.repeat(repeats=[1, 2, 3])\n        0      a\n        1     bb\n        2    ccc\n        dtype: object\n        \"\"\"\n        result = self._array._str_repeat(repeats)\n        return self._wrap_result(result)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def pad(self, width, side=\"left\", fillchar=\" \"):\n        \"\"\"\n        Pad strings in the Series/Index up to width.\n\n        Parameters\n        ----------\n        width : int\n            Minimum width of resulting string; additional characters will be filled\n            with character defined in `fillchar`.\n        side : {'left', 'right', 'both'}, default 'left'\n            Side from which to fill resulting string.\n        fillchar : str, default ' '\n            Additional character for filling, default is whitespace.\n\n        Returns\n        -------\n        Series or Index of object\n            Returns Series or Index with minimum number of char in object.\n\n        See Also\n        --------\n        Series.str.rjust : Fills the left side of strings with an arbitrary\n            character. Equivalent to ``Series.str.pad(side='left')``.\n        Series.str.ljust : Fills the right side of strings with an arbitrary\n            character. Equivalent to ``Series.str.pad(side='right')``.\n        Series.str.center : Fills both sides of strings with an arbitrary\n            character. Equivalent to ``Series.str.pad(side='both')``.\n        Series.str.zfill : Pad strings in the Series/Index by prepending '0'\n            character. Equivalent to ``Series.str.pad(side='left', fillchar='0')``.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"caribou\", \"tiger\"])\n        >>> s\n        0    caribou\n        1      tiger\n        dtype: object\n\n        >>> s.str.pad(width=10)\n        0       caribou\n        1         tiger\n        dtype: object\n\n        >>> s.str.pad(width=10, side='right', fillchar='-')\n        0    caribou---\n        1    tiger-----\n        dtype: object\n\n        >>> s.str.pad(width=10, side='both', fillchar='-')\n        0    -caribou--\n        1    --tiger---\n        dtype: object\n        \"\"\"\n        if not isinstance(fillchar, str):\n            msg = f\"fillchar must be a character, not {type(fillchar).__name__}\"\n            raise TypeError(msg)\n\n        if len(fillchar) != 1:\n            raise TypeError(\"fillchar must be a character, not str\")\n\n        if not is_integer(width):\n            msg = f\"width must be of integer type, not {type(width).__name__}\"\n            raise TypeError(msg)\n\n        result = self._array._str_pad(width, side=side, fillchar=fillchar)\n        return self._wrap_result(result)\n\n    _shared_docs[\n        \"str_pad\"\n    ] = \"\"\"\n    Pad %(side)s side of strings in the Series/Index.\n\n    Equivalent to :meth:`str.%(method)s`.\n\n    Parameters\n    ----------\n    width : int\n        Minimum width of resulting string; additional characters will be filled\n        with ``fillchar``.\n    fillchar : str\n        Additional character for filling, default is whitespace.\n\n    Returns\n    -------\n    filled : Series/Index of objects.\n    \"\"\"\n\n    @Appender(_shared_docs[\"str_pad\"] % {\"side\": \"left and right\", \"method\": \"center\"})\n    @forbid_nonstring_types([\"bytes\"])\n    def center(self, width, fillchar=\" \"):\n        return self.pad(width, side=\"both\", fillchar=fillchar)\n\n    @Appender(_shared_docs[\"str_pad\"] % {\"side\": \"right\", \"method\": \"ljust\"})\n    @forbid_nonstring_types([\"bytes\"])\n    def ljust(self, width, fillchar=\" \"):\n        return self.pad(width, side=\"right\", fillchar=fillchar)\n\n    @Appender(_shared_docs[\"str_pad\"] % {\"side\": \"left\", \"method\": \"rjust\"})\n    @forbid_nonstring_types([\"bytes\"])\n    def rjust(self, width, fillchar=\" \"):\n        return self.pad(width, side=\"left\", fillchar=fillchar)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def zfill(self, width):\n        \"\"\"\n        Pad strings in the Series/Index by prepending '0' characters.\n\n        Strings in the Series/Index are padded with '0' characters on the\n        left of the string to reach a total string length  `width`. Strings\n        in the Series/Index with length greater or equal to `width` are\n        unchanged.\n\n        Parameters\n        ----------\n        width : int\n            Minimum length of resulting string; strings with length less\n            than `width` be prepended with '0' characters.\n\n        Returns\n        -------\n        Series/Index of objects.\n\n        See Also\n        --------\n        Series.str.rjust : Fills the left side of strings with an arbitrary\n            character.\n        Series.str.ljust : Fills the right side of strings with an arbitrary\n            character.\n        Series.str.pad : Fills the specified sides of strings with an arbitrary\n            character.\n        Series.str.center : Fills both sides of strings with an arbitrary\n            character.\n\n        Notes\n        -----\n        Differs from :meth:`str.zfill` which has special handling\n        for '+'/'-' in the string.\n\n        Examples\n        --------\n        >>> s = pd.Series(['-1', '1', '1000', 10, np.nan])\n        >>> s\n        0      -1\n        1       1\n        2    1000\n        3      10\n        4     NaN\n        dtype: object\n\n        Note that ``10`` and ``NaN`` are not strings, therefore they are\n        converted to ``NaN``. The minus sign in ``'-1'`` is treated as a\n        regular character and the zero is added to the left of it\n        (:meth:`str.zfill` would have moved it to the left). ``1000``\n        remains unchanged as it is longer than `width`.\n\n        >>> s.str.zfill(3)\n        0     0-1\n        1     001\n        2    1000\n        3     NaN\n        4     NaN\n        dtype: object\n        \"\"\"\n        result = self.pad(width, side=\"left\", fillchar=\"0\")\n        return self._wrap_result(result)\n\n    def slice(self, start=None, stop=None, step=None):\n        \"\"\"\n        Slice substrings from each element in the Series or Index.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start position for slice operation.\n        stop : int, optional\n            Stop position for slice operation.\n        step : int, optional\n            Step size for slice operation.\n\n        Returns\n        -------\n        Series or Index of object\n            Series or Index from sliced substring from original string object.\n\n        See Also\n        --------\n        Series.str.slice_replace : Replace a slice with a string.\n        Series.str.get : Return element at position.\n            Equivalent to `Series.str.slice(start=i, stop=i+1)` with `i`\n            being the position.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"koala\", \"fox\", \"chameleon\"])\n        >>> s\n        0        koala\n        1          fox\n        2    chameleon\n        dtype: object\n\n        >>> s.str.slice(start=1)\n        0        oala\n        1          ox\n        2    hameleon\n        dtype: object\n\n        >>> s.str.slice(start=-1)\n        0           a\n        1           x\n        2           n\n        dtype: object\n\n        >>> s.str.slice(stop=2)\n        0    ko\n        1    fo\n        2    ch\n        dtype: object\n\n        >>> s.str.slice(step=2)\n        0      kaa\n        1       fx\n        2    caeen\n        dtype: object\n\n        >>> s.str.slice(start=0, stop=5, step=3)\n        0    kl\n        1     f\n        2    cm\n        dtype: object\n\n        Equivalent behaviour to:\n\n        >>> s.str[0:5:3]\n        0    kl\n        1     f\n        2    cm\n        dtype: object\n        \"\"\"\n        result = self._array._str_slice(start, stop, step)\n        return self._wrap_result(result)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def slice_replace(self, start=None, stop=None, repl=None):\n        \"\"\"\n        Replace a positional slice of a string with another value.\n\n        Parameters\n        ----------\n        start : int, optional\n            Left index position to use for the slice. If not specified (None),\n            the slice is unbounded on the left, i.e. slice from the start\n            of the string.\n        stop : int, optional\n            Right index position to use for the slice. If not specified (None),\n            the slice is unbounded on the right, i.e. slice until the\n            end of the string.\n        repl : str, optional\n            String for replacement. If not specified (None), the sliced region\n            is replaced with an empty string.\n\n        Returns\n        -------\n        Series or Index\n            Same type as the original object.\n\n        See Also\n        --------\n        Series.str.slice : Just slicing without replacement.\n\n        Examples\n        --------\n        >>> s = pd.Series(['a', 'ab', 'abc', 'abdc', 'abcde'])\n        >>> s\n        0        a\n        1       ab\n        2      abc\n        3     abdc\n        4    abcde\n        dtype: object\n\n        Specify just `start`, meaning replace `start` until the end of the\n        string with `repl`.\n\n        >>> s.str.slice_replace(1, repl='X')\n        0    aX\n        1    aX\n        2    aX\n        3    aX\n        4    aX\n        dtype: object\n\n        Specify just `stop`, meaning the start of the string to `stop` is replaced\n        with `repl`, and the rest of the string is included.\n\n        >>> s.str.slice_replace(stop=2, repl='X')\n        0       X\n        1       X\n        2      Xc\n        3     Xdc\n        4    Xcde\n        dtype: object\n\n        Specify `start` and `stop`, meaning the slice from `start` to `stop` is\n        replaced with `repl`. Everything before or after `start` and `stop` is\n        included as is.\n\n        >>> s.str.slice_replace(start=1, stop=3, repl='X')\n        0      aX\n        1      aX\n        2      aX\n        3     aXc\n        4    aXde\n        dtype: object\n        \"\"\"\n        result = self._array._str_slice_replace(start, stop, repl)\n        return self._wrap_result(result)\n\n    def decode(self, encoding, errors=\"strict\"):\n        \"\"\"\n        Decode character string in the Series/Index using indicated encoding.\n\n        Equivalent to :meth:`str.decode` in python2 and :meth:`bytes.decode` in\n        python3.\n\n        Parameters\n        ----------\n        encoding : str\n        errors : str, optional\n\n        Returns\n        -------\n        Series or Index\n        \"\"\"\n        # TODO: Add a similar _bytes interface.\n        if encoding in _cpython_optimized_decoders:\n            # CPython optimized implementation\n            f = lambda x: x.decode(encoding, errors)\n        else:\n            decoder = codecs.getdecoder(encoding)\n            f = lambda x: decoder(x, errors)[0]\n        arr = self._array\n        # assert isinstance(arr, (StringArray,))\n        result = arr._str_map(f)\n        return self._wrap_result(result)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def encode(self, encoding, errors=\"strict\"):\n        \"\"\"\n        Encode character string in the Series/Index using indicated encoding.\n\n        Equivalent to :meth:`str.encode`.\n\n        Parameters\n        ----------\n        encoding : str\n        errors : str, optional\n\n        Returns\n        -------\n        encoded : Series/Index of objects\n        \"\"\"\n        result = self._array._str_encode(encoding, errors)\n        return self._wrap_result(result, returns_string=False)\n\n    _shared_docs[\n        \"str_strip\"\n    ] = r\"\"\"\n    Remove %(position)s characters.\n\n    Strip whitespaces (including newlines) or a set of specified characters\n    from each string in the Series/Index from %(side)s.\n    Equivalent to :meth:`str.%(method)s`.\n\n    Parameters\n    ----------\n    to_strip : str or None, default None\n        Specifying the set of characters to be removed.\n        All combinations of this set of characters will be stripped.\n        If None then whitespaces are removed.\n\n    Returns\n    -------\n    Series or Index of object\n\n    See Also\n    --------\n    Series.str.strip : Remove leading and trailing characters in Series/Index.\n    Series.str.lstrip : Remove leading characters in Series/Index.\n    Series.str.rstrip : Remove trailing characters in Series/Index.\n\n    Examples\n    --------\n    >>> s = pd.Series(['1. Ant.  ', '2. Bee!\\n', '3. Cat?\\t', np.nan])\n    >>> s\n    0    1. Ant.\n    1    2. Bee!\\n\n    2    3. Cat?\\t\n    3          NaN\n    dtype: object\n\n    >>> s.str.strip()\n    0    1. Ant.\n    1    2. Bee!\n    2    3. Cat?\n    3        NaN\n    dtype: object\n\n    >>> s.str.lstrip('123.')\n    0    Ant.\n    1    Bee!\\n\n    2    Cat?\\t\n    3       NaN\n    dtype: object\n\n    >>> s.str.rstrip('.!? \\n\\t')\n    0    1. Ant\n    1    2. Bee\n    2    3. Cat\n    3       NaN\n    dtype: object\n\n    >>> s.str.strip('123.!? \\n\\t')\n    0    Ant\n    1    Bee\n    2    Cat\n    3    NaN\n    dtype: object\n    \"\"\"\n\n    @Appender(\n        _shared_docs[\"str_strip\"]\n        % {\n            \"side\": \"left and right sides\",\n            \"method\": \"strip\",\n            \"position\": \"leading and trailing\",\n        }\n    )\n    @forbid_nonstring_types([\"bytes\"])\n    def strip(self, to_strip=None):\n        result = self._array._str_strip(to_strip)\n        return self._wrap_result(result)\n\n    @Appender(\n        _shared_docs[\"str_strip\"]\n        % {\"side\": \"left side\", \"method\": \"lstrip\", \"position\": \"leading\"}\n    )\n    @forbid_nonstring_types([\"bytes\"])\n    def lstrip(self, to_strip=None):\n        result = self._array._str_lstrip(to_strip)\n        return self._wrap_result(result)\n\n    @Appender(\n        _shared_docs[\"str_strip\"]\n        % {\"side\": \"right side\", \"method\": \"rstrip\", \"position\": \"trailing\"}\n    )\n    @forbid_nonstring_types([\"bytes\"])\n    def rstrip(self, to_strip=None):\n        result = self._array._str_rstrip(to_strip)\n        return self._wrap_result(result)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def wrap(self, width, **kwargs):\n        r\"\"\"\n        Wrap strings in Series/Index at specified line width.\n\n        This method has the same keyword parameters and defaults as\n        :class:`textwrap.TextWrapper`.\n\n        Parameters\n        ----------\n        width : int\n            Maximum line width.\n        expand_tabs : bool, optional\n            If True, tab characters will be expanded to spaces (default: True).\n        replace_whitespace : bool, optional\n            If True, each whitespace character (as defined by string.whitespace)\n            remaining after tab expansion will be replaced by a single space\n            (default: True).\n        drop_whitespace : bool, optional\n            If True, whitespace that, after wrapping, happens to end up at the\n            beginning or end of a line is dropped (default: True).\n        break_long_words : bool, optional\n            If True, then words longer than width will be broken in order to ensure\n            that no lines are longer than width. If it is false, long words will\n            not be broken, and some lines may be longer than width (default: True).\n        break_on_hyphens : bool, optional\n            If True, wrapping will occur preferably on whitespace and right after\n            hyphens in compound words, as it is customary in English. If false,\n            only whitespaces will be considered as potentially good places for line\n            breaks, but you need to set break_long_words to false if you want truly\n            insecable words (default: True).\n\n        Returns\n        -------\n        Series or Index\n\n        Notes\n        -----\n        Internally, this method uses a :class:`textwrap.TextWrapper` instance with\n        default settings. To achieve behavior matching R's stringr library str_wrap\n        function, use the arguments:\n\n        - expand_tabs = False\n        - replace_whitespace = True\n        - drop_whitespace = True\n        - break_long_words = False\n        - break_on_hyphens = False\n\n        Examples\n        --------\n        >>> s = pd.Series(['line to be wrapped', 'another line to be wrapped'])\n        >>> s.str.wrap(12)\n        0             line to be\\nwrapped\n        1    another line\\nto be\\nwrapped\n        dtype: object\n        \"\"\"\n        result = self._array._str_wrap(width, **kwargs)\n        return self._wrap_result(result)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def get_dummies(self, sep=\"|\"):\n        \"\"\"\n        Return DataFrame of dummy/indicator variables for Series.\n\n        Each string in Series is split by sep and returned as a DataFrame\n        of dummy/indicator variables.\n\n        Parameters\n        ----------\n        sep : str, default \"|\"\n            String to split on.\n\n        Returns\n        -------\n        DataFrame\n            Dummy variables corresponding to values of the Series.\n\n        See Also\n        --------\n        get_dummies : Convert categorical variable into dummy/indicator\n            variables.\n\n        Examples\n        --------\n        >>> pd.Series(['a|b', 'a', 'a|c']).str.get_dummies()\n        a  b  c\n        0  1  1  0\n        1  1  0  0\n        2  1  0  1\n\n        >>> pd.Series(['a|b', np.nan, 'a|c']).str.get_dummies()\n        a  b  c\n        0  1  1  0\n        1  0  0  0\n        2  1  0  1\n        \"\"\"\n        # we need to cast to Series of strings as only that has all\n        # methods available for making the dummies...\n        result, name = self._array._str_get_dummies(sep)\n        return self._wrap_result(\n            result,\n            name=name,\n            expand=True,\n            returns_string=False,\n        )\n\n    @forbid_nonstring_types([\"bytes\"])\n    def translate(self, table):\n        \"\"\"\n        Map all characters in the string through the given mapping table.\n\n        Equivalent to standard :meth:`str.translate`.\n\n        Parameters\n        ----------\n        table : dict\n            Table is a mapping of Unicode ordinals to Unicode ordinals, strings, or\n            None. Unmapped characters are left untouched.\n            Characters mapped to None are deleted. :meth:`str.maketrans` is a\n            helper function for making translation tables.\n\n        Returns\n        -------\n        Series or Index\n        \"\"\"\n        result = self._array._str_translate(table)\n        return self._wrap_result(result)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def count(self, pat, flags=0):\n        r\"\"\"\n        Count occurrences of pattern in each string of the Series/Index.\n\n        This function is used to count the number of times a particular regex\n        pattern is repeated in each of the string elements of the\n        :class:`~pandas.Series`.\n\n        Parameters\n        ----------\n        pat : str\n            Valid regular expression.\n        flags : int, default 0, meaning no flags\n            Flags for the `re` module. For a complete list, `see here\n            <https://docs.python.org/3/howto/regex.html#compilation-flags>`_.\n        **kwargs\n            For compatibility with other string methods. Not used.\n\n        Returns\n        -------\n        Series or Index\n            Same type as the calling object containing the integer counts.\n\n        See Also\n        --------\n        re : Standard library module for regular expressions.\n        str.count : Standard library version, without regular expression support.\n\n        Notes\n        -----\n        Some characters need to be escaped when passing in `pat`.\n        eg. ``'$'`` has a special meaning in regex and must be escaped when\n        finding this literal character.\n\n        Examples\n        --------\n        >>> s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])\n        >>> s.str.count('a')\n        0    0.0\n        1    0.0\n        2    2.0\n        3    2.0\n        4    NaN\n        5    0.0\n        6    1.0\n        dtype: float64\n\n        Escape ``'$'`` to find the literal dollar sign.\n\n        >>> s = pd.Series(['$', 'B', 'Aab$', '$$ca', 'C$B$', 'cat'])\n        >>> s.str.count('\\\\$')\n        0    1\n        1    0\n        2    1\n        3    2\n        4    2\n        5    0\n        dtype: int64\n\n        This is also available on Index\n\n        >>> pd.Index(['A', 'A', 'Aaba', 'cat']).str.count('a')\n        Int64Index([0, 0, 2, 1], dtype='int64')\n        \"\"\"\n        result = self._array._str_count(pat, flags)\n        return self._wrap_result(result, returns_string=False)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def startswith(self, pat, na=None):\n        \"\"\"\n        Test if the start of each string element matches a pattern.\n\n        Equivalent to :meth:`str.startswith`.\n\n        Parameters\n        ----------\n        pat : str\n            Character sequence. Regular expressions are not accepted.\n        na : object, default NaN\n            Object shown if element tested is not a string. The default depends\n            on dtype of the array. For object-dtype, ``numpy.nan`` is used.\n            For ``StringDtype``, ``pandas.NA`` is used.\n\n        Returns\n        -------\n        Series or Index of bool\n            A Series of booleans indicating whether the given pattern matches\n            the start of each string element.\n\n        See Also\n        --------\n        str.startswith : Python standard library string method.\n        Series.str.endswith : Same as startswith, but tests the end of string.\n        Series.str.contains : Tests if string element contains a pattern.\n\n        Examples\n        --------\n        >>> s = pd.Series(['bat', 'Bear', 'cat', np.nan])\n        >>> s\n        0     bat\n        1    Bear\n        2     cat\n        3     NaN\n        dtype: object\n\n        >>> s.str.startswith('b')\n        0     True\n        1    False\n        2    False\n        3      NaN\n        dtype: object\n\n        Specifying `na` to be `False` instead of `NaN`.\n\n        >>> s.str.startswith('b', na=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        dtype: bool\n        \"\"\"\n        result = self._array._str_startswith(pat, na=na)\n        return self._wrap_result(result, returns_string=False)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def endswith(self, pat, na=None):\n        \"\"\"\n        Test if the end of each string element matches a pattern.\n\n        Equivalent to :meth:`str.endswith`.\n\n        Parameters\n        ----------\n        pat : str\n            Character sequence. Regular expressions are not accepted.\n        na : object, default NaN\n            Object shown if element tested is not a string. The default depends\n            on dtype of the array. For object-dtype, ``numpy.nan`` is used.\n            For ``StringDtype``, ``pandas.NA`` is used.\n\n        Returns\n        -------\n        Series or Index of bool\n            A Series of booleans indicating whether the given pattern matches\n            the end of each string element.\n\n        See Also\n        --------\n        str.endswith : Python standard library string method.\n        Series.str.startswith : Same as endswith, but tests the start of string.\n        Series.str.contains : Tests if string element contains a pattern.\n\n        Examples\n        --------\n        >>> s = pd.Series(['bat', 'bear', 'caT', np.nan])\n        >>> s\n        0     bat\n        1    bear\n        2     caT\n        3     NaN\n        dtype: object\n\n        >>> s.str.endswith('t')\n        0     True\n        1    False\n        2    False\n        3      NaN\n        dtype: object\n\n        Specifying `na` to be `False` instead of `NaN`.\n\n        >>> s.str.endswith('t', na=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        dtype: bool\n        \"\"\"\n        result = self._array._str_endswith(pat, na=na)\n        return self._wrap_result(result, returns_string=False)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def findall(self, pat, flags=0):\n        \"\"\"\n        Find all occurrences of pattern or regular expression in the Series/Index.\n\n        Equivalent to applying :func:`re.findall` to all the elements in the\n        Series/Index.\n\n        Parameters\n        ----------\n        pat : str\n            Pattern or regular expression.\n        flags : int, default 0\n            Flags from ``re`` module, e.g. `re.IGNORECASE` (default is 0, which\n            means no flags).\n\n        Returns\n        -------\n        Series/Index of lists of strings\n            All non-overlapping matches of pattern or regular expression in each\n            string of this Series/Index.\n\n        See Also\n        --------\n        count : Count occurrences of pattern or regular expression in each string\n            of the Series/Index.\n        extractall : For each string in the Series, extract groups from all matches\n            of regular expression and return a DataFrame with one row for each\n            match and one column for each group.\n        re.findall : The equivalent ``re`` function to all non-overlapping matches\n            of pattern or regular expression in string, as a list of strings.\n\n        Examples\n        --------\n        >>> s = pd.Series(['Lion', 'Monkey', 'Rabbit'])\n\n        The search for the pattern 'Monkey' returns one match:\n\n        >>> s.str.findall('Monkey')\n        0          []\n        1    [Monkey]\n        2          []\n        dtype: object\n\n        On the other hand, the search for the pattern 'MONKEY' doesn't return any\n        match:\n\n        >>> s.str.findall('MONKEY')\n        0    []\n        1    []\n        2    []\n        dtype: object\n\n        Flags can be added to the pattern or regular expression. For instance,\n        to find the pattern 'MONKEY' ignoring the case:\n\n        >>> import re\n        >>> s.str.findall('MONKEY', flags=re.IGNORECASE)\n        0          []\n        1    [Monkey]\n        2          []\n        dtype: object\n\n        When the pattern matches more than one string in the Series, all matches\n        are returned:\n\n        >>> s.str.findall('on')\n        0    [on]\n        1    [on]\n        2      []\n        dtype: object\n\n        Regular expressions are supported too. For instance, the search for all the\n        strings ending with the word 'on' is shown next:\n\n        >>> s.str.findall('on$')\n        0    [on]\n        1      []\n        2      []\n        dtype: object\n\n        If the pattern is found more than once in the same string, then a list of\n        multiple strings is returned:\n\n        >>> s.str.findall('b')\n        0        []\n        1        []\n        2    [b, b]\n        dtype: object\n        \"\"\"\n        result = self._array._str_findall(pat, flags)\n        return self._wrap_result(result, returns_string=False)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def extract(self, pat, flags=0, expand=True):\n        r\"\"\"\n        Extract capture groups in the regex `pat` as columns in a DataFrame.\n\n        For each subject string in the Series, extract groups from the\n        first match of regular expression `pat`.\n\n        Parameters\n        ----------\n        pat : str\n            Regular expression pattern with capturing groups.\n        flags : int, default 0 (no flags)\n            Flags from the ``re`` module, e.g. ``re.IGNORECASE``, that\n            modify regular expression matching for things like case,\n            spaces, etc. For more details, see :mod:`re`.\n        expand : bool, default True\n            If True, return DataFrame with one column per capture group.\n            If False, return a Series/Index if there is one capture group\n            or DataFrame if there are multiple capture groups.\n\n        Returns\n        -------\n        DataFrame or Series or Index\n            A DataFrame with one row for each subject string, and one\n            column for each group. Any capture group names in regular\n            expression pat will be used for column names; otherwise\n            capture group numbers will be used. The dtype of each result\n            column is always object, even when no match is found. If\n            ``expand=False`` and pat has only one capture group, then\n            return a Series (if subject is a Series) or Index (if subject\n            is an Index).\n\n        See Also\n        --------\n        extractall : Returns all matches (not just the first match).\n\n        Examples\n        --------\n        A pattern with two groups will return a DataFrame with two columns.\n        Non-matches will be NaN.\n\n        >>> s = pd.Series(['a1', 'b2', 'c3'])\n        >>> s.str.extract(r'([ab])(\\d)')\n            0    1\n        0    a    1\n        1    b    2\n        2  NaN  NaN\n\n        A pattern may contain optional groups.\n\n        >>> s.str.extract(r'([ab])?(\\d)')\n            0  1\n        0    a  1\n        1    b  2\n        2  NaN  3\n\n        Named groups will become column names in the result.\n\n        >>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\\d)')\n        letter digit\n        0      a     1\n        1      b     2\n        2    NaN   NaN\n\n        A pattern with one group will return a DataFrame with one column\n        if expand=True.\n\n        >>> s.str.extract(r'[ab](\\d)', expand=True)\n            0\n        0    1\n        1    2\n        2  NaN\n\n        A pattern with one group will return a Series if expand=False.\n\n        >>> s.str.extract(r'[ab](\\d)', expand=False)\n        0      1\n        1      2\n        2    NaN\n        dtype: object\n        \"\"\"\n        # TODO: dispatch\n        return str_extract(self, pat, flags, expand=expand)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def extractall(self, pat, flags=0):\n        r\"\"\"\n        Extract capture groups in the regex `pat` as columns in DataFrame.\n\n        For each subject string in the Series, extract groups from all\n        matches of regular expression pat. When each subject string in the\n        Series has exactly one match, extractall(pat).xs(0, level='match')\n        is the same as extract(pat).\n\n        Parameters\n        ----------\n        pat : str\n            Regular expression pattern with capturing groups.\n        flags : int, default 0 (no flags)\n            A ``re`` module flag, for example ``re.IGNORECASE``. These allow\n            to modify regular expression matching for things like case, spaces,\n            etc. Multiple flags can be combined with the bitwise OR operator,\n            for example ``re.IGNORECASE | re.MULTILINE``.\n\n        Returns\n        -------\n        DataFrame\n            A ``DataFrame`` with one row for each match, and one column for each\n            group. Its rows have a ``MultiIndex`` with first levels that come from\n            the subject ``Series``. The last level is named 'match' and indexes the\n            matches in each item of the ``Series``. Any capture group names in\n            regular expression pat will be used for column names; otherwise capture\n            group numbers will be used.\n\n        See Also\n        --------\n        extract : Returns first match only (not all matches).\n\n        Examples\n        --------\n        A pattern with one group will return a DataFrame with one column.\n        Indices with no matches will not appear in the result.\n\n        >>> s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"])\n        >>> s.str.extractall(r\"[ab](\\d)\")\n                0\n        match\n        A 0      1\n        1      2\n        B 0      1\n\n        Capture group names are used for column names of the result.\n\n        >>> s.str.extractall(r\"[ab](?P<digit>\\d)\")\n                digit\n        match\n        A 0         1\n        1         2\n        B 0         1\n\n        A pattern with two groups will return a DataFrame with two columns.\n\n        >>> s.str.extractall(r\"(?P<letter>[ab])(?P<digit>\\d)\")\n                letter digit\n        match\n        A 0          a     1\n        1          a     2\n        B 0          b     1\n\n        Optional groups that do not match are NaN in the result.\n\n        >>> s.str.extractall(r\"(?P<letter>[ab])?(?P<digit>\\d)\")\n                letter digit\n        match\n        A 0          a     1\n        1          a     2\n        B 0          b     1\n        C 0        NaN     1\n        \"\"\"\n        # TODO: dispatch\n        return str_extractall(self._orig, pat, flags)\n\n    _shared_docs[\n        \"find\"\n    ] = \"\"\"\n    Return %(side)s indexes in each strings in the Series/Index.\n\n    Each of returned indexes corresponds to the position where the\n    substring is fully contained between [start:end]. Return -1 on\n    failure. Equivalent to standard :meth:`str.%(method)s`.\n\n    Parameters\n    ----------\n    sub : str\n        Substring being searched.\n    start : int\n        Left edge index.\n    end : int\n        Right edge index.\n\n    Returns\n    -------\n    Series or Index of int.\n\n    See Also\n    --------\n    %(also)s\n    \"\"\"\n\n    @Appender(\n        _shared_docs[\"find\"]\n        % {\n            \"side\": \"lowest\",\n            \"method\": \"find\",\n            \"also\": \"rfind : Return highest indexes in each strings.\",\n        }\n    )\n    @forbid_nonstring_types([\"bytes\"])\n    def find(self, sub, start=0, end=None):\n        if not isinstance(sub, str):\n            msg = f\"expected a string object, not {type(sub).__name__}\"\n            raise TypeError(msg)\n\n        result = self._array._str_find(sub, start, end)\n        return self._wrap_result(result, returns_string=False)\n\n    @Appender(\n        _shared_docs[\"find\"]\n        % {\n            \"side\": \"highest\",\n            \"method\": \"rfind\",\n            \"also\": \"find : Return lowest indexes in each strings.\",\n        }\n    )\n    @forbid_nonstring_types([\"bytes\"])\n    def rfind(self, sub, start=0, end=None):\n        if not isinstance(sub, str):\n            msg = f\"expected a string object, not {type(sub).__name__}\"\n            raise TypeError(msg)\n\n        result = self._array._str_rfind(sub, start=start, end=end)\n        return self._wrap_result(result, returns_string=False)\n\n    @forbid_nonstring_types([\"bytes\"])\n    def normalize(self, form):\n        \"\"\"\n        Return the Unicode normal form for the strings in the Series/Index.\n\n        For more information on the forms, see the\n        :func:`unicodedata.normalize`.\n\n        Parameters\n        ----------\n        form : {'NFC', 'NFKC', 'NFD', 'NFKD'}\n            Unicode form.\n\n        Returns\n        -------\n        normalized : Series/Index of objects\n        \"\"\"\n        result = self._array._str_normalize(form)\n        return self._wrap_result(result)\n\n    _shared_docs[\n        \"index\"\n    ] = \"\"\"\n    Return %(side)s indexes in each string in Series/Index.\n\n    Each of the returned indexes corresponds to the position where the\n    substring is fully contained between [start:end]. This is the same\n    as ``str.%(similar)s`` except instead of returning -1, it raises a\n    ValueError when the substring is not found. Equivalent to standard\n    ``str.%(method)s``.\n\n    Parameters\n    ----------\n    sub : str\n        Substring being searched.\n    start : int\n        Left edge index.\n    end : int\n        Right edge index.\n\n    Returns\n    -------\n    Series or Index of object\n\n    See Also\n    --------\n    %(also)s\n    \"\"\"\n\n    @Appender(\n        _shared_docs[\"index\"]\n        % {\n            \"side\": \"lowest\",\n            \"similar\": \"find\",\n            \"method\": \"index\",\n            \"also\": \"rindex : Return highest indexes in each strings.\",\n        }\n    )\n    @forbid_nonstring_types([\"bytes\"])\n    def index(self, sub, start=0, end=None):\n        if not isinstance(sub, str):\n            msg = f\"expected a string object, not {type(sub).__name__}\"\n            raise TypeError(msg)\n\n        result = self._array._str_index(sub, start=start, end=end)\n        return self._wrap_result(result, returns_string=False)\n\n    @Appender(\n        _shared_docs[\"index\"]\n        % {\n            \"side\": \"highest\",\n            \"similar\": \"rfind\",\n            \"method\": \"rindex\",\n            \"also\": \"index : Return lowest indexes in each strings.\",\n        }\n    )\n    @forbid_nonstring_types([\"bytes\"])\n    def rindex(self, sub, start=0, end=None):\n        if not isinstance(sub, str):\n            msg = f\"expected a string object, not {type(sub).__name__}\"\n            raise TypeError(msg)\n\n        result = self._array._str_rindex(sub, start=start, end=end)\n        return self._wrap_result(result, returns_string=False)\n\n    def len(self):\n        \"\"\"\n        Compute the length of each element in the Series/Index.\n\n        The element may be a sequence (such as a string, tuple or list) or a collection\n        (such as a dictionary).\n\n        Returns\n        -------\n        Series or Index of int\n            A Series or Index of integer values indicating the length of each\n            element in the Series or Index.\n\n        See Also\n        --------\n        str.len : Python built-in function returning the length of an object.\n        Series.size : Returns the length of the Series.\n\n        Examples\n        --------\n        Returns the length (number of characters) in a string. Returns the\n        number of entries for dictionaries, lists or tuples.\n\n        >>> s = pd.Series(['dog',\n        ...                 '',\n        ...                 5,\n        ...                 {'foo' : 'bar'},\n        ...                 [2, 3, 5, 7],\n        ...                 ('one', 'two', 'three')])\n        >>> s\n        0                  dog\n        1\n        2                    5\n        3       {'foo': 'bar'}\n        4         [2, 3, 5, 7]\n        5    (one, two, three)\n        dtype: object\n        >>> s.str.len()\n        0    3.0\n        1    0.0\n        2    NaN\n        3    1.0\n        4    4.0\n        5    3.0\n        dtype: float64\n        \"\"\"\n        result = self._array._str_len()\n        return self._wrap_result(result, returns_string=False)\n\n    _shared_docs[\n        \"casemethods\"\n    ] = \"\"\"\n    Convert strings in the Series/Index to %(type)s.\n    %(version)s\n    Equivalent to :meth:`str.%(method)s`.\n\n    Returns\n    -------\n    Series or Index of object\n\n    See Also\n    --------\n    Series.str.lower : Converts all characters to lowercase.\n    Series.str.upper : Converts all characters to uppercase.\n    Series.str.title : Converts first character of each word to uppercase and\n        remaining to lowercase.\n    Series.str.capitalize : Converts first character to uppercase and\n        remaining to lowercase.\n    Series.str.swapcase : Converts uppercase to lowercase and lowercase to\n        uppercase.\n    Series.str.casefold: Removes all case distinctions in the string.\n\n    Examples\n    --------\n    >>> s = pd.Series(['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe'])\n    >>> s\n    0                 lower\n    1              CAPITALS\n    2    this is a sentence\n    3              SwApCaSe\n    dtype: object\n\n    >>> s.str.lower()\n    0                 lower\n    1              capitals\n    2    this is a sentence\n    3              swapcase\n    dtype: object\n\n    >>> s.str.upper()\n    0                 LOWER\n    1              CAPITALS\n    2    THIS IS A SENTENCE\n    3              SWAPCASE\n    dtype: object\n\n    >>> s.str.title()\n    0                 Lower\n    1              Capitals\n    2    This Is A Sentence\n    3              Swapcase\n    dtype: object\n\n    >>> s.str.capitalize()\n    0                 Lower\n    1              Capitals\n    2    This is a sentence\n    3              Swapcase\n    dtype: object\n\n    >>> s.str.swapcase()\n    0                 LOWER\n    1              capitals\n    2    THIS IS A SENTENCE\n    3              sWaPcAsE\n    dtype: object\n    \"\"\"\n    # Types:\n    #   cases:\n    #       upper, lower, title, capitalize, swapcase, casefold\n    #   boolean:\n    #     isalpha, isnumeric isalnum isdigit isdecimal isspace islower isupper istitle\n    # _doc_args holds dict of strings to use in substituting casemethod docs\n    _doc_args: Dict[str, Dict[str, str]] = {}\n    _doc_args[\"lower\"] = {\"type\": \"lowercase\", \"method\": \"lower\", \"version\": \"\"}\n    _doc_args[\"upper\"] = {\"type\": \"uppercase\", \"method\": \"upper\", \"version\": \"\"}\n    _doc_args[\"title\"] = {\"type\": \"titlecase\", \"method\": \"title\", \"version\": \"\"}\n    _doc_args[\"capitalize\"] = {\n        \"type\": \"be capitalized\",\n        \"method\": \"capitalize\",\n        \"version\": \"\",\n    }\n    _doc_args[\"swapcase\"] = {\n        \"type\": \"be swapcased\",\n        \"method\": \"swapcase\",\n        \"version\": \"\",\n    }\n    _doc_args[\"casefold\"] = {\n        \"type\": \"be casefolded\",\n        \"method\": \"casefold\",\n        \"version\": \"\\n    .. versionadded:: 0.25.0\\n\",\n    }\n\n    @Appender(_shared_docs[\"casemethods\"] % _doc_args[\"lower\"])\n    @forbid_nonstring_types([\"bytes\"])\n    def lower(self):\n        result = self._array._str_lower()\n        return self._wrap_result(result)\n\n    @Appender(_shared_docs[\"casemethods\"] % _doc_args[\"upper\"])\n    @forbid_nonstring_types([\"bytes\"])\n    def upper(self):\n        result = self._array._str_upper()\n        return self._wrap_result(result)\n\n    @Appender(_shared_docs[\"casemethods\"] % _doc_args[\"title\"])\n    @forbid_nonstring_types([\"bytes\"])\n    def title(self):\n        result = self._array._str_title()\n        return self._wrap_result(result)\n\n    @Appender(_shared_docs[\"casemethods\"] % _doc_args[\"capitalize\"])\n    @forbid_nonstring_types([\"bytes\"])\n    def capitalize(self):\n        result = self._array._str_capitalize()\n        return self._wrap_result(result)\n\n    @Appender(_shared_docs[\"casemethods\"] % _doc_args[\"swapcase\"])\n    @forbid_nonstring_types([\"bytes\"])\n    def swapcase(self):\n        result = self._array._str_swapcase()\n        return self._wrap_result(result)\n\n    @Appender(_shared_docs[\"casemethods\"] % _doc_args[\"casefold\"])\n    @forbid_nonstring_types([\"bytes\"])\n    def casefold(self):\n        result = self._array._str_casefold()\n        return self._wrap_result(result)\n\n    _shared_docs[\n        \"ismethods\"\n    ] = \"\"\"\n    Check whether all characters in each string are %(type)s.\n\n    This is equivalent to running the Python string method\n    :meth:`str.%(method)s` for each element of the Series/Index. If a string\n    has zero characters, ``False`` is returned for that check.\n\n    Returns\n    -------\n    Series or Index of bool\n        Series or Index of boolean values with the same length as the original\n        Series/Index.\n\n    See Also\n    --------\n    Series.str.isalpha : Check whether all characters are alphabetic.\n    Series.str.isnumeric : Check whether all characters are numeric.\n    Series.str.isalnum : Check whether all characters are alphanumeric.\n    Series.str.isdigit : Check whether all characters are digits.\n    Series.str.isdecimal : Check whether all characters are decimal.\n    Series.str.isspace : Check whether all characters are whitespace.\n    Series.str.islower : Check whether all characters are lowercase.\n    Series.str.isupper : Check whether all characters are uppercase.\n    Series.str.istitle : Check whether all characters are titlecase.\n\n    Examples\n    --------\n    **Checks for Alphabetic and Numeric Characters**\n\n    >>> s1 = pd.Series(['one', 'one1', '1', ''])\n\n    >>> s1.str.isalpha()\n    0     True\n    1    False\n    2    False\n    3    False\n    dtype: bool\n\n    >>> s1.str.isnumeric()\n    0    False\n    1    False\n    2     True\n    3    False\n    dtype: bool\n\n    >>> s1.str.isalnum()\n    0     True\n    1     True\n    2     True\n    3    False\n    dtype: bool\n\n    Note that checks against characters mixed with any additional punctuation\n    or whitespace will evaluate to false for an alphanumeric check.\n\n    >>> s2 = pd.Series(['A B', '1.5', '3,000'])\n    >>> s2.str.isalnum()\n    0    False\n    1    False\n    2    False\n    dtype: bool\n\n    **More Detailed Checks for Numeric Characters**\n\n    There are several different but overlapping sets of numeric characters that\n    can be checked for.\n\n    >>> s3 = pd.Series(['23', '', '', ''])\n\n    The ``s3.str.isdecimal`` method checks for characters used to form numbers\n    in base 10.\n\n    >>> s3.str.isdecimal()\n    0     True\n    1    False\n    2    False\n    3    False\n    dtype: bool\n\n    The ``s.str.isdigit`` method is the same as ``s3.str.isdecimal`` but also\n    includes special digits, like superscripted and subscripted digits in\n    unicode.\n\n    >>> s3.str.isdigit()\n    0     True\n    1     True\n    2    False\n    3    False\n    dtype: bool\n\n    The ``s.str.isnumeric`` method is the same as ``s3.str.isdigit`` but also\n    includes other characters that can represent quantities such as unicode\n    fractions.\n\n    >>> s3.str.isnumeric()\n    0     True\n    1     True\n    2     True\n    3    False\n    dtype: bool\n\n    **Checks for Whitespace**\n\n    >>> s4 = pd.Series([' ', '\\\\t\\\\r\\\\n ', ''])\n    >>> s4.str.isspace()\n    0     True\n    1     True\n    2    False\n    dtype: bool\n\n    **Checks for Character Case**\n\n    >>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])\n\n    >>> s5.str.islower()\n    0     True\n    1    False\n    2    False\n    3    False\n    dtype: bool\n\n    >>> s5.str.isupper()\n    0    False\n    1    False\n    2     True\n    3    False\n    dtype: bool\n\n    The ``s5.str.istitle`` method checks for whether all words are in title\n    case (whether only the first letter of each word is capitalized). Words are\n    assumed to be as any sequence of non-numeric characters separated by\n    whitespace characters.\n\n    >>> s5.str.istitle()\n    0    False\n    1     True\n    2    False\n    3    False\n    dtype: bool\n    \"\"\"\n    _doc_args[\"isalnum\"] = {\"type\": \"alphanumeric\", \"method\": \"isalnum\"}\n    _doc_args[\"isalpha\"] = {\"type\": \"alphabetic\", \"method\": \"isalpha\"}\n    _doc_args[\"isdigit\"] = {\"type\": \"digits\", \"method\": \"isdigit\"}\n    _doc_args[\"isspace\"] = {\"type\": \"whitespace\", \"method\": \"isspace\"}\n    _doc_args[\"islower\"] = {\"type\": \"lowercase\", \"method\": \"islower\"}\n    _doc_args[\"isupper\"] = {\"type\": \"uppercase\", \"method\": \"isupper\"}\n    _doc_args[\"istitle\"] = {\"type\": \"titlecase\", \"method\": \"istitle\"}\n    _doc_args[\"isnumeric\"] = {\"type\": \"numeric\", \"method\": \"isnumeric\"}\n    _doc_args[\"isdecimal\"] = {\"type\": \"decimal\", \"method\": \"isdecimal\"}\n    # force _noarg_wrapper return type with dtype=np.dtype(bool) (GH 29624)\n\n    isalnum = _map_and_wrap(\n        \"isalnum\", docstring=_shared_docs[\"ismethods\"] % _doc_args[\"isalnum\"]\n    )\n    isalpha = _map_and_wrap(\n        \"isalpha\", docstring=_shared_docs[\"ismethods\"] % _doc_args[\"isalpha\"]\n    )\n    isdigit = _map_and_wrap(\n        \"isdigit\", docstring=_shared_docs[\"ismethods\"] % _doc_args[\"isdigit\"]\n    )\n    isspace = _map_and_wrap(\n        \"isspace\", docstring=_shared_docs[\"ismethods\"] % _doc_args[\"isalnum\"]\n    )\n    islower = _map_and_wrap(\n        \"islower\", docstring=_shared_docs[\"ismethods\"] % _doc_args[\"islower\"]\n    )\n    isupper = _map_and_wrap(\n        \"isupper\", docstring=_shared_docs[\"ismethods\"] % _doc_args[\"isupper\"]\n    )\n    istitle = _map_and_wrap(\n        \"istitle\", docstring=_shared_docs[\"ismethods\"] % _doc_args[\"istitle\"]\n    )\n    isnumeric = _map_and_wrap(\n        \"isnumeric\", docstring=_shared_docs[\"ismethods\"] % _doc_args[\"isnumeric\"]\n    )\n    isdecimal = _map_and_wrap(\n        \"isdecimal\", docstring=_shared_docs[\"ismethods\"] % _doc_args[\"isdecimal\"]\n    )\n\n\ndef cat_safe(list_of_columns: List, sep: str):\n    \"\"\"\n    Auxiliary function for :meth:`str.cat`.\n\n    Same signature as cat_core, but handles TypeErrors in concatenation, which\n    happen if the arrays in list_of columns have the wrong dtypes or content.\n\n    Parameters\n    ----------\n    list_of_columns : list of numpy arrays\n        List of arrays to be concatenated with sep;\n        these arrays may not contain NaNs!\n    sep : string\n        The separator string for concatenating the columns.\n\n    Returns\n    -------\n    nd.array\n        The concatenation of list_of_columns with sep.\n    \"\"\"\n    try:\n        result = cat_core(list_of_columns, sep)\n    except TypeError:\n        # if there are any non-string values (wrong dtype or hidden behind\n        # object dtype), np.sum will fail; catch and return with better message\n        for column in list_of_columns:\n            dtype = lib.infer_dtype(column, skipna=True)\n            if dtype not in [\"string\", \"empty\"]:\n                raise TypeError(\n                    \"Concatenation requires list-likes containing only \"\n                    \"strings (or missing values). Offending values found in \"\n                    f\"column {dtype}\"\n                ) from None\n    return result\n\n\ndef cat_core(list_of_columns: List, sep: str):\n    \"\"\"\n    Auxiliary function for :meth:`str.cat`\n\n    Parameters\n    ----------\n    list_of_columns : list of numpy arrays\n        List of arrays to be concatenated with sep;\n        these arrays may not contain NaNs!\n    sep : string\n        The separator string for concatenating the columns.\n\n    Returns\n    -------\n    nd.array\n        The concatenation of list_of_columns with sep.\n    \"\"\"\n    if sep == \"\":\n        # no need to interleave sep if it is empty\n        arr_of_cols = np.asarray(list_of_columns, dtype=object)\n        return np.sum(arr_of_cols, axis=0)\n    list_with_sep = [sep] * (2 * len(list_of_columns) - 1)\n    list_with_sep[::2] = list_of_columns\n    arr_with_sep = np.asarray(list_with_sep, dtype=object)\n    return np.sum(arr_with_sep, axis=0)\n\n\ndef _groups_or_na_fun(regex):\n    \"\"\"Used in both extract_noexpand and extract_frame\"\"\"\n    if regex.groups == 0:\n        raise ValueError(\"pattern contains no capture groups\")\n    empty_row = [np.nan] * regex.groups\n\n    def f(x):\n        if not isinstance(x, str):\n            return empty_row\n        m = regex.search(x)\n        if m:\n            return [np.nan if item is None else item for item in m.groups()]\n        else:\n            return empty_row\n\n    return f\n\n\ndef _result_dtype(arr):\n    # workaround #27953\n    # ideally we just pass `dtype=arr.dtype` unconditionally, but this fails\n    # when the list of values is empty.\n    from pandas.core.arrays.string_ import StringDtype\n\n    if isinstance(arr.dtype, StringDtype):\n        return arr.dtype.name\n    else:\n        return object\n\n\ndef _get_single_group_name(rx):\n    try:\n        return list(rx.groupindex.keys()).pop()\n    except IndexError:\n        return None\n\n\ndef _str_extract_noexpand(arr, pat, flags=0):\n    \"\"\"\n    Find groups in each string in the Series using passed regular\n    expression. This function is called from\n    str_extract(expand=False), and can return Series, DataFrame, or\n    Index.\n\n    \"\"\"\n    from pandas import DataFrame, array\n\n    regex = re.compile(pat, flags=flags)\n    groups_or_na = _groups_or_na_fun(regex)\n    result_dtype = _result_dtype(arr)\n\n    if regex.groups == 1:\n        result = np.array([groups_or_na(val)[0] for val in arr], dtype=object)\n        name = _get_single_group_name(regex)\n        # not dispatching, so we have to reconstruct here.\n        result = array(result, dtype=result_dtype)\n    else:\n        if isinstance(arr, ABCIndexClass):\n            raise ValueError(\"only one regex group is supported with Index\")\n        name = None\n        names = dict(zip(regex.groupindex.values(), regex.groupindex.keys()))\n        columns = [names.get(1 + i, i) for i in range(regex.groups)]\n        if arr.size == 0:\n            result = DataFrame(columns=columns, dtype=object)\n        else:\n            dtype = _result_dtype(arr)\n            result = DataFrame(\n                [groups_or_na(val) for val in arr],\n                columns=columns,\n                index=arr.index,\n                dtype=dtype,\n            )\n    return result, name\n\n\ndef _str_extract_frame(arr, pat, flags=0):\n    \"\"\"\n    For each subject string in the Series, extract groups from the\n    first match of regular expression pat. This function is called from\n    str_extract(expand=True), and always returns a DataFrame.\n\n    \"\"\"\n    from pandas import DataFrame\n\n    regex = re.compile(pat, flags=flags)\n    groups_or_na = _groups_or_na_fun(regex)\n    names = dict(zip(regex.groupindex.values(), regex.groupindex.keys()))\n    columns = [names.get(1 + i, i) for i in range(regex.groups)]\n\n    if len(arr) == 0:\n        return DataFrame(columns=columns, dtype=object)\n    try:\n        result_index = arr.index\n    except AttributeError:\n        result_index = None\n    dtype = _result_dtype(arr)\n    return DataFrame(\n        [groups_or_na(val) for val in arr],\n        columns=columns,\n        index=result_index,\n        dtype=dtype,\n    )\n\n\ndef str_extract(arr, pat, flags=0, expand=True):\n    if not isinstance(expand, bool):\n        raise ValueError(\"expand must be True or False\")\n    if expand:\n        result = _str_extract_frame(arr._orig, pat, flags=flags)\n        return result.__finalize__(arr._orig, method=\"str_extract\")\n    else:\n        result, name = _str_extract_noexpand(arr._orig, pat, flags=flags)\n        return arr._wrap_result(result, name=name, expand=expand)\n\n\ndef str_extractall(arr, pat, flags=0):\n    regex = re.compile(pat, flags=flags)\n    # the regex must contain capture groups.\n    if regex.groups == 0:\n        raise ValueError(\"pattern contains no capture groups\")\n\n    if isinstance(arr, ABCIndexClass):\n        arr = arr.to_series().reset_index(drop=True)\n\n    names = dict(zip(regex.groupindex.values(), regex.groupindex.keys()))\n    columns = [names.get(1 + i, i) for i in range(regex.groups)]\n    match_list = []\n    index_list = []\n    is_mi = arr.index.nlevels > 1\n\n    for subject_key, subject in arr.items():\n        if isinstance(subject, str):\n\n            if not is_mi:\n                subject_key = (subject_key,)\n\n            for match_i, match_tuple in enumerate(regex.findall(subject)):\n                if isinstance(match_tuple, str):\n                    match_tuple = (match_tuple,)\n                na_tuple = [np.NaN if group == \"\" else group for group in match_tuple]\n                match_list.append(na_tuple)\n                result_key = tuple(subject_key + (match_i,))\n                index_list.append(result_key)\n\n    from pandas import MultiIndex\n\n    index = MultiIndex.from_tuples(index_list, names=arr.index.names + [\"match\"])\n    dtype = _result_dtype(arr)\n\n    result = arr._constructor_expanddim(\n        match_list, index=index, columns=columns, dtype=dtype\n    )\n    return result\n"
    },
    {
      "filename": "pandas/io/pytables.py",
      "content": "\"\"\"\nHigh level interface to PyTables for reading and writing pandas data structures\nto disk\n\"\"\"\nfrom contextlib import suppress\nimport copy\nfrom datetime import date, tzinfo\nimport itertools\nimport os\nimport re\nfrom textwrap import dedent\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import config, get_option\n\nfrom pandas._libs import lib, writers as libwriters\nfrom pandas._libs.tslibs import timezones\nfrom pandas._typing import ArrayLike, FrameOrSeries, FrameOrSeriesUnion, Label, Shape\nfrom pandas.compat._optional import import_optional_dependency\nfrom pandas.compat.pickle_compat import patch_pickle\nfrom pandas.errors import PerformanceWarning\nfrom pandas.util._decorators import cache_readonly\n\nfrom pandas.core.dtypes.common import (\n    ensure_object,\n    is_categorical_dtype,\n    is_complex_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_extension_array_dtype,\n    is_list_like,\n    is_string_dtype,\n    is_timedelta64_dtype,\n    needs_i8_conversion,\n)\nfrom pandas.core.dtypes.missing import array_equivalent\n\nfrom pandas import (\n    DataFrame,\n    DatetimeIndex,\n    Index,\n    Int64Index,\n    MultiIndex,\n    PeriodIndex,\n    Series,\n    TimedeltaIndex,\n    concat,\n    isna,\n)\nfrom pandas.core.arrays import Categorical, DatetimeArray, PeriodArray\nimport pandas.core.common as com\nfrom pandas.core.computation.pytables import PyTablesExpr, maybe_expression\nfrom pandas.core.construction import extract_array\nfrom pandas.core.indexes.api import ensure_index\n\nfrom pandas.io.common import stringify_path\nfrom pandas.io.formats.printing import adjoin, pprint_thing\n\nif TYPE_CHECKING:\n    from tables import Col, File, Node\n\n\n# versioning attribute\n_version = \"0.15.2\"\n\n# encoding\n_default_encoding = \"UTF-8\"\n\n\ndef _ensure_decoded(s):\n    \"\"\" if we have bytes, decode them to unicode \"\"\"\n    if isinstance(s, np.bytes_):\n        s = s.decode(\"UTF-8\")\n    return s\n\n\ndef _ensure_encoding(encoding):\n    # set the encoding if we need\n    if encoding is None:\n        encoding = _default_encoding\n\n    return encoding\n\n\ndef _ensure_str(name):\n    \"\"\"\n    Ensure that an index / column name is a str (python 3); otherwise they\n    may be np.string dtype. Non-string dtypes are passed through unchanged.\n\n    https://github.com/pandas-dev/pandas/issues/13492\n    \"\"\"\n    if isinstance(name, str):\n        name = str(name)\n    return name\n\n\nTerm = PyTablesExpr\n\n\ndef _ensure_term(where, scope_level: int):\n    \"\"\"\n    Ensure that the where is a Term or a list of Term.\n\n    This makes sure that we are capturing the scope of variables that are\n    passed create the terms here with a frame_level=2 (we are 2 levels down)\n    \"\"\"\n    # only consider list/tuple here as an ndarray is automatically a coordinate\n    # list\n    level = scope_level + 1\n    if isinstance(where, (list, tuple)):\n        where = [\n            Term(term, scope_level=level + 1) if maybe_expression(term) else term\n            for term in where\n            if term is not None\n        ]\n    elif maybe_expression(where):\n        where = Term(where, scope_level=level)\n    return where if where is None or len(where) else None\n\n\nclass PossibleDataLossError(Exception):\n    pass\n\n\nclass ClosedFileError(Exception):\n    pass\n\n\nclass IncompatibilityWarning(Warning):\n    pass\n\n\nincompatibility_doc = \"\"\"\nwhere criteria is being ignored as this version [%s] is too old (or\nnot-defined), read the file in and write it out to a new file to upgrade (with\nthe copy_to method)\n\"\"\"\n\n\nclass AttributeConflictWarning(Warning):\n    pass\n\n\nattribute_conflict_doc = \"\"\"\nthe [%s] attribute of the existing index is [%s] which conflicts with the new\n[%s], resetting the attribute to None\n\"\"\"\n\n\nclass DuplicateWarning(Warning):\n    pass\n\n\nduplicate_doc = \"\"\"\nduplicate entries in table, taking most recently appended\n\"\"\"\n\nperformance_doc = \"\"\"\nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type->%s,key->%s] [items->%s]\n\"\"\"\n\n# formats\n_FORMAT_MAP = {\"f\": \"fixed\", \"fixed\": \"fixed\", \"t\": \"table\", \"table\": \"table\"}\n\n# axes map\n_AXES_MAP = {DataFrame: [0]}\n\n# register our configuration options\ndropna_doc = \"\"\"\n: boolean\n    drop ALL nan rows when appending to a table\n\"\"\"\nformat_doc = \"\"\"\n: format\n    default format writing format, if None, then\n    put will default to 'fixed' and append will default to 'table'\n\"\"\"\n\nwith config.config_prefix(\"io.hdf\"):\n    config.register_option(\"dropna_table\", False, dropna_doc, validator=config.is_bool)\n    config.register_option(\n        \"default_format\",\n        None,\n        format_doc,\n        validator=config.is_one_of_factory([\"fixed\", \"table\", None]),\n    )\n\n# oh the troubles to reduce import time\n_table_mod = None\n_table_file_open_policy_is_strict = False\n\n\ndef _tables():\n    global _table_mod\n    global _table_file_open_policy_is_strict\n    if _table_mod is None:\n        import tables\n\n        _table_mod = tables\n\n        # set the file open policy\n        # return the file open policy; this changes as of pytables 3.1\n        # depending on the HDF5 version\n        with suppress(AttributeError):\n            _table_file_open_policy_is_strict = (\n                tables.file._FILE_OPEN_POLICY == \"strict\"\n            )\n\n    return _table_mod\n\n\n# interface to/from ###\n\n\ndef to_hdf(\n    path_or_buf,\n    key: str,\n    value: FrameOrSeries,\n    mode: str = \"a\",\n    complevel: Optional[int] = None,\n    complib: Optional[str] = None,\n    append: bool = False,\n    format: Optional[str] = None,\n    index: bool = True,\n    min_itemsize: Optional[Union[int, Dict[str, int]]] = None,\n    nan_rep=None,\n    dropna: Optional[bool] = None,\n    data_columns: Optional[Union[bool, List[str]]] = None,\n    errors: str = \"strict\",\n    encoding: str = \"UTF-8\",\n):\n    \"\"\" store this object, close it if we opened it \"\"\"\n    if append:\n        f = lambda store: store.append(\n            key,\n            value,\n            format=format,\n            index=index,\n            min_itemsize=min_itemsize,\n            nan_rep=nan_rep,\n            dropna=dropna,\n            data_columns=data_columns,\n            errors=errors,\n            encoding=encoding,\n        )\n    else:\n        # NB: dropna is not passed to `put`\n        f = lambda store: store.put(\n            key,\n            value,\n            format=format,\n            index=index,\n            min_itemsize=min_itemsize,\n            nan_rep=nan_rep,\n            data_columns=data_columns,\n            errors=errors,\n            encoding=encoding,\n            dropna=dropna,\n        )\n\n    path_or_buf = stringify_path(path_or_buf)\n    if isinstance(path_or_buf, str):\n        with HDFStore(\n            path_or_buf, mode=mode, complevel=complevel, complib=complib\n        ) as store:\n            f(store)\n    else:\n        f(path_or_buf)\n\n\ndef read_hdf(\n    path_or_buf,\n    key=None,\n    mode: str = \"r\",\n    errors: str = \"strict\",\n    where=None,\n    start: Optional[int] = None,\n    stop: Optional[int] = None,\n    columns=None,\n    iterator=False,\n    chunksize: Optional[int] = None,\n    **kwargs,\n):\n    \"\"\"\n    Read from the store, close it if we opened it.\n\n    Retrieve pandas object stored in file, optionally based on where\n    criteria.\n\n    .. warning::\n\n       Pandas uses PyTables for reading and writing HDF5 files, which allows\n       serializing object-dtype data with pickle when using the \"fixed\" format.\n       Loading pickled data received from untrusted sources can be unsafe.\n\n       See: https://docs.python.org/3/library/pickle.html for more.\n\n    Parameters\n    ----------\n    path_or_buf : str, path object, pandas.HDFStore or file-like object\n        Any valid string path is acceptable. The string could be a URL. Valid\n        URL schemes include http, ftp, s3, and file. For file URLs, a host is\n        expected. A local file could be: ``file://localhost/path/to/table.h5``.\n\n        If you want to pass in a path object, pandas accepts any\n        ``os.PathLike``.\n\n        Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.\n\n        By file-like object, we refer to objects with a ``read()`` method,\n        such as a file handle (e.g. via builtin ``open`` function)\n        or ``StringIO``.\n    key : object, optional\n        The group identifier in the store. Can be omitted if the HDF file\n        contains a single pandas object.\n    mode : {'r', 'r+', 'a'}, default 'r'\n        Mode to use when opening the file. Ignored if path_or_buf is a\n        :class:`pandas.HDFStore`. Default is 'r'.\n    errors : str, default 'strict'\n        Specifies how encoding and decoding errors are to be handled.\n        See the errors argument for :func:`open` for a full list\n        of options.\n    where : list, optional\n        A list of Term (or convertible) objects.\n    start : int, optional\n        Row number to start selection.\n    stop  : int, optional\n        Row number to stop selection.\n    columns : list, optional\n        A list of columns names to return.\n    iterator : bool, optional\n        Return an iterator object.\n    chunksize : int, optional\n        Number of rows to include in an iteration when using an iterator.\n    **kwargs\n        Additional keyword arguments passed to HDFStore.\n\n    Returns\n    -------\n    item : object\n        The selected object. Return type depends on the object stored.\n\n    See Also\n    --------\n    DataFrame.to_hdf : Write a HDF file from a DataFrame.\n    HDFStore : Low-level access to HDF files.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])\n    >>> df.to_hdf('./store.h5', 'data')\n    >>> reread = pd.read_hdf('./store.h5')\n    \"\"\"\n    if mode not in [\"r\", \"r+\", \"a\"]:\n        raise ValueError(\n            f\"mode {mode} is not allowed while performing a read. \"\n            f\"Allowed modes are r, r+ and a.\"\n        )\n    # grab the scope\n    if where is not None:\n        where = _ensure_term(where, scope_level=1)\n\n    if isinstance(path_or_buf, HDFStore):\n        if not path_or_buf.is_open:\n            raise OSError(\"The HDFStore must be open for reading.\")\n\n        store = path_or_buf\n        auto_close = False\n    else:\n        path_or_buf = stringify_path(path_or_buf)\n        if not isinstance(path_or_buf, str):\n            raise NotImplementedError(\n                \"Support for generic buffers has not been implemented.\"\n            )\n        try:\n            exists = os.path.exists(path_or_buf)\n\n        # if filepath is too long\n        except (TypeError, ValueError):\n            exists = False\n\n        if not exists:\n            raise FileNotFoundError(f\"File {path_or_buf} does not exist\")\n\n        store = HDFStore(path_or_buf, mode=mode, errors=errors, **kwargs)\n        # can't auto open/close if we are using an iterator\n        # so delegate to the iterator\n        auto_close = True\n\n    try:\n        if key is None:\n            groups = store.groups()\n            if len(groups) == 0:\n                raise ValueError(\n                    \"Dataset(s) incompatible with Pandas data types, \"\n                    \"not table, or no datasets found in HDF5 file.\"\n                )\n            candidate_only_group = groups[0]\n\n            # For the HDF file to have only one dataset, all other groups\n            # should then be metadata groups for that candidate group. (This\n            # assumes that the groups() method enumerates parent groups\n            # before their children.)\n            for group_to_check in groups[1:]:\n                if not _is_metadata_of(group_to_check, candidate_only_group):\n                    raise ValueError(\n                        \"key must be provided when HDF5 \"\n                        \"file contains multiple datasets.\"\n                    )\n            key = candidate_only_group._v_pathname\n        return store.select(\n            key,\n            where=where,\n            start=start,\n            stop=stop,\n            columns=columns,\n            iterator=iterator,\n            chunksize=chunksize,\n            auto_close=auto_close,\n        )\n    except (ValueError, TypeError, KeyError):\n        if not isinstance(path_or_buf, HDFStore):\n            # if there is an error, close the store if we opened it.\n            with suppress(AttributeError):\n                store.close()\n\n        raise\n\n\ndef _is_metadata_of(group: \"Node\", parent_group: \"Node\") -> bool:\n    \"\"\"Check if a given group is a metadata group for a given parent_group.\"\"\"\n    if group._v_depth <= parent_group._v_depth:\n        return False\n\n    current = group\n    while current._v_depth > 1:\n        parent = current._v_parent\n        if parent == parent_group and current._v_name == \"meta\":\n            return True\n        current = current._v_parent\n    return False\n\n\nclass HDFStore:\n    \"\"\"\n    Dict-like IO interface for storing pandas objects in PyTables.\n\n    Either Fixed or Table format.\n\n    .. warning::\n\n       Pandas uses PyTables for reading and writing HDF5 files, which allows\n       serializing object-dtype data with pickle when using the \"fixed\" format.\n       Loading pickled data received from untrusted sources can be unsafe.\n\n       See: https://docs.python.org/3/library/pickle.html for more.\n\n    Parameters\n    ----------\n    path : str\n        File path to HDF5 file.\n    mode : {'a', 'w', 'r', 'r+'}, default 'a'\n\n        ``'r'``\n            Read-only; no data can be modified.\n        ``'w'``\n            Write; a new file is created (an existing file with the same\n            name would be deleted).\n        ``'a'``\n            Append; an existing file is opened for reading and writing,\n            and if the file does not exist it is created.\n        ``'r+'``\n            It is similar to ``'a'``, but the file must already exist.\n    complevel : int, 0-9, default None\n        Specifies a compression level for data.\n        A value of 0 or None disables compression.\n    complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'\n        Specifies the compression library to be used.\n        As of v0.20.2 these additional compressors for Blosc are supported\n        (default if no compressor specified: 'blosc:blosclz'):\n        {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',\n         'blosc:zlib', 'blosc:zstd'}.\n        Specifying a compression library which is not available issues\n        a ValueError.\n    fletcher32 : bool, default False\n        If applying compression use the fletcher32 checksum.\n    **kwargs\n        These parameters will be passed to the PyTables open_file method.\n\n    Examples\n    --------\n    >>> bar = pd.DataFrame(np.random.randn(10, 4))\n    >>> store = pd.HDFStore('test.h5')\n    >>> store['foo'] = bar   # write to HDF5\n    >>> bar = store['foo']   # retrieve\n    >>> store.close()\n\n    **Create or load HDF5 file in-memory**\n\n    When passing the `driver` option to the PyTables open_file method through\n    **kwargs, the HDF5 file is loaded or created in-memory and will only be\n    written when closed:\n\n    >>> bar = pd.DataFrame(np.random.randn(10, 4))\n    >>> store = pd.HDFStore('test.h5', driver='H5FD_CORE')\n    >>> store['foo'] = bar\n    >>> store.close()   # only now, data is written to disk\n    \"\"\"\n\n    _handle: Optional[\"File\"]\n    _mode: str\n    _complevel: int\n    _fletcher32: bool\n\n    def __init__(\n        self,\n        path,\n        mode: str = \"a\",\n        complevel: Optional[int] = None,\n        complib=None,\n        fletcher32: bool = False,\n        **kwargs,\n    ):\n\n        if \"format\" in kwargs:\n            raise ValueError(\"format is not a defined argument for HDFStore\")\n\n        tables = import_optional_dependency(\"tables\")\n\n        if complib is not None and complib not in tables.filters.all_complibs:\n            raise ValueError(\n                f\"complib only supports {tables.filters.all_complibs} compression.\"\n            )\n\n        if complib is None and complevel is not None:\n            complib = tables.filters.default_complib\n\n        self._path = stringify_path(path)\n        if mode is None:\n            mode = \"a\"\n        self._mode = mode\n        self._handle = None\n        self._complevel = complevel if complevel else 0\n        self._complib = complib\n        self._fletcher32 = fletcher32\n        self._filters = None\n        self.open(mode=mode, **kwargs)\n\n    def __fspath__(self):\n        return self._path\n\n    @property\n    def root(self):\n        \"\"\" return the root node \"\"\"\n        self._check_if_open()\n        assert self._handle is not None  # for mypy\n        return self._handle.root\n\n    @property\n    def filename(self):\n        return self._path\n\n    def __getitem__(self, key: str):\n        return self.get(key)\n\n    def __setitem__(self, key: str, value):\n        self.put(key, value)\n\n    def __delitem__(self, key: str):\n        return self.remove(key)\n\n    def __getattr__(self, name: str):\n        \"\"\" allow attribute access to get stores \"\"\"\n        try:\n            return self.get(name)\n        except (KeyError, ClosedFileError):\n            pass\n        raise AttributeError(\n            f\"'{type(self).__name__}' object has no attribute '{name}'\"\n        )\n\n    def __contains__(self, key: str) -> bool:\n        \"\"\"\n        check for existence of this key\n        can match the exact pathname or the pathnm w/o the leading '/'\n        \"\"\"\n        node = self.get_node(key)\n        if node is not None:\n            name = node._v_pathname\n            if name == key or name[1:] == key:\n                return True\n        return False\n\n    def __len__(self) -> int:\n        return len(self.groups())\n\n    def __repr__(self) -> str:\n        pstr = pprint_thing(self._path)\n        return f\"{type(self)}\\nFile path: {pstr}\\n\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def keys(self, include: str = \"pandas\") -> List[str]:\n        \"\"\"\n        Return a list of keys corresponding to objects stored in HDFStore.\n\n        Parameters\n        ----------\n\n        include : str, default 'pandas'\n                When kind equals 'pandas' return pandas objects.\n                When kind equals 'native' return native HDF5 Table objects.\n\n                .. versionadded:: 1.1.0\n\n        Returns\n        -------\n        list\n            List of ABSOLUTE path-names (e.g. have the leading '/').\n\n        Raises\n        ------\n        raises ValueError if kind has an illegal value\n        \"\"\"\n        if include == \"pandas\":\n            return [n._v_pathname for n in self.groups()]\n\n        elif include == \"native\":\n            assert self._handle is not None  # mypy\n            return [\n                n._v_pathname for n in self._handle.walk_nodes(\"/\", classname=\"Table\")\n            ]\n        raise ValueError(\n            f\"`include` should be either 'pandas' or 'native' but is '{include}'\"\n        )\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def items(self):\n        \"\"\"\n        iterate on key->group\n        \"\"\"\n        for g in self.groups():\n            yield g._v_pathname, g\n\n    iteritems = items\n\n    def open(self, mode: str = \"a\", **kwargs):\n        \"\"\"\n        Open the file in the specified mode\n\n        Parameters\n        ----------\n        mode : {'a', 'w', 'r', 'r+'}, default 'a'\n            See HDFStore docstring or tables.open_file for info about modes\n        **kwargs\n            These parameters will be passed to the PyTables open_file method.\n        \"\"\"\n        tables = _tables()\n\n        if self._mode != mode:\n            # if we are changing a write mode to read, ok\n            if self._mode in [\"a\", \"w\"] and mode in [\"r\", \"r+\"]:\n                pass\n            elif mode in [\"w\"]:\n                # this would truncate, raise here\n                if self.is_open:\n                    raise PossibleDataLossError(\n                        f\"Re-opening the file [{self._path}] with mode [{self._mode}] \"\n                        \"will delete the current file!\"\n                    )\n\n            self._mode = mode\n\n        # close and reopen the handle\n        if self.is_open:\n            self.close()\n\n        if self._complevel and self._complevel > 0:\n            self._filters = _tables().Filters(\n                self._complevel, self._complib, fletcher32=self._fletcher32\n            )\n\n        if _table_file_open_policy_is_strict and self.is_open:\n            msg = (\n                \"Cannot open HDF5 file, which is already opened, \"\n                \"even in read-only mode.\"\n            )\n            raise ValueError(msg)\n\n        self._handle = tables.open_file(self._path, self._mode, **kwargs)\n\n    def close(self):\n        \"\"\"\n        Close the PyTables file handle\n        \"\"\"\n        if self._handle is not None:\n            self._handle.close()\n        self._handle = None\n\n    @property\n    def is_open(self) -> bool:\n        \"\"\"\n        return a boolean indicating whether the file is open\n        \"\"\"\n        if self._handle is None:\n            return False\n        return bool(self._handle.isopen)\n\n    def flush(self, fsync: bool = False):\n        \"\"\"\n        Force all buffered modifications to be written to disk.\n\n        Parameters\n        ----------\n        fsync : bool (default False)\n          call ``os.fsync()`` on the file handle to force writing to disk.\n\n        Notes\n        -----\n        Without ``fsync=True``, flushing may not guarantee that the OS writes\n        to disk. With fsync, the operation will block until the OS claims the\n        file has been written; however, other caching layers may still\n        interfere.\n        \"\"\"\n        if self._handle is not None:\n            self._handle.flush()\n            if fsync:\n                with suppress(OSError):\n                    os.fsync(self._handle.fileno())\n\n    def get(self, key: str):\n        \"\"\"\n        Retrieve pandas object stored in file.\n\n        Parameters\n        ----------\n        key : str\n\n        Returns\n        -------\n        object\n            Same type as object stored in file.\n        \"\"\"\n        with patch_pickle():\n            # GH#31167 Without this patch, pickle doesn't know how to unpickle\n            #  old DateOffset objects now that they are cdef classes.\n            group = self.get_node(key)\n            if group is None:\n                raise KeyError(f\"No object named {key} in the file\")\n            return self._read_group(group)\n\n    def select(\n        self,\n        key: str,\n        where=None,\n        start=None,\n        stop=None,\n        columns=None,\n        iterator=False,\n        chunksize=None,\n        auto_close: bool = False,\n    ):\n        \"\"\"\n        Retrieve pandas object stored in file, optionally based on where criteria.\n\n        .. warning::\n\n           Pandas uses PyTables for reading and writing HDF5 files, which allows\n           serializing object-dtype data with pickle when using the \"fixed\" format.\n           Loading pickled data received from untrusted sources can be unsafe.\n\n           See: https://docs.python.org/3/library/pickle.html for more.\n\n        Parameters\n        ----------\n        key : str\n            Object being retrieved from file.\n        where : list or None\n            List of Term (or convertible) objects, optional.\n        start : int or None\n            Row number to start selection.\n        stop : int, default None\n            Row number to stop selection.\n        columns : list or None\n            A list of columns that if not None, will limit the return columns.\n        iterator : bool or False\n            Returns an iterator.\n        chunksize : int or None\n            Number or rows to include in iteration, return an iterator.\n        auto_close : bool or False\n            Should automatically close the store when finished.\n\n        Returns\n        -------\n        object\n            Retrieved object from file.\n        \"\"\"\n        group = self.get_node(key)\n        if group is None:\n            raise KeyError(f\"No object named {key} in the file\")\n\n        # create the storer and axes\n        where = _ensure_term(where, scope_level=1)\n        s = self._create_storer(group)\n        s.infer_axes()\n\n        # function to call on iteration\n        def func(_start, _stop, _where):\n            return s.read(start=_start, stop=_stop, where=_where, columns=columns)\n\n        # create the iterator\n        it = TableIterator(\n            self,\n            s,\n            func,\n            where=where,\n            nrows=s.nrows,\n            start=start,\n            stop=stop,\n            iterator=iterator,\n            chunksize=chunksize,\n            auto_close=auto_close,\n        )\n\n        return it.get_result()\n\n    def select_as_coordinates(\n        self,\n        key: str,\n        where=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n        \"\"\"\n        return the selection as an Index\n\n        .. warning::\n\n           Pandas uses PyTables for reading and writing HDF5 files, which allows\n           serializing object-dtype data with pickle when using the \"fixed\" format.\n           Loading pickled data received from untrusted sources can be unsafe.\n\n           See: https://docs.python.org/3/library/pickle.html for more.\n\n\n        Parameters\n        ----------\n        key : str\n        where : list of Term (or convertible) objects, optional\n        start : integer (defaults to None), row number to start selection\n        stop  : integer (defaults to None), row number to stop selection\n        \"\"\"\n        where = _ensure_term(where, scope_level=1)\n        tbl = self.get_storer(key)\n        if not isinstance(tbl, Table):\n            raise TypeError(\"can only read_coordinates with a table\")\n        return tbl.read_coordinates(where=where, start=start, stop=stop)\n\n    def select_column(\n        self,\n        key: str,\n        column: str,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n        \"\"\"\n        return a single column from the table. This is generally only useful to\n        select an indexable\n\n        .. warning::\n\n           Pandas uses PyTables for reading and writing HDF5 files, which allows\n           serializing object-dtype data with pickle when using the \"fixed\" format.\n           Loading pickled data received from untrusted sources can be unsafe.\n\n           See: https://docs.python.org/3/library/pickle.html for more.\n\n        Parameters\n        ----------\n        key : str\n        column : str\n            The column of interest.\n        start : int or None, default None\n        stop : int or None, default None\n\n        Raises\n        ------\n        raises KeyError if the column is not found (or key is not a valid\n            store)\n        raises ValueError if the column can not be extracted individually (it\n            is part of a data block)\n\n        \"\"\"\n        tbl = self.get_storer(key)\n        if not isinstance(tbl, Table):\n            raise TypeError(\"can only read_column with a table\")\n        return tbl.read_column(column=column, start=start, stop=stop)\n\n    def select_as_multiple(\n        self,\n        keys,\n        where=None,\n        selector=None,\n        columns=None,\n        start=None,\n        stop=None,\n        iterator=False,\n        chunksize=None,\n        auto_close: bool = False,\n    ):\n        \"\"\"\n        Retrieve pandas objects from multiple tables.\n\n        .. warning::\n\n           Pandas uses PyTables for reading and writing HDF5 files, which allows\n           serializing object-dtype data with pickle when using the \"fixed\" format.\n           Loading pickled data received from untrusted sources can be unsafe.\n\n           See: https://docs.python.org/3/library/pickle.html for more.\n\n        Parameters\n        ----------\n        keys : a list of the tables\n        selector : the table to apply the where criteria (defaults to keys[0]\n            if not supplied)\n        columns : the columns I want back\n        start : integer (defaults to None), row number to start selection\n        stop  : integer (defaults to None), row number to stop selection\n        iterator : boolean, return an iterator, default False\n        chunksize : nrows to include in iteration, return an iterator\n        auto_close : bool, default False\n            Should automatically close the store when finished.\n\n        Raises\n        ------\n        raises KeyError if keys or selector is not found or keys is empty\n        raises TypeError if keys is not a list or tuple\n        raises ValueError if the tables are not ALL THE SAME DIMENSIONS\n        \"\"\"\n        # default to single select\n        where = _ensure_term(where, scope_level=1)\n        if isinstance(keys, (list, tuple)) and len(keys) == 1:\n            keys = keys[0]\n        if isinstance(keys, str):\n            return self.select(\n                key=keys,\n                where=where,\n                columns=columns,\n                start=start,\n                stop=stop,\n                iterator=iterator,\n                chunksize=chunksize,\n                auto_close=auto_close,\n            )\n\n        if not isinstance(keys, (list, tuple)):\n            raise TypeError(\"keys must be a list/tuple\")\n\n        if not len(keys):\n            raise ValueError(\"keys must have a non-zero length\")\n\n        if selector is None:\n            selector = keys[0]\n\n        # collect the tables\n        tbls = [self.get_storer(k) for k in keys]\n        s = self.get_storer(selector)\n\n        # validate rows\n        nrows = None\n        for t, k in itertools.chain([(s, selector)], zip(tbls, keys)):\n            if t is None:\n                raise KeyError(f\"Invalid table [{k}]\")\n            if not t.is_table:\n                raise TypeError(\n                    f\"object [{t.pathname}] is not a table, and cannot be used in all \"\n                    \"select as multiple\"\n                )\n\n            if nrows is None:\n                nrows = t.nrows\n            elif t.nrows != nrows:\n                raise ValueError(\"all tables must have exactly the same nrows!\")\n\n        # The isinstance checks here are redundant with the check above,\n        #  but necessary for mypy; see GH#29757\n        _tbls = [x for x in tbls if isinstance(x, Table)]\n\n        # axis is the concentration axes\n        axis = list({t.non_index_axes[0][0] for t in _tbls})[0]\n\n        def func(_start, _stop, _where):\n\n            # retrieve the objs, _where is always passed as a set of\n            # coordinates here\n            objs = [\n                t.read(where=_where, columns=columns, start=_start, stop=_stop)\n                for t in tbls\n            ]\n\n            # concat and return\n            return concat(objs, axis=axis, verify_integrity=False)._consolidate()\n\n        # create the iterator\n        it = TableIterator(\n            self,\n            s,\n            func,\n            where=where,\n            nrows=nrows,\n            start=start,\n            stop=stop,\n            iterator=iterator,\n            chunksize=chunksize,\n            auto_close=auto_close,\n        )\n\n        return it.get_result(coordinates=True)\n\n    def put(\n        self,\n        key: str,\n        value: FrameOrSeries,\n        format=None,\n        index=True,\n        append=False,\n        complib=None,\n        complevel: Optional[int] = None,\n        min_itemsize: Optional[Union[int, Dict[str, int]]] = None,\n        nan_rep=None,\n        data_columns: Optional[List[str]] = None,\n        encoding=None,\n        errors: str = \"strict\",\n        track_times: bool = True,\n        dropna: bool = False,\n    ):\n        \"\"\"\n        Store object in HDFStore.\n\n        Parameters\n        ----------\n        key : str\n        value : {Series, DataFrame}\n        format : 'fixed(f)|table(t)', default is 'fixed'\n            Format to use when storing object in HDFStore. Value can be one of:\n\n            ``'fixed'``\n                Fixed format.  Fast writing/reading. Not-appendable, nor searchable.\n            ``'table'``\n                Table format.  Write as a PyTables Table structure which may perform\n                worse but allow more flexible operations like searching / selecting\n                subsets of the data.\n        append : bool, default False\n            This will force Table format, append the input data to the existing.\n        data_columns : list, default None\n            List of columns to create as data columns, or True to use all columns.\n            See `here\n            <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#query-via-data-columns>`__.\n        encoding : str, default None\n            Provide an encoding for strings.\n        track_times : bool, default True\n            Parameter is propagated to 'create_table' method of 'PyTables'.\n            If set to False it enables to have the same h5 files (same hashes)\n            independent on creation time.\n\n            .. versionadded:: 1.1.0\n        \"\"\"\n        if format is None:\n            format = get_option(\"io.hdf.default_format\") or \"fixed\"\n        format = self._validate_format(format)\n        self._write_to_group(\n            key,\n            value,\n            format=format,\n            index=index,\n            append=append,\n            complib=complib,\n            complevel=complevel,\n            min_itemsize=min_itemsize,\n            nan_rep=nan_rep,\n            data_columns=data_columns,\n            encoding=encoding,\n            errors=errors,\n            track_times=track_times,\n            dropna=dropna,\n        )\n\n    def remove(self, key: str, where=None, start=None, stop=None):\n        \"\"\"\n        Remove pandas object partially by specifying the where condition\n\n        Parameters\n        ----------\n        key : string\n            Node to remove or delete rows from\n        where : list of Term (or convertible) objects, optional\n        start : integer (defaults to None), row number to start selection\n        stop  : integer (defaults to None), row number to stop selection\n\n        Returns\n        -------\n        number of rows removed (or None if not a Table)\n\n        Raises\n        ------\n        raises KeyError if key is not a valid store\n\n        \"\"\"\n        where = _ensure_term(where, scope_level=1)\n        try:\n            s = self.get_storer(key)\n        except KeyError:\n            # the key is not a valid store, re-raising KeyError\n            raise\n        except AssertionError:\n            # surface any assertion errors for e.g. debugging\n            raise\n        except Exception as err:\n            # In tests we get here with ClosedFileError, TypeError, and\n            #  _table_mod.NoSuchNodeError.  TODO: Catch only these?\n\n            if where is not None:\n                raise ValueError(\n                    \"trying to remove a node with a non-None where clause!\"\n                ) from err\n\n            # we are actually trying to remove a node (with children)\n            node = self.get_node(key)\n            if node is not None:\n                node._f_remove(recursive=True)\n                return None\n\n        # remove the node\n        if com.all_none(where, start, stop):\n            s.group._f_remove(recursive=True)\n\n        # delete from the table\n        else:\n            if not s.is_table:\n                raise ValueError(\n                    \"can only remove with where on objects written as tables\"\n                )\n            return s.delete(where=where, start=start, stop=stop)\n\n    def append(\n        self,\n        key: str,\n        value: FrameOrSeries,\n        format=None,\n        axes=None,\n        index=True,\n        append=True,\n        complib=None,\n        complevel: Optional[int] = None,\n        columns=None,\n        min_itemsize: Optional[Union[int, Dict[str, int]]] = None,\n        nan_rep=None,\n        chunksize=None,\n        expectedrows=None,\n        dropna: Optional[bool] = None,\n        data_columns: Optional[List[str]] = None,\n        encoding=None,\n        errors: str = \"strict\",\n    ):\n        \"\"\"\n        Append to Table in file. Node must already exist and be Table\n        format.\n\n        Parameters\n        ----------\n        key : str\n        value : {Series, DataFrame}\n        format : 'table' is the default\n            Format to use when storing object in HDFStore.  Value can be one of:\n\n            ``'table'``\n                Table format. Write as a PyTables Table structure which may perform\n                worse but allow more flexible operations like searching / selecting\n                subsets of the data.\n        append       : bool, default True\n            Append the input data to the existing.\n        data_columns : list of columns, or True, default None\n            List of columns to create as indexed data columns for on-disk\n            queries, or True to use all columns. By default only the axes\n            of the object are indexed. See `here\n            <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#query-via-data-columns>`__.\n        min_itemsize : dict of columns that specify minimum str sizes\n        nan_rep      : str to use as str nan representation\n        chunksize    : size to chunk the writing\n        expectedrows : expected TOTAL row size of this table\n        encoding     : default None, provide an encoding for str\n        dropna : bool, default False\n            Do not write an ALL nan row to the store settable\n            by the option 'io.hdf.dropna_table'.\n\n        Notes\n        -----\n        Does *not* check if data being appended overlaps with existing\n        data in the table, so be careful\n        \"\"\"\n        if columns is not None:\n            raise TypeError(\n                \"columns is not a supported keyword in append, try data_columns\"\n            )\n\n        if dropna is None:\n            dropna = get_option(\"io.hdf.dropna_table\")\n        if format is None:\n            format = get_option(\"io.hdf.default_format\") or \"table\"\n        format = self._validate_format(format)\n        self._write_to_group(\n            key,\n            value,\n            format=format,\n            axes=axes,\n            index=index,\n            append=append,\n            complib=complib,\n            complevel=complevel,\n            min_itemsize=min_itemsize,\n            nan_rep=nan_rep,\n            chunksize=chunksize,\n            expectedrows=expectedrows,\n            dropna=dropna,\n            data_columns=data_columns,\n            encoding=encoding,\n            errors=errors,\n        )\n\n    def append_to_multiple(\n        self,\n        d: Dict,\n        value,\n        selector,\n        data_columns=None,\n        axes=None,\n        dropna=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Append to multiple tables\n\n        Parameters\n        ----------\n        d : a dict of table_name to table_columns, None is acceptable as the\n            values of one node (this will get all the remaining columns)\n        value : a pandas object\n        selector : a string that designates the indexable table; all of its\n            columns will be designed as data_columns, unless data_columns is\n            passed, in which case these are used\n        data_columns : list of columns to create as data columns, or True to\n            use all columns\n        dropna : if evaluates to True, drop rows from all tables if any single\n                 row in each table has all NaN. Default False.\n\n        Notes\n        -----\n        axes parameter is currently not accepted\n\n        \"\"\"\n        if axes is not None:\n            raise TypeError(\n                \"axes is currently not accepted as a parameter to append_to_multiple; \"\n                \"you can create the tables independently instead\"\n            )\n\n        if not isinstance(d, dict):\n            raise ValueError(\n                \"append_to_multiple must have a dictionary specified as the \"\n                \"way to split the value\"\n            )\n\n        if selector not in d:\n            raise ValueError(\n                \"append_to_multiple requires a selector that is in passed dict\"\n            )\n\n        # figure out the splitting axis (the non_index_axis)\n        axis = list(set(range(value.ndim)) - set(_AXES_MAP[type(value)]))[0]\n\n        # figure out how to split the value\n        remain_key = None\n        remain_values: List = []\n        for k, v in d.items():\n            if v is None:\n                if remain_key is not None:\n                    raise ValueError(\n                        \"append_to_multiple can only have one value in d that is None\"\n                    )\n                remain_key = k\n            else:\n                remain_values.extend(v)\n        if remain_key is not None:\n            ordered = value.axes[axis]\n            ordd = ordered.difference(Index(remain_values))\n            ordd = sorted(ordered.get_indexer(ordd))\n            d[remain_key] = ordered.take(ordd)\n\n        # data_columns\n        if data_columns is None:\n            data_columns = d[selector]\n\n        # ensure rows are synchronized across the tables\n        if dropna:\n            idxs = (value[cols].dropna(how=\"all\").index for cols in d.values())\n            valid_index = next(idxs)\n            for index in idxs:\n                valid_index = valid_index.intersection(index)\n            value = value.loc[valid_index]\n\n        min_itemsize = kwargs.pop(\"min_itemsize\", None)\n\n        # append\n        for k, v in d.items():\n            dc = data_columns if k == selector else None\n\n            # compute the val\n            val = value.reindex(v, axis=axis)\n\n            filtered = (\n                {key: value for (key, value) in min_itemsize.items() if key in v}\n                if min_itemsize is not None\n                else None\n            )\n            self.append(k, val, data_columns=dc, min_itemsize=filtered, **kwargs)\n\n    def create_table_index(\n        self,\n        key: str,\n        columns=None,\n        optlevel: Optional[int] = None,\n        kind: Optional[str] = None,\n    ):\n        \"\"\"\n        Create a pytables index on the table.\n\n        Parameters\n        ----------\n        key : str\n        columns : None, bool, or listlike[str]\n            Indicate which columns to create an index on.\n\n            * False : Do not create any indexes.\n            * True : Create indexes on all columns.\n            * None : Create indexes on all columns.\n            * listlike : Create indexes on the given columns.\n\n        optlevel : int or None, default None\n            Optimization level, if None, pytables defaults to 6.\n        kind : str or None, default None\n            Kind of index, if None, pytables defaults to \"medium\".\n\n        Raises\n        ------\n        TypeError: raises if the node is not a table\n        \"\"\"\n        # version requirements\n        _tables()\n        s = self.get_storer(key)\n        if s is None:\n            return\n\n        if not isinstance(s, Table):\n            raise TypeError(\"cannot create table index on a Fixed format store\")\n        s.create_index(columns=columns, optlevel=optlevel, kind=kind)\n\n    def groups(self):\n        \"\"\"\n        Return a list of all the top-level nodes.\n\n        Each node returned is not a pandas storage object.\n\n        Returns\n        -------\n        list\n            List of objects.\n        \"\"\"\n        _tables()\n        self._check_if_open()\n        assert self._handle is not None  # for mypy\n        assert _table_mod is not None  # for mypy\n        return [\n            g\n            for g in self._handle.walk_groups()\n            if (\n                not isinstance(g, _table_mod.link.Link)\n                and (\n                    getattr(g._v_attrs, \"pandas_type\", None)\n                    or getattr(g, \"table\", None)\n                    or (isinstance(g, _table_mod.table.Table) and g._v_name != \"table\")\n                )\n            )\n        ]\n\n    def walk(self, where=\"/\"):\n        \"\"\"\n        Walk the pytables group hierarchy for pandas objects.\n\n        This generator will yield the group path, subgroups and pandas object\n        names for each group.\n\n        Any non-pandas PyTables objects that are not a group will be ignored.\n\n        The `where` group itself is listed first (preorder), then each of its\n        child groups (following an alphanumerical order) is also traversed,\n        following the same procedure.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        where : str, default \"/\"\n            Group where to start walking.\n\n        Yields\n        ------\n        path : str\n            Full path to a group (without trailing '/').\n        groups : list\n            Names (strings) of the groups contained in `path`.\n        leaves : list\n            Names (strings) of the pandas objects contained in `path`.\n        \"\"\"\n        _tables()\n        self._check_if_open()\n        assert self._handle is not None  # for mypy\n        assert _table_mod is not None  # for mypy\n\n        for g in self._handle.walk_groups(where):\n            if getattr(g._v_attrs, \"pandas_type\", None) is not None:\n                continue\n\n            groups = []\n            leaves = []\n            for child in g._v_children.values():\n                pandas_type = getattr(child._v_attrs, \"pandas_type\", None)\n                if pandas_type is None:\n                    if isinstance(child, _table_mod.group.Group):\n                        groups.append(child._v_name)\n                else:\n                    leaves.append(child._v_name)\n\n            yield (g._v_pathname.rstrip(\"/\"), groups, leaves)\n\n    def get_node(self, key: str) -> Optional[\"Node\"]:\n        \"\"\" return the node with the key or None if it does not exist \"\"\"\n        self._check_if_open()\n        if not key.startswith(\"/\"):\n            key = \"/\" + key\n\n        assert self._handle is not None\n        assert _table_mod is not None  # for mypy\n        try:\n            node = self._handle.get_node(self.root, key)\n        except _table_mod.exceptions.NoSuchNodeError:\n            return None\n\n        assert isinstance(node, _table_mod.Node), type(node)\n        return node\n\n    def get_storer(self, key: str) -> Union[\"GenericFixed\", \"Table\"]:\n        \"\"\" return the storer object for a key, raise if not in the file \"\"\"\n        group = self.get_node(key)\n        if group is None:\n            raise KeyError(f\"No object named {key} in the file\")\n\n        s = self._create_storer(group)\n        s.infer_axes()\n        return s\n\n    def copy(\n        self,\n        file,\n        mode=\"w\",\n        propindexes: bool = True,\n        keys=None,\n        complib=None,\n        complevel: Optional[int] = None,\n        fletcher32: bool = False,\n        overwrite=True,\n    ):\n        \"\"\"\n        Copy the existing store to a new file, updating in place.\n\n        Parameters\n        ----------\n        propindexes : bool, default True\n            Restore indexes in copied file.\n        keys : list, optional\n            List of keys to include in the copy (defaults to all).\n        overwrite : bool, default True\n            Whether to overwrite (remove and replace) existing nodes in the new store.\n        mode, complib, complevel, fletcher32 same as in HDFStore.__init__\n\n        Returns\n        -------\n        open file handle of the new store\n        \"\"\"\n        new_store = HDFStore(\n            file, mode=mode, complib=complib, complevel=complevel, fletcher32=fletcher32\n        )\n        if keys is None:\n            keys = list(self.keys())\n        if not isinstance(keys, (tuple, list)):\n            keys = [keys]\n        for k in keys:\n            s = self.get_storer(k)\n            if s is not None:\n\n                if k in new_store:\n                    if overwrite:\n                        new_store.remove(k)\n\n                data = self.select(k)\n                if isinstance(s, Table):\n\n                    index: Union[bool, List[str]] = False\n                    if propindexes:\n                        index = [a.name for a in s.axes if a.is_indexed]\n                    new_store.append(\n                        k,\n                        data,\n                        index=index,\n                        data_columns=getattr(s, \"data_columns\", None),\n                        encoding=s.encoding,\n                    )\n                else:\n                    new_store.put(k, data, encoding=s.encoding)\n\n        return new_store\n\n    def info(self) -> str:\n        \"\"\"\n        Print detailed information on the store.\n\n        Returns\n        -------\n        str\n        \"\"\"\n        path = pprint_thing(self._path)\n        output = f\"{type(self)}\\nFile path: {path}\\n\"\n\n        if self.is_open:\n            lkeys = sorted(self.keys())\n            if len(lkeys):\n                keys = []\n                values = []\n\n                for k in lkeys:\n                    try:\n                        s = self.get_storer(k)\n                        if s is not None:\n                            keys.append(pprint_thing(s.pathname or k))\n                            values.append(pprint_thing(s or \"invalid_HDFStore node\"))\n                    except AssertionError:\n                        # surface any assertion errors for e.g. debugging\n                        raise\n                    except Exception as detail:\n                        keys.append(k)\n                        dstr = pprint_thing(detail)\n                        values.append(f\"[invalid_HDFStore node: {dstr}]\")\n\n                output += adjoin(12, keys, values)\n            else:\n                output += \"Empty\"\n        else:\n            output += \"File is CLOSED\"\n\n        return output\n\n    # ------------------------------------------------------------------------\n    # private methods\n\n    def _check_if_open(self):\n        if not self.is_open:\n            raise ClosedFileError(f\"{self._path} file is not open!\")\n\n    def _validate_format(self, format: str) -> str:\n        \"\"\" validate / deprecate formats \"\"\"\n        # validate\n        try:\n            format = _FORMAT_MAP[format.lower()]\n        except KeyError as err:\n            raise TypeError(f\"invalid HDFStore format specified [{format}]\") from err\n\n        return format\n\n    def _create_storer(\n        self,\n        group,\n        format=None,\n        value: Optional[FrameOrSeries] = None,\n        encoding: str = \"UTF-8\",\n        errors: str = \"strict\",\n    ) -> Union[\"GenericFixed\", \"Table\"]:\n        \"\"\" return a suitable class to operate \"\"\"\n        cls: Union[Type[\"GenericFixed\"], Type[\"Table\"]]\n\n        if value is not None and not isinstance(value, (Series, DataFrame)):\n            raise TypeError(\"value must be None, Series, or DataFrame\")\n\n        def error(t):\n            # return instead of raising so mypy can tell where we are raising\n            return TypeError(\n                f\"cannot properly create the storer for: [{t}] [group->\"\n                f\"{group},value->{type(value)},format->{format}\"\n            )\n\n        pt = _ensure_decoded(getattr(group._v_attrs, \"pandas_type\", None))\n        tt = _ensure_decoded(getattr(group._v_attrs, \"table_type\", None))\n\n        # infer the pt from the passed value\n        if pt is None:\n            if value is None:\n                _tables()\n                assert _table_mod is not None  # for mypy\n                if getattr(group, \"table\", None) or isinstance(\n                    group, _table_mod.table.Table\n                ):\n                    pt = \"frame_table\"\n                    tt = \"generic_table\"\n                else:\n                    raise TypeError(\n                        \"cannot create a storer if the object is not existing \"\n                        \"nor a value are passed\"\n                    )\n            else:\n                _TYPE_MAP = {Series: \"series\", DataFrame: \"frame\"}\n                pt = _TYPE_MAP[type(value)]\n\n                # we are actually a table\n                if format == \"table\":\n                    pt += \"_table\"\n\n        # a storer node\n        if \"table\" not in pt:\n            _STORER_MAP = {\"series\": SeriesFixed, \"frame\": FrameFixed}\n            try:\n                cls = _STORER_MAP[pt]\n            except KeyError as err:\n                raise error(\"_STORER_MAP\") from err\n            return cls(self, group, encoding=encoding, errors=errors)\n\n        # existing node (and must be a table)\n        if tt is None:\n            # if we are a writer, determine the tt\n            if value is not None:\n                if pt == \"series_table\":\n                    index = getattr(value, \"index\", None)\n                    if index is not None:\n                        if index.nlevels == 1:\n                            tt = \"appendable_series\"\n                        elif index.nlevels > 1:\n                            tt = \"appendable_multiseries\"\n                elif pt == \"frame_table\":\n                    index = getattr(value, \"index\", None)\n                    if index is not None:\n                        if index.nlevels == 1:\n                            tt = \"appendable_frame\"\n                        elif index.nlevels > 1:\n                            tt = \"appendable_multiframe\"\n\n        _TABLE_MAP = {\n            \"generic_table\": GenericTable,\n            \"appendable_series\": AppendableSeriesTable,\n            \"appendable_multiseries\": AppendableMultiSeriesTable,\n            \"appendable_frame\": AppendableFrameTable,\n            \"appendable_multiframe\": AppendableMultiFrameTable,\n            \"worm\": WORMTable,\n        }\n        try:\n            cls = _TABLE_MAP[tt]\n        except KeyError as err:\n            raise error(\"_TABLE_MAP\") from err\n\n        return cls(self, group, encoding=encoding, errors=errors)\n\n    def _write_to_group(\n        self,\n        key: str,\n        value: FrameOrSeries,\n        format,\n        axes=None,\n        index=True,\n        append=False,\n        complib=None,\n        complevel: Optional[int] = None,\n        fletcher32=None,\n        min_itemsize: Optional[Union[int, Dict[str, int]]] = None,\n        chunksize=None,\n        expectedrows=None,\n        dropna=False,\n        nan_rep=None,\n        data_columns=None,\n        encoding=None,\n        errors: str = \"strict\",\n        track_times: bool = True,\n    ):\n        # we don't want to store a table node at all if our object is 0-len\n        # as there are not dtypes\n        if getattr(value, \"empty\", None) and (format == \"table\" or append):\n            return\n\n        group = self._identify_group(key, append)\n\n        s = self._create_storer(group, format, value, encoding=encoding, errors=errors)\n        if append:\n            # raise if we are trying to append to a Fixed format,\n            #       or a table that exists (and we are putting)\n            if not s.is_table or (s.is_table and format == \"fixed\" and s.is_exists):\n                raise ValueError(\"Can only append to Tables\")\n            if not s.is_exists:\n                s.set_object_info()\n        else:\n            s.set_object_info()\n\n        if not s.is_table and complib:\n            raise ValueError(\"Compression not supported on Fixed format stores\")\n\n        # write the object\n        s.write(\n            obj=value,\n            axes=axes,\n            append=append,\n            complib=complib,\n            complevel=complevel,\n            fletcher32=fletcher32,\n            min_itemsize=min_itemsize,\n            chunksize=chunksize,\n            expectedrows=expectedrows,\n            dropna=dropna,\n            nan_rep=nan_rep,\n            data_columns=data_columns,\n            track_times=track_times,\n        )\n\n        if isinstance(s, Table) and index:\n            s.create_index(columns=index)\n\n    def _read_group(self, group: \"Node\"):\n        s = self._create_storer(group)\n        s.infer_axes()\n        return s.read()\n\n    def _identify_group(self, key: str, append: bool) -> \"Node\":\n        \"\"\"Identify HDF5 group based on key, delete/create group if needed.\"\"\"\n        group = self.get_node(key)\n\n        # we make this assertion for mypy; the get_node call will already\n        # have raised if this is incorrect\n        assert self._handle is not None\n\n        # remove the node if we are not appending\n        if group is not None and not append:\n            self._handle.remove_node(group, recursive=True)\n            group = None\n\n        if group is None:\n            group = self._create_nodes_and_group(key)\n\n        return group\n\n    def _create_nodes_and_group(self, key: str) -> \"Node\":\n        \"\"\"Create nodes from key and return group name.\"\"\"\n        # assertion for mypy\n        assert self._handle is not None\n\n        paths = key.split(\"/\")\n        # recursively create the groups\n        path = \"/\"\n        for p in paths:\n            if not len(p):\n                continue\n            new_path = path\n            if not path.endswith(\"/\"):\n                new_path += \"/\"\n            new_path += p\n            group = self.get_node(new_path)\n            if group is None:\n                group = self._handle.create_group(path, p)\n            path = new_path\n        return group\n\n\nclass TableIterator:\n    \"\"\"\n    Define the iteration interface on a table\n\n    Parameters\n    ----------\n    store : HDFStore\n    s     : the referred storer\n    func  : the function to execute the query\n    where : the where of the query\n    nrows : the rows to iterate on\n    start : the passed start value (default is None)\n    stop  : the passed stop value (default is None)\n    iterator : bool, default False\n        Whether to use the default iterator.\n    chunksize : the passed chunking value (default is 100000)\n    auto_close : bool, default False\n        Whether to automatically close the store at the end of iteration.\n    \"\"\"\n\n    chunksize: Optional[int]\n    store: HDFStore\n    s: Union[\"GenericFixed\", \"Table\"]\n\n    def __init__(\n        self,\n        store: HDFStore,\n        s: Union[\"GenericFixed\", \"Table\"],\n        func,\n        where,\n        nrows,\n        start=None,\n        stop=None,\n        iterator: bool = False,\n        chunksize: Optional[int] = None,\n        auto_close: bool = False,\n    ):\n        self.store = store\n        self.s = s\n        self.func = func\n        self.where = where\n\n        # set start/stop if they are not set if we are a table\n        if self.s.is_table:\n            if nrows is None:\n                nrows = 0\n            if start is None:\n                start = 0\n            if stop is None:\n                stop = nrows\n            stop = min(nrows, stop)\n\n        self.nrows = nrows\n        self.start = start\n        self.stop = stop\n\n        self.coordinates = None\n        if iterator or chunksize is not None:\n            if chunksize is None:\n                chunksize = 100000\n            self.chunksize = int(chunksize)\n        else:\n            self.chunksize = None\n\n        self.auto_close = auto_close\n\n    def __iter__(self):\n        # iterate\n        current = self.start\n        if self.coordinates is None:\n            raise ValueError(\"Cannot iterate until get_result is called.\")\n        while current < self.stop:\n            stop = min(current + self.chunksize, self.stop)\n            value = self.func(None, None, self.coordinates[current:stop])\n            current = stop\n            if value is None or not len(value):\n                continue\n\n            yield value\n\n        self.close()\n\n    def close(self):\n        if self.auto_close:\n            self.store.close()\n\n    def get_result(self, coordinates: bool = False):\n        #  return the actual iterator\n        if self.chunksize is not None:\n            if not isinstance(self.s, Table):\n                raise TypeError(\"can only use an iterator or chunksize on a table\")\n\n            self.coordinates = self.s.read_coordinates(where=self.where)\n\n            return self\n\n        # if specified read via coordinates (necessary for multiple selections\n        if coordinates:\n            if not isinstance(self.s, Table):\n                raise TypeError(\"can only read_coordinates on a table\")\n            where = self.s.read_coordinates(\n                where=self.where, start=self.start, stop=self.stop\n            )\n        else:\n            where = self.where\n\n        # directly return the result\n        results = self.func(self.start, self.stop, where)\n        self.close()\n        return results\n\n\nclass IndexCol:\n    \"\"\"\n    an index column description class\n\n    Parameters\n    ----------\n    axis   : axis which I reference\n    values : the ndarray like converted values\n    kind   : a string description of this type\n    typ    : the pytables type\n    pos    : the position in the pytables\n\n    \"\"\"\n\n    is_an_indexable = True\n    is_data_indexable = True\n    _info_fields = [\"freq\", \"tz\", \"index_name\"]\n\n    name: str\n    cname: str\n\n    def __init__(\n        self,\n        name: str,\n        values=None,\n        kind=None,\n        typ=None,\n        cname: Optional[str] = None,\n        axis=None,\n        pos=None,\n        freq=None,\n        tz=None,\n        index_name=None,\n        ordered=None,\n        table=None,\n        meta=None,\n        metadata=None,\n    ):\n\n        if not isinstance(name, str):\n            raise ValueError(\"`name` must be a str.\")\n\n        self.values = values\n        self.kind = kind\n        self.typ = typ\n        self.name = name\n        self.cname = cname or name\n        self.axis = axis\n        self.pos = pos\n        self.freq = freq\n        self.tz = tz\n        self.index_name = index_name\n        self.ordered = ordered\n        self.table = table\n        self.meta = meta\n        self.metadata = metadata\n\n        if pos is not None:\n            self.set_pos(pos)\n\n        # These are ensured as long as the passed arguments match the\n        #  constructor annotations.\n        assert isinstance(self.name, str)\n        assert isinstance(self.cname, str)\n\n    @property\n    def itemsize(self) -> int:\n        # Assumes self.typ has already been initialized\n        return self.typ.itemsize\n\n    @property\n    def kind_attr(self) -> str:\n        return f\"{self.name}_kind\"\n\n    def set_pos(self, pos: int):\n        \"\"\" set the position of this column in the Table \"\"\"\n        self.pos = pos\n        if pos is not None and self.typ is not None:\n            self.typ._v_pos = pos\n\n    def __repr__(self) -> str:\n        temp = tuple(\n            map(pprint_thing, (self.name, self.cname, self.axis, self.pos, self.kind))\n        )\n        return \",\".join(\n            (\n                f\"{key}->{value}\"\n                for key, value in zip([\"name\", \"cname\", \"axis\", \"pos\", \"kind\"], temp)\n            )\n        )\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\" compare 2 col items \"\"\"\n        return all(\n            getattr(self, a, None) == getattr(other, a, None)\n            for a in [\"name\", \"cname\", \"axis\", \"pos\"]\n        )\n\n    def __ne__(self, other) -> bool:\n        return not self.__eq__(other)\n\n    @property\n    def is_indexed(self) -> bool:\n        \"\"\" return whether I am an indexed column \"\"\"\n        if not hasattr(self.table, \"cols\"):\n            # e.g. if infer hasn't been called yet, self.table will be None.\n            return False\n        return getattr(self.table.cols, self.cname).is_indexed\n\n    def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str):\n        \"\"\"\n        Convert the data from this selection to the appropriate pandas type.\n        \"\"\"\n        assert isinstance(values, np.ndarray), type(values)\n\n        # values is a recarray\n        if values.dtype.fields is not None:\n            values = values[self.cname]\n\n        val_kind = _ensure_decoded(self.kind)\n        values = _maybe_convert(values, val_kind, encoding, errors)\n\n        kwargs = {}\n        kwargs[\"name\"] = _ensure_decoded(self.index_name)\n\n        if self.freq is not None:\n            kwargs[\"freq\"] = _ensure_decoded(self.freq)\n\n        # making an Index instance could throw a number of different errors\n        try:\n            new_pd_index = Index(values, **kwargs)\n        except ValueError:\n            # if the output freq is different that what we recorded,\n            # it should be None (see also 'doc example part 2')\n            if \"freq\" in kwargs:\n                kwargs[\"freq\"] = None\n            new_pd_index = Index(values, **kwargs)\n\n        new_pd_index = _set_tz(new_pd_index, self.tz)\n        return new_pd_index, new_pd_index\n\n    def take_data(self):\n        \"\"\" return the values\"\"\"\n        return self.values\n\n    @property\n    def attrs(self):\n        return self.table._v_attrs\n\n    @property\n    def description(self):\n        return self.table.description\n\n    @property\n    def col(self):\n        \"\"\" return my current col description \"\"\"\n        return getattr(self.description, self.cname, None)\n\n    @property\n    def cvalues(self):\n        \"\"\" return my cython values \"\"\"\n        return self.values\n\n    def __iter__(self):\n        return iter(self.values)\n\n    def maybe_set_size(self, min_itemsize=None):\n        \"\"\"\n        maybe set a string col itemsize:\n            min_itemsize can be an integer or a dict with this columns name\n            with an integer size\n        \"\"\"\n        if _ensure_decoded(self.kind) == \"string\":\n            if isinstance(min_itemsize, dict):\n                min_itemsize = min_itemsize.get(self.name)\n\n            if min_itemsize is not None and self.typ.itemsize < min_itemsize:\n                self.typ = _tables().StringCol(itemsize=min_itemsize, pos=self.pos)\n\n    def validate_names(self):\n        pass\n\n    def validate_and_set(self, handler: \"AppendableTable\", append: bool):\n        self.table = handler.table\n        self.validate_col()\n        self.validate_attr(append)\n        self.validate_metadata(handler)\n        self.write_metadata(handler)\n        self.set_attr()\n\n    def validate_col(self, itemsize=None):\n        \"\"\" validate this column: return the compared against itemsize \"\"\"\n        # validate this column for string truncation (or reset to the max size)\n        if _ensure_decoded(self.kind) == \"string\":\n            c = self.col\n            if c is not None:\n                if itemsize is None:\n                    itemsize = self.itemsize\n                if c.itemsize < itemsize:\n                    raise ValueError(\n                        f\"Trying to store a string with len [{itemsize}] in \"\n                        f\"[{self.cname}] column but\\nthis column has a limit of \"\n                        f\"[{c.itemsize}]!\\nConsider using min_itemsize to \"\n                        \"preset the sizes on these columns\"\n                    )\n                return c.itemsize\n\n        return None\n\n    def validate_attr(self, append: bool):\n        # check for backwards incompatibility\n        if append:\n            existing_kind = getattr(self.attrs, self.kind_attr, None)\n            if existing_kind is not None and existing_kind != self.kind:\n                raise TypeError(\n                    f\"incompatible kind in col [{existing_kind} - {self.kind}]\"\n                )\n\n    def update_info(self, info):\n        \"\"\"\n        set/update the info for this indexable with the key/value\n        if there is a conflict raise/warn as needed\n        \"\"\"\n        for key in self._info_fields:\n\n            value = getattr(self, key, None)\n            idx = info.setdefault(self.name, {})\n\n            existing_value = idx.get(key)\n            if key in idx and value is not None and existing_value != value:\n                # frequency/name just warn\n                if key in [\"freq\", \"index_name\"]:\n                    ws = attribute_conflict_doc % (key, existing_value, value)\n                    warnings.warn(ws, AttributeConflictWarning, stacklevel=6)\n\n                    # reset\n                    idx[key] = None\n                    setattr(self, key, None)\n\n                else:\n                    raise ValueError(\n                        f\"invalid info for [{self.name}] for [{key}], \"\n                        f\"existing_value [{existing_value}] conflicts with \"\n                        f\"new value [{value}]\"\n                    )\n            else:\n                if value is not None or existing_value is not None:\n                    idx[key] = value\n\n    def set_info(self, info):\n        \"\"\" set my state from the passed info \"\"\"\n        idx = info.get(self.name)\n        if idx is not None:\n            self.__dict__.update(idx)\n\n    def set_attr(self):\n        \"\"\" set the kind for this column \"\"\"\n        setattr(self.attrs, self.kind_attr, self.kind)\n\n    def validate_metadata(self, handler: \"AppendableTable\"):\n        \"\"\" validate that kind=category does not change the categories \"\"\"\n        if self.meta == \"category\":\n            new_metadata = self.metadata\n            cur_metadata = handler.read_metadata(self.cname)\n            if (\n                new_metadata is not None\n                and cur_metadata is not None\n                and not array_equivalent(new_metadata, cur_metadata)\n            ):\n                raise ValueError(\n                    \"cannot append a categorical with \"\n                    \"different categories to the existing\"\n                )\n\n    def write_metadata(self, handler: \"AppendableTable\"):\n        \"\"\" set the meta data \"\"\"\n        if self.metadata is not None:\n            handler.write_metadata(self.cname, self.metadata)\n\n\nclass GenericIndexCol(IndexCol):\n    \"\"\" an index which is not represented in the data of the table \"\"\"\n\n    @property\n    def is_indexed(self) -> bool:\n        return False\n\n    def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str):\n        \"\"\"\n        Convert the data from this selection to the appropriate pandas type.\n\n        Parameters\n        ----------\n        values : np.ndarray\n        nan_rep : str\n        encoding : str\n        errors : str\n        \"\"\"\n        assert isinstance(values, np.ndarray), type(values)\n\n        values = Int64Index(np.arange(len(values)))\n        return values, values\n\n    def set_attr(self):\n        pass\n\n\nclass DataCol(IndexCol):\n    \"\"\"\n    a data holding column, by definition this is not indexable\n\n    Parameters\n    ----------\n    data   : the actual data\n    cname  : the column name in the table to hold the data (typically\n                values)\n    meta   : a string description of the metadata\n    metadata : the actual metadata\n    \"\"\"\n\n    is_an_indexable = False\n    is_data_indexable = False\n    _info_fields = [\"tz\", \"ordered\"]\n\n    def __init__(\n        self,\n        name: str,\n        values=None,\n        kind=None,\n        typ=None,\n        cname=None,\n        pos=None,\n        tz=None,\n        ordered=None,\n        table=None,\n        meta=None,\n        metadata=None,\n        dtype=None,\n        data=None,\n    ):\n        super().__init__(\n            name=name,\n            values=values,\n            kind=kind,\n            typ=typ,\n            pos=pos,\n            cname=cname,\n            tz=tz,\n            ordered=ordered,\n            table=table,\n            meta=meta,\n            metadata=metadata,\n        )\n        self.dtype = dtype\n        self.data = data\n\n    @property\n    def dtype_attr(self) -> str:\n        return f\"{self.name}_dtype\"\n\n    @property\n    def meta_attr(self) -> str:\n        return f\"{self.name}_meta\"\n\n    def __repr__(self) -> str:\n        temp = tuple(\n            map(\n                pprint_thing, (self.name, self.cname, self.dtype, self.kind, self.shape)\n            )\n        )\n        return \",\".join(\n            (\n                f\"{key}->{value}\"\n                for key, value in zip([\"name\", \"cname\", \"dtype\", \"kind\", \"shape\"], temp)\n            )\n        )\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\" compare 2 col items \"\"\"\n        return all(\n            getattr(self, a, None) == getattr(other, a, None)\n            for a in [\"name\", \"cname\", \"dtype\", \"pos\"]\n        )\n\n    def set_data(self, data: ArrayLike):\n        assert data is not None\n        assert self.dtype is None\n\n        data, dtype_name = _get_data_and_dtype_name(data)\n\n        self.data = data\n        self.dtype = dtype_name\n        self.kind = _dtype_to_kind(dtype_name)\n\n    def take_data(self):\n        \"\"\" return the data \"\"\"\n        return self.data\n\n    @classmethod\n    def _get_atom(cls, values: ArrayLike) -> \"Col\":\n        \"\"\"\n        Get an appropriately typed and shaped pytables.Col object for values.\n        \"\"\"\n        dtype = values.dtype\n        # error: \"ExtensionDtype\" has no attribute \"itemsize\"\n        itemsize = dtype.itemsize  # type: ignore[attr-defined]\n\n        shape = values.shape\n        if values.ndim == 1:\n            # EA, use block shape pretending it is 2D\n            # TODO(EA2D): not necessary with 2D EAs\n            shape = (1, values.size)\n\n        if isinstance(values, Categorical):\n            codes = values.codes\n            atom = cls.get_atom_data(shape, kind=codes.dtype.name)\n        elif is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype):\n            atom = cls.get_atom_datetime64(shape)\n        elif is_timedelta64_dtype(dtype):\n            atom = cls.get_atom_timedelta64(shape)\n        elif is_complex_dtype(dtype):\n            atom = _tables().ComplexCol(itemsize=itemsize, shape=shape[0])\n        elif is_string_dtype(dtype):\n            atom = cls.get_atom_string(shape, itemsize)\n        else:\n            atom = cls.get_atom_data(shape, kind=dtype.name)\n\n        return atom\n\n    @classmethod\n    def get_atom_string(cls, shape, itemsize):\n        return _tables().StringCol(itemsize=itemsize, shape=shape[0])\n\n    @classmethod\n    def get_atom_coltype(cls, kind: str) -> Type[\"Col\"]:\n        \"\"\" return the PyTables column class for this column \"\"\"\n        if kind.startswith(\"uint\"):\n            k4 = kind[4:]\n            col_name = f\"UInt{k4}Col\"\n        elif kind.startswith(\"period\"):\n            # we store as integer\n            col_name = \"Int64Col\"\n        else:\n            kcap = kind.capitalize()\n            col_name = f\"{kcap}Col\"\n\n        return getattr(_tables(), col_name)\n\n    @classmethod\n    def get_atom_data(cls, shape, kind: str) -> \"Col\":\n        return cls.get_atom_coltype(kind=kind)(shape=shape[0])\n\n    @classmethod\n    def get_atom_datetime64(cls, shape):\n        return _tables().Int64Col(shape=shape[0])\n\n    @classmethod\n    def get_atom_timedelta64(cls, shape):\n        return _tables().Int64Col(shape=shape[0])\n\n    @property\n    def shape(self):\n        return getattr(self.data, \"shape\", None)\n\n    @property\n    def cvalues(self):\n        \"\"\" return my cython values \"\"\"\n        return self.data\n\n    def validate_attr(self, append):\n        \"\"\"validate that we have the same order as the existing & same dtype\"\"\"\n        if append:\n            existing_fields = getattr(self.attrs, self.kind_attr, None)\n            if existing_fields is not None and existing_fields != list(self.values):\n                raise ValueError(\"appended items do not match existing items in table!\")\n\n            existing_dtype = getattr(self.attrs, self.dtype_attr, None)\n            if existing_dtype is not None and existing_dtype != self.dtype:\n                raise ValueError(\n                    \"appended items dtype do not match existing items dtype in table!\"\n                )\n\n    def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str):\n        \"\"\"\n        Convert the data from this selection to the appropriate pandas type.\n\n        Parameters\n        ----------\n        values : np.ndarray\n        nan_rep :\n        encoding : str\n        errors : str\n\n        Returns\n        -------\n        index : listlike to become an Index\n        data : ndarraylike to become a column\n        \"\"\"\n        assert isinstance(values, np.ndarray), type(values)\n\n        # values is a recarray\n        if values.dtype.fields is not None:\n            values = values[self.cname]\n\n        assert self.typ is not None\n        if self.dtype is None:\n            # Note: in tests we never have timedelta64 or datetime64,\n            #  so the _get_data_and_dtype_name may be unnecessary\n            converted, dtype_name = _get_data_and_dtype_name(values)\n            kind = _dtype_to_kind(dtype_name)\n        else:\n            converted = values\n            dtype_name = self.dtype\n            kind = self.kind\n\n        assert isinstance(converted, np.ndarray)  # for mypy\n\n        # use the meta if needed\n        meta = _ensure_decoded(self.meta)\n        metadata = self.metadata\n        ordered = self.ordered\n        tz = self.tz\n\n        assert dtype_name is not None\n        # convert to the correct dtype\n        dtype = _ensure_decoded(dtype_name)\n\n        # reverse converts\n        if dtype == \"datetime64\":\n            # recreate with tz if indicated\n            converted = _set_tz(converted, tz, coerce=True)\n\n        elif dtype == \"timedelta64\":\n            converted = np.asarray(converted, dtype=\"m8[ns]\")\n        elif dtype == \"date\":\n            try:\n                converted = np.asarray(\n                    [date.fromordinal(v) for v in converted], dtype=object\n                )\n            except ValueError:\n                converted = np.asarray(\n                    [date.fromtimestamp(v) for v in converted], dtype=object\n                )\n\n        elif meta == \"category\":\n            # we have a categorical\n            categories = metadata\n            codes = converted.ravel()\n\n            # if we have stored a NaN in the categories\n            # then strip it; in theory we could have BOTH\n            # -1s in the codes and nulls :<\n            if categories is None:\n                # Handle case of NaN-only categorical columns in which case\n                # the categories are an empty array; when this is stored,\n                # pytables cannot write a zero-len array, so on readback\n                # the categories would be None and `read_hdf()` would fail.\n                categories = Index([], dtype=np.float64)\n            else:\n                mask = isna(categories)\n                if mask.any():\n                    categories = categories[~mask]\n                    codes[codes != -1] -= mask.astype(int).cumsum()._values\n\n            converted = Categorical.from_codes(\n                codes, categories=categories, ordered=ordered\n            )\n\n        else:\n\n            try:\n                converted = converted.astype(dtype, copy=False)\n            except TypeError:\n                converted = converted.astype(\"O\", copy=False)\n\n        # convert nans / decode\n        if _ensure_decoded(kind) == \"string\":\n            converted = _unconvert_string_array(\n                converted, nan_rep=nan_rep, encoding=encoding, errors=errors\n            )\n\n        return self.values, converted\n\n    def set_attr(self):\n        \"\"\" set the data for this column \"\"\"\n        setattr(self.attrs, self.kind_attr, self.values)\n        setattr(self.attrs, self.meta_attr, self.meta)\n        assert self.dtype is not None\n        setattr(self.attrs, self.dtype_attr, self.dtype)\n\n\nclass DataIndexableCol(DataCol):\n    \"\"\" represent a data column that can be indexed \"\"\"\n\n    is_data_indexable = True\n\n    def validate_names(self):\n        if not Index(self.values).is_object():\n            # TODO: should the message here be more specifically non-str?\n            raise ValueError(\"cannot have non-object label DataIndexableCol\")\n\n    @classmethod\n    def get_atom_string(cls, shape, itemsize):\n        return _tables().StringCol(itemsize=itemsize)\n\n    @classmethod\n    def get_atom_data(cls, shape, kind: str) -> \"Col\":\n        return cls.get_atom_coltype(kind=kind)()\n\n    @classmethod\n    def get_atom_datetime64(cls, shape):\n        return _tables().Int64Col()\n\n    @classmethod\n    def get_atom_timedelta64(cls, shape):\n        return _tables().Int64Col()\n\n\nclass GenericDataIndexableCol(DataIndexableCol):\n    \"\"\" represent a generic pytables data column \"\"\"\n\n    pass\n\n\nclass Fixed:\n    \"\"\"\n    represent an object in my store\n    facilitate read/write of various types of objects\n    this is an abstract base class\n\n    Parameters\n    ----------\n    parent : HDFStore\n    group : Node\n        The group node where the table resides.\n    \"\"\"\n\n    pandas_kind: str\n    format_type: str = \"fixed\"  # GH#30962 needed by dask\n    obj_type: Type[FrameOrSeriesUnion]\n    ndim: int\n    encoding: str\n    parent: HDFStore\n    group: \"Node\"\n    errors: str\n    is_table = False\n\n    def __init__(\n        self,\n        parent: HDFStore,\n        group: \"Node\",\n        encoding: str = \"UTF-8\",\n        errors: str = \"strict\",\n    ):\n        assert isinstance(parent, HDFStore), type(parent)\n        assert _table_mod is not None  # needed for mypy\n        assert isinstance(group, _table_mod.Node), type(group)\n        self.parent = parent\n        self.group = group\n        self.encoding = _ensure_encoding(encoding)\n        self.errors = errors\n\n    @property\n    def is_old_version(self) -> bool:\n        return self.version[0] <= 0 and self.version[1] <= 10 and self.version[2] < 1\n\n    @property\n    def version(self) -> Tuple[int, int, int]:\n        \"\"\" compute and set our version \"\"\"\n        version = _ensure_decoded(getattr(self.group._v_attrs, \"pandas_version\", None))\n        try:\n            version = tuple(int(x) for x in version.split(\".\"))\n            if len(version) == 2:\n                version = version + (0,)\n        except AttributeError:\n            version = (0, 0, 0)\n        return version\n\n    @property\n    def pandas_type(self):\n        return _ensure_decoded(getattr(self.group._v_attrs, \"pandas_type\", None))\n\n    def __repr__(self) -> str:\n        \"\"\" return a pretty representation of myself \"\"\"\n        self.infer_axes()\n        s = self.shape\n        if s is not None:\n            if isinstance(s, (list, tuple)):\n                jshape = \",\".join(pprint_thing(x) for x in s)\n                s = f\"[{jshape}]\"\n            return f\"{self.pandas_type:12.12} (shape->{s})\"\n        return self.pandas_type\n\n    def set_object_info(self):\n        \"\"\" set my pandas type & version \"\"\"\n        self.attrs.pandas_type = str(self.pandas_kind)\n        self.attrs.pandas_version = str(_version)\n\n    def copy(self):\n        new_self = copy.copy(self)\n        return new_self\n\n    @property\n    def shape(self):\n        return self.nrows\n\n    @property\n    def pathname(self):\n        return self.group._v_pathname\n\n    @property\n    def _handle(self):\n        return self.parent._handle\n\n    @property\n    def _filters(self):\n        return self.parent._filters\n\n    @property\n    def _complevel(self) -> int:\n        return self.parent._complevel\n\n    @property\n    def _fletcher32(self) -> bool:\n        return self.parent._fletcher32\n\n    @property\n    def attrs(self):\n        return self.group._v_attrs\n\n    def set_attrs(self):\n        \"\"\" set our object attributes \"\"\"\n        pass\n\n    def get_attrs(self):\n        \"\"\" get our object attributes \"\"\"\n        pass\n\n    @property\n    def storable(self):\n        \"\"\" return my storable \"\"\"\n        return self.group\n\n    @property\n    def is_exists(self) -> bool:\n        return False\n\n    @property\n    def nrows(self):\n        return getattr(self.storable, \"nrows\", None)\n\n    def validate(self, other):\n        \"\"\" validate against an existing storable \"\"\"\n        if other is None:\n            return\n        return True\n\n    def validate_version(self, where=None):\n        \"\"\" are we trying to operate on an old version? \"\"\"\n        return True\n\n    def infer_axes(self):\n        \"\"\"\n        infer the axes of my storer\n        return a boolean indicating if we have a valid storer or not\n        \"\"\"\n        s = self.storable\n        if s is None:\n            return False\n        self.get_attrs()\n        return True\n\n    def read(\n        self,\n        where=None,\n        columns=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n        raise NotImplementedError(\n            \"cannot read on an abstract storer: subclasses should implement\"\n        )\n\n    def write(self, **kwargs):\n        raise NotImplementedError(\n            \"cannot write on an abstract storer: subclasses should implement\"\n        )\n\n    def delete(\n        self, where=None, start: Optional[int] = None, stop: Optional[int] = None\n    ):\n        \"\"\"\n        support fully deleting the node in its entirety (only) - where\n        specification must be None\n        \"\"\"\n        if com.all_none(where, start, stop):\n            self._handle.remove_node(self.group, recursive=True)\n            return None\n\n        raise TypeError(\"cannot delete on an abstract storer\")\n\n\nclass GenericFixed(Fixed):\n    \"\"\" a generified fixed version \"\"\"\n\n    _index_type_map = {DatetimeIndex: \"datetime\", PeriodIndex: \"period\"}\n    _reverse_index_map = {v: k for k, v in _index_type_map.items()}\n    attributes: List[str] = []\n\n    # indexer helpers\n    def _class_to_alias(self, cls) -> str:\n        return self._index_type_map.get(cls, \"\")\n\n    def _alias_to_class(self, alias):\n        if isinstance(alias, type):  # pragma: no cover\n            # compat: for a short period of time master stored types\n            return alias\n        return self._reverse_index_map.get(alias, Index)\n\n    def _get_index_factory(self, klass):\n        if klass == DatetimeIndex:\n\n            def f(values, freq=None, tz=None):\n                # data are already in UTC, localize and convert if tz present\n                dta = DatetimeArray._simple_new(values.values, freq=freq)\n                result = DatetimeIndex._simple_new(dta, name=None)\n                if tz is not None:\n                    result = result.tz_localize(\"UTC\").tz_convert(tz)\n                return result\n\n            return f\n        elif klass == PeriodIndex:\n\n            def f(values, freq=None, tz=None):\n                parr = PeriodArray._simple_new(values, freq=freq)\n                return PeriodIndex._simple_new(parr, name=None)\n\n            return f\n\n        return klass\n\n    def validate_read(self, columns, where):\n        \"\"\"\n        raise if any keywords are passed which are not-None\n        \"\"\"\n        if columns is not None:\n            raise TypeError(\n                \"cannot pass a column specification when reading \"\n                \"a Fixed format store. this store must be selected in its entirety\"\n            )\n        if where is not None:\n            raise TypeError(\n                \"cannot pass a where specification when reading \"\n                \"from a Fixed format store. this store must be selected in its entirety\"\n            )\n\n    @property\n    def is_exists(self) -> bool:\n        return True\n\n    def set_attrs(self):\n        \"\"\" set our object attributes \"\"\"\n        self.attrs.encoding = self.encoding\n        self.attrs.errors = self.errors\n\n    def get_attrs(self):\n        \"\"\" retrieve our attributes \"\"\"\n        self.encoding = _ensure_encoding(getattr(self.attrs, \"encoding\", None))\n        self.errors = _ensure_decoded(getattr(self.attrs, \"errors\", \"strict\"))\n        for n in self.attributes:\n            setattr(self, n, _ensure_decoded(getattr(self.attrs, n, None)))\n\n    def write(self, obj, **kwargs):\n        self.set_attrs()\n\n    def read_array(\n        self, key: str, start: Optional[int] = None, stop: Optional[int] = None\n    ):\n        \"\"\" read an array for the specified node (off of group \"\"\"\n        import tables\n\n        node = getattr(self.group, key)\n        attrs = node._v_attrs\n\n        transposed = getattr(attrs, \"transposed\", False)\n\n        if isinstance(node, tables.VLArray):\n            ret = node[0][start:stop]\n        else:\n            dtype = _ensure_decoded(getattr(attrs, \"value_type\", None))\n            shape = getattr(attrs, \"shape\", None)\n\n            if shape is not None:\n                # length 0 axis\n                ret = np.empty(shape, dtype=dtype)\n            else:\n                ret = node[start:stop]\n\n            if dtype == \"datetime64\":\n                # reconstruct a timezone if indicated\n                tz = getattr(attrs, \"tz\", None)\n                ret = _set_tz(ret, tz, coerce=True)\n\n            elif dtype == \"timedelta64\":\n                ret = np.asarray(ret, dtype=\"m8[ns]\")\n\n        if transposed:\n            return ret.T\n        else:\n            return ret\n\n    def read_index(\n        self, key: str, start: Optional[int] = None, stop: Optional[int] = None\n    ) -> Index:\n        variety = _ensure_decoded(getattr(self.attrs, f\"{key}_variety\"))\n\n        if variety == \"multi\":\n            return self.read_multi_index(key, start=start, stop=stop)\n        elif variety == \"regular\":\n            node = getattr(self.group, key)\n            index = self.read_index_node(node, start=start, stop=stop)\n            return index\n        else:  # pragma: no cover\n            raise TypeError(f\"unrecognized index variety: {variety}\")\n\n    def write_index(self, key: str, index: Index):\n        if isinstance(index, MultiIndex):\n            setattr(self.attrs, f\"{key}_variety\", \"multi\")\n            self.write_multi_index(key, index)\n        else:\n            setattr(self.attrs, f\"{key}_variety\", \"regular\")\n            converted = _convert_index(\"index\", index, self.encoding, self.errors)\n\n            self.write_array(key, converted.values)\n\n            node = getattr(self.group, key)\n            node._v_attrs.kind = converted.kind\n            node._v_attrs.name = index.name\n\n            if isinstance(index, (DatetimeIndex, PeriodIndex)):\n                node._v_attrs.index_class = self._class_to_alias(type(index))\n\n            if isinstance(index, (DatetimeIndex, PeriodIndex, TimedeltaIndex)):\n                node._v_attrs.freq = index.freq\n\n            if isinstance(index, DatetimeIndex) and index.tz is not None:\n                node._v_attrs.tz = _get_tz(index.tz)\n\n    def write_multi_index(self, key: str, index: MultiIndex):\n        setattr(self.attrs, f\"{key}_nlevels\", index.nlevels)\n\n        for i, (lev, level_codes, name) in enumerate(\n            zip(index.levels, index.codes, index.names)\n        ):\n            # write the level\n            if is_extension_array_dtype(lev):\n                raise NotImplementedError(\n                    \"Saving a MultiIndex with an extension dtype is not supported.\"\n                )\n            level_key = f\"{key}_level{i}\"\n            conv_level = _convert_index(level_key, lev, self.encoding, self.errors)\n            self.write_array(level_key, conv_level.values)\n            node = getattr(self.group, level_key)\n            node._v_attrs.kind = conv_level.kind\n            node._v_attrs.name = name\n\n            # write the name\n            setattr(node._v_attrs, f\"{key}_name{name}\", name)\n\n            # write the labels\n            label_key = f\"{key}_label{i}\"\n            self.write_array(label_key, level_codes)\n\n    def read_multi_index(\n        self, key: str, start: Optional[int] = None, stop: Optional[int] = None\n    ) -> MultiIndex:\n        nlevels = getattr(self.attrs, f\"{key}_nlevels\")\n\n        levels = []\n        codes = []\n        names: List[Label] = []\n        for i in range(nlevels):\n            level_key = f\"{key}_level{i}\"\n            node = getattr(self.group, level_key)\n            lev = self.read_index_node(node, start=start, stop=stop)\n            levels.append(lev)\n            names.append(lev.name)\n\n            label_key = f\"{key}_label{i}\"\n            level_codes = self.read_array(label_key, start=start, stop=stop)\n            codes.append(level_codes)\n\n        return MultiIndex(\n            levels=levels, codes=codes, names=names, verify_integrity=True\n        )\n\n    def read_index_node(\n        self, node: \"Node\", start: Optional[int] = None, stop: Optional[int] = None\n    ) -> Index:\n        data = node[start:stop]\n        # If the index was an empty array write_array_empty() will\n        # have written a sentinel. Here we replace it with the original.\n        if \"shape\" in node._v_attrs and np.prod(node._v_attrs.shape) == 0:\n            data = np.empty(node._v_attrs.shape, dtype=node._v_attrs.value_type)\n        kind = _ensure_decoded(node._v_attrs.kind)\n        name = None\n\n        if \"name\" in node._v_attrs:\n            name = _ensure_str(node._v_attrs.name)\n            name = _ensure_decoded(name)\n\n        index_class = self._alias_to_class(\n            _ensure_decoded(getattr(node._v_attrs, \"index_class\", \"\"))\n        )\n        factory = self._get_index_factory(index_class)\n\n        kwargs = {}\n        if \"freq\" in node._v_attrs:\n            kwargs[\"freq\"] = node._v_attrs[\"freq\"]\n\n        if \"tz\" in node._v_attrs:\n            if isinstance(node._v_attrs[\"tz\"], bytes):\n                # created by python2\n                kwargs[\"tz\"] = node._v_attrs[\"tz\"].decode(\"utf-8\")\n            else:\n                # created by python3\n                kwargs[\"tz\"] = node._v_attrs[\"tz\"]\n\n        if kind == \"date\":\n            index = factory(\n                _unconvert_index(\n                    data, kind, encoding=self.encoding, errors=self.errors\n                ),\n                dtype=object,\n                **kwargs,\n            )\n        else:\n            index = factory(\n                _unconvert_index(\n                    data, kind, encoding=self.encoding, errors=self.errors\n                ),\n                **kwargs,\n            )\n\n        index.name = name\n\n        return index\n\n    def write_array_empty(self, key: str, value: ArrayLike):\n        \"\"\" write a 0-len array \"\"\"\n        # ugly hack for length 0 axes\n        arr = np.empty((1,) * value.ndim)\n        self._handle.create_array(self.group, key, arr)\n        node = getattr(self.group, key)\n        node._v_attrs.value_type = str(value.dtype)\n        node._v_attrs.shape = value.shape\n\n    def write_array(self, key: str, obj: FrameOrSeries, items: Optional[Index] = None):\n        # TODO: we only have a few tests that get here, the only EA\n        #  that gets passed is DatetimeArray, and we never have\n        #  both self._filters and EA\n\n        value = extract_array(obj, extract_numpy=True)\n\n        if key in self.group:\n            self._handle.remove_node(self.group, key)\n\n        # Transform needed to interface with pytables row/col notation\n        empty_array = value.size == 0\n        transposed = False\n\n        if is_categorical_dtype(value.dtype):\n            raise NotImplementedError(\n                \"Cannot store a category dtype in a HDF5 dataset that uses format=\"\n                '\"fixed\". Use format=\"table\".'\n            )\n        if not empty_array:\n            if hasattr(value, \"T\"):\n                # ExtensionArrays (1d) may not have transpose.\n                value = value.T\n                transposed = True\n\n        atom = None\n        if self._filters is not None:\n            with suppress(ValueError):\n                # get the atom for this datatype\n                atom = _tables().Atom.from_dtype(value.dtype)\n\n        if atom is not None:\n            # We only get here if self._filters is non-None and\n            #  the Atom.from_dtype call succeeded\n\n            # create an empty chunked array and fill it from value\n            if not empty_array:\n                ca = self._handle.create_carray(\n                    self.group, key, atom, value.shape, filters=self._filters\n                )\n                ca[:] = value\n\n            else:\n                self.write_array_empty(key, value)\n\n        elif value.dtype.type == np.object_:\n            # infer the type, warn if we have a non-string type here (for\n            # performance)\n            inferred_type = lib.infer_dtype(value, skipna=False)\n            if empty_array:\n                pass\n            elif inferred_type == \"string\":\n                pass\n            else:\n                ws = performance_doc % (inferred_type, key, items)\n                warnings.warn(ws, PerformanceWarning, stacklevel=7)\n\n            vlarr = self._handle.create_vlarray(self.group, key, _tables().ObjectAtom())\n            vlarr.append(value)\n\n        elif is_datetime64_dtype(value.dtype):\n            self._handle.create_array(self.group, key, value.view(\"i8\"))\n            getattr(self.group, key)._v_attrs.value_type = \"datetime64\"\n        elif is_datetime64tz_dtype(value.dtype):\n            # store as UTC\n            # with a zone\n            self._handle.create_array(self.group, key, value.asi8)\n\n            node = getattr(self.group, key)\n            node._v_attrs.tz = _get_tz(value.tz)\n            node._v_attrs.value_type = \"datetime64\"\n        elif is_timedelta64_dtype(value.dtype):\n            self._handle.create_array(self.group, key, value.view(\"i8\"))\n            getattr(self.group, key)._v_attrs.value_type = \"timedelta64\"\n        elif empty_array:\n            self.write_array_empty(key, value)\n        else:\n            self._handle.create_array(self.group, key, value)\n\n        getattr(self.group, key)._v_attrs.transposed = transposed\n\n\nclass SeriesFixed(GenericFixed):\n    pandas_kind = \"series\"\n    attributes = [\"name\"]\n\n    name: Label\n\n    @property\n    def shape(self):\n        try:\n            return (len(self.group.values),)\n        except (TypeError, AttributeError):\n            return None\n\n    def read(\n        self,\n        where=None,\n        columns=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n        self.validate_read(columns, where)\n        index = self.read_index(\"index\", start=start, stop=stop)\n        values = self.read_array(\"values\", start=start, stop=stop)\n        return Series(values, index=index, name=self.name)\n\n    def write(self, obj, **kwargs):\n        super().write(obj, **kwargs)\n        self.write_index(\"index\", obj.index)\n        self.write_array(\"values\", obj)\n        self.attrs.name = obj.name\n\n\nclass BlockManagerFixed(GenericFixed):\n    attributes = [\"ndim\", \"nblocks\"]\n\n    nblocks: int\n\n    @property\n    def shape(self) -> Optional[Shape]:\n        try:\n            ndim = self.ndim\n\n            # items\n            items = 0\n            for i in range(self.nblocks):\n                node = getattr(self.group, f\"block{i}_items\")\n                shape = getattr(node, \"shape\", None)\n                if shape is not None:\n                    items += shape[0]\n\n            # data shape\n            node = self.group.block0_values\n            shape = getattr(node, \"shape\", None)\n            if shape is not None:\n                shape = list(shape[0 : (ndim - 1)])\n            else:\n                shape = []\n\n            shape.append(items)\n\n            return shape\n        except AttributeError:\n            return None\n\n    def read(\n        self,\n        where=None,\n        columns=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n        # start, stop applied to rows, so 0th axis only\n        self.validate_read(columns, where)\n        select_axis = self.obj_type()._get_block_manager_axis(0)\n\n        axes = []\n        for i in range(self.ndim):\n\n            _start, _stop = (start, stop) if i == select_axis else (None, None)\n            ax = self.read_index(f\"axis{i}\", start=_start, stop=_stop)\n            axes.append(ax)\n\n        items = axes[0]\n        dfs = []\n\n        for i in range(self.nblocks):\n\n            blk_items = self.read_index(f\"block{i}_items\")\n            values = self.read_array(f\"block{i}_values\", start=_start, stop=_stop)\n\n            columns = items[items.get_indexer(blk_items)]\n            df = DataFrame(values.T, columns=columns, index=axes[1])\n            dfs.append(df)\n\n        if len(dfs) > 0:\n            out = concat(dfs, axis=1)\n            out = out.reindex(columns=items, copy=False)\n            return out\n\n        return DataFrame(columns=axes[0], index=axes[1])\n\n    def write(self, obj, **kwargs):\n        super().write(obj, **kwargs)\n        data = obj._mgr\n        if not data.is_consolidated():\n            data = data.consolidate()\n\n        self.attrs.ndim = data.ndim\n        for i, ax in enumerate(data.axes):\n            if i == 0 and (not ax.is_unique):\n                raise ValueError(\"Columns index has to be unique for fixed format\")\n            self.write_index(f\"axis{i}\", ax)\n\n        # Supporting mixed-type DataFrame objects...nontrivial\n        self.attrs.nblocks = len(data.blocks)\n        for i, blk in enumerate(data.blocks):\n            # I have no idea why, but writing values before items fixed #2299\n            blk_items = data.items.take(blk.mgr_locs)\n            self.write_array(f\"block{i}_values\", blk.values, items=blk_items)\n            self.write_index(f\"block{i}_items\", blk_items)\n\n\nclass FrameFixed(BlockManagerFixed):\n    pandas_kind = \"frame\"\n    obj_type = DataFrame\n\n\nclass Table(Fixed):\n    \"\"\"\n    represent a table:\n        facilitate read/write of various types of tables\n\n    Attrs in Table Node\n    -------------------\n    These are attributes that are store in the main table node, they are\n    necessary to recreate these tables when read back in.\n\n    index_axes    : a list of tuples of the (original indexing axis and\n        index column)\n    non_index_axes: a list of tuples of the (original index axis and\n        columns on a non-indexing axis)\n    values_axes   : a list of the columns which comprise the data of this\n        table\n    data_columns  : a list of the columns that we are allowing indexing\n        (these become single columns in values_axes), or True to force all\n        columns\n    nan_rep       : the string to use for nan representations for string\n        objects\n    levels        : the names of levels\n    metadata      : the names of the metadata columns\n    \"\"\"\n\n    pandas_kind = \"wide_table\"\n    format_type: str = \"table\"  # GH#30962 needed by dask\n    table_type: str\n    levels: Union[int, List[Label]] = 1\n    is_table = True\n\n    index_axes: List[IndexCol]\n    non_index_axes: List[Tuple[int, Any]]\n    values_axes: List[DataCol]\n    data_columns: List\n    metadata: List\n    info: Dict\n\n    def __init__(\n        self,\n        parent: HDFStore,\n        group: \"Node\",\n        encoding=None,\n        errors: str = \"strict\",\n        index_axes=None,\n        non_index_axes=None,\n        values_axes=None,\n        data_columns=None,\n        info=None,\n        nan_rep=None,\n    ):\n        super().__init__(parent, group, encoding=encoding, errors=errors)\n        self.index_axes = index_axes or []\n        self.non_index_axes = non_index_axes or []\n        self.values_axes = values_axes or []\n        self.data_columns = data_columns or []\n        self.info = info or {}\n        self.nan_rep = nan_rep\n\n    @property\n    def table_type_short(self) -> str:\n        return self.table_type.split(\"_\")[0]\n\n    def __repr__(self) -> str:\n        \"\"\" return a pretty representation of myself \"\"\"\n        self.infer_axes()\n        jdc = \",\".join(self.data_columns) if len(self.data_columns) else \"\"\n        dc = f\",dc->[{jdc}]\"\n\n        ver = \"\"\n        if self.is_old_version:\n            jver = \".\".join(str(x) for x in self.version)\n            ver = f\"[{jver}]\"\n\n        jindex_axes = \",\".join(a.name for a in self.index_axes)\n        return (\n            f\"{self.pandas_type:12.12}{ver} \"\n            f\"(typ->{self.table_type_short},nrows->{self.nrows},\"\n            f\"ncols->{self.ncols},indexers->[{jindex_axes}]{dc})\"\n        )\n\n    def __getitem__(self, c: str):\n        \"\"\" return the axis for c \"\"\"\n        for a in self.axes:\n            if c == a.name:\n                return a\n        return None\n\n    def validate(self, other):\n        \"\"\" validate against an existing table \"\"\"\n        if other is None:\n            return\n\n        if other.table_type != self.table_type:\n            raise TypeError(\n                \"incompatible table_type with existing \"\n                f\"[{other.table_type} - {self.table_type}]\"\n            )\n\n        for c in [\"index_axes\", \"non_index_axes\", \"values_axes\"]:\n            sv = getattr(self, c, None)\n            ov = getattr(other, c, None)\n            if sv != ov:\n\n                # show the error for the specific axes\n                for i, sax in enumerate(sv):\n                    oax = ov[i]\n                    if sax != oax:\n                        raise ValueError(\n                            f\"invalid combination of [{c}] on appending data \"\n                            f\"[{sax}] vs current table [{oax}]\"\n                        )\n\n                # should never get here\n                raise Exception(\n                    f\"invalid combination of [{c}] on appending data [{sv}] vs \"\n                    f\"current table [{ov}]\"\n                )\n\n    @property\n    def is_multi_index(self) -> bool:\n        \"\"\"the levels attribute is 1 or a list in the case of a multi-index\"\"\"\n        return isinstance(self.levels, list)\n\n    def validate_multiindex(\n        self, obj: FrameOrSeriesUnion\n    ) -> Tuple[DataFrame, List[Label]]:\n        \"\"\"\n        validate that we can store the multi-index; reset and return the\n        new object\n        \"\"\"\n        levels = [\n            l if l is not None else f\"level_{i}\" for i, l in enumerate(obj.index.names)\n        ]\n        try:\n            reset_obj = obj.reset_index()\n        except ValueError as err:\n            raise ValueError(\n                \"duplicate names/columns in the multi-index when storing as a table\"\n            ) from err\n        assert isinstance(reset_obj, DataFrame)  # for mypy\n        return reset_obj, levels\n\n    @property\n    def nrows_expected(self) -> int:\n        \"\"\" based on our axes, compute the expected nrows \"\"\"\n        return np.prod([i.cvalues.shape[0] for i in self.index_axes])\n\n    @property\n    def is_exists(self) -> bool:\n        \"\"\" has this table been created \"\"\"\n        return \"table\" in self.group\n\n    @property\n    def storable(self):\n        return getattr(self.group, \"table\", None)\n\n    @property\n    def table(self):\n        \"\"\" return the table group (this is my storable) \"\"\"\n        return self.storable\n\n    @property\n    def dtype(self):\n        return self.table.dtype\n\n    @property\n    def description(self):\n        return self.table.description\n\n    @property\n    def axes(self):\n        return itertools.chain(self.index_axes, self.values_axes)\n\n    @property\n    def ncols(self) -> int:\n        \"\"\" the number of total columns in the values axes \"\"\"\n        return sum(len(a.values) for a in self.values_axes)\n\n    @property\n    def is_transposed(self) -> bool:\n        return False\n\n    @property\n    def data_orientation(self):\n        \"\"\"return a tuple of my permutated axes, non_indexable at the front\"\"\"\n        return tuple(\n            itertools.chain(\n                [int(a[0]) for a in self.non_index_axes],\n                [int(a.axis) for a in self.index_axes],\n            )\n        )\n\n    def queryables(self) -> Dict[str, Any]:\n        \"\"\" return a dict of the kinds allowable columns for this object \"\"\"\n        # mypy doesn't recognize DataFrame._AXIS_NAMES, so we re-write it here\n        axis_names = {0: \"index\", 1: \"columns\"}\n\n        # compute the values_axes queryables\n        d1 = [(a.cname, a) for a in self.index_axes]\n        d2 = [(axis_names[axis], None) for axis, values in self.non_index_axes]\n        d3 = [\n            (v.cname, v) for v in self.values_axes if v.name in set(self.data_columns)\n        ]\n\n        # error: Unsupported operand types for + (\"List[Tuple[str, IndexCol]]\"\n        # and \"List[Tuple[str, None]]\")\n        return dict(d1 + d2 + d3)  # type: ignore[operator]\n\n    def index_cols(self):\n        \"\"\" return a list of my index cols \"\"\"\n        # Note: each `i.cname` below is assured to be a str.\n        return [(i.axis, i.cname) for i in self.index_axes]\n\n    def values_cols(self) -> List[str]:\n        \"\"\" return a list of my values cols \"\"\"\n        return [i.cname for i in self.values_axes]\n\n    def _get_metadata_path(self, key: str) -> str:\n        \"\"\" return the metadata pathname for this key \"\"\"\n        group = self.group._v_pathname\n        return f\"{group}/meta/{key}/meta\"\n\n    def write_metadata(self, key: str, values: np.ndarray):\n        \"\"\"\n        Write out a metadata array to the key as a fixed-format Series.\n\n        Parameters\n        ----------\n        key : str\n        values : ndarray\n        \"\"\"\n        values = Series(values)\n        self.parent.put(\n            self._get_metadata_path(key),\n            values,\n            format=\"table\",\n            encoding=self.encoding,\n            errors=self.errors,\n            nan_rep=self.nan_rep,\n        )\n\n    def read_metadata(self, key: str):\n        \"\"\" return the meta data array for this key \"\"\"\n        if getattr(getattr(self.group, \"meta\", None), key, None) is not None:\n            return self.parent.select(self._get_metadata_path(key))\n        return None\n\n    def set_attrs(self):\n        \"\"\" set our table type & indexables \"\"\"\n        self.attrs.table_type = str(self.table_type)\n        self.attrs.index_cols = self.index_cols()\n        self.attrs.values_cols = self.values_cols()\n        self.attrs.non_index_axes = self.non_index_axes\n        self.attrs.data_columns = self.data_columns\n        self.attrs.nan_rep = self.nan_rep\n        self.attrs.encoding = self.encoding\n        self.attrs.errors = self.errors\n        self.attrs.levels = self.levels\n        self.attrs.info = self.info\n\n    def get_attrs(self):\n        \"\"\" retrieve our attributes \"\"\"\n        self.non_index_axes = getattr(self.attrs, \"non_index_axes\", None) or []\n        self.data_columns = getattr(self.attrs, \"data_columns\", None) or []\n        self.info = getattr(self.attrs, \"info\", None) or {}\n        self.nan_rep = getattr(self.attrs, \"nan_rep\", None)\n        self.encoding = _ensure_encoding(getattr(self.attrs, \"encoding\", None))\n        self.errors = _ensure_decoded(getattr(self.attrs, \"errors\", \"strict\"))\n        self.levels: List[Label] = getattr(self.attrs, \"levels\", None) or []\n        self.index_axes = [a for a in self.indexables if a.is_an_indexable]\n        self.values_axes = [a for a in self.indexables if not a.is_an_indexable]\n\n    def validate_version(self, where=None):\n        \"\"\" are we trying to operate on an old version? \"\"\"\n        if where is not None:\n            if self.version[0] <= 0 and self.version[1] <= 10 and self.version[2] < 1:\n                ws = incompatibility_doc % \".\".join([str(x) for x in self.version])\n                warnings.warn(ws, IncompatibilityWarning)\n\n    def validate_min_itemsize(self, min_itemsize):\n        \"\"\"\n        validate the min_itemsize doesn't contain items that are not in the\n        axes this needs data_columns to be defined\n        \"\"\"\n        if min_itemsize is None:\n            return\n        if not isinstance(min_itemsize, dict):\n            return\n\n        q = self.queryables()\n        for k, v in min_itemsize.items():\n\n            # ok, apply generally\n            if k == \"values\":\n                continue\n            if k not in q:\n                raise ValueError(\n                    f\"min_itemsize has the key [{k}] which is not an axis or \"\n                    \"data_column\"\n                )\n\n    @cache_readonly\n    def indexables(self):\n        \"\"\" create/cache the indexables if they don't exist \"\"\"\n        _indexables = []\n\n        desc = self.description\n        table_attrs = self.table.attrs\n\n        # Note: each of the `name` kwargs below are str, ensured\n        #  by the definition in index_cols.\n        # index columns\n        for i, (axis, name) in enumerate(self.attrs.index_cols):\n            atom = getattr(desc, name)\n            md = self.read_metadata(name)\n            meta = \"category\" if md is not None else None\n\n            kind_attr = f\"{name}_kind\"\n            kind = getattr(table_attrs, kind_attr, None)\n\n            index_col = IndexCol(\n                name=name,\n                axis=axis,\n                pos=i,\n                kind=kind,\n                typ=atom,\n                table=self.table,\n                meta=meta,\n                metadata=md,\n            )\n            _indexables.append(index_col)\n\n        # values columns\n        dc = set(self.data_columns)\n        base_pos = len(_indexables)\n\n        def f(i, c):\n            assert isinstance(c, str)\n            klass = DataCol\n            if c in dc:\n                klass = DataIndexableCol\n\n            atom = getattr(desc, c)\n            adj_name = _maybe_adjust_name(c, self.version)\n\n            # TODO: why kind_attr here?\n            values = getattr(table_attrs, f\"{adj_name}_kind\", None)\n            dtype = getattr(table_attrs, f\"{adj_name}_dtype\", None)\n            kind = _dtype_to_kind(dtype)\n\n            md = self.read_metadata(c)\n            # TODO: figure out why these two versions of `meta` dont always match.\n            #  meta = \"category\" if md is not None else None\n            meta = getattr(table_attrs, f\"{adj_name}_meta\", None)\n\n            obj = klass(\n                name=adj_name,\n                cname=c,\n                values=values,\n                kind=kind,\n                pos=base_pos + i,\n                typ=atom,\n                table=self.table,\n                meta=meta,\n                metadata=md,\n                dtype=dtype,\n            )\n            return obj\n\n        # Note: the definition of `values_cols` ensures that each\n        #  `c` below is a str.\n        _indexables.extend([f(i, c) for i, c in enumerate(self.attrs.values_cols)])\n\n        return _indexables\n\n    def create_index(self, columns=None, optlevel=None, kind: Optional[str] = None):\n        \"\"\"\n        Create a pytables index on the specified columns.\n\n        Parameters\n        ----------\n        columns : None, bool, or listlike[str]\n            Indicate which columns to create an index on.\n\n            * False : Do not create any indexes.\n            * True : Create indexes on all columns.\n            * None : Create indexes on all columns.\n            * listlike : Create indexes on the given columns.\n\n        optlevel : int or None, default None\n            Optimization level, if None, pytables defaults to 6.\n        kind : str or None, default None\n            Kind of index, if None, pytables defaults to \"medium\".\n\n        Raises\n        ------\n        TypeError if trying to create an index on a complex-type column.\n\n        Notes\n        -----\n        Cannot index Time64Col or ComplexCol.\n        Pytables must be >= 3.0.\n        \"\"\"\n        if not self.infer_axes():\n            return\n        if columns is False:\n            return\n\n        # index all indexables and data_columns\n        if columns is None or columns is True:\n            columns = [a.cname for a in self.axes if a.is_data_indexable]\n        if not isinstance(columns, (tuple, list)):\n            columns = [columns]\n\n        kw = {}\n        if optlevel is not None:\n            kw[\"optlevel\"] = optlevel\n        if kind is not None:\n            kw[\"kind\"] = kind\n\n        table = self.table\n        for c in columns:\n            v = getattr(table.cols, c, None)\n            if v is not None:\n                # remove the index if the kind/optlevel have changed\n                if v.is_indexed:\n                    index = v.index\n                    cur_optlevel = index.optlevel\n                    cur_kind = index.kind\n\n                    if kind is not None and cur_kind != kind:\n                        v.remove_index()\n                    else:\n                        kw[\"kind\"] = cur_kind\n\n                    if optlevel is not None and cur_optlevel != optlevel:\n                        v.remove_index()\n                    else:\n                        kw[\"optlevel\"] = cur_optlevel\n\n                # create the index\n                if not v.is_indexed:\n                    if v.type.startswith(\"complex\"):\n                        raise TypeError(\n                            \"Columns containing complex values can be stored but \"\n                            \"cannot be indexed when using table format. Either use \"\n                            \"fixed format, set index=False, or do not include \"\n                            \"the columns containing complex values to \"\n                            \"data_columns when initializing the table.\"\n                        )\n                    v.create_index(**kw)\n            elif c in self.non_index_axes[0][1]:\n                # GH 28156\n                raise AttributeError(\n                    f\"column {c} is not a data_column.\\n\"\n                    f\"In order to read column {c} you must reload the dataframe \\n\"\n                    f\"into HDFStore and include {c} with the data_columns argument.\"\n                )\n\n    def _read_axes(\n        self, where, start: Optional[int] = None, stop: Optional[int] = None\n    ) -> List[Tuple[ArrayLike, ArrayLike]]:\n        \"\"\"\n        Create the axes sniffed from the table.\n\n        Parameters\n        ----------\n        where : ???\n        start : int or None, default None\n        stop : int or None, default None\n\n        Returns\n        -------\n        List[Tuple[index_values, column_values]]\n        \"\"\"\n        # create the selection\n        selection = Selection(self, where=where, start=start, stop=stop)\n        values = selection.select()\n\n        results = []\n        # convert the data\n        for a in self.axes:\n            a.set_info(self.info)\n            res = a.convert(\n                values,\n                nan_rep=self.nan_rep,\n                encoding=self.encoding,\n                errors=self.errors,\n            )\n            results.append(res)\n\n        return results\n\n    @classmethod\n    def get_object(cls, obj, transposed: bool):\n        \"\"\" return the data for this obj \"\"\"\n        return obj\n\n    def validate_data_columns(self, data_columns, min_itemsize, non_index_axes):\n        \"\"\"\n        take the input data_columns and min_itemize and create a data\n        columns spec\n        \"\"\"\n        if not len(non_index_axes):\n            return []\n\n        axis, axis_labels = non_index_axes[0]\n        info = self.info.get(axis, {})\n        if info.get(\"type\") == \"MultiIndex\" and data_columns:\n            raise ValueError(\n                f\"cannot use a multi-index on axis [{axis}] with \"\n                f\"data_columns {data_columns}\"\n            )\n\n        # evaluate the passed data_columns, True == use all columns\n        # take only valid axis labels\n        if data_columns is True:\n            data_columns = list(axis_labels)\n        elif data_columns is None:\n            data_columns = []\n\n        # if min_itemsize is a dict, add the keys (exclude 'values')\n        if isinstance(min_itemsize, dict):\n            existing_data_columns = set(data_columns)\n            data_columns = list(data_columns)  # ensure we do not modify\n            data_columns.extend(\n                [\n                    k\n                    for k in min_itemsize.keys()\n                    if k != \"values\" and k not in existing_data_columns\n                ]\n            )\n\n        # return valid columns in the order of our axis\n        return [c for c in data_columns if c in axis_labels]\n\n    def _create_axes(\n        self,\n        axes,\n        obj: DataFrame,\n        validate: bool = True,\n        nan_rep=None,\n        data_columns=None,\n        min_itemsize=None,\n    ):\n        \"\"\"\n        Create and return the axes.\n\n        Parameters\n        ----------\n        axes: list or None\n            The names or numbers of the axes to create.\n        obj : DataFrame\n            The object to create axes on.\n        validate: bool, default True\n            Whether to validate the obj against an existing object already written.\n        nan_rep :\n            A value to use for string column nan_rep.\n        data_columns : List[str], True, or None, default None\n            Specify the columns that we want to create to allow indexing on.\n\n            * True : Use all available columns.\n            * None : Use no columns.\n            * List[str] : Use the specified columns.\n\n        min_itemsize: Dict[str, int] or None, default None\n            The min itemsize for a column in bytes.\n        \"\"\"\n        if not isinstance(obj, DataFrame):\n            group = self.group._v_name\n            raise TypeError(\n                f\"cannot properly create the storer for: [group->{group},\"\n                f\"value->{type(obj)}]\"\n            )\n\n        # set the default axes if needed\n        if axes is None:\n            axes = [0]\n\n        # map axes to numbers\n        axes = [obj._get_axis_number(a) for a in axes]\n\n        # do we have an existing table (if so, use its axes & data_columns)\n        if self.infer_axes():\n            table_exists = True\n            axes = [a.axis for a in self.index_axes]\n            data_columns = list(self.data_columns)\n            nan_rep = self.nan_rep\n            # TODO: do we always have validate=True here?\n        else:\n            table_exists = False\n\n        new_info = self.info\n\n        assert self.ndim == 2  # with next check, we must have len(axes) == 1\n        # currently support on ndim-1 axes\n        if len(axes) != self.ndim - 1:\n            raise ValueError(\n                \"currently only support ndim-1 indexers in an AppendableTable\"\n            )\n\n        # create according to the new data\n        new_non_index_axes: List = []\n\n        # nan_representation\n        if nan_rep is None:\n            nan_rep = \"nan\"\n\n        # We construct the non-index-axis first, since that alters new_info\n        idx = [x for x in [0, 1] if x not in axes][0]\n\n        a = obj.axes[idx]\n        # we might be able to change the axes on the appending data if necessary\n        append_axis = list(a)\n        if table_exists:\n            indexer = len(new_non_index_axes)  # i.e. 0\n            exist_axis = self.non_index_axes[indexer][1]\n            if not array_equivalent(np.array(append_axis), np.array(exist_axis)):\n\n                # ahah! -> reindex\n                if array_equivalent(\n                    np.array(sorted(append_axis)), np.array(sorted(exist_axis))\n                ):\n                    append_axis = exist_axis\n\n        # the non_index_axes info\n        info = new_info.setdefault(idx, {})\n        info[\"names\"] = list(a.names)\n        info[\"type\"] = type(a).__name__\n\n        new_non_index_axes.append((idx, append_axis))\n\n        # Now we can construct our new index axis\n        idx = axes[0]\n        a = obj.axes[idx]\n        axis_name = obj._get_axis_name(idx)\n        new_index = _convert_index(axis_name, a, self.encoding, self.errors)\n        new_index.axis = idx\n\n        # Because we are always 2D, there is only one new_index, so\n        #  we know it will have pos=0\n        new_index.set_pos(0)\n        new_index.update_info(new_info)\n        new_index.maybe_set_size(min_itemsize)  # check for column conflicts\n\n        new_index_axes = [new_index]\n        j = len(new_index_axes)  # i.e. 1\n        assert j == 1\n\n        # reindex by our non_index_axes & compute data_columns\n        assert len(new_non_index_axes) == 1\n        for a in new_non_index_axes:\n            obj = _reindex_axis(obj, a[0], a[1])\n\n        def get_blk_items(mgr, blocks):\n            return [mgr.items.take(blk.mgr_locs) for blk in blocks]\n\n        transposed = new_index.axis == 1\n\n        # figure out data_columns and get out blocks\n        data_columns = self.validate_data_columns(\n            data_columns, min_itemsize, new_non_index_axes\n        )\n\n        block_obj = self.get_object(obj, transposed)._consolidate()\n\n        blocks, blk_items = self._get_blocks_and_items(\n            block_obj, table_exists, new_non_index_axes, self.values_axes, data_columns\n        )\n\n        # add my values\n        vaxes = []\n        for i, (b, b_items) in enumerate(zip(blocks, blk_items)):\n\n            # shape of the data column are the indexable axes\n            klass = DataCol\n            name = None\n\n            # we have a data_column\n            if data_columns and len(b_items) == 1 and b_items[0] in data_columns:\n                klass = DataIndexableCol\n                name = b_items[0]\n                if not (name is None or isinstance(name, str)):\n                    # TODO: should the message here be more specifically non-str?\n                    raise ValueError(\"cannot have non-object label DataIndexableCol\")\n\n            # make sure that we match up the existing columns\n            # if we have an existing table\n            existing_col: Optional[DataCol]\n\n            if table_exists and validate:\n                try:\n                    existing_col = self.values_axes[i]\n                except (IndexError, KeyError) as err:\n                    raise ValueError(\n                        f\"Incompatible appended table [{blocks}]\"\n                        f\"with existing table [{self.values_axes}]\"\n                    ) from err\n            else:\n                existing_col = None\n\n            new_name = name or f\"values_block_{i}\"\n            data_converted = _maybe_convert_for_string_atom(\n                new_name,\n                b,\n                existing_col=existing_col,\n                min_itemsize=min_itemsize,\n                nan_rep=nan_rep,\n                encoding=self.encoding,\n                errors=self.errors,\n            )\n            adj_name = _maybe_adjust_name(new_name, self.version)\n\n            typ = klass._get_atom(data_converted)\n            kind = _dtype_to_kind(data_converted.dtype.name)\n            tz = _get_tz(data_converted.tz) if hasattr(data_converted, \"tz\") else None\n\n            meta = metadata = ordered = None\n            if is_categorical_dtype(data_converted.dtype):\n                ordered = data_converted.ordered\n                meta = \"category\"\n                metadata = np.array(data_converted.categories, copy=False).ravel()\n\n            data, dtype_name = _get_data_and_dtype_name(data_converted)\n\n            col = klass(\n                name=adj_name,\n                cname=new_name,\n                values=list(b_items),\n                typ=typ,\n                pos=j,\n                kind=kind,\n                tz=tz,\n                ordered=ordered,\n                meta=meta,\n                metadata=metadata,\n                dtype=dtype_name,\n                data=data,\n            )\n            col.update_info(new_info)\n\n            vaxes.append(col)\n\n            j += 1\n\n        dcs = [col.name for col in vaxes if col.is_data_indexable]\n\n        new_table = type(self)(\n            parent=self.parent,\n            group=self.group,\n            encoding=self.encoding,\n            errors=self.errors,\n            index_axes=new_index_axes,\n            non_index_axes=new_non_index_axes,\n            values_axes=vaxes,\n            data_columns=dcs,\n            info=new_info,\n            nan_rep=nan_rep,\n        )\n        if hasattr(self, \"levels\"):\n            # TODO: get this into constructor, only for appropriate subclass\n            new_table.levels = self.levels\n\n        new_table.validate_min_itemsize(min_itemsize)\n\n        if validate and table_exists:\n            new_table.validate(self)\n\n        return new_table\n\n    @staticmethod\n    def _get_blocks_and_items(\n        block_obj, table_exists, new_non_index_axes, values_axes, data_columns\n    ):\n        # Helper to clarify non-state-altering parts of _create_axes\n\n        def get_blk_items(mgr, blocks):\n            return [mgr.items.take(blk.mgr_locs) for blk in blocks]\n\n        blocks = block_obj._mgr.blocks\n        blk_items = get_blk_items(block_obj._mgr, blocks)\n\n        if len(data_columns):\n            axis, axis_labels = new_non_index_axes[0]\n            new_labels = Index(axis_labels).difference(Index(data_columns))\n            mgr = block_obj.reindex(new_labels, axis=axis)._mgr\n\n            blocks = list(mgr.blocks)\n            blk_items = get_blk_items(mgr, blocks)\n            for c in data_columns:\n                mgr = block_obj.reindex([c], axis=axis)._mgr\n                blocks.extend(mgr.blocks)\n                blk_items.extend(get_blk_items(mgr, mgr.blocks))\n\n        # reorder the blocks in the same order as the existing table if we can\n        if table_exists:\n            by_items = {\n                tuple(b_items.tolist()): (b, b_items)\n                for b, b_items in zip(blocks, blk_items)\n            }\n            new_blocks = []\n            new_blk_items = []\n            for ea in values_axes:\n                items = tuple(ea.values)\n                try:\n                    b, b_items = by_items.pop(items)\n                    new_blocks.append(b)\n                    new_blk_items.append(b_items)\n                except (IndexError, KeyError) as err:\n                    jitems = \",\".join(pprint_thing(item) for item in items)\n                    raise ValueError(\n                        f\"cannot match existing table structure for [{jitems}] \"\n                        \"on appending data\"\n                    ) from err\n            blocks = new_blocks\n            blk_items = new_blk_items\n\n        return blocks, blk_items\n\n    def process_axes(self, obj, selection: \"Selection\", columns=None):\n        \"\"\" process axes filters \"\"\"\n        # make a copy to avoid side effects\n        if columns is not None:\n            columns = list(columns)\n\n        # make sure to include levels if we have them\n        if columns is not None and self.is_multi_index:\n            assert isinstance(self.levels, list)  # assured by is_multi_index\n            for n in self.levels:\n                if n not in columns:\n                    columns.insert(0, n)\n\n        # reorder by any non_index_axes & limit to the select columns\n        for axis, labels in self.non_index_axes:\n            obj = _reindex_axis(obj, axis, labels, columns)\n\n        # apply the selection filters (but keep in the same order)\n        if selection.filter is not None:\n            for field, op, filt in selection.filter.format():\n\n                def process_filter(field, filt):\n\n                    for axis_name in obj._AXIS_ORDERS:\n                        axis_number = obj._get_axis_number(axis_name)\n                        axis_values = obj._get_axis(axis_name)\n                        assert axis_number is not None\n\n                        # see if the field is the name of an axis\n                        if field == axis_name:\n\n                            # if we have a multi-index, then need to include\n                            # the levels\n                            if self.is_multi_index:\n                                filt = filt.union(Index(self.levels))\n\n                            takers = op(axis_values, filt)\n                            return obj.loc(axis=axis_number)[takers]\n\n                        # this might be the name of a file IN an axis\n                        elif field in axis_values:\n\n                            # we need to filter on this dimension\n                            values = ensure_index(getattr(obj, field).values)\n                            filt = ensure_index(filt)\n\n                            # hack until we support reversed dim flags\n                            if isinstance(obj, DataFrame):\n                                axis_number = 1 - axis_number\n                            takers = op(values, filt)\n                            return obj.loc(axis=axis_number)[takers]\n\n                    raise ValueError(f\"cannot find the field [{field}] for filtering!\")\n\n                obj = process_filter(field, filt)\n\n        return obj\n\n    def create_description(\n        self,\n        complib,\n        complevel: Optional[int],\n        fletcher32: bool,\n        expectedrows: Optional[int],\n    ) -> Dict[str, Any]:\n        \"\"\" create the description of the table from the axes & values \"\"\"\n        # provided expected rows if its passed\n        if expectedrows is None:\n            expectedrows = max(self.nrows_expected, 10000)\n\n        d = {\"name\": \"table\", \"expectedrows\": expectedrows}\n\n        # description from the axes & values\n        d[\"description\"] = {a.cname: a.typ for a in self.axes}\n\n        if complib:\n            if complevel is None:\n                complevel = self._complevel or 9\n            filters = _tables().Filters(\n                complevel=complevel,\n                complib=complib,\n                fletcher32=fletcher32 or self._fletcher32,\n            )\n            d[\"filters\"] = filters\n        elif self._filters is not None:\n            d[\"filters\"] = self._filters\n\n        return d\n\n    def read_coordinates(\n        self, where=None, start: Optional[int] = None, stop: Optional[int] = None\n    ):\n        \"\"\"\n        select coordinates (row numbers) from a table; return the\n        coordinates object\n        \"\"\"\n        # validate the version\n        self.validate_version(where)\n\n        # infer the data kind\n        if not self.infer_axes():\n            return False\n\n        # create the selection\n        selection = Selection(self, where=where, start=start, stop=stop)\n        coords = selection.select_coords()\n        if selection.filter is not None:\n            for field, op, filt in selection.filter.format():\n                data = self.read_column(\n                    field, start=coords.min(), stop=coords.max() + 1\n                )\n                coords = coords[op(data.iloc[coords - coords.min()], filt).values]\n\n        return Index(coords)\n\n    def read_column(\n        self,\n        column: str,\n        where=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n        \"\"\"\n        return a single column from the table, generally only indexables\n        are interesting\n        \"\"\"\n        # validate the version\n        self.validate_version()\n\n        # infer the data kind\n        if not self.infer_axes():\n            return False\n\n        if where is not None:\n            raise TypeError(\"read_column does not currently accept a where clause\")\n\n        # find the axes\n        for a in self.axes:\n            if column == a.name:\n                if not a.is_data_indexable:\n                    raise ValueError(\n                        f\"column [{column}] can not be extracted individually; \"\n                        \"it is not data indexable\"\n                    )\n\n                # column must be an indexable or a data column\n                c = getattr(self.table.cols, column)\n                a.set_info(self.info)\n                col_values = a.convert(\n                    c[start:stop],\n                    nan_rep=self.nan_rep,\n                    encoding=self.encoding,\n                    errors=self.errors,\n                )\n                return Series(_set_tz(col_values[1], a.tz), name=column)\n\n        raise KeyError(f\"column [{column}] not found in the table\")\n\n\nclass WORMTable(Table):\n    \"\"\"\n    a write-once read-many table: this format DOES NOT ALLOW appending to a\n    table. writing is a one-time operation the data are stored in a format\n    that allows for searching the data on disk\n    \"\"\"\n\n    table_type = \"worm\"\n\n    def read(\n        self,\n        where=None,\n        columns=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n        \"\"\"\n        read the indices and the indexing array, calculate offset rows and return\n        \"\"\"\n        raise NotImplementedError(\"WORMTable needs to implement read\")\n\n    def write(self, **kwargs):\n        \"\"\"\n        write in a format that we can search later on (but cannot append\n        to): write out the indices and the values using _write_array\n        (e.g. a CArray) create an indexing table so that we can search\n        \"\"\"\n        raise NotImplementedError(\"WORMTable needs to implement write\")\n\n\nclass AppendableTable(Table):\n    \"\"\" support the new appendable table formats \"\"\"\n\n    table_type = \"appendable\"\n\n    def write(\n        self,\n        obj,\n        axes=None,\n        append=False,\n        complib=None,\n        complevel=None,\n        fletcher32=None,\n        min_itemsize=None,\n        chunksize=None,\n        expectedrows=None,\n        dropna=False,\n        nan_rep=None,\n        data_columns=None,\n        track_times=True,\n    ):\n        if not append and self.is_exists:\n            self._handle.remove_node(self.group, \"table\")\n\n        # create the axes\n        table = self._create_axes(\n            axes=axes,\n            obj=obj,\n            validate=append,\n            min_itemsize=min_itemsize,\n            nan_rep=nan_rep,\n            data_columns=data_columns,\n        )\n\n        for a in table.axes:\n            a.validate_names()\n\n        if not table.is_exists:\n\n            # create the table\n            options = table.create_description(\n                complib=complib,\n                complevel=complevel,\n                fletcher32=fletcher32,\n                expectedrows=expectedrows,\n            )\n\n            # set the table attributes\n            table.set_attrs()\n\n            options[\"track_times\"] = track_times\n\n            # create the table\n            table._handle.create_table(table.group, **options)\n\n        # update my info\n        table.attrs.info = table.info\n\n        # validate the axes and set the kinds\n        for a in table.axes:\n            a.validate_and_set(table, append)\n\n        # add the rows\n        table.write_data(chunksize, dropna=dropna)\n\n    def write_data(self, chunksize: Optional[int], dropna: bool = False):\n        \"\"\"\n        we form the data into a 2-d including indexes,values,mask write chunk-by-chunk\n        \"\"\"\n        names = self.dtype.names\n        nrows = self.nrows_expected\n\n        # if dropna==True, then drop ALL nan rows\n        masks = []\n        if dropna:\n            for a in self.values_axes:\n                # figure the mask: only do if we can successfully process this\n                # column, otherwise ignore the mask\n                mask = isna(a.data).all(axis=0)\n                if isinstance(mask, np.ndarray):\n                    masks.append(mask.astype(\"u1\", copy=False))\n\n        # consolidate masks\n        if len(masks):\n            mask = masks[0]\n            for m in masks[1:]:\n                mask = mask & m\n            mask = mask.ravel()\n        else:\n            mask = None\n\n        # broadcast the indexes if needed\n        indexes = [a.cvalues for a in self.index_axes]\n        nindexes = len(indexes)\n        assert nindexes == 1, nindexes  # ensures we dont need to broadcast\n\n        # transpose the values so first dimension is last\n        # reshape the values if needed\n        values = [a.take_data() for a in self.values_axes]\n        values = [v.transpose(np.roll(np.arange(v.ndim), v.ndim - 1)) for v in values]\n        bvalues = []\n        for i, v in enumerate(values):\n            new_shape = (nrows,) + self.dtype[names[nindexes + i]].shape\n            bvalues.append(values[i].reshape(new_shape))\n\n        # write the chunks\n        if chunksize is None:\n            chunksize = 100000\n\n        rows = np.empty(min(chunksize, nrows), dtype=self.dtype)\n        chunks = nrows // chunksize + 1\n        for i in range(chunks):\n            start_i = i * chunksize\n            end_i = min((i + 1) * chunksize, nrows)\n            if start_i >= end_i:\n                break\n\n            self.write_data_chunk(\n                rows,\n                indexes=[a[start_i:end_i] for a in indexes],\n                mask=mask[start_i:end_i] if mask is not None else None,\n                values=[v[start_i:end_i] for v in bvalues],\n            )\n\n    def write_data_chunk(\n        self,\n        rows: np.ndarray,\n        indexes: List[np.ndarray],\n        mask: Optional[np.ndarray],\n        values: List[np.ndarray],\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        rows : an empty memory space where we are putting the chunk\n        indexes : an array of the indexes\n        mask : an array of the masks\n        values : an array of the values\n        \"\"\"\n        # 0 len\n        for v in values:\n            if not np.prod(v.shape):\n                return\n\n        nrows = indexes[0].shape[0]\n        if nrows != len(rows):\n            rows = np.empty(nrows, dtype=self.dtype)\n        names = self.dtype.names\n        nindexes = len(indexes)\n\n        # indexes\n        for i, idx in enumerate(indexes):\n            rows[names[i]] = idx\n\n        # values\n        for i, v in enumerate(values):\n            rows[names[i + nindexes]] = v\n\n        # mask\n        if mask is not None:\n            m = ~mask.ravel().astype(bool, copy=False)\n            if not m.all():\n                rows = rows[m]\n\n        if len(rows):\n            self.table.append(rows)\n            self.table.flush()\n\n    def delete(\n        self, where=None, start: Optional[int] = None, stop: Optional[int] = None\n    ):\n\n        # delete all rows (and return the nrows)\n        if where is None or not len(where):\n            if start is None and stop is None:\n                nrows = self.nrows\n                self._handle.remove_node(self.group, recursive=True)\n            else:\n                # pytables<3.0 would remove a single row with stop=None\n                if stop is None:\n                    stop = self.nrows\n                nrows = self.table.remove_rows(start=start, stop=stop)\n                self.table.flush()\n            return nrows\n\n        # infer the data kind\n        if not self.infer_axes():\n            return None\n\n        # create the selection\n        table = self.table\n        selection = Selection(self, where, start=start, stop=stop)\n        values = selection.select_coords()\n\n        # delete the rows in reverse order\n        sorted_series = Series(values).sort_values()\n        ln = len(sorted_series)\n\n        if ln:\n\n            # construct groups of consecutive rows\n            diff = sorted_series.diff()\n            groups = list(diff[diff > 1].index)\n\n            # 1 group\n            if not len(groups):\n                groups = [0]\n\n            # final element\n            if groups[-1] != ln:\n                groups.append(ln)\n\n            # initial element\n            if groups[0] != 0:\n                groups.insert(0, 0)\n\n            # we must remove in reverse order!\n            pg = groups.pop()\n            for g in reversed(groups):\n                rows = sorted_series.take(range(g, pg))\n                table.remove_rows(\n                    start=rows[rows.index[0]], stop=rows[rows.index[-1]] + 1\n                )\n                pg = g\n\n            self.table.flush()\n\n        # return the number of rows removed\n        return ln\n\n\nclass AppendableFrameTable(AppendableTable):\n    \"\"\" support the new appendable table formats \"\"\"\n\n    pandas_kind = \"frame_table\"\n    table_type = \"appendable_frame\"\n    ndim = 2\n    obj_type: Type[FrameOrSeriesUnion] = DataFrame\n\n    @property\n    def is_transposed(self) -> bool:\n        return self.index_axes[0].axis == 1\n\n    @classmethod\n    def get_object(cls, obj, transposed: bool):\n        \"\"\" these are written transposed \"\"\"\n        if transposed:\n            obj = obj.T\n        return obj\n\n    def read(\n        self,\n        where=None,\n        columns=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n\n        # validate the version\n        self.validate_version(where)\n\n        # infer the data kind\n        if not self.infer_axes():\n            return None\n\n        result = self._read_axes(where=where, start=start, stop=stop)\n\n        info = (\n            self.info.get(self.non_index_axes[0][0], {})\n            if len(self.non_index_axes)\n            else {}\n        )\n\n        inds = [i for i, ax in enumerate(self.axes) if ax is self.index_axes[0]]\n        assert len(inds) == 1\n        ind = inds[0]\n\n        index = result[ind][0]\n\n        frames = []\n        for i, a in enumerate(self.axes):\n            if a not in self.values_axes:\n                continue\n            index_vals, cvalues = result[i]\n\n            # we could have a multi-index constructor here\n            # ensure_index doesn't recognized our list-of-tuples here\n            if info.get(\"type\") == \"MultiIndex\":\n                cols = MultiIndex.from_tuples(index_vals)\n            else:\n                cols = Index(index_vals)\n\n            names = info.get(\"names\")\n            if names is not None:\n                cols.set_names(names, inplace=True)\n\n            if self.is_transposed:\n                values = cvalues\n                index_ = cols\n                cols_ = Index(index, name=getattr(index, \"name\", None))\n            else:\n                values = cvalues.T\n                index_ = Index(index, name=getattr(index, \"name\", None))\n                cols_ = cols\n\n            # if we have a DataIndexableCol, its shape will only be 1 dim\n            if values.ndim == 1 and isinstance(values, np.ndarray):\n                values = values.reshape((1, values.shape[0]))\n\n            if isinstance(values, np.ndarray):\n                df = DataFrame(values.T, columns=cols_, index=index_)\n            elif isinstance(values, Index):\n                df = DataFrame(values, columns=cols_, index=index_)\n            else:\n                # Categorical\n                df = DataFrame([values], columns=cols_, index=index_)\n            assert (df.dtypes == values.dtype).all(), (df.dtypes, values.dtype)\n            frames.append(df)\n\n        if len(frames) == 1:\n            df = frames[0]\n        else:\n            df = concat(frames, axis=1)\n\n        selection = Selection(self, where=where, start=start, stop=stop)\n        # apply the selection filters & axis orderings\n        df = self.process_axes(df, selection=selection, columns=columns)\n\n        return df\n\n\nclass AppendableSeriesTable(AppendableFrameTable):\n    \"\"\" support the new appendable table formats \"\"\"\n\n    pandas_kind = \"series_table\"\n    table_type = \"appendable_series\"\n    ndim = 2\n    obj_type = Series\n\n    @property\n    def is_transposed(self) -> bool:\n        return False\n\n    @classmethod\n    def get_object(cls, obj, transposed: bool):\n        return obj\n\n    def write(self, obj, data_columns=None, **kwargs):\n        \"\"\" we are going to write this as a frame table \"\"\"\n        if not isinstance(obj, DataFrame):\n            name = obj.name or \"values\"\n            obj = obj.to_frame(name)\n        return super().write(obj=obj, data_columns=obj.columns.tolist(), **kwargs)\n\n    def read(\n        self,\n        where=None,\n        columns=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ) -> Series:\n\n        is_multi_index = self.is_multi_index\n        if columns is not None and is_multi_index:\n            assert isinstance(self.levels, list)  # needed for mypy\n            for n in self.levels:\n                if n not in columns:\n                    columns.insert(0, n)\n        s = super().read(where=where, columns=columns, start=start, stop=stop)\n        if is_multi_index:\n            s.set_index(self.levels, inplace=True)\n\n        s = s.iloc[:, 0]\n\n        # remove the default name\n        if s.name == \"values\":\n            s.name = None\n        return s\n\n\nclass AppendableMultiSeriesTable(AppendableSeriesTable):\n    \"\"\" support the new appendable table formats \"\"\"\n\n    pandas_kind = \"series_table\"\n    table_type = \"appendable_multiseries\"\n\n    def write(self, obj, **kwargs):\n        \"\"\" we are going to write this as a frame table \"\"\"\n        name = obj.name or \"values\"\n        newobj, self.levels = self.validate_multiindex(obj)\n        assert isinstance(self.levels, list)  # for mypy\n        cols = list(self.levels)\n        cols.append(name)\n        newobj.columns = Index(cols)\n        return super().write(obj=newobj, **kwargs)\n\n\nclass GenericTable(AppendableFrameTable):\n    \"\"\" a table that read/writes the generic pytables table format \"\"\"\n\n    pandas_kind = \"frame_table\"\n    table_type = \"generic_table\"\n    ndim = 2\n    obj_type = DataFrame\n    levels: List[Label]\n\n    @property\n    def pandas_type(self) -> str:\n        return self.pandas_kind\n\n    @property\n    def storable(self):\n        return getattr(self.group, \"table\", None) or self.group\n\n    def get_attrs(self):\n        \"\"\" retrieve our attributes \"\"\"\n        self.non_index_axes = []\n        self.nan_rep = None\n        self.levels = []\n\n        self.index_axes = [a for a in self.indexables if a.is_an_indexable]\n        self.values_axes = [a for a in self.indexables if not a.is_an_indexable]\n        self.data_columns = [a.name for a in self.values_axes]\n\n    @cache_readonly\n    def indexables(self):\n        \"\"\" create the indexables from the table description \"\"\"\n        d = self.description\n\n        # TODO: can we get a typ for this?  AFAICT it is the only place\n        #  where we aren't passing one\n        # the index columns is just a simple index\n        md = self.read_metadata(\"index\")\n        meta = \"category\" if md is not None else None\n        index_col = GenericIndexCol(\n            name=\"index\", axis=0, table=self.table, meta=meta, metadata=md\n        )\n\n        _indexables: List[Union[GenericIndexCol, GenericDataIndexableCol]] = [index_col]\n\n        for i, n in enumerate(d._v_names):\n            assert isinstance(n, str)\n\n            atom = getattr(d, n)\n            md = self.read_metadata(n)\n            meta = \"category\" if md is not None else None\n            dc = GenericDataIndexableCol(\n                name=n,\n                pos=i,\n                values=[n],\n                typ=atom,\n                table=self.table,\n                meta=meta,\n                metadata=md,\n            )\n            _indexables.append(dc)\n\n        return _indexables\n\n    def write(self, **kwargs):\n        raise NotImplementedError(\"cannot write on an generic table\")\n\n\nclass AppendableMultiFrameTable(AppendableFrameTable):\n    \"\"\" a frame with a multi-index \"\"\"\n\n    table_type = \"appendable_multiframe\"\n    obj_type = DataFrame\n    ndim = 2\n    _re_levels = re.compile(r\"^level_\\d+$\")\n\n    @property\n    def table_type_short(self) -> str:\n        return \"appendable_multi\"\n\n    def write(self, obj, data_columns=None, **kwargs):\n        if data_columns is None:\n            data_columns = []\n        elif data_columns is True:\n            data_columns = obj.columns.tolist()\n        obj, self.levels = self.validate_multiindex(obj)\n        assert isinstance(self.levels, list)  # for mypy\n        for n in self.levels:\n            if n not in data_columns:\n                data_columns.insert(0, n)\n        return super().write(obj=obj, data_columns=data_columns, **kwargs)\n\n    def read(\n        self,\n        where=None,\n        columns=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n\n        df = super().read(where=where, columns=columns, start=start, stop=stop)\n        df = df.set_index(self.levels)\n\n        # remove names for 'level_%d'\n        df.index = df.index.set_names(\n            [None if self._re_levels.search(name) else name for name in df.index.names]\n        )\n\n        return df\n\n\ndef _reindex_axis(obj: DataFrame, axis: int, labels: Index, other=None) -> DataFrame:\n    ax = obj._get_axis(axis)\n    labels = ensure_index(labels)\n\n    # try not to reindex even if other is provided\n    # if it equals our current index\n    if other is not None:\n        other = ensure_index(other)\n    if (other is None or labels.equals(other)) and labels.equals(ax):\n        return obj\n\n    labels = ensure_index(labels.unique())\n    if other is not None:\n        labels = ensure_index(other.unique()).intersection(labels, sort=False)\n    if not labels.equals(ax):\n        slicer: List[Union[slice, Index]] = [slice(None, None)] * obj.ndim\n        slicer[axis] = labels\n        obj = obj.loc[tuple(slicer)]\n    return obj\n\n\n# tz to/from coercion\n\n\ndef _get_tz(tz: tzinfo) -> Union[str, tzinfo]:\n    \"\"\" for a tz-aware type, return an encoded zone \"\"\"\n    zone = timezones.get_timezone(tz)\n    return zone\n\n\ndef _set_tz(\n    values: Union[np.ndarray, Index],\n    tz: Optional[Union[str, tzinfo]],\n    coerce: bool = False,\n) -> Union[np.ndarray, DatetimeIndex]:\n    \"\"\"\n    coerce the values to a DatetimeIndex if tz is set\n    preserve the input shape if possible\n\n    Parameters\n    ----------\n    values : ndarray or Index\n    tz : str or tzinfo\n    coerce : if we do not have a passed timezone, coerce to M8[ns] ndarray\n    \"\"\"\n    if isinstance(values, DatetimeIndex):\n        # If values is tzaware, the tz gets dropped in the values.ravel()\n        #  call below (which returns an ndarray).  So we are only non-lossy\n        #  if `tz` matches `values.tz`.\n        assert values.tz is None or values.tz == tz\n\n    if tz is not None:\n        if isinstance(values, DatetimeIndex):\n            name = values.name\n            values = values.asi8\n        else:\n            name = None\n            values = values.ravel()\n\n        tz = _ensure_decoded(tz)\n        values = DatetimeIndex(values, name=name)\n        values = values.tz_localize(\"UTC\").tz_convert(tz)\n    elif coerce:\n        values = np.asarray(values, dtype=\"M8[ns]\")\n\n    return values\n\n\ndef _convert_index(name: str, index: Index, encoding: str, errors: str) -> IndexCol:\n    assert isinstance(name, str)\n\n    index_name = index.name\n    converted, dtype_name = _get_data_and_dtype_name(index)\n    kind = _dtype_to_kind(dtype_name)\n    atom = DataIndexableCol._get_atom(converted)\n\n    if isinstance(index, Int64Index) or needs_i8_conversion(index.dtype):\n        # Includes Int64Index, RangeIndex, DatetimeIndex, TimedeltaIndex, PeriodIndex,\n        #  in which case \"kind\" is \"integer\", \"integer\", \"datetime64\",\n        #  \"timedelta64\", and \"integer\", respectively.\n        return IndexCol(\n            name,\n            values=converted,\n            kind=kind,\n            typ=atom,\n            freq=getattr(index, \"freq\", None),\n            tz=getattr(index, \"tz\", None),\n            index_name=index_name,\n        )\n\n    if isinstance(index, MultiIndex):\n        raise TypeError(\"MultiIndex not supported here!\")\n\n    inferred_type = lib.infer_dtype(index, skipna=False)\n    # we won't get inferred_type of \"datetime64\" or \"timedelta64\" as these\n    #  would go through the DatetimeIndex/TimedeltaIndex paths above\n\n    values = np.asarray(index)\n\n    if inferred_type == \"date\":\n        converted = np.asarray([v.toordinal() for v in values], dtype=np.int32)\n        return IndexCol(\n            name, converted, \"date\", _tables().Time32Col(), index_name=index_name\n        )\n    elif inferred_type == \"string\":\n\n        converted = _convert_string_array(values, encoding, errors)\n        itemsize = converted.dtype.itemsize\n        return IndexCol(\n            name,\n            converted,\n            \"string\",\n            _tables().StringCol(itemsize),\n            index_name=index_name,\n        )\n\n    elif inferred_type in [\"integer\", \"floating\"]:\n        return IndexCol(\n            name, values=converted, kind=kind, typ=atom, index_name=index_name\n        )\n    else:\n        assert isinstance(converted, np.ndarray) and converted.dtype == object\n        assert kind == \"object\", kind\n        atom = _tables().ObjectAtom()\n        return IndexCol(name, converted, kind, atom, index_name=index_name)\n\n\ndef _unconvert_index(\n    data, kind: str, encoding: str, errors: str\n) -> Union[np.ndarray, Index]:\n    index: Union[Index, np.ndarray]\n\n    if kind == \"datetime64\":\n        index = DatetimeIndex(data)\n    elif kind == \"timedelta64\":\n        index = TimedeltaIndex(data)\n    elif kind == \"date\":\n        try:\n            index = np.asarray([date.fromordinal(v) for v in data], dtype=object)\n        except (ValueError):\n            index = np.asarray([date.fromtimestamp(v) for v in data], dtype=object)\n    elif kind in (\"integer\", \"float\"):\n        index = np.asarray(data)\n    elif kind in (\"string\"):\n        index = _unconvert_string_array(\n            data, nan_rep=None, encoding=encoding, errors=errors\n        )\n    elif kind == \"object\":\n        index = np.asarray(data[0])\n    else:  # pragma: no cover\n        raise ValueError(f\"unrecognized index type {kind}\")\n    return index\n\n\ndef _maybe_convert_for_string_atom(\n    name: str, block, existing_col, min_itemsize, nan_rep, encoding, errors\n):\n    if not block.is_object:\n        return block.values\n\n    dtype_name = block.dtype.name\n    inferred_type = lib.infer_dtype(block.values, skipna=False)\n\n    if inferred_type == \"date\":\n        raise TypeError(\"[date] is not implemented as a table column\")\n    elif inferred_type == \"datetime\":\n        # after GH#8260\n        # this only would be hit for a multi-timezone dtype which is an error\n        raise TypeError(\n            \"too many timezones in this block, create separate data columns\"\n        )\n\n    elif not (inferred_type == \"string\" or dtype_name == \"object\"):\n        return block.values\n\n    block = block.fillna(nan_rep, downcast=False)\n    if isinstance(block, list):\n        # Note: because block is always object dtype, fillna goes\n        #  through a path such that the result is always a 1-element list\n        block = block[0]\n    data = block.values\n\n    # see if we have a valid string type\n    inferred_type = lib.infer_dtype(data, skipna=False)\n    if inferred_type != \"string\":\n\n        # we cannot serialize this data, so report an exception on a column\n        # by column basis\n        for i in range(len(block.shape[0])):\n            col = block.iget(i)\n            inferred_type = lib.infer_dtype(col, skipna=False)\n            if inferred_type != \"string\":\n                iloc = block.mgr_locs.indexer[i]\n                raise TypeError(\n                    f\"Cannot serialize the column [{iloc}] because\\n\"\n                    f\"its data contents are [{inferred_type}] object dtype\"\n                )\n\n    # itemsize is the maximum length of a string (along any dimension)\n    data_converted = _convert_string_array(data, encoding, errors).reshape(data.shape)\n    assert data_converted.shape == block.shape, (data_converted.shape, block.shape)\n    itemsize = data_converted.itemsize\n\n    # specified min_itemsize?\n    if isinstance(min_itemsize, dict):\n        min_itemsize = int(min_itemsize.get(name) or min_itemsize.get(\"values\") or 0)\n    itemsize = max(min_itemsize or 0, itemsize)\n\n    # check for column in the values conflicts\n    if existing_col is not None:\n        eci = existing_col.validate_col(itemsize)\n        if eci > itemsize:\n            itemsize = eci\n\n    data_converted = data_converted.astype(f\"|S{itemsize}\", copy=False)\n    return data_converted\n\n\ndef _convert_string_array(data: np.ndarray, encoding: str, errors: str) -> np.ndarray:\n    \"\"\"\n    Take a string-like that is object dtype and coerce to a fixed size string type.\n\n    Parameters\n    ----------\n    data : np.ndarray[object]\n    encoding : str\n    errors : str\n        Handler for encoding errors.\n\n    Returns\n    -------\n    np.ndarray[fixed-length-string]\n    \"\"\"\n    # encode if needed\n    if len(data):\n        data = (\n            Series(data.ravel())\n            .str.encode(encoding, errors)\n            ._values.reshape(data.shape)\n        )\n\n    # create the sized dtype\n    ensured = ensure_object(data.ravel())\n    itemsize = max(1, libwriters.max_len_string_array(ensured))\n\n    data = np.asarray(data, dtype=f\"S{itemsize}\")\n    return data\n\n\ndef _unconvert_string_array(\n    data: np.ndarray, nan_rep, encoding: str, errors: str\n) -> np.ndarray:\n    \"\"\"\n    Inverse of _convert_string_array.\n\n    Parameters\n    ----------\n    data : np.ndarray[fixed-length-string]\n    nan_rep : the storage repr of NaN\n    encoding : str\n    errors : str\n        Handler for encoding errors.\n\n    Returns\n    -------\n    np.ndarray[object]\n        Decoded data.\n    \"\"\"\n    shape = data.shape\n    data = np.asarray(data.ravel(), dtype=object)\n\n    if len(data):\n\n        itemsize = libwriters.max_len_string_array(ensure_object(data))\n        dtype = f\"U{itemsize}\"\n\n        if isinstance(data[0], bytes):\n            data = Series(data).str.decode(encoding, errors=errors)._values\n        else:\n            data = data.astype(dtype, copy=False).astype(object, copy=False)\n\n    if nan_rep is None:\n        nan_rep = \"nan\"\n\n    data = libwriters.string_array_replace_from_nan_rep(data, nan_rep)\n    return data.reshape(shape)\n\n\ndef _maybe_convert(values: np.ndarray, val_kind: str, encoding: str, errors: str):\n    assert isinstance(val_kind, str), type(val_kind)\n    if _need_convert(val_kind):\n        conv = _get_converter(val_kind, encoding, errors)\n        values = conv(values)\n    return values\n\n\ndef _get_converter(kind: str, encoding: str, errors: str):\n    if kind == \"datetime64\":\n        return lambda x: np.asarray(x, dtype=\"M8[ns]\")\n    elif kind == \"string\":\n        return lambda x: _unconvert_string_array(\n            x, nan_rep=None, encoding=encoding, errors=errors\n        )\n    else:  # pragma: no cover\n        raise ValueError(f\"invalid kind {kind}\")\n\n\ndef _need_convert(kind: str) -> bool:\n    if kind in (\"datetime64\", \"string\"):\n        return True\n    return False\n\n\ndef _maybe_adjust_name(name: str, version: Sequence[int]) -> str:\n    \"\"\"\n    Prior to 0.10.1, we named values blocks like: values_block_0 an the\n    name values_0, adjust the given name if necessary.\n\n    Parameters\n    ----------\n    name : str\n    version : Tuple[int, int, int]\n\n    Returns\n    -------\n    str\n    \"\"\"\n    if isinstance(version, str) or len(version) < 3:\n        raise ValueError(\"Version is incorrect, expected sequence of 3 integers.\")\n\n    if version[0] == 0 and version[1] <= 10 and version[2] == 0:\n        m = re.search(r\"values_block_(\\d+)\", name)\n        if m:\n            grp = m.groups()[0]\n            name = f\"values_{grp}\"\n    return name\n\n\ndef _dtype_to_kind(dtype_str: str) -> str:\n    \"\"\"\n    Find the \"kind\" string describing the given dtype name.\n    \"\"\"\n    dtype_str = _ensure_decoded(dtype_str)\n\n    if dtype_str.startswith(\"string\") or dtype_str.startswith(\"bytes\"):\n        kind = \"string\"\n    elif dtype_str.startswith(\"float\"):\n        kind = \"float\"\n    elif dtype_str.startswith(\"complex\"):\n        kind = \"complex\"\n    elif dtype_str.startswith(\"int\") or dtype_str.startswith(\"uint\"):\n        kind = \"integer\"\n    elif dtype_str.startswith(\"datetime64\"):\n        kind = \"datetime64\"\n    elif dtype_str.startswith(\"timedelta\"):\n        kind = \"timedelta64\"\n    elif dtype_str.startswith(\"bool\"):\n        kind = \"bool\"\n    elif dtype_str.startswith(\"category\"):\n        kind = \"category\"\n    elif dtype_str.startswith(\"period\"):\n        # We store the `freq` attr so we can restore from integers\n        kind = \"integer\"\n    elif dtype_str == \"object\":\n        kind = \"object\"\n    else:\n        raise ValueError(f\"cannot interpret dtype of [{dtype_str}]\")\n\n    return kind\n\n\ndef _get_data_and_dtype_name(data: ArrayLike):\n    \"\"\"\n    Convert the passed data into a storable form and a dtype string.\n    \"\"\"\n    if isinstance(data, Categorical):\n        data = data.codes\n\n    # For datetime64tz we need to drop the TZ in tests TODO: why?\n    dtype_name = data.dtype.name.split(\"[\")[0]\n\n    if data.dtype.kind in [\"m\", \"M\"]:\n        data = np.asarray(data.view(\"i8\"))\n        # TODO: we used to reshape for the dt64tz case, but no longer\n        #  doing that doesn't seem to break anything.  why?\n\n    elif isinstance(data, PeriodIndex):\n        data = data.asi8\n\n    data = np.asarray(data)\n    return data, dtype_name\n\n\nclass Selection:\n    \"\"\"\n    Carries out a selection operation on a tables.Table object.\n\n    Parameters\n    ----------\n    table : a Table object\n    where : list of Terms (or convertible to)\n    start, stop: indices to start and/or stop selection\n\n    \"\"\"\n\n    def __init__(\n        self,\n        table: Table,\n        where=None,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n    ):\n        self.table = table\n        self.where = where\n        self.start = start\n        self.stop = stop\n        self.condition = None\n        self.filter = None\n        self.terms = None\n        self.coordinates = None\n\n        if is_list_like(where):\n\n            # see if we have a passed coordinate like\n            with suppress(ValueError):\n                inferred = lib.infer_dtype(where, skipna=False)\n                if inferred == \"integer\" or inferred == \"boolean\":\n                    where = np.asarray(where)\n                    if where.dtype == np.bool_:\n                        start, stop = self.start, self.stop\n                        if start is None:\n                            start = 0\n                        if stop is None:\n                            stop = self.table.nrows\n                        self.coordinates = np.arange(start, stop)[where]\n                    elif issubclass(where.dtype.type, np.integer):\n                        if (self.start is not None and (where < self.start).any()) or (\n                            self.stop is not None and (where >= self.stop).any()\n                        ):\n                            raise ValueError(\n                                \"where must have index locations >= start and < stop\"\n                            )\n                        self.coordinates = where\n\n        if self.coordinates is None:\n\n            self.terms = self.generate(where)\n\n            # create the numexpr & the filter\n            if self.terms is not None:\n                self.condition, self.filter = self.terms.evaluate()\n\n    def generate(self, where):\n        \"\"\" where can be a : dict,list,tuple,string \"\"\"\n        if where is None:\n            return None\n\n        q = self.table.queryables()\n        try:\n            return PyTablesExpr(where, queryables=q, encoding=self.table.encoding)\n        except NameError as err:\n            # raise a nice message, suggesting that the user should use\n            # data_columns\n            qkeys = \",\".join(q.keys())\n            msg = dedent(\n                f\"\"\"\\\n                The passed where expression: {where}\n                            contains an invalid variable reference\n                            all of the variable references must be a reference to\n                            an axis (e.g. 'index' or 'columns'), or a data_column\n                            The currently defined references are: {qkeys}\n                \"\"\"\n            )\n            raise ValueError(msg) from err\n\n    def select(self):\n        \"\"\"\n        generate the selection\n        \"\"\"\n        if self.condition is not None:\n            return self.table.table.read_where(\n                self.condition.format(), start=self.start, stop=self.stop\n            )\n        elif self.coordinates is not None:\n            return self.table.table.read_coordinates(self.coordinates)\n        return self.table.table.read(start=self.start, stop=self.stop)\n\n    def select_coords(self):\n        \"\"\"\n        generate the selection\n        \"\"\"\n        start, stop = self.start, self.stop\n        nrows = self.table.nrows\n        if start is None:\n            start = 0\n        elif start < 0:\n            start += nrows\n        if stop is None:\n            stop = nrows\n        elif stop < 0:\n            stop += nrows\n\n        if self.condition is not None:\n            return self.table.table.get_where_list(\n                self.condition.format(), start=start, stop=stop, sort=True\n            )\n        elif self.coordinates is not None:\n            return self.coordinates\n\n        return np.arange(start, stop)\n"
    },
    {
      "filename": "pandas/tests/dtypes/test_inference.py",
      "content": "\"\"\"\nThese the test the public routines exposed in types/common.py\nrelated to inference and not otherwise tested in types/test_common.py\n\n\"\"\"\nimport collections\nfrom collections import namedtuple\nfrom datetime import date, datetime, time, timedelta\nfrom decimal import Decimal\nfrom fractions import Fraction\nfrom io import StringIO\nfrom numbers import Number\nimport re\n\nimport numpy as np\nimport pytest\nimport pytz\n\nfrom pandas._libs import lib, missing as libmissing\nimport pandas.util._test_decorators as td\n\nfrom pandas.core.dtypes import inference\nfrom pandas.core.dtypes.common import (\n    ensure_int32,\n    is_bool,\n    is_datetime64_any_dtype,\n    is_datetime64_dtype,\n    is_datetime64_ns_dtype,\n    is_datetime64tz_dtype,\n    is_float,\n    is_integer,\n    is_number,\n    is_scalar,\n    is_scipy_sparse,\n    is_timedelta64_dtype,\n    is_timedelta64_ns_dtype,\n)\n\nimport pandas as pd\nfrom pandas import (\n    Categorical,\n    DataFrame,\n    DateOffset,\n    DatetimeIndex,\n    Index,\n    Interval,\n    Period,\n    PeriodIndex,\n    Series,\n    Timedelta,\n    TimedeltaIndex,\n    Timestamp,\n)\nimport pandas._testing as tm\nfrom pandas.core.arrays import IntegerArray\n\n\n@pytest.fixture(params=[True, False], ids=str)\ndef coerce(request):\n    return request.param\n\n\n# collect all objects to be tested for list-like-ness; use tuples of objects,\n# whether they are list-like or not (special casing for sets), and their ID\nll_params = [\n    ([1], True, \"list\"),\n    ([], True, \"list-empty\"),\n    ((1,), True, \"tuple\"),\n    ((), True, \"tuple-empty\"),\n    ({\"a\": 1}, True, \"dict\"),\n    ({}, True, \"dict-empty\"),\n    ({\"a\", 1}, \"set\", \"set\"),\n    (set(), \"set\", \"set-empty\"),\n    (frozenset({\"a\", 1}), \"set\", \"frozenset\"),\n    (frozenset(), \"set\", \"frozenset-empty\"),\n    (iter([1, 2]), True, \"iterator\"),\n    (iter([]), True, \"iterator-empty\"),\n    ((x for x in [1, 2]), True, \"generator\"),\n    ((_ for _ in []), True, \"generator-empty\"),\n    (Series([1]), True, \"Series\"),\n    (Series([], dtype=object), True, \"Series-empty\"),\n    (Series([\"a\"]).str, True, \"StringMethods\"),\n    (Series([], dtype=\"O\").str, True, \"StringMethods-empty\"),\n    (Index([1]), True, \"Index\"),\n    (Index([]), True, \"Index-empty\"),\n    (DataFrame([[1]]), True, \"DataFrame\"),\n    (DataFrame(), True, \"DataFrame-empty\"),\n    (np.ndarray((2,) * 1), True, \"ndarray-1d\"),\n    (np.array([]), True, \"ndarray-1d-empty\"),\n    (np.ndarray((2,) * 2), True, \"ndarray-2d\"),\n    (np.array([[]]), True, \"ndarray-2d-empty\"),\n    (np.ndarray((2,) * 3), True, \"ndarray-3d\"),\n    (np.array([[[]]]), True, \"ndarray-3d-empty\"),\n    (np.ndarray((2,) * 4), True, \"ndarray-4d\"),\n    (np.array([[[[]]]]), True, \"ndarray-4d-empty\"),\n    (np.array(2), False, \"ndarray-0d\"),\n    (1, False, \"int\"),\n    (b\"123\", False, \"bytes\"),\n    (b\"\", False, \"bytes-empty\"),\n    (\"123\", False, \"string\"),\n    (\"\", False, \"string-empty\"),\n    (str, False, \"string-type\"),\n    (object(), False, \"object\"),\n    (np.nan, False, \"NaN\"),\n    (None, False, \"None\"),\n]\nobjs, expected, ids = zip(*ll_params)\n\n\n@pytest.fixture(params=zip(objs, expected), ids=ids)\ndef maybe_list_like(request):\n    return request.param\n\n\ndef test_is_list_like(maybe_list_like):\n    obj, expected = maybe_list_like\n    expected = True if expected == \"set\" else expected\n    assert inference.is_list_like(obj) == expected\n\n\ndef test_is_list_like_disallow_sets(maybe_list_like):\n    obj, expected = maybe_list_like\n    expected = False if expected == \"set\" else expected\n    assert inference.is_list_like(obj, allow_sets=False) == expected\n\n\ndef test_is_list_like_recursion():\n    # GH 33721\n    # interpreter would crash with SIGABRT\n    def foo():\n        inference.is_list_like([])\n        foo()\n\n    with pytest.raises(RecursionError):\n        foo()\n\n\ndef test_is_sequence():\n    is_seq = inference.is_sequence\n    assert is_seq((1, 2))\n    assert is_seq([1, 2])\n    assert not is_seq(\"abcd\")\n    assert not is_seq(np.int64)\n\n    class A:\n        def __getitem__(self):\n            return 1\n\n    assert not is_seq(A())\n\n\ndef test_is_array_like():\n    assert inference.is_array_like(Series([], dtype=object))\n    assert inference.is_array_like(Series([1, 2]))\n    assert inference.is_array_like(np.array([\"a\", \"b\"]))\n    assert inference.is_array_like(Index([\"2016-01-01\"]))\n\n    class DtypeList(list):\n        dtype = \"special\"\n\n    assert inference.is_array_like(DtypeList())\n\n    assert not inference.is_array_like([1, 2, 3])\n    assert not inference.is_array_like(())\n    assert not inference.is_array_like(\"foo\")\n    assert not inference.is_array_like(123)\n\n\n@pytest.mark.parametrize(\n    \"inner\",\n    [\n        [],\n        [1],\n        (1,),\n        (1, 2),\n        {\"a\": 1},\n        {1, \"a\"},\n        Series([1]),\n        Series([], dtype=object),\n        Series([\"a\"]).str,\n        (x for x in range(5)),\n    ],\n)\n@pytest.mark.parametrize(\"outer\", [list, Series, np.array, tuple])\ndef test_is_nested_list_like_passes(inner, outer):\n    result = outer([inner for _ in range(5)])\n    assert inference.is_list_like(result)\n\n\n@pytest.mark.parametrize(\n    \"obj\",\n    [\n        \"abc\",\n        [],\n        [1],\n        (1,),\n        [\"a\"],\n        \"a\",\n        {\"a\"},\n        [1, 2, 3],\n        Series([1]),\n        DataFrame({\"A\": [1]}),\n        ([1, 2] for _ in range(5)),\n    ],\n)\ndef test_is_nested_list_like_fails(obj):\n    assert not inference.is_nested_list_like(obj)\n\n\n@pytest.mark.parametrize(\"ll\", [{}, {\"A\": 1}, Series([1]), collections.defaultdict()])\ndef test_is_dict_like_passes(ll):\n    assert inference.is_dict_like(ll)\n\n\n@pytest.mark.parametrize(\n    \"ll\",\n    [\n        \"1\",\n        1,\n        [1, 2],\n        (1, 2),\n        range(2),\n        Index([1]),\n        dict,\n        collections.defaultdict,\n        Series,\n    ],\n)\ndef test_is_dict_like_fails(ll):\n    assert not inference.is_dict_like(ll)\n\n\n@pytest.mark.parametrize(\"has_keys\", [True, False])\n@pytest.mark.parametrize(\"has_getitem\", [True, False])\n@pytest.mark.parametrize(\"has_contains\", [True, False])\ndef test_is_dict_like_duck_type(has_keys, has_getitem, has_contains):\n    class DictLike:\n        def __init__(self, d):\n            self.d = d\n\n        if has_keys:\n\n            def keys(self):\n                return self.d.keys()\n\n        if has_getitem:\n\n            def __getitem__(self, key):\n                return self.d.__getitem__(key)\n\n        if has_contains:\n\n            def __contains__(self, key) -> bool:\n                return self.d.__contains__(key)\n\n    d = DictLike({1: 2})\n    result = inference.is_dict_like(d)\n    expected = has_keys and has_getitem and has_contains\n\n    assert result is expected\n\n\ndef test_is_file_like():\n    class MockFile:\n        pass\n\n    is_file = inference.is_file_like\n\n    data = StringIO(\"data\")\n    assert is_file(data)\n\n    # No read / write attributes\n    # No iterator attributes\n    m = MockFile()\n    assert not is_file(m)\n\n    MockFile.write = lambda self: 0\n\n    # Write attribute but not an iterator\n    m = MockFile()\n    assert not is_file(m)\n\n    # gh-16530: Valid iterator just means we have the\n    # __iter__ attribute for our purposes.\n    MockFile.__iter__ = lambda self: self\n\n    # Valid write-only file\n    m = MockFile()\n    assert is_file(m)\n\n    del MockFile.write\n    MockFile.read = lambda self: 0\n\n    # Valid read-only file\n    m = MockFile()\n    assert is_file(m)\n\n    # Iterator but no read / write attributes\n    data = [1, 2, 3]\n    assert not is_file(data)\n\n\ntest_tuple = collections.namedtuple(\"Test\", [\"a\", \"b\", \"c\"])\n\n\n@pytest.mark.parametrize(\"ll\", [test_tuple(1, 2, 3)])\ndef test_is_names_tuple_passes(ll):\n    assert inference.is_named_tuple(ll)\n\n\n@pytest.mark.parametrize(\"ll\", [(1, 2, 3), \"a\", Series({\"pi\": 3.14})])\ndef test_is_names_tuple_fails(ll):\n    assert not inference.is_named_tuple(ll)\n\n\ndef test_is_hashable():\n\n    # all new-style classes are hashable by default\n    class HashableClass:\n        pass\n\n    class UnhashableClass1:\n        __hash__ = None\n\n    class UnhashableClass2:\n        def __hash__(self):\n            raise TypeError(\"Not hashable\")\n\n    hashable = (1, 3.14, np.float64(3.14), \"a\", (), (1,), HashableClass())\n    not_hashable = ([], UnhashableClass1())\n    abc_hashable_not_really_hashable = (([],), UnhashableClass2())\n\n    for i in hashable:\n        assert inference.is_hashable(i)\n    for i in not_hashable:\n        assert not inference.is_hashable(i)\n    for i in abc_hashable_not_really_hashable:\n        assert not inference.is_hashable(i)\n\n    # numpy.array is no longer collections.abc.Hashable as of\n    # https://github.com/numpy/numpy/pull/5326, just test\n    # is_hashable()\n    assert not inference.is_hashable(np.array([]))\n\n\n@pytest.mark.parametrize(\"ll\", [re.compile(\"ad\")])\ndef test_is_re_passes(ll):\n    assert inference.is_re(ll)\n\n\n@pytest.mark.parametrize(\"ll\", [\"x\", 2, 3, object()])\ndef test_is_re_fails(ll):\n    assert not inference.is_re(ll)\n\n\n@pytest.mark.parametrize(\n    \"ll\", [r\"a\", \"x\", r\"asdf\", re.compile(\"adsf\"), r\"\\u2233\\s*\", re.compile(r\"\")]\n)\ndef test_is_recompilable_passes(ll):\n    assert inference.is_re_compilable(ll)\n\n\n@pytest.mark.parametrize(\"ll\", [1, [], object()])\ndef test_is_recompilable_fails(ll):\n    assert not inference.is_re_compilable(ll)\n\n\nclass TestInference:\n    @pytest.mark.parametrize(\n        \"arr\",\n        [\n            np.array(list(\"abc\"), dtype=\"S1\"),\n            np.array(list(\"abc\"), dtype=\"S1\").astype(object),\n            [b\"a\", np.nan, b\"c\"],\n        ],\n    )\n    def test_infer_dtype_bytes(self, arr):\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"bytes\"\n\n    @pytest.mark.parametrize(\n        \"value, expected\",\n        [\n            (float(\"inf\"), True),\n            (np.inf, True),\n            (-np.inf, False),\n            (1, False),\n            (\"a\", False),\n        ],\n    )\n    def test_isposinf_scalar(self, value, expected):\n        # GH 11352\n        result = libmissing.isposinf_scalar(value)\n        assert result is expected\n\n    @pytest.mark.parametrize(\n        \"value, expected\",\n        [\n            (float(\"-inf\"), True),\n            (-np.inf, True),\n            (np.inf, False),\n            (1, False),\n            (\"a\", False),\n        ],\n    )\n    def test_isneginf_scalar(self, value, expected):\n        result = libmissing.isneginf_scalar(value)\n        assert result is expected\n\n    @pytest.mark.parametrize(\"coerce_numeric\", [True, False])\n    @pytest.mark.parametrize(\n        \"infinity\", [\"inf\", \"inF\", \"iNf\", \"Inf\", \"iNF\", \"InF\", \"INf\", \"INF\"]\n    )\n    @pytest.mark.parametrize(\"prefix\", [\"\", \"-\", \"+\"])\n    def test_maybe_convert_numeric_infinities(self, coerce_numeric, infinity, prefix):\n        # see gh-13274\n        result = lib.maybe_convert_numeric(\n            np.array([prefix + infinity], dtype=object),\n            na_values={\"\", \"NULL\", \"nan\"},\n            coerce_numeric=coerce_numeric,\n        )\n        expected = np.array([np.inf if prefix in [\"\", \"+\"] else -np.inf])\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_maybe_convert_numeric_infinities_raises(self):\n        msg = \"Unable to parse string\"\n        with pytest.raises(ValueError, match=msg):\n            lib.maybe_convert_numeric(\n                np.array([\"foo_inf\"], dtype=object),\n                na_values={\"\", \"NULL\", \"nan\"},\n                coerce_numeric=False,\n            )\n\n    def test_maybe_convert_numeric_post_floatify_nan(self, coerce):\n        # see gh-13314\n        data = np.array([\"1.200\", \"-999.000\", \"4.500\"], dtype=object)\n        expected = np.array([1.2, np.nan, 4.5], dtype=np.float64)\n        nan_values = {-999, -999.0}\n\n        out = lib.maybe_convert_numeric(data, nan_values, coerce)\n        tm.assert_numpy_array_equal(out, expected)\n\n    def test_convert_infs(self):\n        arr = np.array([\"inf\", \"inf\", \"inf\"], dtype=\"O\")\n        result = lib.maybe_convert_numeric(arr, set(), False)\n        assert result.dtype == np.float64\n\n        arr = np.array([\"-inf\", \"-inf\", \"-inf\"], dtype=\"O\")\n        result = lib.maybe_convert_numeric(arr, set(), False)\n        assert result.dtype == np.float64\n\n    def test_scientific_no_exponent(self):\n        # See PR 12215\n        arr = np.array([\"42E\", \"2E\", \"99e\", \"6e\"], dtype=\"O\")\n        result = lib.maybe_convert_numeric(arr, set(), False, True)\n        assert np.all(np.isnan(result))\n\n    def test_convert_non_hashable(self):\n        # GH13324\n        # make sure that we are handing non-hashables\n        arr = np.array([[10.0, 2], 1.0, \"apple\"], dtype=object)\n        result = lib.maybe_convert_numeric(arr, set(), False, True)\n        tm.assert_numpy_array_equal(result, np.array([np.nan, 1.0, np.nan]))\n\n    def test_convert_numeric_uint64(self):\n        arr = np.array([2 ** 63], dtype=object)\n        exp = np.array([2 ** 63], dtype=np.uint64)\n        tm.assert_numpy_array_equal(lib.maybe_convert_numeric(arr, set()), exp)\n\n        arr = np.array([str(2 ** 63)], dtype=object)\n        exp = np.array([2 ** 63], dtype=np.uint64)\n        tm.assert_numpy_array_equal(lib.maybe_convert_numeric(arr, set()), exp)\n\n        arr = np.array([np.uint64(2 ** 63)], dtype=object)\n        exp = np.array([2 ** 63], dtype=np.uint64)\n        tm.assert_numpy_array_equal(lib.maybe_convert_numeric(arr, set()), exp)\n\n    @pytest.mark.parametrize(\n        \"arr\",\n        [\n            np.array([2 ** 63, np.nan], dtype=object),\n            np.array([str(2 ** 63), np.nan], dtype=object),\n            np.array([np.nan, 2 ** 63], dtype=object),\n            np.array([np.nan, str(2 ** 63)], dtype=object),\n        ],\n    )\n    def test_convert_numeric_uint64_nan(self, coerce, arr):\n        expected = arr.astype(float) if coerce else arr.copy()\n        result = lib.maybe_convert_numeric(arr, set(), coerce_numeric=coerce)\n        tm.assert_almost_equal(result, expected)\n\n    def test_convert_numeric_uint64_nan_values(self, coerce):\n        arr = np.array([2 ** 63, 2 ** 63 + 1], dtype=object)\n        na_values = {2 ** 63}\n\n        expected = (\n            np.array([np.nan, 2 ** 63 + 1], dtype=float) if coerce else arr.copy()\n        )\n        result = lib.maybe_convert_numeric(arr, na_values, coerce_numeric=coerce)\n        tm.assert_almost_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"case\",\n        [\n            np.array([2 ** 63, -1], dtype=object),\n            np.array([str(2 ** 63), -1], dtype=object),\n            np.array([str(2 ** 63), str(-1)], dtype=object),\n            np.array([-1, 2 ** 63], dtype=object),\n            np.array([-1, str(2 ** 63)], dtype=object),\n            np.array([str(-1), str(2 ** 63)], dtype=object),\n        ],\n    )\n    def test_convert_numeric_int64_uint64(self, case, coerce):\n        expected = case.astype(float) if coerce else case.copy()\n        result = lib.maybe_convert_numeric(case, set(), coerce_numeric=coerce)\n        tm.assert_almost_equal(result, expected)\n\n    def test_convert_numeric_string_uint64(self):\n        # GH32394\n        result = lib.maybe_convert_numeric(\n            np.array([\"uint64\"], dtype=object), set(), coerce_numeric=True\n        )\n        assert np.isnan(result)\n\n    @pytest.mark.parametrize(\"value\", [-(2 ** 63) - 1, 2 ** 64])\n    def test_convert_int_overflow(self, value):\n        # see gh-18584\n        arr = np.array([value], dtype=object)\n        result = lib.maybe_convert_objects(arr)\n        tm.assert_numpy_array_equal(arr, result)\n\n    def test_maybe_convert_objects_uint64(self):\n        # see gh-4471\n        arr = np.array([2 ** 63], dtype=object)\n        exp = np.array([2 ** 63], dtype=np.uint64)\n        tm.assert_numpy_array_equal(lib.maybe_convert_objects(arr), exp)\n\n        # NumPy bug: can't compare uint64 to int64, as that\n        # results in both casting to float64, so we should\n        # make sure that this function is robust against it\n        arr = np.array([np.uint64(2 ** 63)], dtype=object)\n        exp = np.array([2 ** 63], dtype=np.uint64)\n        tm.assert_numpy_array_equal(lib.maybe_convert_objects(arr), exp)\n\n        arr = np.array([2, -1], dtype=object)\n        exp = np.array([2, -1], dtype=np.int64)\n        tm.assert_numpy_array_equal(lib.maybe_convert_objects(arr), exp)\n\n        arr = np.array([2 ** 63, -1], dtype=object)\n        exp = np.array([2 ** 63, -1], dtype=object)\n        tm.assert_numpy_array_equal(lib.maybe_convert_objects(arr), exp)\n\n    def test_maybe_convert_objects_datetime(self):\n        # GH27438\n        arr = np.array(\n            [np.datetime64(\"2000-01-01\"), np.timedelta64(1, \"s\")], dtype=object\n        )\n        exp = arr.copy()\n        out = lib.maybe_convert_objects(arr, convert_datetime=1, convert_timedelta=1)\n        tm.assert_numpy_array_equal(out, exp)\n\n        arr = np.array([pd.NaT, np.timedelta64(1, \"s\")], dtype=object)\n        exp = np.array([np.timedelta64(\"NaT\"), np.timedelta64(1, \"s\")], dtype=\"m8[ns]\")\n        out = lib.maybe_convert_objects(arr, convert_datetime=1, convert_timedelta=1)\n        tm.assert_numpy_array_equal(out, exp)\n\n        arr = np.array([np.timedelta64(1, \"s\"), np.nan], dtype=object)\n        exp = arr.copy()\n        out = lib.maybe_convert_objects(arr, convert_datetime=1, convert_timedelta=1)\n        tm.assert_numpy_array_equal(out, exp)\n\n    @pytest.mark.parametrize(\n        \"exp\",\n        [\n            IntegerArray(np.array([2, 0], dtype=\"i8\"), np.array([False, True])),\n            IntegerArray(np.array([2, 0], dtype=\"int64\"), np.array([False, True])),\n        ],\n    )\n    def test_maybe_convert_objects_nullable_integer(self, exp):\n        # GH27335\n        arr = np.array([2, np.NaN], dtype=object)\n        result = lib.maybe_convert_objects(arr, convert_to_nullable_integer=1)\n\n        tm.assert_extension_array_equal(result, exp)\n\n    def test_maybe_convert_objects_bool_nan(self):\n        # GH32146\n        ind = Index([True, False, np.nan], dtype=object)\n        exp = np.array([True, False, np.nan], dtype=object)\n        out = lib.maybe_convert_objects(ind.values, safe=1)\n        tm.assert_numpy_array_equal(out, exp)\n\n    def test_mixed_dtypes_remain_object_array(self):\n        # GH14956\n        array = np.array([datetime(2015, 1, 1, tzinfo=pytz.utc), 1], dtype=object)\n        result = lib.maybe_convert_objects(array, convert_datetime=1)\n        tm.assert_numpy_array_equal(result, array)\n\n\nclass TestTypeInference:\n\n    # Dummy class used for testing with Python objects\n    class Dummy:\n        pass\n\n    def test_inferred_dtype_fixture(self, any_skipna_inferred_dtype):\n        # see pandas/conftest.py\n        inferred_dtype, values = any_skipna_inferred_dtype\n\n        # make sure the inferred dtype of the fixture is as requested\n        assert inferred_dtype == lib.infer_dtype(values, skipna=True)\n\n    @pytest.mark.parametrize(\"skipna\", [True, False])\n    def test_length_zero(self, skipna):\n        result = lib.infer_dtype(np.array([], dtype=\"i4\"), skipna=skipna)\n        assert result == \"integer\"\n\n        result = lib.infer_dtype([], skipna=skipna)\n        assert result == \"empty\"\n\n        # GH 18004\n        arr = np.array([np.array([], dtype=object), np.array([], dtype=object)])\n        result = lib.infer_dtype(arr, skipna=skipna)\n        assert result == \"empty\"\n\n    def test_integers(self):\n        arr = np.array([1, 2, 3, np.int64(4), np.int32(5)], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"integer\"\n\n        arr = np.array([1, 2, 3, np.int64(4), np.int32(5), \"foo\"], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"mixed-integer\"\n\n        arr = np.array([1, 2, 3, 4, 5], dtype=\"i4\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"integer\"\n\n    @pytest.mark.parametrize(\n        \"arr, skipna\",\n        [\n            (np.array([1, 2, np.nan, np.nan, 3], dtype=\"O\"), False),\n            (np.array([1, 2, np.nan, np.nan, 3], dtype=\"O\"), True),\n            (np.array([1, 2, 3, np.int64(4), np.int32(5), np.nan], dtype=\"O\"), False),\n            (np.array([1, 2, 3, np.int64(4), np.int32(5), np.nan], dtype=\"O\"), True),\n        ],\n    )\n    def test_integer_na(self, arr, skipna):\n        # GH 27392\n        result = lib.infer_dtype(arr, skipna=skipna)\n        expected = \"integer\" if skipna else \"integer-na\"\n        assert result == expected\n\n    def test_infer_dtype_skipna_default(self):\n        # infer_dtype `skipna` default deprecated in GH#24050,\n        #  changed to True in GH#29876\n        arr = np.array([1, 2, 3, np.nan], dtype=object)\n\n        result = lib.infer_dtype(arr)\n        assert result == \"integer\"\n\n    def test_bools(self):\n        arr = np.array([True, False, True, True, True], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"boolean\"\n\n        arr = np.array([np.bool_(True), np.bool_(False)], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"boolean\"\n\n        arr = np.array([True, False, True, \"foo\"], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"mixed\"\n\n        arr = np.array([True, False, True], dtype=bool)\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"boolean\"\n\n        arr = np.array([True, np.nan, False], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"boolean\"\n\n        result = lib.infer_dtype(arr, skipna=False)\n        assert result == \"mixed\"\n\n    def test_floats(self):\n        arr = np.array([1.0, 2.0, 3.0, np.float64(4), np.float32(5)], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"floating\"\n\n        arr = np.array([1, 2, 3, np.float64(4), np.float32(5), \"foo\"], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"mixed-integer\"\n\n        arr = np.array([1, 2, 3, 4, 5], dtype=\"f4\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"floating\"\n\n        arr = np.array([1, 2, 3, 4, 5], dtype=\"f8\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"floating\"\n\n    def test_decimals(self):\n        # GH15690\n        arr = np.array([Decimal(1), Decimal(2), Decimal(3)])\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"decimal\"\n\n        arr = np.array([1.0, 2.0, Decimal(3)])\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"mixed\"\n\n        result = lib.infer_dtype(arr[::-1], skipna=True)\n        assert result == \"mixed\"\n\n        arr = np.array([Decimal(1), Decimal(\"NaN\"), Decimal(3)])\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"decimal\"\n\n        arr = np.array([Decimal(1), np.nan, Decimal(3)], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"decimal\"\n\n    # complex is compatible with nan, so skipna has no effect\n    @pytest.mark.parametrize(\"skipna\", [True, False])\n    def test_complex(self, skipna):\n        # gets cast to complex on array construction\n        arr = np.array([1.0, 2.0, 1 + 1j])\n        result = lib.infer_dtype(arr, skipna=skipna)\n        assert result == \"complex\"\n\n        arr = np.array([1.0, 2.0, 1 + 1j], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=skipna)\n        assert result == \"mixed\"\n\n        result = lib.infer_dtype(arr[::-1], skipna=skipna)\n        assert result == \"mixed\"\n\n        # gets cast to complex on array construction\n        arr = np.array([1, np.nan, 1 + 1j])\n        result = lib.infer_dtype(arr, skipna=skipna)\n        assert result == \"complex\"\n\n        arr = np.array([1.0, np.nan, 1 + 1j], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=skipna)\n        assert result == \"mixed\"\n\n        # complex with nans stays complex\n        arr = np.array([1 + 1j, np.nan, 3 + 3j], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=skipna)\n        assert result == \"complex\"\n\n        # test smaller complex dtype; will pass through _try_infer_map fastpath\n        arr = np.array([1 + 1j, np.nan, 3 + 3j], dtype=np.complex64)\n        result = lib.infer_dtype(arr, skipna=skipna)\n        assert result == \"complex\"\n\n    def test_string(self):\n        pass\n\n    def test_unicode(self):\n        arr = [\"a\", np.nan, \"c\"]\n        result = lib.infer_dtype(arr, skipna=False)\n        # This currently returns \"mixed\", but it's not clear that's optimal.\n        # This could also return \"string\" or \"mixed-string\"\n        assert result == \"mixed\"\n\n        arr = [\"a\", np.nan, \"c\"]\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"string\"\n\n        arr = [\"a\", \"c\"]\n        result = lib.infer_dtype(arr, skipna=False)\n        assert result == \"string\"\n\n    @pytest.mark.parametrize(\n        \"dtype, missing, skipna, expected\",\n        [\n            (float, np.nan, False, \"floating\"),\n            (float, np.nan, True, \"floating\"),\n            (object, np.nan, False, \"floating\"),\n            (object, np.nan, True, \"empty\"),\n            (object, None, False, \"mixed\"),\n            (object, None, True, \"empty\"),\n        ],\n    )\n    @pytest.mark.parametrize(\"box\", [pd.Series, np.array])\n    def test_object_empty(self, box, missing, dtype, skipna, expected):\n        # GH 23421\n        arr = box([missing, missing], dtype=dtype)\n\n        result = lib.infer_dtype(arr, skipna=skipna)\n        assert result == expected\n\n    def test_datetime(self):\n\n        dates = [datetime(2012, 1, x) for x in range(1, 20)]\n        index = Index(dates)\n        assert index.inferred_type == \"datetime64\"\n\n    def test_infer_dtype_datetime64(self):\n        arr = np.array(\n            [np.datetime64(\"2011-01-01\"), np.datetime64(\"2011-01-01\")], dtype=object\n        )\n        assert lib.infer_dtype(arr, skipna=True) == \"datetime64\"\n\n    @pytest.mark.parametrize(\"na_value\", [pd.NaT, np.nan])\n    def test_infer_dtype_datetime64_with_na(self, na_value):\n        # starts with nan\n        arr = np.array([na_value, np.datetime64(\"2011-01-02\")])\n        assert lib.infer_dtype(arr, skipna=True) == \"datetime64\"\n\n        arr = np.array([na_value, np.datetime64(\"2011-01-02\"), na_value])\n        assert lib.infer_dtype(arr, skipna=True) == \"datetime64\"\n\n    @pytest.mark.parametrize(\n        \"arr\",\n        [\n            np.array(\n                [np.timedelta64(\"nat\"), np.datetime64(\"2011-01-02\")], dtype=object\n            ),\n            np.array(\n                [np.datetime64(\"2011-01-02\"), np.timedelta64(\"nat\")], dtype=object\n            ),\n            np.array([np.datetime64(\"2011-01-01\"), Timestamp(\"2011-01-02\")]),\n            np.array([Timestamp(\"2011-01-02\"), np.datetime64(\"2011-01-01\")]),\n            np.array([np.nan, Timestamp(\"2011-01-02\"), 1.1]),\n            np.array([np.nan, \"2011-01-01\", Timestamp(\"2011-01-02\")]),\n            np.array([np.datetime64(\"nat\"), np.timedelta64(1, \"D\")], dtype=object),\n            np.array([np.timedelta64(1, \"D\"), np.datetime64(\"nat\")], dtype=object),\n        ],\n    )\n    def test_infer_datetimelike_dtype_mixed(self, arr):\n        assert lib.infer_dtype(arr, skipna=False) == \"mixed\"\n\n    def test_infer_dtype_mixed_integer(self):\n        arr = np.array([np.nan, Timestamp(\"2011-01-02\"), 1])\n        assert lib.infer_dtype(arr, skipna=True) == \"mixed-integer\"\n\n    @pytest.mark.parametrize(\n        \"arr\",\n        [\n            np.array([Timestamp(\"2011-01-01\"), Timestamp(\"2011-01-02\")]),\n            np.array([datetime(2011, 1, 1), datetime(2012, 2, 1)]),\n            np.array([datetime(2011, 1, 1), Timestamp(\"2011-01-02\")]),\n        ],\n    )\n    def test_infer_dtype_datetime(self, arr):\n        assert lib.infer_dtype(arr, skipna=True) == \"datetime\"\n\n    @pytest.mark.parametrize(\"na_value\", [pd.NaT, np.nan])\n    @pytest.mark.parametrize(\n        \"time_stamp\", [Timestamp(\"2011-01-01\"), datetime(2011, 1, 1)]\n    )\n    def test_infer_dtype_datetime_with_na(self, na_value, time_stamp):\n        # starts with nan\n        arr = np.array([na_value, time_stamp])\n        assert lib.infer_dtype(arr, skipna=True) == \"datetime\"\n\n        arr = np.array([na_value, time_stamp, na_value])\n        assert lib.infer_dtype(arr, skipna=True) == \"datetime\"\n\n    @pytest.mark.parametrize(\n        \"arr\",\n        [\n            np.array([Timedelta(\"1 days\"), Timedelta(\"2 days\")]),\n            np.array([np.timedelta64(1, \"D\"), np.timedelta64(2, \"D\")], dtype=object),\n            np.array([timedelta(1), timedelta(2)]),\n        ],\n    )\n    def test_infer_dtype_timedelta(self, arr):\n        assert lib.infer_dtype(arr, skipna=True) == \"timedelta\"\n\n    @pytest.mark.parametrize(\"na_value\", [pd.NaT, np.nan])\n    @pytest.mark.parametrize(\n        \"delta\", [Timedelta(\"1 days\"), np.timedelta64(1, \"D\"), timedelta(1)]\n    )\n    def test_infer_dtype_timedelta_with_na(self, na_value, delta):\n        # starts with nan\n        arr = np.array([na_value, delta])\n        assert lib.infer_dtype(arr, skipna=True) == \"timedelta\"\n\n        arr = np.array([na_value, delta, na_value])\n        assert lib.infer_dtype(arr, skipna=True) == \"timedelta\"\n\n    def test_infer_dtype_period(self):\n        # GH 13664\n        arr = np.array([Period(\"2011-01\", freq=\"D\"), Period(\"2011-02\", freq=\"D\")])\n        assert lib.infer_dtype(arr, skipna=True) == \"period\"\n\n        arr = np.array([Period(\"2011-01\", freq=\"D\"), Period(\"2011-02\", freq=\"M\")])\n        assert lib.infer_dtype(arr, skipna=True) == \"period\"\n\n    def test_infer_dtype_period_mixed(self):\n        arr = np.array(\n            [Period(\"2011-01\", freq=\"M\"), np.datetime64(\"nat\")], dtype=object\n        )\n        assert lib.infer_dtype(arr, skipna=False) == \"mixed\"\n\n        arr = np.array(\n            [np.datetime64(\"nat\"), Period(\"2011-01\", freq=\"M\")], dtype=object\n        )\n        assert lib.infer_dtype(arr, skipna=False) == \"mixed\"\n\n    @pytest.mark.parametrize(\"na_value\", [pd.NaT, np.nan])\n    def test_infer_dtype_period_with_na(self, na_value):\n        # starts with nan\n        arr = np.array([na_value, Period(\"2011-01\", freq=\"D\")])\n        assert lib.infer_dtype(arr, skipna=True) == \"period\"\n\n        arr = np.array([na_value, Period(\"2011-01\", freq=\"D\"), na_value])\n        assert lib.infer_dtype(arr, skipna=True) == \"period\"\n\n    @pytest.mark.parametrize(\n        \"data\",\n        [\n            [datetime(2017, 6, 12, 19, 30), datetime(2017, 3, 11, 1, 15)],\n            [Timestamp(\"20170612\"), Timestamp(\"20170311\")],\n            [\n                Timestamp(\"20170612\", tz=\"US/Eastern\"),\n                Timestamp(\"20170311\", tz=\"US/Eastern\"),\n            ],\n            [date(2017, 6, 12), Timestamp(\"20170311\", tz=\"US/Eastern\")],\n            [np.datetime64(\"2017-06-12\"), np.datetime64(\"2017-03-11\")],\n            [np.datetime64(\"2017-06-12\"), datetime(2017, 3, 11, 1, 15)],\n        ],\n    )\n    def test_infer_datetimelike_array_datetime(self, data):\n        assert lib.infer_datetimelike_array(data) == \"datetime\"\n\n    @pytest.mark.parametrize(\n        \"data\",\n        [\n            [timedelta(2017, 6, 12), timedelta(2017, 3, 11)],\n            [timedelta(2017, 6, 12), date(2017, 3, 11)],\n            [np.timedelta64(2017, \"D\"), np.timedelta64(6, \"s\")],\n            [np.timedelta64(2017, \"D\"), timedelta(2017, 3, 11)],\n        ],\n    )\n    def test_infer_datetimelike_array_timedelta(self, data):\n        assert lib.infer_datetimelike_array(data) == \"timedelta\"\n\n    def test_infer_datetimelike_array_date(self):\n        arr = [date(2017, 6, 12), date(2017, 3, 11)]\n        assert lib.infer_datetimelike_array(arr) == \"date\"\n\n    @pytest.mark.parametrize(\n        \"data\",\n        [\n            [\"2017-06-12\", \"2017-03-11\"],\n            [20170612, 20170311],\n            [20170612.5, 20170311.8],\n            [Dummy(), Dummy()],\n            [Timestamp(\"20170612\"), Timestamp(\"20170311\", tz=\"US/Eastern\")],\n            [Timestamp(\"20170612\"), 20170311],\n            [timedelta(2017, 6, 12), Timestamp(\"20170311\", tz=\"US/Eastern\")],\n        ],\n    )\n    def test_infer_datetimelike_array_mixed(self, data):\n        assert lib.infer_datetimelike_array(data) == \"mixed\"\n\n    @pytest.mark.parametrize(\n        \"first, expected\",\n        [\n            [[None], \"mixed\"],\n            [[np.nan], \"mixed\"],\n            [[pd.NaT], \"nat\"],\n            [[datetime(2017, 6, 12, 19, 30), pd.NaT], \"datetime\"],\n            [[np.datetime64(\"2017-06-12\"), pd.NaT], \"datetime\"],\n            [[date(2017, 6, 12), pd.NaT], \"date\"],\n            [[timedelta(2017, 6, 12), pd.NaT], \"timedelta\"],\n            [[np.timedelta64(2017, \"D\"), pd.NaT], \"timedelta\"],\n        ],\n    )\n    @pytest.mark.parametrize(\"second\", [None, np.nan])\n    def test_infer_datetimelike_array_nan_nat_like(self, first, second, expected):\n        first.append(second)\n        assert lib.infer_datetimelike_array(first) == expected\n\n    def test_infer_dtype_all_nan_nat_like(self):\n        arr = np.array([np.nan, np.nan])\n        assert lib.infer_dtype(arr, skipna=True) == \"floating\"\n\n        # nan and None mix are result in mixed\n        arr = np.array([np.nan, np.nan, None])\n        assert lib.infer_dtype(arr, skipna=True) == \"empty\"\n        assert lib.infer_dtype(arr, skipna=False) == \"mixed\"\n\n        arr = np.array([None, np.nan, np.nan])\n        assert lib.infer_dtype(arr, skipna=True) == \"empty\"\n        assert lib.infer_dtype(arr, skipna=False) == \"mixed\"\n\n        # pd.NaT\n        arr = np.array([pd.NaT])\n        assert lib.infer_dtype(arr, skipna=False) == \"datetime\"\n\n        arr = np.array([pd.NaT, np.nan])\n        assert lib.infer_dtype(arr, skipna=False) == \"datetime\"\n\n        arr = np.array([np.nan, pd.NaT])\n        assert lib.infer_dtype(arr, skipna=False) == \"datetime\"\n\n        arr = np.array([np.nan, pd.NaT, np.nan])\n        assert lib.infer_dtype(arr, skipna=False) == \"datetime\"\n\n        arr = np.array([None, pd.NaT, None])\n        assert lib.infer_dtype(arr, skipna=False) == \"datetime\"\n\n        # np.datetime64(nat)\n        arr = np.array([np.datetime64(\"nat\")])\n        assert lib.infer_dtype(arr, skipna=False) == \"datetime64\"\n\n        for n in [np.nan, pd.NaT, None]:\n            arr = np.array([n, np.datetime64(\"nat\"), n])\n            assert lib.infer_dtype(arr, skipna=False) == \"datetime64\"\n\n            arr = np.array([pd.NaT, n, np.datetime64(\"nat\"), n])\n            assert lib.infer_dtype(arr, skipna=False) == \"datetime64\"\n\n        arr = np.array([np.timedelta64(\"nat\")], dtype=object)\n        assert lib.infer_dtype(arr, skipna=False) == \"timedelta\"\n\n        for n in [np.nan, pd.NaT, None]:\n            arr = np.array([n, np.timedelta64(\"nat\"), n])\n            assert lib.infer_dtype(arr, skipna=False) == \"timedelta\"\n\n            arr = np.array([pd.NaT, n, np.timedelta64(\"nat\"), n])\n            assert lib.infer_dtype(arr, skipna=False) == \"timedelta\"\n\n        # datetime / timedelta mixed\n        arr = np.array([pd.NaT, np.datetime64(\"nat\"), np.timedelta64(\"nat\"), np.nan])\n        assert lib.infer_dtype(arr, skipna=False) == \"mixed\"\n\n        arr = np.array([np.timedelta64(\"nat\"), np.datetime64(\"nat\")], dtype=object)\n        assert lib.infer_dtype(arr, skipna=False) == \"mixed\"\n\n    def test_is_datetimelike_array_all_nan_nat_like(self):\n        arr = np.array([np.nan, pd.NaT, np.datetime64(\"nat\")])\n        assert lib.is_datetime_array(arr)\n        assert lib.is_datetime64_array(arr)\n        assert not lib.is_timedelta_or_timedelta64_array(arr)\n\n        arr = np.array([np.nan, pd.NaT, np.timedelta64(\"nat\")])\n        assert not lib.is_datetime_array(arr)\n        assert not lib.is_datetime64_array(arr)\n        assert lib.is_timedelta_or_timedelta64_array(arr)\n\n        arr = np.array([np.nan, pd.NaT, np.datetime64(\"nat\"), np.timedelta64(\"nat\")])\n        assert not lib.is_datetime_array(arr)\n        assert not lib.is_datetime64_array(arr)\n        assert not lib.is_timedelta_or_timedelta64_array(arr)\n\n        arr = np.array([np.nan, pd.NaT])\n        assert lib.is_datetime_array(arr)\n        assert lib.is_datetime64_array(arr)\n        assert lib.is_timedelta_or_timedelta64_array(arr)\n\n        arr = np.array([np.nan, np.nan], dtype=object)\n        assert not lib.is_datetime_array(arr)\n        assert not lib.is_datetime64_array(arr)\n        assert not lib.is_timedelta_or_timedelta64_array(arr)\n\n        assert lib.is_datetime_with_singletz_array(\n            np.array(\n                [\n                    Timestamp(\"20130101\", tz=\"US/Eastern\"),\n                    Timestamp(\"20130102\", tz=\"US/Eastern\"),\n                ],\n                dtype=object,\n            )\n        )\n        assert not lib.is_datetime_with_singletz_array(\n            np.array(\n                [\n                    Timestamp(\"20130101\", tz=\"US/Eastern\"),\n                    Timestamp(\"20130102\", tz=\"CET\"),\n                ],\n                dtype=object,\n            )\n        )\n\n    @pytest.mark.parametrize(\n        \"func\",\n        [\n            \"is_datetime_array\",\n            \"is_datetime64_array\",\n            \"is_bool_array\",\n            \"is_timedelta_or_timedelta64_array\",\n            \"is_date_array\",\n            \"is_time_array\",\n            \"is_interval_array\",\n            \"is_period_array\",\n        ],\n    )\n    def test_other_dtypes_for_array(self, func):\n        func = getattr(lib, func)\n        arr = np.array([\"foo\", \"bar\"])\n        assert not func(arr)\n\n        arr = np.array([1, 2])\n        assert not func(arr)\n\n    def test_date(self):\n\n        dates = [date(2012, 1, day) for day in range(1, 20)]\n        index = Index(dates)\n        assert index.inferred_type == \"date\"\n\n        dates = [date(2012, 1, day) for day in range(1, 20)] + [np.nan]\n        result = lib.infer_dtype(dates, skipna=False)\n        assert result == \"mixed\"\n\n        result = lib.infer_dtype(dates, skipna=True)\n        assert result == \"date\"\n\n    @pytest.mark.parametrize(\n        \"values\",\n        [\n            [date(2020, 1, 1), Timestamp(\"2020-01-01\")],\n            [Timestamp(\"2020-01-01\"), date(2020, 1, 1)],\n            [date(2020, 1, 1), pd.NaT],\n            [pd.NaT, date(2020, 1, 1)],\n        ],\n    )\n    @pytest.mark.parametrize(\"skipna\", [True, False])\n    def test_infer_dtype_date_order_invariant(self, values, skipna):\n        # https://github.com/pandas-dev/pandas/issues/33741\n        result = lib.infer_dtype(values, skipna=skipna)\n        assert result == \"date\"\n\n    def test_is_numeric_array(self):\n\n        assert lib.is_float_array(np.array([1, 2.0]))\n        assert lib.is_float_array(np.array([1, 2.0, np.nan]))\n        assert not lib.is_float_array(np.array([1, 2]))\n\n        assert lib.is_integer_array(np.array([1, 2]))\n        assert not lib.is_integer_array(np.array([1, 2.0]))\n\n    def test_is_string_array(self):\n\n        assert lib.is_string_array(np.array([\"foo\", \"bar\"]))\n        assert not lib.is_string_array(\n            np.array([\"foo\", \"bar\", pd.NA], dtype=object), skipna=False\n        )\n        assert lib.is_string_array(\n            np.array([\"foo\", \"bar\", pd.NA], dtype=object), skipna=True\n        )\n        # NaN is not valid for string array, just NA\n        assert not lib.is_string_array(\n            np.array([\"foo\", \"bar\", np.nan], dtype=object), skipna=True\n        )\n\n        assert not lib.is_string_array(np.array([1, 2]))\n\n    def test_to_object_array_tuples(self):\n        r = (5, 6)\n        values = [r]\n        lib.to_object_array_tuples(values)\n\n        # make sure record array works\n        record = namedtuple(\"record\", \"x y\")\n        r = record(5, 6)\n        values = [r]\n        lib.to_object_array_tuples(values)\n\n    def test_object(self):\n\n        # GH 7431\n        # cannot infer more than this as only a single element\n        arr = np.array([None], dtype=\"O\")\n        result = lib.infer_dtype(arr, skipna=False)\n        assert result == \"mixed\"\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"empty\"\n\n    def test_to_object_array_width(self):\n        # see gh-13320\n        rows = [[1, 2, 3], [4, 5, 6]]\n\n        expected = np.array(rows, dtype=object)\n        out = lib.to_object_array(rows)\n        tm.assert_numpy_array_equal(out, expected)\n\n        expected = np.array(rows, dtype=object)\n        out = lib.to_object_array(rows, min_width=1)\n        tm.assert_numpy_array_equal(out, expected)\n\n        expected = np.array(\n            [[1, 2, 3, None, None], [4, 5, 6, None, None]], dtype=object\n        )\n        out = lib.to_object_array(rows, min_width=5)\n        tm.assert_numpy_array_equal(out, expected)\n\n    def test_is_period(self):\n        assert lib.is_period(Period(\"2011-01\", freq=\"M\"))\n        assert not lib.is_period(PeriodIndex([\"2011-01\"], freq=\"M\"))\n        assert not lib.is_period(Timestamp(\"2011-01\"))\n        assert not lib.is_period(1)\n        assert not lib.is_period(np.nan)\n\n    def test_categorical(self):\n\n        # GH 8974\n        arr = Categorical(list(\"abc\"))\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"categorical\"\n\n        result = lib.infer_dtype(Series(arr), skipna=True)\n        assert result == \"categorical\"\n\n        arr = Categorical(list(\"abc\"), categories=[\"cegfab\"], ordered=True)\n        result = lib.infer_dtype(arr, skipna=True)\n        assert result == \"categorical\"\n\n        result = lib.infer_dtype(Series(arr), skipna=True)\n        assert result == \"categorical\"\n\n    def test_interval(self):\n        idx = pd.IntervalIndex.from_breaks(range(5), closed=\"both\")\n        inferred = lib.infer_dtype(idx, skipna=False)\n        assert inferred == \"interval\"\n\n        inferred = lib.infer_dtype(idx._data, skipna=False)\n        assert inferred == \"interval\"\n\n        inferred = lib.infer_dtype(Series(idx), skipna=False)\n        assert inferred == \"interval\"\n\n    @pytest.mark.parametrize(\"klass\", [pd.array, pd.Series])\n    @pytest.mark.parametrize(\"skipna\", [True, False])\n    @pytest.mark.parametrize(\"data\", [[\"a\", \"b\", \"c\"], [\"a\", \"b\", pd.NA]])\n    def test_string_dtype(self, data, skipna, klass):\n        # StringArray\n        val = klass(data, dtype=\"string\")\n        inferred = lib.infer_dtype(val, skipna=skipna)\n        assert inferred == \"string\"\n\n    @pytest.mark.parametrize(\"klass\", [pd.array, pd.Series])\n    @pytest.mark.parametrize(\"skipna\", [True, False])\n    @pytest.mark.parametrize(\"data\", [[True, False, True], [True, False, pd.NA]])\n    def test_boolean_dtype(self, data, skipna, klass):\n        # BooleanArray\n        val = klass(data, dtype=\"boolean\")\n        inferred = lib.infer_dtype(val, skipna=skipna)\n        assert inferred == \"boolean\"\n\n\nclass TestNumberScalar:\n    def test_is_number(self):\n\n        assert is_number(True)\n        assert is_number(1)\n        assert is_number(1.1)\n        assert is_number(1 + 3j)\n        assert is_number(np.int64(1))\n        assert is_number(np.float64(1.1))\n        assert is_number(np.complex128(1 + 3j))\n        assert is_number(np.nan)\n\n        assert not is_number(None)\n        assert not is_number(\"x\")\n        assert not is_number(datetime(2011, 1, 1))\n        assert not is_number(np.datetime64(\"2011-01-01\"))\n        assert not is_number(Timestamp(\"2011-01-01\"))\n        assert not is_number(Timestamp(\"2011-01-01\", tz=\"US/Eastern\"))\n        assert not is_number(timedelta(1000))\n        assert not is_number(Timedelta(\"1 days\"))\n\n        # questionable\n        assert not is_number(np.bool_(False))\n        assert is_number(np.timedelta64(1, \"D\"))\n\n    def test_is_bool(self):\n        assert is_bool(True)\n        assert is_bool(False)\n        assert is_bool(np.bool_(False))\n\n        assert not is_bool(1)\n        assert not is_bool(1.1)\n        assert not is_bool(1 + 3j)\n        assert not is_bool(np.int64(1))\n        assert not is_bool(np.float64(1.1))\n        assert not is_bool(np.complex128(1 + 3j))\n        assert not is_bool(np.nan)\n        assert not is_bool(None)\n        assert not is_bool(\"x\")\n        assert not is_bool(datetime(2011, 1, 1))\n        assert not is_bool(np.datetime64(\"2011-01-01\"))\n        assert not is_bool(Timestamp(\"2011-01-01\"))\n        assert not is_bool(Timestamp(\"2011-01-01\", tz=\"US/Eastern\"))\n        assert not is_bool(timedelta(1000))\n        assert not is_bool(np.timedelta64(1, \"D\"))\n        assert not is_bool(Timedelta(\"1 days\"))\n\n    def test_is_integer(self):\n        assert is_integer(1)\n        assert is_integer(np.int64(1))\n\n        assert not is_integer(True)\n        assert not is_integer(1.1)\n        assert not is_integer(1 + 3j)\n        assert not is_integer(False)\n        assert not is_integer(np.bool_(False))\n        assert not is_integer(np.float64(1.1))\n        assert not is_integer(np.complex128(1 + 3j))\n        assert not is_integer(np.nan)\n        assert not is_integer(None)\n        assert not is_integer(\"x\")\n        assert not is_integer(datetime(2011, 1, 1))\n        assert not is_integer(np.datetime64(\"2011-01-01\"))\n        assert not is_integer(Timestamp(\"2011-01-01\"))\n        assert not is_integer(Timestamp(\"2011-01-01\", tz=\"US/Eastern\"))\n        assert not is_integer(timedelta(1000))\n        assert not is_integer(Timedelta(\"1 days\"))\n        assert not is_integer(np.timedelta64(1, \"D\"))\n\n    def test_is_float(self):\n        assert is_float(1.1)\n        assert is_float(np.float64(1.1))\n        assert is_float(np.nan)\n\n        assert not is_float(True)\n        assert not is_float(1)\n        assert not is_float(1 + 3j)\n        assert not is_float(False)\n        assert not is_float(np.bool_(False))\n        assert not is_float(np.int64(1))\n        assert not is_float(np.complex128(1 + 3j))\n        assert not is_float(None)\n        assert not is_float(\"x\")\n        assert not is_float(datetime(2011, 1, 1))\n        assert not is_float(np.datetime64(\"2011-01-01\"))\n        assert not is_float(Timestamp(\"2011-01-01\"))\n        assert not is_float(Timestamp(\"2011-01-01\", tz=\"US/Eastern\"))\n        assert not is_float(timedelta(1000))\n        assert not is_float(np.timedelta64(1, \"D\"))\n        assert not is_float(Timedelta(\"1 days\"))\n\n    def test_is_datetime_dtypes(self):\n\n        ts = pd.date_range(\"20130101\", periods=3)\n        tsa = pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\")\n\n        assert is_datetime64_dtype(\"datetime64\")\n        assert is_datetime64_dtype(\"datetime64[ns]\")\n        assert is_datetime64_dtype(ts)\n        assert not is_datetime64_dtype(tsa)\n\n        assert not is_datetime64_ns_dtype(\"datetime64\")\n        assert is_datetime64_ns_dtype(\"datetime64[ns]\")\n        assert is_datetime64_ns_dtype(ts)\n        assert is_datetime64_ns_dtype(tsa)\n\n        assert is_datetime64_any_dtype(\"datetime64\")\n        assert is_datetime64_any_dtype(\"datetime64[ns]\")\n        assert is_datetime64_any_dtype(ts)\n        assert is_datetime64_any_dtype(tsa)\n\n        assert not is_datetime64tz_dtype(\"datetime64\")\n        assert not is_datetime64tz_dtype(\"datetime64[ns]\")\n        assert not is_datetime64tz_dtype(ts)\n        assert is_datetime64tz_dtype(tsa)\n\n        for tz in [\"US/Eastern\", \"UTC\"]:\n            dtype = f\"datetime64[ns, {tz}]\"\n            assert not is_datetime64_dtype(dtype)\n            assert is_datetime64tz_dtype(dtype)\n            assert is_datetime64_ns_dtype(dtype)\n            assert is_datetime64_any_dtype(dtype)\n\n    def test_is_timedelta(self):\n        assert is_timedelta64_dtype(\"timedelta64\")\n        assert is_timedelta64_dtype(\"timedelta64[ns]\")\n        assert not is_timedelta64_ns_dtype(\"timedelta64\")\n        assert is_timedelta64_ns_dtype(\"timedelta64[ns]\")\n\n        tdi = TimedeltaIndex([1e14, 2e14], dtype=\"timedelta64[ns]\")\n        assert is_timedelta64_dtype(tdi)\n        assert is_timedelta64_ns_dtype(tdi)\n        assert is_timedelta64_ns_dtype(tdi.astype(\"timedelta64[ns]\"))\n\n        # Conversion to Int64Index:\n        assert not is_timedelta64_ns_dtype(tdi.astype(\"timedelta64\"))\n        assert not is_timedelta64_ns_dtype(tdi.astype(\"timedelta64[h]\"))\n\n\nclass TestIsScalar:\n    def test_is_scalar_builtin_scalars(self):\n        assert is_scalar(None)\n        assert is_scalar(True)\n        assert is_scalar(False)\n        assert is_scalar(Fraction())\n        assert is_scalar(0.0)\n        assert is_scalar(1)\n        assert is_scalar(complex(2))\n        assert is_scalar(float(\"NaN\"))\n        assert is_scalar(np.nan)\n        assert is_scalar(\"foobar\")\n        assert is_scalar(b\"foobar\")\n        assert is_scalar(datetime(2014, 1, 1))\n        assert is_scalar(date(2014, 1, 1))\n        assert is_scalar(time(12, 0))\n        assert is_scalar(timedelta(hours=1))\n        assert is_scalar(pd.NaT)\n        assert is_scalar(pd.NA)\n\n    def test_is_scalar_builtin_nonscalars(self):\n        assert not is_scalar({})\n        assert not is_scalar([])\n        assert not is_scalar([1])\n        assert not is_scalar(())\n        assert not is_scalar((1,))\n        assert not is_scalar(slice(None))\n        assert not is_scalar(Ellipsis)\n\n    def test_is_scalar_numpy_array_scalars(self):\n        assert is_scalar(np.int64(1))\n        assert is_scalar(np.float64(1.0))\n        assert is_scalar(np.int32(1))\n        assert is_scalar(np.complex64(2))\n        assert is_scalar(np.object_(\"foobar\"))\n        assert is_scalar(np.str_(\"foobar\"))\n        assert is_scalar(np.unicode_(\"foobar\"))\n        assert is_scalar(np.bytes_(b\"foobar\"))\n        assert is_scalar(np.datetime64(\"2014-01-01\"))\n        assert is_scalar(np.timedelta64(1, \"h\"))\n\n    def test_is_scalar_numpy_zerodim_arrays(self):\n        for zerodim in [\n            np.array(1),\n            np.array(\"foobar\"),\n            np.array(np.datetime64(\"2014-01-01\")),\n            np.array(np.timedelta64(1, \"h\")),\n            np.array(np.datetime64(\"NaT\")),\n        ]:\n            assert not is_scalar(zerodim)\n            assert is_scalar(lib.item_from_zerodim(zerodim))\n\n    @pytest.mark.filterwarnings(\"ignore::PendingDeprecationWarning\")\n    def test_is_scalar_numpy_arrays(self):\n        assert not is_scalar(np.array([]))\n        assert not is_scalar(np.array([[]]))\n        assert not is_scalar(np.matrix(\"1; 2\"))\n\n    def test_is_scalar_pandas_scalars(self):\n        assert is_scalar(Timestamp(\"2014-01-01\"))\n        assert is_scalar(Timedelta(hours=1))\n        assert is_scalar(Period(\"2014-01-01\"))\n        assert is_scalar(Interval(left=0, right=1))\n        assert is_scalar(DateOffset(days=1))\n        assert is_scalar(pd.offsets.Minute(3))\n\n    def test_is_scalar_pandas_containers(self):\n        assert not is_scalar(Series(dtype=object))\n        assert not is_scalar(Series([1]))\n        assert not is_scalar(DataFrame())\n        assert not is_scalar(DataFrame([[1]]))\n        assert not is_scalar(Index([]))\n        assert not is_scalar(Index([1]))\n        assert not is_scalar(Categorical([]))\n        assert not is_scalar(DatetimeIndex([])._data)\n        assert not is_scalar(TimedeltaIndex([])._data)\n        assert not is_scalar(DatetimeIndex([])._data.to_period(\"D\"))\n        assert not is_scalar(pd.array([1, 2, 3]))\n\n    def test_is_scalar_number(self):\n        # Number() is not recognied by PyNumber_Check, so by extension\n        #  is not recognized by is_scalar, but instances of non-abstract\n        #  subclasses are.\n\n        class Numeric(Number):\n            def __init__(self, value):\n                self.value = value\n\n            def __int__(self):\n                return self.value\n\n        num = Numeric(1)\n        assert is_scalar(num)\n\n\ndef test_datetimeindex_from_empty_datetime64_array():\n    for unit in [\"ms\", \"us\", \"ns\"]:\n        idx = DatetimeIndex(np.array([], dtype=f\"datetime64[{unit}]\"))\n        assert len(idx) == 0\n\n\ndef test_nan_to_nat_conversions():\n\n    df = DataFrame(\n        {\"A\": np.asarray(range(10), dtype=\"float64\"), \"B\": Timestamp(\"20010101\")}\n    )\n    df.iloc[3:6, :] = np.nan\n    result = df.loc[4, \"B\"]\n    assert result is pd.NaT\n\n    s = df[\"B\"].copy()\n    s[8:9] = np.nan\n    assert s[8] is pd.NaT\n\n\n@td.skip_if_no_scipy\n@pytest.mark.filterwarnings(\"ignore::PendingDeprecationWarning\")\ndef test_is_scipy_sparse(spmatrix):\n    assert is_scipy_sparse(spmatrix([[0, 1]]))\n    assert not is_scipy_sparse(np.array([1]))\n\n\ndef test_ensure_int32():\n    values = np.arange(10, dtype=np.int32)\n    result = ensure_int32(values)\n    assert result.dtype == np.int32\n\n    values = np.arange(10, dtype=np.int64)\n    result = ensure_int32(values)\n    assert result.dtype == np.int32\n"
    },
    {
      "filename": "pandas/tests/frame/methods/test_to_records.py",
      "content": "from collections import abc\n\nimport numpy as np\nimport pytest\n\nfrom pandas import (\n    CategoricalDtype,\n    DataFrame,\n    MultiIndex,\n    Series,\n    Timestamp,\n    date_range,\n)\nimport pandas._testing as tm\n\n\nclass TestDataFrameToRecords:\n    def test_to_records_dt64(self):\n        df = DataFrame(\n            [[\"one\", \"two\", \"three\"], [\"four\", \"five\", \"six\"]],\n            index=date_range(\"2012-01-01\", \"2012-01-02\"),\n        )\n\n        expected = df.index.values[0]\n        result = df.to_records()[\"index\"][0]\n        assert expected == result\n\n    def test_to_records_dt64tz_column(self):\n        # GH#32535 dont less tz in to_records\n        df = DataFrame({\"A\": date_range(\"2012-01-01\", \"2012-01-02\", tz=\"US/Eastern\")})\n\n        result = df.to_records()\n\n        assert result.dtype[\"A\"] == object\n        val = result[0][1]\n        assert isinstance(val, Timestamp)\n        assert val == df.loc[0, \"A\"]\n\n    def test_to_records_with_multindex(self):\n        # GH#3189\n        index = [\n            [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n            [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n        ]\n        data = np.zeros((8, 4))\n        df = DataFrame(data, index=index)\n        r = df.to_records(index=True)[\"level_0\"]\n        assert \"bar\" in r\n        assert \"one\" not in r\n\n    def test_to_records_with_Mapping_type(self):\n        import email\n        from email.parser import Parser\n\n        abc.Mapping.register(email.message.Message)\n\n        headers = Parser().parsestr(\n            \"From: <user@example.com>\\n\"\n            \"To: <someone_else@example.com>\\n\"\n            \"Subject: Test message\\n\"\n            \"\\n\"\n            \"Body would go here\\n\"\n        )\n\n        frame = DataFrame.from_records([headers])\n        all(x in frame for x in [\"Type\", \"Subject\", \"From\"])\n\n    def test_to_records_floats(self):\n        df = DataFrame(np.random.rand(10, 10))\n        df.to_records()\n\n    def test_to_records_index_name(self):\n        df = DataFrame(np.random.randn(3, 3))\n        df.index.name = \"X\"\n        rs = df.to_records()\n        assert \"X\" in rs.dtype.fields\n\n        df = DataFrame(np.random.randn(3, 3))\n        rs = df.to_records()\n        assert \"index\" in rs.dtype.fields\n\n        df.index = MultiIndex.from_tuples([(\"a\", \"x\"), (\"a\", \"y\"), (\"b\", \"z\")])\n        df.index.names = [\"A\", None]\n        rs = df.to_records()\n        assert \"level_0\" in rs.dtype.fields\n\n    def test_to_records_with_unicode_index(self):\n        # GH#13172\n        # unicode_literals conflict with to_records\n        result = DataFrame([{\"a\": \"x\", \"b\": \"y\"}]).set_index(\"a\").to_records()\n        expected = np.rec.array([(\"x\", \"y\")], dtype=[(\"a\", \"O\"), (\"b\", \"O\")])\n        tm.assert_almost_equal(result, expected)\n\n    def test_to_records_with_unicode_column_names(self):\n        # xref issue: https://github.com/numpy/numpy/issues/2407\n        # Issue GH#11879. to_records used to raise an exception when used\n        # with column names containing non-ascii characters in Python 2\n        result = DataFrame(data={\"accented_name_\": [1.0]}).to_records()\n\n        # Note that numpy allows for unicode field names but dtypes need\n        # to be specified using dictionary instead of list of tuples.\n        expected = np.rec.array(\n            [(0, 1.0)],\n            dtype={\"names\": [\"index\", \"accented_name_\"], \"formats\": [\"=i8\", \"=f8\"]},\n        )\n        tm.assert_almost_equal(result, expected)\n\n    def test_to_records_with_categorical(self):\n        # GH#8626\n\n        # dict creation\n        df = DataFrame({\"A\": list(\"abc\")}, dtype=\"category\")\n        expected = Series(list(\"abc\"), dtype=\"category\", name=\"A\")\n        tm.assert_series_equal(df[\"A\"], expected)\n\n        # list-like creation\n        df = DataFrame(list(\"abc\"), dtype=\"category\")\n        expected = Series(list(\"abc\"), dtype=\"category\", name=0)\n        tm.assert_series_equal(df[0], expected)\n\n        # to record array\n        # this coerces\n        result = df.to_records()\n        expected = np.rec.array(\n            [(0, \"a\"), (1, \"b\"), (2, \"c\")], dtype=[(\"index\", \"=i8\"), (\"0\", \"O\")]\n        )\n        tm.assert_almost_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"kwargs,expected\",\n        [\n            # No dtypes --> default to array dtypes.\n            (\n                {},\n                np.rec.array(\n                    [(0, 1, 0.2, \"a\"), (1, 2, 1.5, \"bc\")],\n                    dtype=[(\"index\", \"<i8\"), (\"A\", \"<i8\"), (\"B\", \"<f8\"), (\"C\", \"O\")],\n                ),\n            ),\n            # Should have no effect in this case.\n            (\n                {\"index\": True},\n                np.rec.array(\n                    [(0, 1, 0.2, \"a\"), (1, 2, 1.5, \"bc\")],\n                    dtype=[(\"index\", \"<i8\"), (\"A\", \"<i8\"), (\"B\", \"<f8\"), (\"C\", \"O\")],\n                ),\n            ),\n            # Column dtype applied across the board. Index unaffected.\n            (\n                {\"column_dtypes\": \"<U4\"},\n                np.rec.array(\n                    [(\"0\", \"1\", \"0.2\", \"a\"), (\"1\", \"2\", \"1.5\", \"bc\")],\n                    dtype=[(\"index\", \"<i8\"), (\"A\", \"<U4\"), (\"B\", \"<U4\"), (\"C\", \"<U4\")],\n                ),\n            ),\n            # Index dtype applied across the board. Columns unaffected.\n            (\n                {\"index_dtypes\": \"<U1\"},\n                np.rec.array(\n                    [(\"0\", 1, 0.2, \"a\"), (\"1\", 2, 1.5, \"bc\")],\n                    dtype=[(\"index\", \"<U1\"), (\"A\", \"<i8\"), (\"B\", \"<f8\"), (\"C\", \"O\")],\n                ),\n            ),\n            # Pass in a type instance.\n            (\n                {\"column_dtypes\": str},\n                np.rec.array(\n                    [(\"0\", \"1\", \"0.2\", \"a\"), (\"1\", \"2\", \"1.5\", \"bc\")],\n                    dtype=[(\"index\", \"<i8\"), (\"A\", \"<U\"), (\"B\", \"<U\"), (\"C\", \"<U\")],\n                ),\n            ),\n            # Pass in a dtype instance.\n            (\n                {\"column_dtypes\": np.dtype(\"unicode\")},\n                np.rec.array(\n                    [(\"0\", \"1\", \"0.2\", \"a\"), (\"1\", \"2\", \"1.5\", \"bc\")],\n                    dtype=[(\"index\", \"<i8\"), (\"A\", \"<U\"), (\"B\", \"<U\"), (\"C\", \"<U\")],\n                ),\n            ),\n            # Pass in a dictionary (name-only).\n            (\n                {\"column_dtypes\": {\"A\": np.int8, \"B\": np.float32, \"C\": \"<U2\"}},\n                np.rec.array(\n                    [(\"0\", \"1\", \"0.2\", \"a\"), (\"1\", \"2\", \"1.5\", \"bc\")],\n                    dtype=[(\"index\", \"<i8\"), (\"A\", \"i1\"), (\"B\", \"<f4\"), (\"C\", \"<U2\")],\n                ),\n            ),\n            # Pass in a dictionary (indices-only).\n            (\n                {\"index_dtypes\": {0: \"int16\"}},\n                np.rec.array(\n                    [(0, 1, 0.2, \"a\"), (1, 2, 1.5, \"bc\")],\n                    dtype=[(\"index\", \"i2\"), (\"A\", \"<i8\"), (\"B\", \"<f8\"), (\"C\", \"O\")],\n                ),\n            ),\n            # Ignore index mappings if index is not True.\n            (\n                {\"index\": False, \"index_dtypes\": \"<U2\"},\n                np.rec.array(\n                    [(1, 0.2, \"a\"), (2, 1.5, \"bc\")],\n                    dtype=[(\"A\", \"<i8\"), (\"B\", \"<f8\"), (\"C\", \"O\")],\n                ),\n            ),\n            # Non-existent names / indices in mapping should not error.\n            (\n                {\"index_dtypes\": {0: \"int16\", \"not-there\": \"float32\"}},\n                np.rec.array(\n                    [(0, 1, 0.2, \"a\"), (1, 2, 1.5, \"bc\")],\n                    dtype=[(\"index\", \"i2\"), (\"A\", \"<i8\"), (\"B\", \"<f8\"), (\"C\", \"O\")],\n                ),\n            ),\n            # Names / indices not in mapping default to array dtype.\n            (\n                {\"column_dtypes\": {\"A\": np.int8, \"B\": np.float32}},\n                np.rec.array(\n                    [(\"0\", \"1\", \"0.2\", \"a\"), (\"1\", \"2\", \"1.5\", \"bc\")],\n                    dtype=[(\"index\", \"<i8\"), (\"A\", \"i1\"), (\"B\", \"<f4\"), (\"C\", \"O\")],\n                ),\n            ),\n            # Names / indices not in dtype mapping default to array dtype.\n            (\n                {\"column_dtypes\": {\"A\": np.dtype(\"int8\"), \"B\": np.dtype(\"float32\")}},\n                np.rec.array(\n                    [(\"0\", \"1\", \"0.2\", \"a\"), (\"1\", \"2\", \"1.5\", \"bc\")],\n                    dtype=[(\"index\", \"<i8\"), (\"A\", \"i1\"), (\"B\", \"<f4\"), (\"C\", \"O\")],\n                ),\n            ),\n            # Mixture of everything.\n            (\n                {\n                    \"column_dtypes\": {\"A\": np.int8, \"B\": np.float32},\n                    \"index_dtypes\": \"<U2\",\n                },\n                np.rec.array(\n                    [(\"0\", \"1\", \"0.2\", \"a\"), (\"1\", \"2\", \"1.5\", \"bc\")],\n                    dtype=[(\"index\", \"<U2\"), (\"A\", \"i1\"), (\"B\", \"<f4\"), (\"C\", \"O\")],\n                ),\n            ),\n            # Invalid dype values.\n            (\n                {\"index\": False, \"column_dtypes\": []},\n                (ValueError, \"Invalid dtype \\\\[\\\\] specified for column A\"),\n            ),\n            (\n                {\"index\": False, \"column_dtypes\": {\"A\": \"int32\", \"B\": 5}},\n                (ValueError, \"Invalid dtype 5 specified for column B\"),\n            ),\n            # Numpy can't handle EA types, so check error is raised\n            (\n                {\n                    \"index\": False,\n                    \"column_dtypes\": {\"A\": \"int32\", \"B\": CategoricalDtype([\"a\", \"b\"])},\n                },\n                (ValueError, \"Invalid dtype category specified for column B\"),\n            ),\n            # Check that bad types raise\n            (\n                {\"index\": False, \"column_dtypes\": {\"A\": \"int32\", \"B\": \"foo\"}},\n                (TypeError, \"data type [\\\"']foo[\\\"'] not understood\"),\n            ),\n        ],\n    )\n    def test_to_records_dtype(self, kwargs, expected):\n        # see GH#18146\n        df = DataFrame({\"A\": [1, 2], \"B\": [0.2, 1.5], \"C\": [\"a\", \"bc\"]})\n\n        if not isinstance(expected, np.recarray):\n            with pytest.raises(expected[0], match=expected[1]):\n                df.to_records(**kwargs)\n        else:\n            result = df.to_records(**kwargs)\n            tm.assert_almost_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"df,kwargs,expected\",\n        [\n            # MultiIndex in the index.\n            (\n                DataFrame(\n                    [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list(\"abc\")\n                ).set_index([\"a\", \"b\"]),\n                {\"column_dtypes\": \"float64\", \"index_dtypes\": {0: \"int32\", 1: \"int8\"}},\n                np.rec.array(\n                    [(1, 2, 3.0), (4, 5, 6.0), (7, 8, 9.0)],\n                    dtype=[(\"a\", \"<i4\"), (\"b\", \"i1\"), (\"c\", \"<f8\")],\n                ),\n            ),\n            # MultiIndex in the columns.\n            (\n                DataFrame(\n                    [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                    columns=MultiIndex.from_tuples(\n                        [(\"a\", \"d\"), (\"b\", \"e\"), (\"c\", \"f\")]\n                    ),\n                ),\n                {\"column_dtypes\": {0: \"<U1\", 2: \"float32\"}, \"index_dtypes\": \"float32\"},\n                np.rec.array(\n                    [(0.0, \"1\", 2, 3.0), (1.0, \"4\", 5, 6.0), (2.0, \"7\", 8, 9.0)],\n                    dtype=[\n                        (\"index\", \"<f4\"),\n                        (\"('a', 'd')\", \"<U1\"),\n                        (\"('b', 'e')\", \"<i8\"),\n                        (\"('c', 'f')\", \"<f4\"),\n                    ],\n                ),\n            ),\n            # MultiIndex in both the columns and index.\n            (\n                DataFrame(\n                    [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                    columns=MultiIndex.from_tuples(\n                        [(\"a\", \"d\"), (\"b\", \"e\"), (\"c\", \"f\")], names=list(\"ab\")\n                    ),\n                    index=MultiIndex.from_tuples(\n                        [(\"d\", -4), (\"d\", -5), (\"f\", -6)], names=list(\"cd\")\n                    ),\n                ),\n                {\"column_dtypes\": \"float64\", \"index_dtypes\": {0: \"<U2\", 1: \"int8\"}},\n                np.rec.array(\n                    [\n                        (\"d\", -4, 1.0, 2.0, 3.0),\n                        (\"d\", -5, 4.0, 5.0, 6.0),\n                        (\"f\", -6, 7, 8, 9.0),\n                    ],\n                    dtype=[\n                        (\"c\", \"<U2\"),\n                        (\"d\", \"i1\"),\n                        (\"('a', 'd')\", \"<f8\"),\n                        (\"('b', 'e')\", \"<f8\"),\n                        (\"('c', 'f')\", \"<f8\"),\n                    ],\n                ),\n            ),\n        ],\n    )\n    def test_to_records_dtype_mi(self, df, kwargs, expected):\n        # see GH#18146\n        result = df.to_records(**kwargs)\n        tm.assert_almost_equal(result, expected)\n\n    def test_to_records_dict_like(self):\n        # see GH#18146\n        class DictLike:\n            def __init__(self, **kwargs):\n                self.d = kwargs.copy()\n\n            def __getitem__(self, key):\n                return self.d.__getitem__(key)\n\n            def __contains__(self, key) -> bool:\n                return key in self.d\n\n            def keys(self):\n                return self.d.keys()\n\n        df = DataFrame({\"A\": [1, 2], \"B\": [0.2, 1.5], \"C\": [\"a\", \"bc\"]})\n\n        dtype_mappings = {\n            \"column_dtypes\": DictLike(**{\"A\": np.int8, \"B\": np.float32}),\n            \"index_dtypes\": \"<U2\",\n        }\n\n        result = df.to_records(**dtype_mappings)\n        expected = np.rec.array(\n            [(\"0\", \"1\", \"0.2\", \"a\"), (\"1\", \"2\", \"1.5\", \"bc\")],\n            dtype=[(\"index\", \"<U2\"), (\"A\", \"i1\"), (\"B\", \"<f4\"), (\"C\", \"O\")],\n        )\n        tm.assert_almost_equal(result, expected)\n\n    @pytest.mark.parametrize(\"tz\", [\"UTC\", \"GMT\", \"US/Eastern\"])\n    def test_to_records_datetimeindex_with_tz(self, tz):\n        # GH#13937\n        dr = date_range(\"2016-01-01\", periods=10, freq=\"S\", tz=tz)\n\n        df = DataFrame({\"datetime\": dr}, index=dr)\n\n        expected = df.to_records()\n        result = df.tz_convert(\"UTC\").to_records()\n\n        # both converted to UTC, so they are equal\n        tm.assert_numpy_array_equal(result, expected)\n"
    },
    {
      "filename": "pandas/tests/groupby/transform/test_transform.py",
      "content": "\"\"\" test with the .transform \"\"\"\nfrom io import StringIO\n\nimport numpy as np\nimport pytest\n\nfrom pandas.core.dtypes.common import ensure_platform_int, is_timedelta64_dtype\n\nimport pandas as pd\nfrom pandas import (\n    Categorical,\n    DataFrame,\n    MultiIndex,\n    Series,\n    Timestamp,\n    concat,\n    date_range,\n)\nimport pandas._testing as tm\nfrom pandas.core.groupby.groupby import DataError\n\n\ndef assert_fp_equal(a, b):\n    assert (np.abs(a - b) < 1e-12).all()\n\n\ndef test_transform():\n    data = Series(np.arange(9) // 3, index=np.arange(9))\n\n    index = np.arange(9)\n    np.random.shuffle(index)\n    data = data.reindex(index)\n\n    grouped = data.groupby(lambda x: x // 3)\n\n    transformed = grouped.transform(lambda x: x * x.sum())\n    assert transformed[7] == 12\n\n    # GH 8046\n    # make sure that we preserve the input order\n\n    df = DataFrame(\n        np.arange(6, dtype=\"int64\").reshape(3, 2), columns=[\"a\", \"b\"], index=[0, 2, 1]\n    )\n    key = [0, 0, 1]\n    expected = (\n        df.sort_index()\n        .groupby(key)\n        .transform(lambda x: x - x.mean())\n        .groupby(key)\n        .mean()\n    )\n    result = df.groupby(key).transform(lambda x: x - x.mean()).groupby(key).mean()\n    tm.assert_frame_equal(result, expected)\n\n    def demean(arr):\n        return arr - arr.mean()\n\n    people = DataFrame(\n        np.random.randn(5, 5),\n        columns=[\"a\", \"b\", \"c\", \"d\", \"e\"],\n        index=[\"Joe\", \"Steve\", \"Wes\", \"Jim\", \"Travis\"],\n    )\n    key = [\"one\", \"two\", \"one\", \"two\", \"one\"]\n    result = people.groupby(key).transform(demean).groupby(key).mean()\n    expected = people.groupby(key).apply(demean).groupby(key).mean()\n    tm.assert_frame_equal(result, expected)\n\n    # GH 8430\n    df = tm.makeTimeDataFrame()\n    g = df.groupby(pd.Grouper(freq=\"M\"))\n    g.transform(lambda x: x - 1)\n\n    # GH 9700\n    df = DataFrame({\"a\": range(5, 10), \"b\": range(5)})\n    result = df.groupby(\"a\").transform(max)\n    expected = DataFrame({\"b\": range(5)})\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_transform_fast():\n\n    df = DataFrame({\"id\": np.arange(100000) / 3, \"val\": np.random.randn(100000)})\n\n    grp = df.groupby(\"id\")[\"val\"]\n\n    values = np.repeat(grp.mean().values, ensure_platform_int(grp.count().values))\n    expected = Series(values, index=df.index, name=\"val\")\n\n    result = grp.transform(np.mean)\n    tm.assert_series_equal(result, expected)\n\n    result = grp.transform(\"mean\")\n    tm.assert_series_equal(result, expected)\n\n    # GH 12737\n    df = DataFrame(\n        {\n            \"grouping\": [0, 1, 1, 3],\n            \"f\": [1.1, 2.1, 3.1, 4.5],\n            \"d\": pd.date_range(\"2014-1-1\", \"2014-1-4\"),\n            \"i\": [1, 2, 3, 4],\n        },\n        columns=[\"grouping\", \"f\", \"i\", \"d\"],\n    )\n    result = df.groupby(\"grouping\").transform(\"first\")\n\n    dates = [\n        Timestamp(\"2014-1-1\"),\n        Timestamp(\"2014-1-2\"),\n        Timestamp(\"2014-1-2\"),\n        Timestamp(\"2014-1-4\"),\n    ]\n    expected = DataFrame(\n        {\"f\": [1.1, 2.1, 2.1, 4.5], \"d\": dates, \"i\": [1, 2, 2, 4]},\n        columns=[\"f\", \"i\", \"d\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n    # selection\n    result = df.groupby(\"grouping\")[[\"f\", \"i\"]].transform(\"first\")\n    expected = expected[[\"f\", \"i\"]]\n    tm.assert_frame_equal(result, expected)\n\n    # dup columns\n    df = DataFrame([[1, 2, 3], [4, 5, 6]], columns=[\"g\", \"a\", \"a\"])\n    result = df.groupby(\"g\").transform(\"first\")\n    expected = df.drop(\"g\", axis=1)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_transform_broadcast(tsframe, ts):\n    grouped = ts.groupby(lambda x: x.month)\n    result = grouped.transform(np.mean)\n\n    tm.assert_index_equal(result.index, ts.index)\n    for _, gp in grouped:\n        assert_fp_equal(result.reindex(gp.index), gp.mean())\n\n    grouped = tsframe.groupby(lambda x: x.month)\n    result = grouped.transform(np.mean)\n    tm.assert_index_equal(result.index, tsframe.index)\n    for _, gp in grouped:\n        agged = gp.mean()\n        res = result.reindex(gp.index)\n        for col in tsframe:\n            assert_fp_equal(res[col], agged[col])\n\n    # group columns\n    grouped = tsframe.groupby({\"A\": 0, \"B\": 0, \"C\": 1, \"D\": 1}, axis=1)\n    result = grouped.transform(np.mean)\n    tm.assert_index_equal(result.index, tsframe.index)\n    tm.assert_index_equal(result.columns, tsframe.columns)\n    for _, gp in grouped:\n        agged = gp.mean(1)\n        res = result.reindex(columns=gp.columns)\n        for idx in gp.index:\n            assert_fp_equal(res.xs(idx), agged[idx])\n\n\ndef test_transform_axis_1(transformation_func):\n    # GH 36308\n    if transformation_func == \"tshift\":\n        pytest.xfail(\"tshift is deprecated\")\n    args = (\"ffill\",) if transformation_func == \"fillna\" else ()\n\n    df = DataFrame({\"a\": [1, 2], \"b\": [3, 4], \"c\": [5, 6]}, index=[\"x\", \"y\"])\n    result = df.groupby([0, 0, 1], axis=1).transform(transformation_func, *args)\n    expected = df.T.groupby([0, 0, 1]).transform(transformation_func, *args).T\n\n    if transformation_func == \"diff\":\n        # Result contains nans, so transpose coerces to float\n        expected[\"b\"] = expected[\"b\"].astype(\"int64\")\n\n    # cumcount returns Series; the rest are DataFrame\n    tm.assert_equal(result, expected)\n\n\ndef test_transform_axis_ts(tsframe):\n\n    # make sure that we are setting the axes\n    # correctly when on axis=0 or 1\n    # in the presence of a non-monotonic indexer\n    # GH12713\n\n    base = tsframe.iloc[0:5]\n    r = len(base.index)\n    c = len(base.columns)\n    tso = DataFrame(\n        np.random.randn(r, c), index=base.index, columns=base.columns, dtype=\"float64\"\n    )\n    # monotonic\n    ts = tso\n    grouped = ts.groupby(lambda x: x.weekday())\n    result = ts - grouped.transform(\"mean\")\n    expected = grouped.apply(lambda x: x - x.mean())\n    tm.assert_frame_equal(result, expected)\n\n    ts = ts.T\n    grouped = ts.groupby(lambda x: x.weekday(), axis=1)\n    result = ts - grouped.transform(\"mean\")\n    expected = grouped.apply(lambda x: (x.T - x.mean(1)).T)\n    tm.assert_frame_equal(result, expected)\n\n    # non-monotonic\n    ts = tso.iloc[[1, 0] + list(range(2, len(base)))]\n    grouped = ts.groupby(lambda x: x.weekday())\n    result = ts - grouped.transform(\"mean\")\n    expected = grouped.apply(lambda x: x - x.mean())\n    tm.assert_frame_equal(result, expected)\n\n    ts = ts.T\n    grouped = ts.groupby(lambda x: x.weekday(), axis=1)\n    result = ts - grouped.transform(\"mean\")\n    expected = grouped.apply(lambda x: (x.T - x.mean(1)).T)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_transform_dtype():\n    # GH 9807\n    # Check transform dtype output is preserved\n    df = DataFrame([[1, 3], [2, 3]])\n    result = df.groupby(1).transform(\"mean\")\n    expected = DataFrame([[1.5], [1.5]])\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_transform_bug():\n    # GH 5712\n    # transforming on a datetime column\n    df = DataFrame({\"A\": Timestamp(\"20130101\"), \"B\": np.arange(5)})\n    result = df.groupby(\"A\")[\"B\"].transform(lambda x: x.rank(ascending=False))\n    expected = Series(np.arange(5, 0, step=-1), name=\"B\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_transform_numeric_to_boolean():\n    # GH 16875\n    # inconsistency in transforming boolean values\n    expected = Series([True, True], name=\"A\")\n\n    df = DataFrame({\"A\": [1.1, 2.2], \"B\": [1, 2]})\n    result = df.groupby(\"B\").A.transform(lambda x: True)\n    tm.assert_series_equal(result, expected)\n\n    df = DataFrame({\"A\": [1, 2], \"B\": [1, 2]})\n    result = df.groupby(\"B\").A.transform(lambda x: True)\n    tm.assert_series_equal(result, expected)\n\n\ndef test_transform_datetime_to_timedelta():\n    # GH 15429\n    # transforming a datetime to timedelta\n    df = DataFrame({\"A\": Timestamp(\"20130101\"), \"B\": np.arange(5)})\n    expected = Series([Timestamp(\"20130101\") - Timestamp(\"20130101\")] * 5, name=\"A\")\n\n    # this does date math without changing result type in transform\n    base_time = df[\"A\"][0]\n    result = (\n        df.groupby(\"A\")[\"A\"].transform(lambda x: x.max() - x.min() + base_time)\n        - base_time\n    )\n    tm.assert_series_equal(result, expected)\n\n    # this does date math and causes the transform to return timedelta\n    result = df.groupby(\"A\")[\"A\"].transform(lambda x: x.max() - x.min())\n    tm.assert_series_equal(result, expected)\n\n\ndef test_transform_datetime_to_numeric():\n    # GH 10972\n    # convert dt to float\n    df = DataFrame({\"a\": 1, \"b\": date_range(\"2015-01-01\", periods=2, freq=\"D\")})\n    result = df.groupby(\"a\").b.transform(\n        lambda x: x.dt.dayofweek - x.dt.dayofweek.mean()\n    )\n\n    expected = Series([-0.5, 0.5], name=\"b\")\n    tm.assert_series_equal(result, expected)\n\n    # convert dt to int\n    df = DataFrame({\"a\": 1, \"b\": date_range(\"2015-01-01\", periods=2, freq=\"D\")})\n    result = df.groupby(\"a\").b.transform(\n        lambda x: x.dt.dayofweek - x.dt.dayofweek.min()\n    )\n\n    expected = Series([0, 1], name=\"b\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_transform_casting():\n    # 13046\n    data = \"\"\"\n    idx     A         ID3              DATETIME\n    0   B-028  b76cd912ff \"2014-10-08 13:43:27\"\n    1   B-054  4a57ed0b02 \"2014-10-08 14:26:19\"\n    2   B-076  1a682034f8 \"2014-10-08 14:29:01\"\n    3   B-023  b76cd912ff \"2014-10-08 18:39:34\"\n    4   B-023  f88g8d7sds \"2014-10-08 18:40:18\"\n    5   B-033  b76cd912ff \"2014-10-08 18:44:30\"\n    6   B-032  b76cd912ff \"2014-10-08 18:46:00\"\n    7   B-037  b76cd912ff \"2014-10-08 18:52:15\"\n    8   B-046  db959faf02 \"2014-10-08 18:59:59\"\n    9   B-053  b76cd912ff \"2014-10-08 19:17:48\"\n    10  B-065  b76cd912ff \"2014-10-08 19:21:38\"\n    \"\"\"\n    df = pd.read_csv(\n        StringIO(data), sep=r\"\\s+\", index_col=[0], parse_dates=[\"DATETIME\"]\n    )\n\n    result = df.groupby(\"ID3\")[\"DATETIME\"].transform(lambda x: x.diff())\n    assert is_timedelta64_dtype(result.dtype)\n\n    result = df[[\"ID3\", \"DATETIME\"]].groupby(\"ID3\").transform(lambda x: x.diff())\n    assert is_timedelta64_dtype(result.DATETIME.dtype)\n\n\ndef test_transform_multiple(ts):\n    grouped = ts.groupby([lambda x: x.year, lambda x: x.month])\n\n    grouped.transform(lambda x: x * 2)\n    grouped.transform(np.mean)\n\n\ndef test_dispatch_transform(tsframe):\n    df = tsframe[::5].reindex(tsframe.index)\n\n    grouped = df.groupby(lambda x: x.month)\n\n    filled = grouped.fillna(method=\"pad\")\n    fillit = lambda x: x.fillna(method=\"pad\")\n    expected = df.groupby(lambda x: x.month).transform(fillit)\n    tm.assert_frame_equal(filled, expected)\n\n\ndef test_transform_transformation_func(transformation_func):\n    # GH 30918\n    df = DataFrame(\n        {\n            \"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"baz\"],\n            \"B\": [1, 2, np.nan, 3, 3, np.nan, 4],\n        },\n        index=pd.date_range(\"2020-01-01\", \"2020-01-07\"),\n    )\n\n    if transformation_func == \"cumcount\":\n        test_op = lambda x: x.transform(\"cumcount\")\n        mock_op = lambda x: Series(range(len(x)), x.index)\n    elif transformation_func == \"fillna\":\n        test_op = lambda x: x.transform(\"fillna\", value=0)\n        mock_op = lambda x: x.fillna(value=0)\n    elif transformation_func == \"tshift\":\n        msg = (\n            \"Current behavior of groupby.tshift is inconsistent with other \"\n            \"transformations. See GH34452 for more details\"\n        )\n        pytest.xfail(msg)\n    else:\n        test_op = lambda x: x.transform(transformation_func)\n        mock_op = lambda x: getattr(x, transformation_func)()\n\n    result = test_op(df.groupby(\"A\"))\n    groups = [df[[\"B\"]].iloc[:4], df[[\"B\"]].iloc[4:6], df[[\"B\"]].iloc[6:]]\n    expected = concat([mock_op(g) for g in groups])\n\n    if transformation_func == \"cumcount\":\n        tm.assert_series_equal(result, expected)\n    else:\n        tm.assert_frame_equal(result, expected)\n\n\ndef test_transform_select_columns(df):\n    f = lambda x: x.mean()\n    result = df.groupby(\"A\")[[\"C\", \"D\"]].transform(f)\n\n    selection = df[[\"C\", \"D\"]]\n    expected = selection.groupby(df[\"A\"]).transform(f)\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_transform_exclude_nuisance(df):\n\n    # this also tests orderings in transform between\n    # series/frame to make sure it's consistent\n    expected = {}\n    grouped = df.groupby(\"A\")\n    expected[\"C\"] = grouped[\"C\"].transform(np.mean)\n    expected[\"D\"] = grouped[\"D\"].transform(np.mean)\n    expected = DataFrame(expected)\n    result = df.groupby(\"A\").transform(np.mean)\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_transform_function_aliases(df):\n    result = df.groupby(\"A\").transform(\"mean\")\n    expected = df.groupby(\"A\").transform(np.mean)\n    tm.assert_frame_equal(result, expected)\n\n    result = df.groupby(\"A\")[\"C\"].transform(\"mean\")\n    expected = df.groupby(\"A\")[\"C\"].transform(np.mean)\n    tm.assert_series_equal(result, expected)\n\n\ndef test_series_fast_transform_date():\n    # GH 13191\n    df = DataFrame(\n        {\"grouping\": [np.nan, 1, 1, 3], \"d\": pd.date_range(\"2014-1-1\", \"2014-1-4\")}\n    )\n    result = df.groupby(\"grouping\")[\"d\"].transform(\"first\")\n    dates = [\n        pd.NaT,\n        Timestamp(\"2014-1-2\"),\n        Timestamp(\"2014-1-2\"),\n        Timestamp(\"2014-1-4\"),\n    ]\n    expected = Series(dates, name=\"d\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_transform_length():\n    # GH 9697\n    df = DataFrame({\"col1\": [1, 1, 2, 2], \"col2\": [1, 2, 3, np.nan]})\n    expected = Series([3.0] * 4)\n\n    def nsum(x):\n        return np.nansum(x)\n\n    results = [\n        df.groupby(\"col1\").transform(sum)[\"col2\"],\n        df.groupby(\"col1\")[\"col2\"].transform(sum),\n        df.groupby(\"col1\").transform(nsum)[\"col2\"],\n        df.groupby(\"col1\")[\"col2\"].transform(nsum),\n    ]\n    for result in results:\n        tm.assert_series_equal(result, expected, check_names=False)\n\n\ndef test_transform_coercion():\n\n    # 14457\n    # when we are transforming be sure to not coerce\n    # via assignment\n    df = DataFrame({\"A\": [\"a\", \"a\"], \"B\": [0, 1]})\n    g = df.groupby(\"A\")\n\n    expected = g.transform(np.mean)\n    result = g.transform(lambda x: np.mean(x))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_transform_with_int():\n\n    # GH 3740, make sure that we might upcast on item-by-item transform\n\n    # floats\n    df = DataFrame(\n        {\n            \"A\": [1, 1, 1, 2, 2, 2],\n            \"B\": Series(1, dtype=\"float64\"),\n            \"C\": Series([1, 2, 3, 1, 2, 3], dtype=\"float64\"),\n            \"D\": \"foo\",\n        }\n    )\n    with np.errstate(all=\"ignore\"):\n        result = df.groupby(\"A\").transform(lambda x: (x - x.mean()) / x.std())\n    expected = DataFrame(\n        {\"B\": np.nan, \"C\": Series([-1, 0, 1, -1, 0, 1], dtype=\"float64\")}\n    )\n    tm.assert_frame_equal(result, expected)\n\n    # int case\n    df = DataFrame(\n        {\n            \"A\": [1, 1, 1, 2, 2, 2],\n            \"B\": 1,\n            \"C\": [1, 2, 3, 1, 2, 3],\n            \"D\": \"foo\",\n        }\n    )\n    with np.errstate(all=\"ignore\"):\n        result = df.groupby(\"A\").transform(lambda x: (x - x.mean()) / x.std())\n    expected = DataFrame({\"B\": np.nan, \"C\": [-1, 0, 1, -1, 0, 1]})\n    tm.assert_frame_equal(result, expected)\n\n    # int that needs float conversion\n    s = Series([2, 3, 4, 10, 5, -1])\n    df = DataFrame({\"A\": [1, 1, 1, 2, 2, 2], \"B\": 1, \"C\": s, \"D\": \"foo\"})\n    with np.errstate(all=\"ignore\"):\n        result = df.groupby(\"A\").transform(lambda x: (x - x.mean()) / x.std())\n\n    s1 = s.iloc[0:3]\n    s1 = (s1 - s1.mean()) / s1.std()\n    s2 = s.iloc[3:6]\n    s2 = (s2 - s2.mean()) / s2.std()\n    expected = DataFrame({\"B\": np.nan, \"C\": concat([s1, s2])})\n    tm.assert_frame_equal(result, expected)\n\n    # int downcasting\n    result = df.groupby(\"A\").transform(lambda x: x * 2 / 2)\n    expected = DataFrame({\"B\": 1, \"C\": [2, 3, 4, 10, 5, -1]})\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_groupby_transform_with_nan_group():\n    # GH 9941\n    df = DataFrame({\"a\": range(10), \"b\": [1, 1, 2, 3, np.nan, 4, 4, 5, 5, 5]})\n    result = df.groupby(df.b)[\"a\"].transform(max)\n    expected = Series([1.0, 1.0, 2.0, 3.0, np.nan, 6.0, 6.0, 9.0, 9.0, 9.0], name=\"a\")\n    tm.assert_series_equal(result, expected)\n\n\ndef test_transform_mixed_type():\n    index = MultiIndex.from_arrays([[0, 0, 0, 1, 1, 1], [1, 2, 3, 1, 2, 3]])\n    df = DataFrame(\n        {\n            \"d\": [1.0, 1.0, 1.0, 2.0, 2.0, 2.0],\n            \"c\": np.tile([\"a\", \"b\", \"c\"], 2),\n            \"v\": np.arange(1.0, 7.0),\n        },\n        index=index,\n    )\n\n    def f(group):\n        group[\"g\"] = group[\"d\"] * 2\n        return group[:1]\n\n    grouped = df.groupby(\"c\")\n    result = grouped.apply(f)\n\n    assert result[\"d\"].dtype == np.float64\n\n    # this is by definition a mutating operation!\n    with pd.option_context(\"mode.chained_assignment\", None):\n        for key, group in grouped:\n            res = f(group)\n            tm.assert_frame_equal(res, result.loc[key])\n\n\n@pytest.mark.parametrize(\n    \"op, args, targop\",\n    [\n        (\"cumprod\", (), lambda x: x.cumprod()),\n        (\"cumsum\", (), lambda x: x.cumsum()),\n        (\"shift\", (-1,), lambda x: x.shift(-1)),\n        (\"shift\", (1,), lambda x: x.shift()),\n    ],\n)\ndef test_cython_transform_series(op, args, targop):\n    # GH 4095\n    s = Series(np.random.randn(1000))\n    s_missing = s.copy()\n    s_missing.iloc[2:10] = np.nan\n    labels = np.random.randint(0, 50, size=1000).astype(float)\n\n    # series\n    for data in [s, s_missing]:\n        # print(data.head())\n        expected = data.groupby(labels).transform(targop)\n\n        tm.assert_series_equal(expected, data.groupby(labels).transform(op, *args))\n        tm.assert_series_equal(expected, getattr(data.groupby(labels), op)(*args))\n\n\n@pytest.mark.parametrize(\"op\", [\"cumprod\", \"cumsum\"])\n@pytest.mark.parametrize(\"skipna\", [False, True])\n@pytest.mark.parametrize(\n    \"input, exp\",\n    [\n        # When everything is NaN\n        ({\"key\": [\"b\"] * 10, \"value\": np.nan}, Series([np.nan] * 10, name=\"value\")),\n        # When there is a single NaN\n        (\n            {\"key\": [\"b\"] * 10 + [\"a\"] * 2, \"value\": [3] * 3 + [np.nan] + [3] * 8},\n            {\n                (\"cumprod\", False): [3.0, 9.0, 27.0] + [np.nan] * 7 + [3.0, 9.0],\n                (\"cumprod\", True): [\n                    3.0,\n                    9.0,\n                    27.0,\n                    np.nan,\n                    81.0,\n                    243.0,\n                    729.0,\n                    2187.0,\n                    6561.0,\n                    19683.0,\n                    3.0,\n                    9.0,\n                ],\n                (\"cumsum\", False): [3.0, 6.0, 9.0] + [np.nan] * 7 + [3.0, 6.0],\n                (\"cumsum\", True): [\n                    3.0,\n                    6.0,\n                    9.0,\n                    np.nan,\n                    12.0,\n                    15.0,\n                    18.0,\n                    21.0,\n                    24.0,\n                    27.0,\n                    3.0,\n                    6.0,\n                ],\n            },\n        ),\n    ],\n)\ndef test_groupby_cum_skipna(op, skipna, input, exp):\n    df = DataFrame(input)\n    result = df.groupby(\"key\")[\"value\"].transform(op, skipna=skipna)\n    if isinstance(exp, dict):\n        expected = exp[(op, skipna)]\n    else:\n        expected = exp\n    expected = Series(expected, name=\"value\")\n    tm.assert_series_equal(expected, result)\n\n\n@pytest.mark.arm_slow\n@pytest.mark.parametrize(\n    \"op, args, targop\",\n    [\n        (\"cumprod\", (), lambda x: x.cumprod()),\n        (\"cumsum\", (), lambda x: x.cumsum()),\n        (\"shift\", (-1,), lambda x: x.shift(-1)),\n        (\"shift\", (1,), lambda x: x.shift()),\n    ],\n)\ndef test_cython_transform_frame(op, args, targop):\n    s = Series(np.random.randn(1000))\n    s_missing = s.copy()\n    s_missing.iloc[2:10] = np.nan\n    labels = np.random.randint(0, 50, size=1000).astype(float)\n    strings = list(\"qwertyuiopasdfghjklz\")\n    strings_missing = strings[:]\n    strings_missing[5] = np.nan\n    df = DataFrame(\n        {\n            \"float\": s,\n            \"float_missing\": s_missing,\n            \"int\": [1, 1, 1, 1, 2] * 200,\n            \"datetime\": pd.date_range(\"1990-1-1\", periods=1000),\n            \"timedelta\": pd.timedelta_range(1, freq=\"s\", periods=1000),\n            \"string\": strings * 50,\n            \"string_missing\": strings_missing * 50,\n        },\n        columns=[\n            \"float\",\n            \"float_missing\",\n            \"int\",\n            \"datetime\",\n            \"timedelta\",\n            \"string\",\n            \"string_missing\",\n        ],\n    )\n    df[\"cat\"] = df[\"string\"].astype(\"category\")\n\n    df2 = df.copy()\n    df2.index = pd.MultiIndex.from_product([range(100), range(10)])\n\n    # DataFrame - Single and MultiIndex,\n    # group by values, index level, columns\n    for df in [df, df2]:\n        for gb_target in [\n            {\"by\": labels},\n            {\"level\": 0},\n            {\"by\": \"string\"},\n        ]:  # {\"by\": 'string_missing'}]:\n            # {\"by\": ['int','string']}]:\n\n            gb = df.groupby(**gb_target)\n            # allowlisted methods set the selection before applying\n            # bit a of hack to make sure the cythonized shift\n            # is equivalent to pre 0.17.1 behavior\n            if op == \"shift\":\n                gb._set_group_selection()\n\n            if op != \"shift\" and \"int\" not in gb_target:\n                # numeric apply fastpath promotes dtype so have\n                # to apply separately and concat\n                i = gb[[\"int\"]].apply(targop)\n                f = gb[[\"float\", \"float_missing\"]].apply(targop)\n                expected = pd.concat([f, i], axis=1)\n            else:\n                expected = gb.apply(targop)\n\n            expected = expected.sort_index(axis=1)\n            tm.assert_frame_equal(expected, gb.transform(op, *args).sort_index(axis=1))\n            tm.assert_frame_equal(expected, getattr(gb, op)(*args).sort_index(axis=1))\n            # individual columns\n            for c in df:\n                if c not in [\"float\", \"int\", \"float_missing\"] and op != \"shift\":\n                    msg = \"No numeric types to aggregate\"\n                    with pytest.raises(DataError, match=msg):\n                        gb[c].transform(op)\n                    with pytest.raises(DataError, match=msg):\n                        getattr(gb[c], op)()\n                else:\n                    expected = gb[c].apply(targop)\n                    expected.name = c\n                    tm.assert_series_equal(expected, gb[c].transform(op, *args))\n                    tm.assert_series_equal(expected, getattr(gb[c], op)(*args))\n\n\ndef test_transform_with_non_scalar_group():\n    # GH 10165\n    cols = pd.MultiIndex.from_tuples(\n        [\n            (\"syn\", \"A\"),\n            (\"mis\", \"A\"),\n            (\"non\", \"A\"),\n            (\"syn\", \"C\"),\n            (\"mis\", \"C\"),\n            (\"non\", \"C\"),\n            (\"syn\", \"T\"),\n            (\"mis\", \"T\"),\n            (\"non\", \"T\"),\n            (\"syn\", \"G\"),\n            (\"mis\", \"G\"),\n            (\"non\", \"G\"),\n        ]\n    )\n    df = DataFrame(\n        np.random.randint(1, 10, (4, 12)), columns=cols, index=[\"A\", \"C\", \"G\", \"T\"]\n    )\n\n    msg = \"transform must return a scalar value for each group.*\"\n    with pytest.raises(ValueError, match=msg):\n        df.groupby(axis=1, level=1).transform(lambda z: z.div(z.sum(axis=1), axis=0))\n\n\n@pytest.mark.parametrize(\n    \"cols,exp,comp_func\",\n    [\n        (\"a\", Series([1, 1, 1], name=\"a\"), tm.assert_series_equal),\n        (\n            [\"a\", \"c\"],\n            DataFrame({\"a\": [1, 1, 1], \"c\": [1, 1, 1]}),\n            tm.assert_frame_equal,\n        ),\n    ],\n)\n@pytest.mark.parametrize(\"agg_func\", [\"count\", \"rank\", \"size\"])\ndef test_transform_numeric_ret(cols, exp, comp_func, agg_func, request):\n    if agg_func == \"size\" and isinstance(cols, list):\n        # https://github.com/pytest-dev/pytest/issues/6300\n        # workaround to xfail fixture/param permutations\n        reason = \"'size' transformation not supported with NDFrameGroupy\"\n        request.node.add_marker(pytest.mark.xfail(reason=reason))\n\n    # GH 19200\n    df = DataFrame(\n        {\"a\": pd.date_range(\"2018-01-01\", periods=3), \"b\": range(3), \"c\": range(7, 10)}\n    )\n\n    result = df.groupby(\"b\")[cols].transform(agg_func)\n\n    if agg_func == \"rank\":\n        exp = exp.astype(\"float\")\n\n    comp_func(result, exp)\n\n\n@pytest.mark.parametrize(\"mix_groupings\", [True, False])\n@pytest.mark.parametrize(\"as_series\", [True, False])\n@pytest.mark.parametrize(\"val1,val2\", [(\"foo\", \"bar\"), (1, 2), (1.0, 2.0)])\n@pytest.mark.parametrize(\n    \"fill_method,limit,exp_vals\",\n    [\n        (\n            \"ffill\",\n            None,\n            [np.nan, np.nan, \"val1\", \"val1\", \"val1\", \"val2\", \"val2\", \"val2\"],\n        ),\n        (\"ffill\", 1, [np.nan, np.nan, \"val1\", \"val1\", np.nan, \"val2\", \"val2\", np.nan]),\n        (\n            \"bfill\",\n            None,\n            [\"val1\", \"val1\", \"val1\", \"val2\", \"val2\", \"val2\", np.nan, np.nan],\n        ),\n        (\"bfill\", 1, [np.nan, \"val1\", \"val1\", np.nan, \"val2\", \"val2\", np.nan, np.nan]),\n    ],\n)\ndef test_group_fill_methods(\n    mix_groupings, as_series, val1, val2, fill_method, limit, exp_vals\n):\n    vals = [np.nan, np.nan, val1, np.nan, np.nan, val2, np.nan, np.nan]\n    _exp_vals = list(exp_vals)\n    # Overwrite placeholder values\n    for index, exp_val in enumerate(_exp_vals):\n        if exp_val == \"val1\":\n            _exp_vals[index] = val1\n        elif exp_val == \"val2\":\n            _exp_vals[index] = val2\n\n    # Need to modify values and expectations depending on the\n    # Series / DataFrame that we ultimately want to generate\n    if mix_groupings:  # ['a', 'b', 'a, 'b', ...]\n        keys = [\"a\", \"b\"] * len(vals)\n\n        def interweave(list_obj):\n            temp = []\n            for x in list_obj:\n                temp.extend([x, x])\n\n            return temp\n\n        _exp_vals = interweave(_exp_vals)\n        vals = interweave(vals)\n    else:  # ['a', 'a', 'a', ... 'b', 'b', 'b']\n        keys = [\"a\"] * len(vals) + [\"b\"] * len(vals)\n        _exp_vals = _exp_vals * 2\n        vals = vals * 2\n\n    df = DataFrame({\"key\": keys, \"val\": vals})\n    if as_series:\n        result = getattr(df.groupby(\"key\")[\"val\"], fill_method)(limit=limit)\n        exp = Series(_exp_vals, name=\"val\")\n        tm.assert_series_equal(result, exp)\n    else:\n        result = getattr(df.groupby(\"key\"), fill_method)(limit=limit)\n        exp = DataFrame({\"val\": _exp_vals})\n        tm.assert_frame_equal(result, exp)\n\n\n@pytest.mark.parametrize(\"fill_method\", [\"ffill\", \"bfill\"])\ndef test_pad_stable_sorting(fill_method):\n    # GH 21207\n    x = [0] * 20\n    y = [np.nan] * 10 + [1] * 10\n\n    if fill_method == \"bfill\":\n        y = y[::-1]\n\n    df = DataFrame({\"x\": x, \"y\": y})\n    expected = df.drop(\"x\", 1)\n\n    result = getattr(df.groupby(\"x\"), fill_method)()\n\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"test_series\", [True, False])\n@pytest.mark.parametrize(\n    \"freq\",\n    [\n        None,\n        pytest.param(\n            \"D\",\n            marks=pytest.mark.xfail(\n                reason=\"GH#23918 before method uses freq in vectorized approach\"\n            ),\n        ),\n    ],\n)\n@pytest.mark.parametrize(\"periods\", [1, -1])\n@pytest.mark.parametrize(\"fill_method\", [\"ffill\", \"bfill\", None])\n@pytest.mark.parametrize(\"limit\", [None, 1])\ndef test_pct_change(test_series, freq, periods, fill_method, limit):\n    # GH  21200, 21621, 30463\n    vals = [3, np.nan, np.nan, np.nan, 1, 2, 4, 10, np.nan, 4]\n    keys = [\"a\", \"b\"]\n    key_v = np.repeat(keys, len(vals))\n    df = DataFrame({\"key\": key_v, \"vals\": vals * 2})\n\n    df_g = df\n    if fill_method is not None:\n        df_g = getattr(df.groupby(\"key\"), fill_method)(limit=limit)\n    grp = df_g.groupby(df.key)\n\n    expected = grp[\"vals\"].obj / grp[\"vals\"].shift(periods) - 1\n\n    if test_series:\n        result = df.groupby(\"key\")[\"vals\"].pct_change(\n            periods=periods, fill_method=fill_method, limit=limit, freq=freq\n        )\n        tm.assert_series_equal(result, expected)\n    else:\n        result = df.groupby(\"key\").pct_change(\n            periods=periods, fill_method=fill_method, limit=limit, freq=freq\n        )\n        tm.assert_frame_equal(result, expected.to_frame(\"vals\"))\n\n\n@pytest.mark.parametrize(\n    \"func, expected_status\",\n    [\n        (\"ffill\", [\"shrt\", \"shrt\", \"lng\", np.nan, \"shrt\", \"ntrl\", \"ntrl\"]),\n        (\"bfill\", [\"shrt\", \"lng\", \"lng\", \"shrt\", \"shrt\", \"ntrl\", np.nan]),\n    ],\n)\ndef test_ffill_bfill_non_unique_multilevel(func, expected_status):\n    # GH 19437\n    date = pd.to_datetime(\n        [\n            \"2018-01-01\",\n            \"2018-01-01\",\n            \"2018-01-01\",\n            \"2018-01-01\",\n            \"2018-01-02\",\n            \"2018-01-01\",\n            \"2018-01-02\",\n        ]\n    )\n    symbol = [\"MSFT\", \"MSFT\", \"MSFT\", \"AAPL\", \"AAPL\", \"TSLA\", \"TSLA\"]\n    status = [\"shrt\", np.nan, \"lng\", np.nan, \"shrt\", \"ntrl\", np.nan]\n\n    df = DataFrame({\"date\": date, \"symbol\": symbol, \"status\": status})\n    df = df.set_index([\"date\", \"symbol\"])\n    result = getattr(df.groupby(\"symbol\")[\"status\"], func)()\n\n    index = MultiIndex.from_tuples(\n        tuples=list(zip(*[date, symbol])), names=[\"date\", \"symbol\"]\n    )\n    expected = Series(expected_status, index=index, name=\"status\")\n\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"func\", [np.any, np.all])\ndef test_any_all_np_func(func):\n    # GH 20653\n    df = DataFrame(\n        [[\"foo\", True], [np.nan, True], [\"foo\", True]], columns=[\"key\", \"val\"]\n    )\n\n    exp = Series([True, np.nan, True], name=\"val\")\n\n    res = df.groupby(\"key\")[\"val\"].transform(func)\n    tm.assert_series_equal(res, exp)\n\n\ndef test_groupby_transform_rename():\n    # https://github.com/pandas-dev/pandas/issues/23461\n    def demean_rename(x):\n        result = x - x.mean()\n\n        if isinstance(x, pd.Series):\n            return result\n\n        result = result.rename(columns={c: \"{c}_demeaned\" for c in result.columns})\n\n        return result\n\n    df = DataFrame({\"group\": list(\"ababa\"), \"value\": [1, 1, 1, 2, 2]})\n    expected = DataFrame({\"value\": [-1.0 / 3, -0.5, -1.0 / 3, 0.5, 2.0 / 3]})\n\n    result = df.groupby(\"group\").transform(demean_rename)\n    tm.assert_frame_equal(result, expected)\n    result_single = df.groupby(\"group\").value.transform(demean_rename)\n    tm.assert_series_equal(result_single, expected[\"value\"])\n\n\n@pytest.mark.parametrize(\"func\", [min, max, np.min, np.max, \"first\", \"last\"])\ndef test_groupby_transform_timezone_column(func):\n    # GH 24198\n    ts = pd.to_datetime(\"now\", utc=True).tz_convert(\"Asia/Singapore\")\n    result = DataFrame({\"end_time\": [ts], \"id\": [1]})\n    result[\"max_end_time\"] = result.groupby(\"id\").end_time.transform(func)\n    expected = DataFrame([[ts, 1, ts]], columns=[\"end_time\", \"id\", \"max_end_time\"])\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"func, values\",\n    [\n        (\"idxmin\", [\"1/1/2011\"] * 2 + [\"1/3/2011\"] * 7 + [\"1/10/2011\"]),\n        (\"idxmax\", [\"1/2/2011\"] * 2 + [\"1/9/2011\"] * 7 + [\"1/10/2011\"]),\n    ],\n)\ndef test_groupby_transform_with_datetimes(func, values):\n    # GH 15306\n    dates = pd.date_range(\"1/1/2011\", periods=10, freq=\"D\")\n\n    stocks = DataFrame({\"price\": np.arange(10.0)}, index=dates)\n    stocks[\"week_id\"] = dates.isocalendar().week\n\n    result = stocks.groupby(stocks[\"week_id\"])[\"price\"].transform(func)\n\n    expected = Series(data=pd.to_datetime(values), index=dates, name=\"price\")\n\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"func\", [\"cumsum\", \"cumprod\", \"cummin\", \"cummax\"])\ndef test_transform_absent_categories(func):\n    # GH 16771\n    # cython transforms with more groups than rows\n    x_vals = [1]\n    x_cats = range(2)\n    y = [1]\n    df = DataFrame({\"x\": Categorical(x_vals, x_cats), \"y\": y})\n    result = getattr(df.y.groupby(df.x), func)()\n    expected = df.y\n    tm.assert_series_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"func\", [\"ffill\", \"bfill\", \"shift\"])\n@pytest.mark.parametrize(\"key, val\", [(\"level\", 0), (\"by\", Series([0]))])\ndef test_ffill_not_in_axis(func, key, val):\n    # GH 21521\n    df = DataFrame([[np.nan]])\n    result = getattr(df.groupby(**{key: val}), func)()\n    expected = df\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_transform_invalid_name_raises():\n    # GH#27486\n    df = DataFrame({\"a\": [0, 1, 1, 2]})\n    g = df.groupby([\"a\", \"b\", \"b\", \"c\"])\n    with pytest.raises(ValueError, match=\"not a valid function name\"):\n        g.transform(\"some_arbitrary_name\")\n\n    # method exists on the object, but is not a valid transformation/agg\n    assert hasattr(g, \"aggregate\")  # make sure the method exists\n    with pytest.raises(ValueError, match=\"not a valid function name\"):\n        g.transform(\"aggregate\")\n\n    # Test SeriesGroupBy\n    g = df[\"a\"].groupby([\"a\", \"b\", \"b\", \"c\"])\n    with pytest.raises(ValueError, match=\"not a valid function name\"):\n        g.transform(\"some_arbitrary_name\")\n\n\n@pytest.mark.parametrize(\n    \"obj\",\n    [\n        DataFrame(\n            {\"a\": [0, 0, 0, 1, 1, 1], \"b\": range(6)},\n            index=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n        ),\n        Series([0, 0, 0, 1, 1, 1], index=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]),\n    ],\n)\ndef test_transform_agg_by_name(reduction_func, obj):\n    func = reduction_func\n    g = obj.groupby(np.repeat([0, 1], 3))\n\n    if func == \"ngroup\":  # GH#27468\n        pytest.xfail(\"TODO: g.transform('ngroup') doesn't work\")\n    if func == \"size\":  # GH#27469\n        pytest.xfail(\"TODO: g.transform('size') doesn't work\")\n    if func == \"corrwith\" and isinstance(obj, Series):  # GH#32293\n        pytest.xfail(\"TODO: implement SeriesGroupBy.corrwith\")\n\n    args = {\"nth\": [0], \"quantile\": [0.5], \"corrwith\": [obj]}.get(func, [])\n\n    result = g.transform(func, *args)\n\n    # this is the *definition* of a transformation\n    tm.assert_index_equal(result.index, obj.index)\n    if hasattr(obj, \"columns\"):\n        tm.assert_index_equal(result.columns, obj.columns)\n\n    # verify that values were broadcasted across each group\n    assert len(set(DataFrame(result).iloc[-3:, -1])) == 1\n\n\ndef test_transform_lambda_with_datetimetz():\n    # GH 27496\n    df = DataFrame(\n        {\n            \"time\": [\n                Timestamp(\"2010-07-15 03:14:45\"),\n                Timestamp(\"2010-11-19 18:47:06\"),\n            ],\n            \"timezone\": [\"Etc/GMT+4\", \"US/Eastern\"],\n        }\n    )\n    result = df.groupby([\"timezone\"])[\"time\"].transform(\n        lambda x: x.dt.tz_localize(x.name)\n    )\n    expected = Series(\n        [\n            Timestamp(\"2010-07-15 03:14:45\", tz=\"Etc/GMT+4\"),\n            Timestamp(\"2010-11-19 18:47:06\", tz=\"US/Eastern\"),\n        ],\n        name=\"time\",\n    )\n    tm.assert_series_equal(result, expected)\n\n\ndef test_transform_fastpath_raises():\n    # GH#29631 case where fastpath defined in groupby.generic _choose_path\n    #  raises, but slow_path does not\n\n    df = DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\n    gb = df.groupby(\"A\")\n\n    def func(grp):\n        # we want a function such that func(frame) fails but func.apply(frame)\n        #  works\n        if grp.ndim == 2:\n            # Ensure that fast_path fails\n            raise NotImplementedError(\"Don't cross the streams\")\n        return grp * 2\n\n    # Check that the fastpath raises, see _transform_general\n    obj = gb._obj_with_exclusions\n    gen = gb.grouper.get_iterator(obj, axis=gb.axis)\n    fast_path, slow_path = gb._define_paths(func)\n    _, group = next(gen)\n\n    with pytest.raises(NotImplementedError, match=\"Don't cross the streams\"):\n        fast_path(group)\n\n    result = gb.transform(func)\n\n    expected = DataFrame([2, -2, 2, 4], columns=[\"B\"])\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_transform_lambda_indexing():\n    # GH 7883\n    df = DataFrame(\n        {\n            \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"flux\", \"foo\", \"flux\"],\n            \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"six\", \"five\", \"three\"],\n            \"C\": range(8),\n            \"D\": range(8),\n            \"E\": range(8),\n        }\n    )\n    df = df.set_index([\"A\", \"B\"])\n    df = df.sort_index()\n    result = df.groupby(level=\"A\").transform(lambda x: x.iloc[-1])\n    expected = DataFrame(\n        {\n            \"C\": [3, 3, 7, 7, 4, 4, 4, 4],\n            \"D\": [3, 3, 7, 7, 4, 4, 4, 4],\n            \"E\": [3, 3, 7, 7, 4, 4, 4, 4],\n        },\n        index=MultiIndex.from_tuples(\n            [\n                (\"bar\", \"one\"),\n                (\"bar\", \"three\"),\n                (\"flux\", \"six\"),\n                (\"flux\", \"three\"),\n                (\"foo\", \"five\"),\n                (\"foo\", \"one\"),\n                (\"foo\", \"two\"),\n                (\"foo\", \"two\"),\n            ],\n            names=[\"A\", \"B\"],\n        ),\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_categorical_and_not_categorical_key(observed):\n    # Checks that groupby-transform, when grouping by both a categorical\n    # and a non-categorical key, doesn't try to expand the output to include\n    # non-observed categories but instead matches the input shape.\n    # GH 32494\n    df_with_categorical = DataFrame(\n        {\n            \"A\": Categorical([\"a\", \"b\", \"a\"], categories=[\"a\", \"b\", \"c\"]),\n            \"B\": [1, 2, 3],\n            \"C\": [\"a\", \"b\", \"a\"],\n        }\n    )\n    df_without_categorical = DataFrame(\n        {\"A\": [\"a\", \"b\", \"a\"], \"B\": [1, 2, 3], \"C\": [\"a\", \"b\", \"a\"]}\n    )\n\n    # DataFrame case\n    result = df_with_categorical.groupby([\"A\", \"C\"], observed=observed).transform(\"sum\")\n    expected = df_without_categorical.groupby([\"A\", \"C\"]).transform(\"sum\")\n    tm.assert_frame_equal(result, expected)\n    expected_explicit = DataFrame({\"B\": [4, 2, 4]})\n    tm.assert_frame_equal(result, expected_explicit)\n\n    # Series case\n    result = df_with_categorical.groupby([\"A\", \"C\"], observed=observed)[\"B\"].transform(\n        \"sum\"\n    )\n    expected = df_without_categorical.groupby([\"A\", \"C\"])[\"B\"].transform(\"sum\")\n    tm.assert_series_equal(result, expected)\n    expected_explicit = Series([4, 2, 4], name=\"B\")\n    tm.assert_series_equal(result, expected_explicit)\n"
    },
    {
      "filename": "pandas/tests/io/generate_legacy_storage_files.py",
      "content": "\"\"\"\nself-contained to write legacy storage pickle files\n\nTo use this script. Create an environment where you want\ngenerate pickles, say its for 0.20.3, with your pandas clone\nin ~/pandas\n\n. activate pandas_0.20.3\ncd ~/pandas/pandas\n\n$ python -m tests.io.generate_legacy_storage_files \\\n    tests/io/data/legacy_pickle/0.20.3/ pickle\n\nThis script generates a storage file for the current arch, system,\nand python version\n  pandas version: 0.20.3\n  output dir    : pandas/pandas/tests/io/data/legacy_pickle/0.20.3/\n  storage format: pickle\ncreated pickle file: 0.20.3_x86_64_darwin_3.5.2.pickle\n\nThe idea here is you are using the *current* version of the\ngenerate_legacy_storage_files with an *older* version of pandas to\ngenerate a pickle file. We will then check this file into a current\nbranch, and test using test_pickle.py. This will load the *older*\npickles and test versus the current data that is generated\n(with master). These are then compared.\n\nIf we have cases where we changed the signature (e.g. we renamed\noffset -> freq in Timestamp). Then we have to conditionally execute\nin the generate_legacy_storage_files.py to make it\nrun under the older AND the newer version.\n\n\"\"\"\n\nfrom datetime import timedelta\nfrom distutils.version import LooseVersion\nimport os\nimport pickle\nimport platform as pl\nimport sys\n\nimport numpy as np\n\nimport pandas\nfrom pandas import (\n    Categorical,\n    DataFrame,\n    Index,\n    MultiIndex,\n    NaT,\n    Period,\n    RangeIndex,\n    Series,\n    Timestamp,\n    bdate_range,\n    date_range,\n    period_range,\n    timedelta_range,\n)\n\nfrom pandas.tseries.offsets import (\n    FY5253,\n    BusinessDay,\n    BusinessHour,\n    CustomBusinessDay,\n    DateOffset,\n    Day,\n    Easter,\n    Hour,\n    LastWeekOfMonth,\n    Minute,\n    MonthBegin,\n    MonthEnd,\n    QuarterBegin,\n    QuarterEnd,\n    SemiMonthBegin,\n    SemiMonthEnd,\n    Week,\n    WeekOfMonth,\n    YearBegin,\n    YearEnd,\n)\n\ntry:\n    # TODO: remove try/except when 0.24.0 is the legacy version.\n    from pandas.arrays import SparseArray\nexcept ImportError:\n    from pandas.core.sparse.api import SparseArray\n\n\n_loose_version = LooseVersion(pandas.__version__)\n\n\ndef _create_sp_series():\n    nan = np.nan\n\n    # nan-based\n    arr = np.arange(15, dtype=np.float64)\n    arr[7:12] = nan\n    arr[-1:] = nan\n\n    bseries = Series(SparseArray(arr, kind=\"block\"))\n    bseries.name = \"bseries\"\n    return bseries\n\n\ndef _create_sp_tsseries():\n    nan = np.nan\n\n    # nan-based\n    arr = np.arange(15, dtype=np.float64)\n    arr[7:12] = nan\n    arr[-1:] = nan\n\n    date_index = bdate_range(\"1/1/2011\", periods=len(arr))\n    bseries = Series(SparseArray(arr, kind=\"block\"), index=date_index)\n    bseries.name = \"btsseries\"\n    return bseries\n\n\ndef _create_sp_frame():\n    nan = np.nan\n\n    data = {\n        \"A\": [nan, nan, nan, 0, 1, 2, 3, 4, 5, 6],\n        \"B\": [0, 1, 2, nan, nan, nan, 3, 4, 5, 6],\n        \"C\": np.arange(10).astype(np.int64),\n        \"D\": [0, 1, 2, 3, 4, 5, nan, nan, nan, nan],\n    }\n\n    dates = bdate_range(\"1/1/2011\", periods=10)\n    return DataFrame(data, index=dates).apply(SparseArray)\n\n\ndef create_data():\n    \"\"\" create the pickle data \"\"\"\n    data = {\n        \"A\": [0.0, 1.0, 2.0, 3.0, np.nan],\n        \"B\": [0, 1, 0, 1, 0],\n        \"C\": [\"foo1\", \"foo2\", \"foo3\", \"foo4\", \"foo5\"],\n        \"D\": date_range(\"1/1/2009\", periods=5),\n        \"E\": [0.0, 1, Timestamp(\"20100101\"), \"foo\", 2.0],\n    }\n\n    scalars = {\"timestamp\": Timestamp(\"20130101\"), \"period\": Period(\"2012\", \"M\")}\n\n    index = {\n        \"int\": Index(np.arange(10)),\n        \"date\": date_range(\"20130101\", periods=10),\n        \"period\": period_range(\"2013-01-01\", freq=\"M\", periods=10),\n        \"float\": Index(np.arange(10, dtype=np.float64)),\n        \"uint\": Index(np.arange(10, dtype=np.uint64)),\n        \"timedelta\": timedelta_range(\"00:00:00\", freq=\"30T\", periods=10),\n        \"range\": RangeIndex(10),\n    }\n\n    if _loose_version >= LooseVersion(\"0.21\"):\n        from pandas import interval_range\n\n        index[\"interval\"] = interval_range(0, periods=10)\n\n    mi = {\n        \"reg2\": MultiIndex.from_tuples(\n            tuple(\n                zip(\n                    *[\n                        [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n                        [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n                    ]\n                )\n            ),\n            names=[\"first\", \"second\"],\n        )\n    }\n\n    series = {\n        \"float\": Series(data[\"A\"]),\n        \"int\": Series(data[\"B\"]),\n        \"mixed\": Series(data[\"E\"]),\n        \"ts\": Series(\n            np.arange(10).astype(np.int64), index=date_range(\"20130101\", periods=10)\n        ),\n        \"mi\": Series(\n            np.arange(5).astype(np.float64),\n            index=MultiIndex.from_tuples(\n                tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])), names=[\"one\", \"two\"]\n            ),\n        ),\n        \"dup\": Series(np.arange(5).astype(np.float64), index=[\"A\", \"B\", \"C\", \"D\", \"A\"]),\n        \"cat\": Series(Categorical([\"foo\", \"bar\", \"baz\"])),\n        \"dt\": Series(date_range(\"20130101\", periods=5)),\n        \"dt_tz\": Series(date_range(\"20130101\", periods=5, tz=\"US/Eastern\")),\n        \"period\": Series([Period(\"2000Q1\")] * 5),\n    }\n\n    mixed_dup_df = DataFrame(data)\n    mixed_dup_df.columns = list(\"ABCDA\")\n    frame = {\n        \"float\": DataFrame({\"A\": series[\"float\"], \"B\": series[\"float\"] + 1}),\n        \"int\": DataFrame({\"A\": series[\"int\"], \"B\": series[\"int\"] + 1}),\n        \"mixed\": DataFrame({k: data[k] for k in [\"A\", \"B\", \"C\", \"D\"]}),\n        \"mi\": DataFrame(\n            {\"A\": np.arange(5).astype(np.float64), \"B\": np.arange(5).astype(np.int64)},\n            index=MultiIndex.from_tuples(\n                tuple(\n                    zip(\n                        *[\n                            [\"bar\", \"bar\", \"baz\", \"baz\", \"baz\"],\n                            [\"one\", \"two\", \"one\", \"two\", \"three\"],\n                        ]\n                    )\n                ),\n                names=[\"first\", \"second\"],\n            ),\n        ),\n        \"dup\": DataFrame(\n            np.arange(15).reshape(5, 3).astype(np.float64), columns=[\"A\", \"B\", \"A\"]\n        ),\n        \"cat_onecol\": DataFrame({\"A\": Categorical([\"foo\", \"bar\"])}),\n        \"cat_and_float\": DataFrame(\n            {\n                \"A\": Categorical([\"foo\", \"bar\", \"baz\"]),\n                \"B\": np.arange(3).astype(np.int64),\n            }\n        ),\n        \"mixed_dup\": mixed_dup_df,\n        \"dt_mixed_tzs\": DataFrame(\n            {\n                \"A\": Timestamp(\"20130102\", tz=\"US/Eastern\"),\n                \"B\": Timestamp(\"20130603\", tz=\"CET\"),\n            },\n            index=range(5),\n        ),\n        \"dt_mixed2_tzs\": DataFrame(\n            {\n                \"A\": Timestamp(\"20130102\", tz=\"US/Eastern\"),\n                \"B\": Timestamp(\"20130603\", tz=\"CET\"),\n                \"C\": Timestamp(\"20130603\", tz=\"UTC\"),\n            },\n            index=range(5),\n        ),\n    }\n\n    cat = {\n        \"int8\": Categorical(list(\"abcdefg\")),\n        \"int16\": Categorical(np.arange(1000)),\n        \"int32\": Categorical(np.arange(10000)),\n    }\n\n    timestamp = {\n        \"normal\": Timestamp(\"2011-01-01\"),\n        \"nat\": NaT,\n        \"tz\": Timestamp(\"2011-01-01\", tz=\"US/Eastern\"),\n        \"freq\": Timestamp(\"2011-01-01\", freq=\"D\"),\n        \"both\": Timestamp(\"2011-01-01\", tz=\"Asia/Tokyo\", freq=\"M\"),\n    }\n\n    off = {\n        \"DateOffset\": DateOffset(years=1),\n        \"DateOffset_h_ns\": DateOffset(hour=6, nanoseconds=5824),\n        \"BusinessDay\": BusinessDay(offset=timedelta(seconds=9)),\n        \"BusinessHour\": BusinessHour(normalize=True, n=6, end=\"15:14\"),\n        \"CustomBusinessDay\": CustomBusinessDay(weekmask=\"Mon Fri\"),\n        \"SemiMonthBegin\": SemiMonthBegin(day_of_month=9),\n        \"SemiMonthEnd\": SemiMonthEnd(day_of_month=24),\n        \"MonthBegin\": MonthBegin(1),\n        \"MonthEnd\": MonthEnd(1),\n        \"QuarterBegin\": QuarterBegin(1),\n        \"QuarterEnd\": QuarterEnd(1),\n        \"Day\": Day(1),\n        \"YearBegin\": YearBegin(1),\n        \"YearEnd\": YearEnd(1),\n        \"Week\": Week(1),\n        \"Week_Tues\": Week(2, normalize=False, weekday=1),\n        \"WeekOfMonth\": WeekOfMonth(week=3, weekday=4),\n        \"LastWeekOfMonth\": LastWeekOfMonth(n=1, weekday=3),\n        \"FY5253\": FY5253(n=2, weekday=6, startingMonth=7, variation=\"last\"),\n        \"Easter\": Easter(),\n        \"Hour\": Hour(1),\n        \"Minute\": Minute(1),\n    }\n\n    return {\n        \"series\": series,\n        \"frame\": frame,\n        \"index\": index,\n        \"scalars\": scalars,\n        \"mi\": mi,\n        \"sp_series\": {\"float\": _create_sp_series(), \"ts\": _create_sp_tsseries()},\n        \"sp_frame\": {\"float\": _create_sp_frame()},\n        \"cat\": cat,\n        \"timestamp\": timestamp,\n        \"offsets\": off,\n    }\n\n\ndef create_pickle_data():\n    data = create_data()\n\n    return data\n\n\ndef platform_name():\n    return \"_\".join(\n        [\n            str(pandas.__version__),\n            str(pl.machine()),\n            str(pl.system().lower()),\n            str(pl.python_version()),\n        ]\n    )\n\n\ndef write_legacy_pickles(output_dir):\n\n    version = pandas.__version__\n\n    print(\n        \"This script generates a storage file for the current arch, system, \"\n        \"and python version\"\n    )\n    print(f\"  pandas version: {version}\")\n    print(f\"  output dir    : {output_dir}\")\n    print(\"  storage format: pickle\")\n\n    pth = f\"{platform_name()}.pickle\"\n\n    fh = open(os.path.join(output_dir, pth), \"wb\")\n    pickle.dump(create_pickle_data(), fh, pickle.DEFAULT_PROTOCOL)\n    fh.close()\n\n    print(f\"created pickle file: {pth}\")\n\n\ndef write_legacy_file():\n    # force our cwd to be the first searched\n    sys.path.insert(0, \".\")\n\n    if not (3 <= len(sys.argv) <= 4):\n        exit(\n            \"Specify output directory and storage type: generate_legacy_\"\n            \"storage_files.py <output_dir> <storage_type> \"\n        )\n\n    output_dir = str(sys.argv[1])\n    storage_type = str(sys.argv[2])\n\n    if storage_type == \"pickle\":\n        write_legacy_pickles(output_dir=output_dir)\n    else:\n        exit(\"storage_type must be one of {'pickle'}\")\n\n\nif __name__ == \"__main__\":\n    write_legacy_file()\n"
    }
  ],
  "questions": [
    "@zanuttin hey, `./pandas/core/arrays/interval.py` is already up for merge in #38263.\r\nJust to avoid rework, maybe you could take something else if possible?",
    "Hello @jreback , I just submitted a PR for ./pandas/core/strings/accessor.py and it passed all checks! \r\nif you have a chance could you please check it?\r\n\r\n#38138"
  ],
  "golden_answers": [
    "Hello @jreback , I just submitted a PR for ./pandas/core/strings/accessor.py and it passed all checks! \r\nif you have a chance could you please check it?\r\n\r\n#38138",
    "Hi again @jreback, I checked the file ./pandas/core/dtypes/dtypes.py and it has no unnecessary dict calls\r\n\r\nalso added 5 more files that passed all checks:\r\n\r\npandas/core/arrays/base.py\r\npandas/core/computation/pytables.py\r\npandas/core/indexes/base.py\r\npandas/core/ops/methods.py\r\npandas/io/pytables.py"
  ],
  "questions_generated": [
    "What is the purpose of the C408 warning in the context of the pandas repository?",
    "How can one verify if a file contains C408 warnings in the pandas repository?",
    "What steps should a first-time contributor follow to resolve C408 warnings in the pandas repository?",
    "Why might the pandas repository prefer dictionary literals over the `dict()` function?",
    "In the provided code context from pandas/core/arrays/base.py, identify a specific instance where the C408 warning applies.",
    "What is the significance of the setup.cfg file in the context of resolving C408 warnings?",
    "How does fixing C408 warnings contribute to the overall quality of the pandas codebase?"
  ],
  "golden_answers_generated": [
    "The C408 warning indicates an unnecessary use of the `dict()` call when a dictionary literal `{}` could be used instead. This is a code style issue flagged by flake8, suggesting a more efficient and Pythonic way to initialize dictionaries. Addressing these warnings helps improve code readability and performance across the pandas codebase.",
    "To check if a file contains C408 warnings, you can use the command `flake8 [file_path]`. This will run the flake8 tool on the specified file and report any style violations, including C408 warnings for unnecessary `dict()` calls.",
    "A first-time contributor should locate the files with C408 warnings, as listed in the issue description. They should replace the unnecessary `dict()` calls with dictionary literals `{}`. After making these changes, they can run `flake8 [file_path]` to ensure the warnings have been resolved. Finally, they should commit their changes and submit a pull request for review.",
    "Dictionary literals `{}` are generally more efficient and faster than using the `dict()` function. They require fewer characters to write and are a more idiomatic way to define dictionaries in Python. By preferring literals, the pandas repository maintains cleaner and more performant code.",
    "In the code context of pandas/core/arrays/base.py, the `_extension_array_shared_docs` variable is defined as `_extension_array_shared_docs: Dict[str, str] = {}`. This line correctly uses a dictionary literal `{}` and does not trigger a C408 warning. However, if it were initialized using `dict()`, it would produce a C408 warning.",
    "The setup.cfg file in the pandas repository likely contains configuration settings for tools like flake8. Removing the ignore setting for C408 from this file ensures that flake8 will check for and report these warnings, prompting contributors to address unnecessary `dict()` calls throughout the codebase.",
    "Fixing C408 warnings by replacing `dict()` calls with dictionary literals enhances the readability, efficiency, and maintainability of the code. It aligns the codebase with Python's best practices, reduces the likelihood of introducing subtle bugs, and improves performance by using more efficient data structures."
  ]
}