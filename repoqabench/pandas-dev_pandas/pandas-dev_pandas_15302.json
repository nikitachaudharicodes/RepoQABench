{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "15302",
  "issue_description": "# BUG: read_csv with date_parser lock file open on failure\n\n#### Problem description\r\nWhen using the date_parser functionality of read_csv() if the file read fails then the file is left locked open. My use case is that I am trying to enforce a strict datetime format that must include the time zone offset. Another thread stated that the way to accomplish this is with date_parser. My issue is that I would like to move a file that fails being loaded to another directory, but I cannot because the failure in the date_parser keeps the file open until the python session is terminated.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime\r\n\r\ndef strict_parser(dates):\r\n    datetimes = [pd.Timestamp(datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S%z'), tz='UTC') for date in dates]\r\n    return pd.DatetimeIndex(datetimes)\r\n\r\nfilename = 'c:/temp/data.csv'\r\ndata = pd.read_csv(filename, parse_dates=['datetime'], index_col=['datetime'], date_parser=strict_parser)\r\n\r\n```\r\nThe file data.csv is:\r\n\r\ndatetime,data\r\n2010-05-05 09:30:00-0500,10\r\n2010-05-05 09:35:00-0500,20\r\n2010-05-05 09:40:00,30\r\n\r\n#### Output of ``pd.show_versions()``\r\npandas: 0.19.1\r\n\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 277372519,
      "user": "jreback",
      "body": "We had a very similar issue, can't seem to find this now. But on windows, this is fixed I *think* in 0.19.2, but maybe 0.20.0. (still in dev)\r\n\r\ncan you try?"
    },
    {
      "id": 277556985,
      "user": "rsheftel",
      "body": "I tried with 0.19.2 and the problem remains. When 0.20.0 becomes available on one of my systems I will give that a try."
    },
    {
      "id": 277696719,
      "user": "jreback",
      "body": "I thought this was solved, but was able to repro on windows.\r\n\r\nSo it seems that we are not closing on calling a ``date_parser`` function (we raise, but the top-level needs a try/finally around this to close the handle). We do this for other errors, this must have been missed.\r\n\r\n@rsheftel would you like to do a pull-request to fix?\r\n"
    },
    {
      "id": 277823123,
      "user": "rsheftel",
      "body": "I don't think I am enough of an expert in the pandas code to submit a change that fixes this. Do you want me to just create a blank pull-request? (Sorry I'm new to the GitHub / collaborative world and how exactly it works)"
    },
    {
      "id": 277824362,
      "user": "jreback",
      "body": "no this would be a regular pull-request that has tests and makes the change.\r\n\r\nhttp://pandas.pydata.org/pandas-docs/stable/contributing.html"
    },
    {
      "id": 279182769,
      "user": "rsheftel",
      "body": "If I gain expertise in the code base and feel I can confidently contribute I will. Thanks."
    },
    {
      "id": 282600697,
      "user": "cpcloud",
      "body": "I looked into this with the help of @faizanv and finally got to the bottom of what's happening in pandas, but I'm still trying to figure out what's happening in the C API.\r\n\r\nThe proximate cause of this issue is that when we're reading a standard CSV file from a user provided path, we open a file handle in C (using `fopen`) and do not close it if there's an exception.\r\n\r\nIf there's no exception (or we catch the exception) the file handle is closed by a call to `parser_free()` (which calls `del_file_source` -- this function contains a call to `fclose`) in `TextReader.__dealloc__`. `__dealloc__` is a special Cython method that more or less sits in the [`tp_dealloc`](https://docs.python.org/2/c-api/typeobj.html#c.PyTypeObject.tp_dealloc) slot of `PyTypeObject`s.\r\n\r\nIt's not clear to me why it gets called when we catch the exception versus when we don't."
    },
    {
      "id": 282601974,
      "user": "jreback",
      "body": "hmm, I think we wrapped most/all of the calls for the low-level ``.read`` in try/excepts to do exactly this (in fact you get ResourceWarnings in py3 if you *don't* close things).\r\n\r\nAll that said @cpcloud something fishy is going on. maybe @gfyoung has an idea."
    },
    {
      "id": 282602026,
      "user": "cpcloud",
      "body": "@jreback The issue is that Python knows nothing about any calls to `fopen` or `fclose`."
    },
    {
      "id": 282602225,
      "user": "cpcloud",
      "body": "We do something like this:\r\n\r\n```python\r\ntry:\r\n    rows = parser.read()\r\nfinally:\r\n    parser.close()\r\n```\r\nbut we don't track the file handle of files opened by the `parser_t*` related code so we only close it when `__dealloc__` is called. Since `__dealloc__` isn't called when there's an exception, the handle never closes."
    },
    {
      "id": 282603627,
      "user": "jreback",
      "body": "you could try explicitly closing IN the actual parser when the .close method is called (ios do cleanup), but would prob have to set a flag so that you don't do it again in dealloc"
    },
    {
      "id": 282603983,
      "user": "cpcloud",
      "body": ":) Funny you mention that. That is my current working solution"
    },
    {
      "id": 917682983,
      "user": "twoertwein",
      "body": "I'm quite sure this should work with 1.3.2, but I'm not sure whether the following is a sufficient test for this:\r\n\r\nPYTHONWARNINGS=\"default\" python test.py\r\n\r\ntest.py:\r\n```py\r\nfrom io import StringIO\r\n\r\nimport pandas as pd\r\n\r\n\r\ndef strict_parser(dates):\r\n    assert False\r\n\r\n\r\nwith StringIO(\"a,b,c,datetime\") as file:\r\n    pd.read_csv(\r\n        file,\r\n        parse_dates=[\"datetime\"],\r\n        index_col=[\"datetime\"],\r\n        date_parser=strict_parser,\r\n    )\r\n```\r\nThis should print a `ResourceWarning` (even on unix) if we do not close the file handle."
    },
    {
      "id": 1075016408,
      "user": "roberthdevries",
      "body": "Cannot reproduce anymore with v1.5.0.dev0 on Linux.\r\nI modified the test script on line 6 slightly to adapt to the current pandas version and added a manual test to determine open file descriptors.\r\nAttached the test script.\r\n```\r\nimport pandas as pd\r\nimport datetime\r\nimport psutil\r\n\r\ndef strict_parser(dates):\r\n    datetimes = [pd.to_datetime(datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S%z'), utc=True) for date in dates]\r\n    return pd.DatetimeIndex(datetimes)\r\n\r\nfilename = 'bug-15302.csv'\r\n\r\nproc = psutil.Process()\r\nprint(\"open files\", proc.open_files())\r\n\r\ntry:\r\n    data = pd.read_csv(filename, parse_dates=['datetime'], index_col=['datetime'], date_parser=strict_parser)\r\n    print(data)\r\nexcept Exception as e:\r\n    print(\"------------------------------------------------------------------\")\r\n    print(e)\r\n    print(\"------------------------------------------------------------------\")\r\n\r\nprint(\"==============================================\")\r\nprint(\"open files\", proc.open_files())\r\n```"
    },
    {
      "id": 1667654893,
      "user": "LeilaShahmoradi",
      "body": "take"
    },
    {
      "id": 2041767985,
      "user": "jasonmokk",
      "body": "take"
    },
    {
      "id": 2043382223,
      "user": "aabhinavg",
      "body": "**Issue Description:**\r\n\r\nHere what I observed is that when we are using the `date_parser` functionality of `read_csv()`, if the file read fails due to timezone which has a missing offset in the datetime strings, the file remains locked open, preventing it from being moved to another directory. This occurs because the failure in the `date_parser` keeps the file open until the Python session is terminated.\r\n\r\n**Proposed Solution:**\r\n\r\nTo handle this issue gracefully, we can modify the `strict_parser` function to attempt parsing with timezone offset and, if that fails, try parsing without timezone offset. This approach ensures that files with datetime strings lacking timezone offset are parsed correctly.\r\n\r\n```python\r\ndef strict_parser(dates):\r\n    try:\r\n        # Attempt to parse with timezone offset\r\n        datetimes = [pd.Timestamp(datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S%z'), tz='UTC') for date in dates]\r\n        return pd.DatetimeIndex(datetimes)\r\n    except ValueError:\r\n        # If parsing with timezone offset fails, try parsing without timezone offset\r\n        datetimes = [pd.Timestamp(datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S'), tz='UTC') for date in dates]\r\n        return pd.DatetimeIndex(datetimes)\r\n"
    },
    {
      "id": 2099415776,
      "user": "mroeschke",
      "body": "`date_parser`  will be removed in pandas 3.0 so closing as a won't fix https://github.com/pandas-dev/pandas/pull/51019"
    }
  ],
  "text_context": "# BUG: read_csv with date_parser lock file open on failure\n\n#### Problem description\r\nWhen using the date_parser functionality of read_csv() if the file read fails then the file is left locked open. My use case is that I am trying to enforce a strict datetime format that must include the time zone offset. Another thread stated that the way to accomplish this is with date_parser. My issue is that I would like to move a file that fails being loaded to another directory, but I cannot because the failure in the date_parser keeps the file open until the python session is terminated.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime\r\n\r\ndef strict_parser(dates):\r\n    datetimes = [pd.Timestamp(datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S%z'), tz='UTC') for date in dates]\r\n    return pd.DatetimeIndex(datetimes)\r\n\r\nfilename = 'c:/temp/data.csv'\r\ndata = pd.read_csv(filename, parse_dates=['datetime'], index_col=['datetime'], date_parser=strict_parser)\r\n\r\n```\r\nThe file data.csv is:\r\n\r\ndatetime,data\r\n2010-05-05 09:30:00-0500,10\r\n2010-05-05 09:35:00-0500,20\r\n2010-05-05 09:40:00,30\r\n\r\n#### Output of ``pd.show_versions()``\r\npandas: 0.19.1\r\n\r\n</details>\r\n\n\nWe had a very similar issue, can't seem to find this now. But on windows, this is fixed I *think* in 0.19.2, but maybe 0.20.0. (still in dev)\r\n\r\ncan you try?\n\nI tried with 0.19.2 and the problem remains. When 0.20.0 becomes available on one of my systems I will give that a try.\n\nI thought this was solved, but was able to repro on windows.\r\n\r\nSo it seems that we are not closing on calling a ``date_parser`` function (we raise, but the top-level needs a try/finally around this to close the handle). We do this for other errors, this must have been missed.\r\n\r\n@rsheftel would you like to do a pull-request to fix?\r\n\n\nI don't think I am enough of an expert in the pandas code to submit a change that fixes this. Do you want me to just create a blank pull-request? (Sorry I'm new to the GitHub / collaborative world and how exactly it works)\n\nno this would be a regular pull-request that has tests and makes the change.\r\n\r\nhttp://pandas.pydata.org/pandas-docs/stable/contributing.html\n\nIf I gain expertise in the code base and feel I can confidently contribute I will. Thanks.\n\nI looked into this with the help of @faizanv and finally got to the bottom of what's happening in pandas, but I'm still trying to figure out what's happening in the C API.\r\n\r\nThe proximate cause of this issue is that when we're reading a standard CSV file from a user provided path, we open a file handle in C (using `fopen`) and do not close it if there's an exception.\r\n\r\nIf there's no exception (or we catch the exception) the file handle is closed by a call to `parser_free()` (which calls `del_file_source` -- this function contains a call to `fclose`) in `TextReader.__dealloc__`. `__dealloc__` is a special Cython method that more or less sits in the [`tp_dealloc`](https://docs.python.org/2/c-api/typeobj.html#c.PyTypeObject.tp_dealloc) slot of `PyTypeObject`s.\r\n\r\nIt's not clear to me why it gets called when we catch the exception versus when we don't.\n\nhmm, I think we wrapped most/all of the calls for the low-level ``.read`` in try/excepts to do exactly this (in fact you get ResourceWarnings in py3 if you *don't* close things).\r\n\r\nAll that said @cpcloud something fishy is going on. maybe @gfyoung has an idea.\n\n@jreback The issue is that Python knows nothing about any calls to `fopen` or `fclose`.\n\nWe do something like this:\r\n\r\n```python\r\ntry:\r\n    rows = parser.read()\r\nfinally:\r\n    parser.close()\r\n```\r\nbut we don't track the file handle of files opened by the `parser_t*` related code so we only close it when `__dealloc__` is called. Since `__dealloc__` isn't called when there's an exception, the handle never closes.\n\nyou could try explicitly closing IN the actual parser when the .close method is called (ios do cleanup), but would prob have to set a flag so that you don't do it again in dealloc\n\n:) Funny you mention that. That is my current working solution\n\nI'm quite sure this should work with 1.3.2, but I'm not sure whether the following is a sufficient test for this:\r\n\r\nPYTHONWARNINGS=\"default\" python test.py\r\n\r\ntest.py:\r\n```py\r\nfrom io import StringIO\r\n\r\nimport pandas as pd\r\n\r\n\r\ndef strict_parser(dates):\r\n    assert False\r\n\r\n\r\nwith StringIO(\"a,b,c,datetime\") as file:\r\n    pd.read_csv(\r\n        file,\r\n        parse_dates=[\"datetime\"],\r\n        index_col=[\"datetime\"],\r\n        date_parser=strict_parser,\r\n    )\r\n```\r\nThis should print a `ResourceWarning` (even on unix) if we do not close the file handle.\n\nCannot reproduce anymore with v1.5.0.dev0 on Linux.\r\nI modified the test script on line 6 slightly to adapt to the current pandas version and added a manual test to determine open file descriptors.\r\nAttached the test script.\r\n```\r\nimport pandas as pd\r\nimport datetime\r\nimport psutil\r\n\r\ndef strict_parser(dates):\r\n    datetimes = [pd.to_datetime(datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S%z'), utc=True) for date in dates]\r\n    return pd.DatetimeIndex(datetimes)\r\n\r\nfilename = 'bug-15302.csv'\r\n\r\nproc = psutil.Process()\r\nprint(\"open files\", proc.open_files())\r\n\r\ntry:\r\n    data = pd.read_csv(filename, parse_dates=['datetime'], index_col=['datetime'], date_parser=strict_parser)\r\n    print(data)\r\nexcept Exception as e:\r\n    print(\"------------------------------------------------------------------\")\r\n    print(e)\r\n    print(\"------------------------------------------------------------------\")\r\n\r\nprint(\"==============================================\")\r\nprint(\"open files\", proc.open_files())\r\n```\n\ntake\n\ntake\n\n**Issue Description:**\r\n\r\nHere what I observed is that when we are using the `date_parser` functionality of `read_csv()`, if the file read fails due to timezone which has a missing offset in the datetime strings, the file remains locked open, preventing it from being moved to another directory. This occurs because the failure in the `date_parser` keeps the file open until the Python session is terminated.\r\n\r\n**Proposed Solution:**\r\n\r\nTo handle this issue gracefully, we can modify the `strict_parser` function to attempt parsing with timezone offset and, if that fails, try parsing without timezone offset. This approach ensures that files with datetime strings lacking timezone offset are parsed correctly.\r\n\r\n```python\r\ndef strict_parser(dates):\r\n    try:\r\n        # Attempt to parse with timezone offset\r\n        datetimes = [pd.Timestamp(datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S%z'), tz='UTC') for date in dates]\r\n        return pd.DatetimeIndex(datetimes)\r\n    except ValueError:\r\n        # If parsing with timezone offset fails, try parsing without timezone offset\r\n        datetimes = [pd.Timestamp(datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S'), tz='UTC') for date in dates]\r\n        return pd.DatetimeIndex(datetimes)\r\n\n\n`date_parser`  will be removed in pandas 3.0 so closing as a won't fix https://github.com/pandas-dev/pandas/pull/51019",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/51019",
  "code_context": [
    {
      "filename": "pandas/io/excel/_base.py",
      "content": "from __future__ import annotations\n\nimport abc\nimport datetime\nfrom functools import partial\nfrom io import BytesIO\nimport os\nfrom textwrap import fill\nfrom types import TracebackType\nfrom typing import (\n    IO,\n    Any,\n    Callable,\n    Hashable,\n    Iterable,\n    List,\n    Literal,\n    Mapping,\n    Sequence,\n    Union,\n    cast,\n    overload,\n)\nimport zipfile\n\nfrom pandas._config import (\n    config,\n    using_nullable_dtypes,\n)\n\nfrom pandas._libs import lib\nfrom pandas._libs.parsers import STR_NA_VALUES\nfrom pandas._typing import (\n    DtypeArg,\n    FilePath,\n    IntStrT,\n    ReadBuffer,\n    StorageOptions,\n    WriteExcelBuffer,\n)\nfrom pandas.compat._optional import (\n    get_version,\n    import_optional_dependency,\n)\nfrom pandas.errors import EmptyDataError\nfrom pandas.util._decorators import (\n    Appender,\n    doc,\n)\n\nfrom pandas.core.dtypes.common import (\n    is_bool,\n    is_float,\n    is_integer,\n    is_list_like,\n)\n\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.shared_docs import _shared_docs\nfrom pandas.util.version import Version\n\nfrom pandas.io.common import (\n    IOHandles,\n    get_handle,\n    stringify_path,\n    validate_header_arg,\n)\nfrom pandas.io.excel._util import (\n    fill_mi_header,\n    get_default_engine,\n    get_writer,\n    maybe_convert_usecols,\n    pop_header_name,\n)\nfrom pandas.io.parsers import TextParser\nfrom pandas.io.parsers.readers import validate_integer\n\n_read_excel_doc = (\n    \"\"\"\nRead an Excel file into a pandas DataFrame.\n\nSupports `xls`, `xlsx`, `xlsm`, `xlsb`, `odf`, `ods` and `odt` file extensions\nread from a local filesystem or URL. Supports an option to read\na single sheet or a list of sheets.\n\nParameters\n----------\nio : str, bytes, ExcelFile, xlrd.Book, path object, or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n    expected. A local file could be: ``file://localhost/path/to/table.xlsx``.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method,\n    such as a file handle (e.g. via builtin ``open`` function)\n    or ``StringIO``.\nsheet_name : str, int, list, or None, default 0\n    Strings are used for sheet names. Integers are used in zero-indexed\n    sheet positions (chart sheets do not count as a sheet position).\n    Lists of strings/integers are used to request multiple sheets.\n    Specify None to get all worksheets.\n\n    Available cases:\n\n    * Defaults to ``0``: 1st sheet as a `DataFrame`\n    * ``1``: 2nd sheet as a `DataFrame`\n    * ``\"Sheet1\"``: Load sheet with name \"Sheet1\"\n    * ``[0, 1, \"Sheet5\"]``: Load first, second and sheet named \"Sheet5\"\n      as a dict of `DataFrame`\n    * None: All worksheets.\n\nheader : int, list of int, default 0\n    Row (0-indexed) to use for the column labels of the parsed\n    DataFrame. If a list of integers is passed those row positions will\n    be combined into a ``MultiIndex``. Use None if there is no header.\nnames : array-like, default None\n    List of column names to use. If file contains no header row,\n    then you should explicitly pass header=None.\nindex_col : int, list of int, default None\n    Column (0-indexed) to use as the row labels of the DataFrame.\n    Pass None if there is no such column.  If a list is passed,\n    those columns will be combined into a ``MultiIndex``.  If a\n    subset of data is selected with ``usecols``, index_col\n    is based on the subset.\n\n    Missing values will be forward filled to allow roundtripping with\n    ``to_excel`` for ``merged_cells=True``. To avoid forward filling the\n    missing values use ``set_index`` after reading the data instead of\n    ``index_col``.\nusecols : str, list-like, or callable, default None\n    * If None, then parse all columns.\n    * If str, then indicates comma separated list of Excel column letters\n      and column ranges (e.g. \"A:E\" or \"A,C,E:F\"). Ranges are inclusive of\n      both sides.\n    * If list of int, then indicates list of column numbers to be parsed\n      (0-indexed).\n    * If list of string, then indicates list of column names to be parsed.\n    * If callable, then evaluate each column name against it and parse the\n      column if the callable returns ``True``.\n\n    Returns a subset of the columns according to behavior above.\nsqueeze : bool, default False\n    If the parsed data only contains one column then return a Series.\n\n    .. deprecated:: 1.4.0\n       Append ``.squeeze(\"columns\")`` to the call to ``read_excel`` to squeeze\n       the data.\ndtype : Type name or dict of column -> type, default None\n    Data type for data or columns. E.g. {{'a': np.float64, 'b': np.int32}}\n    Use `object` to preserve data as stored in Excel and not interpret dtype.\n    If converters are specified, they will be applied INSTEAD\n    of dtype conversion.\nengine : str, default None\n    If io is not a buffer or path, this must be set to identify io.\n    Supported engines: \"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\".\n    Engine compatibility :\n\n    - \"xlrd\" supports old-style Excel files (.xls).\n    - \"openpyxl\" supports newer Excel file formats.\n    - \"odf\" supports OpenDocument file formats (.odf, .ods, .odt).\n    - \"pyxlsb\" supports Binary Excel files.\n\n    .. versionchanged:: 1.2.0\n        The engine `xlrd <https://xlrd.readthedocs.io/en/latest/>`_\n        now only supports old-style ``.xls`` files.\n        When ``engine=None``, the following logic will be\n        used to determine the engine:\n\n       - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n         then `odf <https://pypi.org/project/odfpy/>`_ will be used.\n       - Otherwise if ``path_or_buffer`` is an xls format,\n         ``xlrd`` will be used.\n       - Otherwise if ``path_or_buffer`` is in xlsb format,\n         ``pyxlsb`` will be used.\n\n         .. versionadded:: 1.3.0\n       - Otherwise ``openpyxl`` will be used.\n\n         .. versionchanged:: 1.3.0\n\nconverters : dict, default None\n    Dict of functions for converting values in certain columns. Keys can\n    either be integers or column labels, values are functions that take one\n    input argument, the Excel cell content, and return the transformed\n    content.\ntrue_values : list, default None\n    Values to consider as True.\nfalse_values : list, default None\n    Values to consider as False.\nskiprows : list-like, int, or callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (int) at the\n    start of the file. If callable, the callable function will be evaluated\n    against the row indices, returning True if the row should be skipped and\n    False otherwise. An example of a valid callable argument would be ``lambda\n    x: x in [0, 2]``.\nnrows : int, default None\n    Number of rows to parse.\nna_values : scalar, str, list-like, or dict, default None\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values. By default the following values are interpreted\n    as NaN: '\"\"\"\n    + fill(\"', '\".join(sorted(STR_NA_VALUES)), 70, subsequent_indent=\"    \")\n    + \"\"\"'.\nkeep_default_na : bool, default True\n    Whether or not to include the default NaN values when parsing the data.\n    Depending on whether `na_values` is passed in, the behavior is as follows:\n\n    * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n      is appended to the default NaN values used for parsing.\n    * If `keep_default_na` is True, and `na_values` are not specified, only\n      the default NaN values are used for parsing.\n    * If `keep_default_na` is False, and `na_values` are specified, only\n      the NaN values specified `na_values` are used for parsing.\n    * If `keep_default_na` is False, and `na_values` are not specified, no\n      strings will be parsed as NaN.\n\n    Note that if `na_filter` is passed in as False, the `keep_default_na` and\n    `na_values` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of na_values). In\n    data without any NAs, passing na_filter=False can improve the performance\n    of reading a large file.\nverbose : bool, default False\n    Indicate number of NA values placed in non-numeric columns.\nparse_dates : bool, list-like, or dict, default False\n    The behavior is as follows:\n\n    * bool. If True -> try parsing the index.\n    * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n      a single date column.\n    * dict, e.g. {{'foo' : [1, 3]}} -> parse columns 1, 3 as date and call\n      result 'foo'\n\n    If a column or index contains an unparsable date, the entire column or\n    index will be returned unaltered as an object data type. If you don`t want to\n    parse some cells as date just change their type in Excel to \"Text\".\n    For non-standard datetime parsing, use ``pd.to_datetime`` after ``pd.read_excel``.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ndate_parser : function, optional\n    Function to use for converting a sequence of string columns to an array of\n    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. Pandas will try to call `date_parser` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by `parse_dates` into a single array\n    and pass that; and 3) call `date_parser` once for each row using one or\n    more strings (corresponding to the columns defined by `parse_dates`) as\n    arguments.\n\n  .. deprecated:: 2.0.0\n   Use ``date_format`` instead, or read in as ``object`` and then apply\n   :func:`to_datetime` as-needed.\ndate_format : str, default ``None``\n   If used in conjunction with ``parse_dates``, will parse dates according to this\n   format. For anything more complex (e.g. different formats for different columns),\n   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n\n    .. versionadded:: 2.0.0\nthousands : str, default None\n    Thousands separator for parsing string columns to numeric.  Note that\n    this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.\ndecimal : str, default '.'\n    Character to recognize as decimal point for parsing string columns to numeric.\n    Note that this parameter is only necessary for columns stored as TEXT in Excel,\n    any numeric columns will automatically be parsed, regardless of display\n    format.(e.g. use ',' for European data).\n\n    .. versionadded:: 1.4.0\n\ncomment : str, default None\n    Comments out remainder of line. Pass a character or characters to this\n    argument to indicate comments in the input file. Any data between the\n    comment string and the end of the current line is ignored.\nskipfooter : int, default 0\n    Rows at the end to skip (0-indexed).\n{storage_options}\n\n    .. versionadded:: 1.2.0\n\nuse_nullable_dtypes : bool, default False\n    Whether or not to use nullable dtypes as default when reading data. If\n    set to True, nullable dtypes are used for all dtypes that have a nullable\n    implementation, even if no nulls are present. Dtype takes precedence if given.\n\n    .. note::\n\n        The nullable dtype implementation can be configured by calling\n        ``pd.set_option(\"mode.dtype_backend\", \"pandas\")`` to use\n        numpy-backed nullable dtypes or\n        ``pd.set_option(\"mode.dtype_backend\", \"pyarrow\")`` to use\n        pyarrow-backed nullable dtypes (using ``pd.ArrowDtype``).\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or dict of DataFrames\n    DataFrame from the passed in Excel file. See notes in sheet_name\n    argument for more information on when a dict of DataFrames is returned.\n\nSee Also\n--------\nDataFrame.to_excel : Write DataFrame to an Excel file.\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\nThe file can be read using the file name as string or an open file object:\n\n>>> pd.read_excel('tmp.xlsx', index_col=0)  # doctest: +SKIP\n       Name  Value\n0   string1      1\n1   string2      2\n2  #Comment      3\n\n>>> pd.read_excel(open('tmp.xlsx', 'rb'),\n...               sheet_name='Sheet3')  # doctest: +SKIP\n   Unnamed: 0      Name  Value\n0           0   string1      1\n1           1   string2      2\n2           2  #Comment      3\n\nIndex and header can be specified via the `index_col` and `header` arguments\n\n>>> pd.read_excel('tmp.xlsx', index_col=None, header=None)  # doctest: +SKIP\n     0         1      2\n0  NaN      Name  Value\n1  0.0   string1      1\n2  1.0   string2      2\n3  2.0  #Comment      3\n\nColumn types are inferred but can be explicitly specified\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               dtype={{'Name': str, 'Value': float}})  # doctest: +SKIP\n       Name  Value\n0   string1    1.0\n1   string2    2.0\n2  #Comment    3.0\n\nTrue, False, and NA values, and thousands separators have defaults,\nbut can be explicitly specified, too. Supply the values you would like\nas strings or lists of strings!\n\n>>> pd.read_excel('tmp.xlsx', index_col=0,\n...               na_values=['string1', 'string2'])  # doctest: +SKIP\n       Name  Value\n0       NaN      1\n1       NaN      2\n2  #Comment      3\n\nComment lines in the excel input file can be skipped using the `comment` kwarg\n\n>>> pd.read_excel('tmp.xlsx', index_col=0, comment='#')  # doctest: +SKIP\n      Name  Value\n0  string1    1.0\n1  string2    2.0\n2     None    NaN\n\"\"\"\n)\n\n\n@overload\ndef read_excel(\n    io,\n    # sheet name is str or int -> DataFrame\n    sheet_name: str | int = ...,\n    *,\n    header: int | Sequence[int] | None = ...,\n    names: list[str] | None = ...,\n    index_col: int | Sequence[int] | None = ...,\n    usecols: int\n    | str\n    | Sequence[int]\n    | Sequence[str]\n    | Callable[[str], bool]\n    | None = ...,\n    squeeze: bool | None = ...,\n    dtype: DtypeArg | None = ...,\n    engine: Literal[\"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\"] | None = ...,\n    converters: dict[str, Callable] | dict[int, Callable] | None = ...,\n    true_values: Iterable[Hashable] | None = ...,\n    false_values: Iterable[Hashable] | None = ...,\n    skiprows: Sequence[int] | int | Callable[[int], object] | None = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    parse_dates: list | dict | bool = ...,\n    date_parser: Callable | lib.NoDefault = ...,\n    date_format: str | None = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    comment: str | None = ...,\n    skipfooter: int = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> DataFrame:\n    ...\n\n\n@overload\ndef read_excel(\n    io,\n    # sheet name is list or None -> dict[IntStrT, DataFrame]\n    sheet_name: list[IntStrT] | None,\n    *,\n    header: int | Sequence[int] | None = ...,\n    names: list[str] | None = ...,\n    index_col: int | Sequence[int] | None = ...,\n    usecols: int\n    | str\n    | Sequence[int]\n    | Sequence[str]\n    | Callable[[str], bool]\n    | None = ...,\n    squeeze: bool | None = ...,\n    dtype: DtypeArg | None = ...,\n    engine: Literal[\"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\"] | None = ...,\n    converters: dict[str, Callable] | dict[int, Callable] | None = ...,\n    true_values: Iterable[Hashable] | None = ...,\n    false_values: Iterable[Hashable] | None = ...,\n    skiprows: Sequence[int] | int | Callable[[int], object] | None = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    parse_dates: list | dict | bool = ...,\n    date_parser: Callable | lib.NoDefault = ...,\n    date_format: str | None = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    comment: str | None = ...,\n    skipfooter: int = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> dict[IntStrT, DataFrame]:\n    ...\n\n\n@doc(storage_options=_shared_docs[\"storage_options\"])\n@Appender(_read_excel_doc)\ndef read_excel(\n    io,\n    sheet_name: str | int | list[IntStrT] | None = 0,\n    *,\n    header: int | Sequence[int] | None = 0,\n    names: list[str] | None = None,\n    index_col: int | Sequence[int] | None = None,\n    usecols: int\n    | str\n    | Sequence[int]\n    | Sequence[str]\n    | Callable[[str], bool]\n    | None = None,\n    squeeze: bool | None = None,\n    dtype: DtypeArg | None = None,\n    engine: Literal[\"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\"] | None = None,\n    converters: dict[str, Callable] | dict[int, Callable] | None = None,\n    true_values: Iterable[Hashable] | None = None,\n    false_values: Iterable[Hashable] | None = None,\n    skiprows: Sequence[int] | int | Callable[[int], object] | None = None,\n    nrows: int | None = None,\n    na_values=None,\n    keep_default_na: bool = True,\n    na_filter: bool = True,\n    verbose: bool = False,\n    parse_dates: list | dict | bool = False,\n    date_parser: Callable | lib.NoDefault = lib.no_default,\n    date_format: str | None = None,\n    thousands: str | None = None,\n    decimal: str = \".\",\n    comment: str | None = None,\n    skipfooter: int = 0,\n    storage_options: StorageOptions = None,\n    use_nullable_dtypes: bool | lib.NoDefault = lib.no_default,\n) -> DataFrame | dict[IntStrT, DataFrame]:\n    should_close = False\n    if not isinstance(io, ExcelFile):\n        should_close = True\n        io = ExcelFile(io, storage_options=storage_options, engine=engine)\n    elif engine and engine != io.engine:\n        raise ValueError(\n            \"Engine should not be specified when passing \"\n            \"an ExcelFile - ExcelFile already has the engine set\"\n        )\n\n    use_nullable_dtypes = (\n        use_nullable_dtypes\n        if use_nullable_dtypes is not lib.no_default\n        else using_nullable_dtypes()\n    )\n\n    try:\n        data = io.parse(\n            sheet_name=sheet_name,\n            header=header,\n            names=names,\n            index_col=index_col,\n            usecols=usecols,\n            squeeze=squeeze,\n            dtype=dtype,\n            converters=converters,\n            true_values=true_values,\n            false_values=false_values,\n            skiprows=skiprows,\n            nrows=nrows,\n            na_values=na_values,\n            keep_default_na=keep_default_na,\n            na_filter=na_filter,\n            verbose=verbose,\n            parse_dates=parse_dates,\n            date_parser=date_parser,\n            date_format=date_format,\n            thousands=thousands,\n            decimal=decimal,\n            comment=comment,\n            skipfooter=skipfooter,\n            use_nullable_dtypes=use_nullable_dtypes,\n        )\n    finally:\n        # make sure to close opened file handles\n        if should_close:\n            io.close()\n    return data\n\n\nclass BaseExcelReader(metaclass=abc.ABCMeta):\n    def __init__(\n        self, filepath_or_buffer, storage_options: StorageOptions = None\n    ) -> None:\n        # First argument can also be bytes, so create a buffer\n        if isinstance(filepath_or_buffer, bytes):\n            filepath_or_buffer = BytesIO(filepath_or_buffer)\n\n        self.handles = IOHandles(\n            handle=filepath_or_buffer, compression={\"method\": None}\n        )\n        if not isinstance(filepath_or_buffer, (ExcelFile, self._workbook_class)):\n            self.handles = get_handle(\n                filepath_or_buffer, \"rb\", storage_options=storage_options, is_text=False\n            )\n\n        if isinstance(self.handles.handle, self._workbook_class):\n            self.book = self.handles.handle\n        elif hasattr(self.handles.handle, \"read\"):\n            # N.B. xlrd.Book has a read attribute too\n            self.handles.handle.seek(0)\n            try:\n                self.book = self.load_workbook(self.handles.handle)\n            except Exception:\n                self.close()\n                raise\n        else:\n            raise ValueError(\n                \"Must explicitly set engine if not passing in buffer or path for io.\"\n            )\n\n    @property\n    @abc.abstractmethod\n    def _workbook_class(self):\n        pass\n\n    @abc.abstractmethod\n    def load_workbook(self, filepath_or_buffer):\n        pass\n\n    def close(self) -> None:\n        if hasattr(self, \"book\"):\n            if hasattr(self.book, \"close\"):\n                # pyxlsb: opens a TemporaryFile\n                # openpyxl: https://stackoverflow.com/questions/31416842/\n                #     openpyxl-does-not-close-excel-workbook-in-read-only-mode\n                self.book.close()\n            elif hasattr(self.book, \"release_resources\"):\n                # xlrd\n                # https://github.com/python-excel/xlrd/blob/2.0.1/xlrd/book.py#L548\n                self.book.release_resources()\n        self.handles.close()\n\n    @property\n    @abc.abstractmethod\n    def sheet_names(self) -> list[str]:\n        pass\n\n    @abc.abstractmethod\n    def get_sheet_by_name(self, name: str):\n        pass\n\n    @abc.abstractmethod\n    def get_sheet_by_index(self, index: int):\n        pass\n\n    @abc.abstractmethod\n    def get_sheet_data(self, sheet, rows: int | None = None):\n        pass\n\n    def raise_if_bad_sheet_by_index(self, index: int) -> None:\n        n_sheets = len(self.sheet_names)\n        if index >= n_sheets:\n            raise ValueError(\n                f\"Worksheet index {index} is invalid, {n_sheets} worksheets found\"\n            )\n\n    def raise_if_bad_sheet_by_name(self, name: str) -> None:\n        if name not in self.sheet_names:\n            raise ValueError(f\"Worksheet named '{name}' not found\")\n\n    def _check_skiprows_func(\n        self,\n        skiprows: Callable,\n        rows_to_use: int,\n    ) -> int:\n        \"\"\"\n        Determine how many file rows are required to obtain `nrows` data\n        rows when `skiprows` is a function.\n\n        Parameters\n        ----------\n        skiprows : function\n            The function passed to read_excel by the user.\n        rows_to_use : int\n            The number of rows that will be needed for the header and\n            the data.\n\n        Returns\n        -------\n        int\n        \"\"\"\n        i = 0\n        rows_used_so_far = 0\n        while rows_used_so_far < rows_to_use:\n            if not skiprows(i):\n                rows_used_so_far += 1\n            i += 1\n        return i\n\n    def _calc_rows(\n        self,\n        header: int | Sequence[int] | None,\n        index_col: int | Sequence[int] | None,\n        skiprows: Sequence[int] | int | Callable[[int], object] | None,\n        nrows: int | None,\n    ) -> int | None:\n        \"\"\"\n        If nrows specified, find the number of rows needed from the\n        file, otherwise return None.\n\n\n        Parameters\n        ----------\n        header : int, list of int, or None\n            See read_excel docstring.\n        index_col : int, list of int, or None\n            See read_excel docstring.\n        skiprows : list-like, int, callable, or None\n            See read_excel docstring.\n        nrows : int or None\n            See read_excel docstring.\n\n        Returns\n        -------\n        int or None\n        \"\"\"\n        if nrows is None:\n            return None\n        if header is None:\n            header_rows = 1\n        elif is_integer(header):\n            header = cast(int, header)\n            header_rows = 1 + header\n        else:\n            header = cast(Sequence, header)\n            header_rows = 1 + header[-1]\n        # If there is a MultiIndex header and an index then there is also\n        # a row containing just the index name(s)\n        if is_list_like(header) and index_col is not None:\n            header = cast(Sequence, header)\n            if len(header) > 1:\n                header_rows += 1\n        if skiprows is None:\n            return header_rows + nrows\n        if is_integer(skiprows):\n            skiprows = cast(int, skiprows)\n            return header_rows + nrows + skiprows\n        if is_list_like(skiprows):\n\n            def f(skiprows: Sequence, x: int) -> bool:\n                return x in skiprows\n\n            skiprows = cast(Sequence, skiprows)\n            return self._check_skiprows_func(partial(f, skiprows), header_rows + nrows)\n        if callable(skiprows):\n            return self._check_skiprows_func(\n                skiprows,\n                header_rows + nrows,\n            )\n        # else unexpected skiprows type: read_excel will not optimize\n        # the number of rows read from file\n        return None\n\n    def parse(\n        self,\n        sheet_name: str | int | list[int] | list[str] | None = 0,\n        header: int | Sequence[int] | None = 0,\n        names=None,\n        index_col: int | Sequence[int] | None = None,\n        usecols=None,\n        squeeze: bool | None = None,\n        dtype: DtypeArg | None = None,\n        true_values: Iterable[Hashable] | None = None,\n        false_values: Iterable[Hashable] | None = None,\n        skiprows: Sequence[int] | int | Callable[[int], object] | None = None,\n        nrows: int | None = None,\n        na_values=None,\n        verbose: bool = False,\n        parse_dates: list | dict | bool = False,\n        date_parser: Callable | lib.NoDefault = lib.no_default,\n        date_format: str | None = None,\n        thousands: str | None = None,\n        decimal: str = \".\",\n        comment: str | None = None,\n        skipfooter: int = 0,\n        use_nullable_dtypes: bool = False,\n        **kwds,\n    ):\n        validate_header_arg(header)\n        validate_integer(\"nrows\", nrows)\n\n        ret_dict = False\n\n        # Keep sheetname to maintain backwards compatibility.\n        sheets: list[int] | list[str]\n        if isinstance(sheet_name, list):\n            sheets = sheet_name\n            ret_dict = True\n        elif sheet_name is None:\n            sheets = self.sheet_names\n            ret_dict = True\n        elif isinstance(sheet_name, str):\n            sheets = [sheet_name]\n        else:\n            sheets = [sheet_name]\n\n        # handle same-type duplicates.\n        sheets = cast(Union[List[int], List[str]], list(dict.fromkeys(sheets).keys()))\n\n        output = {}\n\n        last_sheetname = None\n        for asheetname in sheets:\n            last_sheetname = asheetname\n            if verbose:\n                print(f\"Reading sheet {asheetname}\")\n\n            if isinstance(asheetname, str):\n                sheet = self.get_sheet_by_name(asheetname)\n            else:  # assume an integer if not a string\n                sheet = self.get_sheet_by_index(asheetname)\n\n            file_rows_needed = self._calc_rows(header, index_col, skiprows, nrows)\n            data = self.get_sheet_data(sheet, file_rows_needed)\n            if hasattr(sheet, \"close\"):\n                # pyxlsb opens two TemporaryFiles\n                sheet.close()\n            usecols = maybe_convert_usecols(usecols)\n\n            if not data:\n                output[asheetname] = DataFrame()\n                continue\n\n            is_list_header = False\n            is_len_one_list_header = False\n            if is_list_like(header):\n                assert isinstance(header, Sequence)\n                is_list_header = True\n                if len(header) == 1:\n                    is_len_one_list_header = True\n\n            if is_len_one_list_header:\n                header = cast(Sequence[int], header)[0]\n\n            # forward fill and pull out names for MultiIndex column\n            header_names = None\n            if header is not None and is_list_like(header):\n                assert isinstance(header, Sequence)\n\n                header_names = []\n                control_row = [True] * len(data[0])\n\n                for row in header:\n                    if is_integer(skiprows):\n                        assert isinstance(skiprows, int)\n                        row += skiprows\n\n                    if row > len(data) - 1:\n                        raise ValueError(\n                            f\"header index {row} exceeds maximum index \"\n                            f\"{len(data) - 1} of data.\",\n                        )\n\n                    data[row], control_row = fill_mi_header(data[row], control_row)\n\n                    if index_col is not None:\n                        header_name, _ = pop_header_name(data[row], index_col)\n                        header_names.append(header_name)\n\n            # If there is a MultiIndex header and an index then there is also\n            # a row containing just the index name(s)\n            has_index_names = False\n            if is_list_header and not is_len_one_list_header and index_col is not None:\n                index_col_list: Sequence[int]\n                if isinstance(index_col, int):\n                    index_col_list = [index_col]\n                else:\n                    assert isinstance(index_col, Sequence)\n                    index_col_list = index_col\n\n                # We have to handle mi without names. If any of the entries in the data\n                # columns are not empty, this is a regular row\n                assert isinstance(header, Sequence)\n                if len(header) < len(data):\n                    potential_index_names = data[len(header)]\n                    potential_data = [\n                        x\n                        for i, x in enumerate(potential_index_names)\n                        if not control_row[i] and i not in index_col_list\n                    ]\n                    has_index_names = all(x == \"\" or x is None for x in potential_data)\n\n            if is_list_like(index_col):\n                # Forward fill values for MultiIndex index.\n                if header is None:\n                    offset = 0\n                elif isinstance(header, int):\n                    offset = 1 + header\n                else:\n                    offset = 1 + max(header)\n\n                # GH34673: if MultiIndex names present and not defined in the header,\n                # offset needs to be incremented so that forward filling starts\n                # from the first MI value instead of the name\n                if has_index_names:\n                    offset += 1\n\n                # Check if we have an empty dataset\n                # before trying to collect data.\n                if offset < len(data):\n                    assert isinstance(index_col, Sequence)\n\n                    for col in index_col:\n                        last = data[offset][col]\n\n                        for row in range(offset + 1, len(data)):\n                            if data[row][col] == \"\" or data[row][col] is None:\n                                data[row][col] = last\n                            else:\n                                last = data[row][col]\n\n            # GH 12292 : error when read one empty column from excel file\n            try:\n                parser = TextParser(\n                    data,\n                    names=names,\n                    header=header,\n                    index_col=index_col,\n                    has_index_names=has_index_names,\n                    squeeze=squeeze,\n                    dtype=dtype,\n                    true_values=true_values,\n                    false_values=false_values,\n                    skiprows=skiprows,\n                    nrows=nrows,\n                    na_values=na_values,\n                    skip_blank_lines=False,  # GH 39808\n                    parse_dates=parse_dates,\n                    date_parser=date_parser,\n                    date_format=date_format,\n                    thousands=thousands,\n                    decimal=decimal,\n                    comment=comment,\n                    skipfooter=skipfooter,\n                    usecols=usecols,\n                    use_nullable_dtypes=use_nullable_dtypes,\n                    **kwds,\n                )\n\n                output[asheetname] = parser.read(nrows=nrows)\n\n                if not squeeze or isinstance(output[asheetname], DataFrame):\n                    if header_names:\n                        output[asheetname].columns = output[\n                            asheetname\n                        ].columns.set_names(header_names)\n\n            except EmptyDataError:\n                # No Data, return an empty DataFrame\n                output[asheetname] = DataFrame()\n\n            except Exception as err:\n                err.args = (f\"{err.args[0]} (sheet: {asheetname})\", *err.args[1:])\n                raise err\n\n        if last_sheetname is None:\n            raise ValueError(\"Sheet name is an empty list\")\n\n        if ret_dict:\n            return output\n        else:\n            return output[last_sheetname]\n\n\n@doc(storage_options=_shared_docs[\"storage_options\"])\nclass ExcelWriter(metaclass=abc.ABCMeta):\n    \"\"\"\n    Class for writing DataFrame objects into excel sheets.\n\n    Default is to use:\n\n    * `xlsxwriter <https://pypi.org/project/XlsxWriter/>`__ for xlsx files if xlsxwriter\n      is installed otherwise `openpyxl <https://pypi.org/project/openpyxl/>`__\n    * `odswriter <https://pypi.org/project/odswriter/>`__ for ods files\n\n    See ``DataFrame.to_excel`` for typical usage.\n\n    The writer should be used as a context manager. Otherwise, call `close()` to save\n    and close any opened file handles.\n\n    Parameters\n    ----------\n    path : str or typing.BinaryIO\n        Path to xls or xlsx or ods file.\n    engine : str (optional)\n        Engine to use for writing. If None, defaults to\n        ``io.excel.<extension>.writer``.  NOTE: can only be passed as a keyword\n        argument.\n    date_format : str, default None\n        Format string for dates written into Excel files (e.g. 'YYYY-MM-DD').\n    datetime_format : str, default None\n        Format string for datetime objects written into Excel files.\n        (e.g. 'YYYY-MM-DD HH:MM:SS').\n    mode : {{'w', 'a'}}, default 'w'\n        File mode to use (write or append). Append does not work with fsspec URLs.\n    {storage_options}\n\n        .. versionadded:: 1.2.0\n\n    if_sheet_exists : {{'error', 'new', 'replace', 'overlay'}}, default 'error'\n        How to behave when trying to write to a sheet that already\n        exists (append mode only).\n\n        * error: raise a ValueError.\n        * new: Create a new sheet, with a name determined by the engine.\n        * replace: Delete the contents of the sheet before writing to it.\n        * overlay: Write contents to the existing sheet without removing the old\n          contents.\n\n        .. versionadded:: 1.3.0\n\n        .. versionchanged:: 1.4.0\n\n           Added ``overlay`` option\n\n    engine_kwargs : dict, optional\n        Keyword arguments to be passed into the engine. These will be passed to\n        the following functions of the respective engines:\n\n        * xlsxwriter: ``xlsxwriter.Workbook(file, **engine_kwargs)``\n        * openpyxl (write mode): ``openpyxl.Workbook(**engine_kwargs)``\n        * openpyxl (append mode): ``openpyxl.load_workbook(file, **engine_kwargs)``\n        * odswriter: ``odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs)``\n\n        .. versionadded:: 1.3.0\n\n    Notes\n    -----\n    For compatibility with CSV writers, ExcelWriter serializes lists\n    and dicts to strings before writing.\n\n    Examples\n    --------\n    Default usage:\n\n    >>> df = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])  # doctest: +SKIP\n    >>> with pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n    ...     df.to_excel(writer)  # doctest: +SKIP\n\n    To write to separate sheets in a single file:\n\n    >>> df1 = pd.DataFrame([[\"AAA\", \"BBB\"]], columns=[\"Spam\", \"Egg\"])  # doctest: +SKIP\n    >>> df2 = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])  # doctest: +SKIP\n    >>> with pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n    ...     df1.to_excel(writer, sheet_name=\"Sheet1\")  # doctest: +SKIP\n    ...     df2.to_excel(writer, sheet_name=\"Sheet2\")  # doctest: +SKIP\n\n    You can set the date format or datetime format:\n\n    >>> from datetime import date, datetime  # doctest: +SKIP\n    >>> df = pd.DataFrame(\n    ...     [\n    ...         [date(2014, 1, 31), date(1999, 9, 24)],\n    ...         [datetime(1998, 5, 26, 23, 33, 4), datetime(2014, 2, 28, 13, 5, 13)],\n    ...     ],\n    ...     index=[\"Date\", \"Datetime\"],\n    ...     columns=[\"X\", \"Y\"],\n    ... )  # doctest: +SKIP\n    >>> with pd.ExcelWriter(\n    ...     \"path_to_file.xlsx\",\n    ...     date_format=\"YYYY-MM-DD\",\n    ...     datetime_format=\"YYYY-MM-DD HH:MM:SS\"\n    ... ) as writer:\n    ...     df.to_excel(writer)  # doctest: +SKIP\n\n    You can also append to an existing Excel file:\n\n    >>> with pd.ExcelWriter(\"path_to_file.xlsx\", mode=\"a\", engine=\"openpyxl\") as writer:\n    ...     df.to_excel(writer, sheet_name=\"Sheet3\")  # doctest: +SKIP\n\n    Here, the `if_sheet_exists` parameter can be set to replace a sheet if it\n    already exists:\n\n    >>> with ExcelWriter(\n    ...     \"path_to_file.xlsx\",\n    ...     mode=\"a\",\n    ...     engine=\"openpyxl\",\n    ...     if_sheet_exists=\"replace\",\n    ... ) as writer:\n    ...     df.to_excel(writer, sheet_name=\"Sheet1\")  # doctest: +SKIP\n\n    You can also write multiple DataFrames to a single sheet. Note that the\n    ``if_sheet_exists`` parameter needs to be set to ``overlay``:\n\n    >>> with ExcelWriter(\"path_to_file.xlsx\",\n    ...     mode=\"a\",\n    ...     engine=\"openpyxl\",\n    ...     if_sheet_exists=\"overlay\",\n    ... ) as writer:\n    ...     df1.to_excel(writer, sheet_name=\"Sheet1\")\n    ...     df2.to_excel(writer, sheet_name=\"Sheet1\", startcol=3)  # doctest: +SKIP\n\n    You can store Excel file in RAM:\n\n    >>> import io\n    >>> df = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])\n    >>> buffer = io.BytesIO()\n    >>> with pd.ExcelWriter(buffer) as writer:\n    ...     df.to_excel(writer)\n\n    You can pack Excel file into zip archive:\n\n    >>> import zipfile  # doctest: +SKIP\n    >>> df = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])  # doctest: +SKIP\n    >>> with zipfile.ZipFile(\"path_to_file.zip\", \"w\") as zf:\n    ...     with zf.open(\"filename.xlsx\", \"w\") as buffer:\n    ...         with pd.ExcelWriter(buffer) as writer:\n    ...             df.to_excel(writer)  # doctest: +SKIP\n\n    You can specify additional arguments to the underlying engine:\n\n    >>> with pd.ExcelWriter(\n    ...     \"path_to_file.xlsx\",\n    ...     engine=\"xlsxwriter\",\n    ...     engine_kwargs={{\"options\": {{\"nan_inf_to_errors\": True}}}}\n    ... ) as writer:\n    ...     df.to_excel(writer)  # doctest: +SKIP\n\n    In append mode, ``engine_kwargs`` are passed through to\n    openpyxl's ``load_workbook``:\n\n    >>> with pd.ExcelWriter(\n    ...     \"path_to_file.xlsx\",\n    ...     engine=\"openpyxl\",\n    ...     mode=\"a\",\n    ...     engine_kwargs={{\"keep_vba\": True}}\n    ... ) as writer:\n    ...     df.to_excel(writer, sheet_name=\"Sheet2\")  # doctest: +SKIP\n    \"\"\"\n\n    # Defining an ExcelWriter implementation (see abstract methods for more...)\n\n    # - Mandatory\n    #   - ``write_cells(self, cells, sheet_name=None, startrow=0, startcol=0)``\n    #     --> called to write additional DataFrames to disk\n    #   - ``_supported_extensions`` (tuple of supported extensions), used to\n    #      check that engine supports the given extension.\n    #   - ``_engine`` - string that gives the engine name. Necessary to\n    #     instantiate class directly and bypass ``ExcelWriterMeta`` engine\n    #     lookup.\n    #   - ``save(self)`` --> called to save file to disk\n    # - Mostly mandatory (i.e. should at least exist)\n    #   - book, cur_sheet, path\n\n    # - Optional:\n    #   - ``__init__(self, path, engine=None, **kwargs)`` --> always called\n    #     with path as first argument.\n\n    # You also need to register the class with ``register_writer()``.\n    # Technically, ExcelWriter implementations don't need to subclass\n    # ExcelWriter.\n\n    _engine: str\n    _supported_extensions: tuple[str, ...]\n\n    def __new__(\n        cls: type[ExcelWriter],\n        path: FilePath | WriteExcelBuffer | ExcelWriter,\n        engine: str | None = None,\n        date_format: str | None = None,\n        datetime_format: str | None = None,\n        mode: str = \"w\",\n        storage_options: StorageOptions = None,\n        if_sheet_exists: Literal[\"error\", \"new\", \"replace\", \"overlay\"] | None = None,\n        engine_kwargs: dict | None = None,\n    ) -> ExcelWriter:\n        # only switch class if generic(ExcelWriter)\n        if cls is ExcelWriter:\n            if engine is None or (isinstance(engine, str) and engine == \"auto\"):\n                if isinstance(path, str):\n                    ext = os.path.splitext(path)[-1][1:]\n                else:\n                    ext = \"xlsx\"\n\n                try:\n                    engine = config.get_option(f\"io.excel.{ext}.writer\", silent=True)\n                    if engine == \"auto\":\n                        engine = get_default_engine(ext, mode=\"writer\")\n                except KeyError as err:\n                    raise ValueError(f\"No engine for filetype: '{ext}'\") from err\n\n            # for mypy\n            assert engine is not None\n            cls = get_writer(engine)\n\n        return object.__new__(cls)\n\n    # declare external properties you can count on\n    _path = None\n\n    @property\n    def supported_extensions(self) -> tuple[str, ...]:\n        \"\"\"Extensions that writer engine supports.\"\"\"\n        return self._supported_extensions\n\n    @property\n    def engine(self) -> str:\n        \"\"\"Name of engine.\"\"\"\n        return self._engine\n\n    @property\n    @abc.abstractmethod\n    def sheets(self) -> dict[str, Any]:\n        \"\"\"Mapping of sheet names to sheet objects.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def book(self):\n        \"\"\"\n        Book instance. Class type will depend on the engine used.\n\n        This attribute can be used to access engine-specific features.\n        \"\"\"\n\n    @abc.abstractmethod\n    def _write_cells(\n        self,\n        cells,\n        sheet_name: str | None = None,\n        startrow: int = 0,\n        startcol: int = 0,\n        freeze_panes: tuple[int, int] | None = None,\n    ) -> None:\n        \"\"\"\n        Write given formatted cells into Excel an excel sheet\n\n        Parameters\n        ----------\n        cells : generator\n            cell of formatted data to save to Excel sheet\n        sheet_name : str, default None\n            Name of Excel sheet, if None, then use self.cur_sheet\n        startrow : upper left cell row to dump data frame\n        startcol : upper left cell column to dump data frame\n        freeze_panes: int tuple of length 2\n            contains the bottom-most row and right-most column to freeze\n        \"\"\"\n\n    @abc.abstractmethod\n    def _save(self) -> None:\n        \"\"\"\n        Save workbook to disk.\n        \"\"\"\n\n    def __init__(\n        self,\n        path: FilePath | WriteExcelBuffer | ExcelWriter,\n        engine: str | None = None,\n        date_format: str | None = None,\n        datetime_format: str | None = None,\n        mode: str = \"w\",\n        storage_options: StorageOptions = None,\n        if_sheet_exists: str | None = None,\n        engine_kwargs: dict[str, Any] | None = None,\n    ) -> None:\n        # validate that this engine can handle the extension\n        if isinstance(path, str):\n            ext = os.path.splitext(path)[-1]\n            self.check_extension(ext)\n\n        # use mode to open the file\n        if \"b\" not in mode:\n            mode += \"b\"\n        # use \"a\" for the user to append data to excel but internally use \"r+\" to let\n        # the excel backend first read the existing file and then write any data to it\n        mode = mode.replace(\"a\", \"r+\")\n\n        # cast ExcelWriter to avoid adding 'if self._handles is not None'\n        self._handles = IOHandles(\n            cast(IO[bytes], path), compression={\"compression\": None}\n        )\n        if not isinstance(path, ExcelWriter):\n            self._handles = get_handle(\n                path, mode, storage_options=storage_options, is_text=False\n            )\n        self._cur_sheet = None\n\n        if date_format is None:\n            self._date_format = \"YYYY-MM-DD\"\n        else:\n            self._date_format = date_format\n        if datetime_format is None:\n            self._datetime_format = \"YYYY-MM-DD HH:MM:SS\"\n        else:\n            self._datetime_format = datetime_format\n\n        self._mode = mode\n\n        if if_sheet_exists not in (None, \"error\", \"new\", \"replace\", \"overlay\"):\n            raise ValueError(\n                f\"'{if_sheet_exists}' is not valid for if_sheet_exists. \"\n                \"Valid options are 'error', 'new', 'replace' and 'overlay'.\"\n            )\n        if if_sheet_exists and \"r+\" not in mode:\n            raise ValueError(\"if_sheet_exists is only valid in append mode (mode='a')\")\n        if if_sheet_exists is None:\n            if_sheet_exists = \"error\"\n        self._if_sheet_exists = if_sheet_exists\n\n    @property\n    def date_format(self) -> str:\n        \"\"\"\n        Format string for dates written into Excel files (e.g. ‘YYYY-MM-DD’).\n        \"\"\"\n        return self._date_format\n\n    @property\n    def datetime_format(self) -> str:\n        \"\"\"\n        Format string for dates written into Excel files (e.g. ‘YYYY-MM-DD’).\n        \"\"\"\n        return self._datetime_format\n\n    @property\n    def if_sheet_exists(self) -> str:\n        \"\"\"\n        How to behave when writing to a sheet that already exists in append mode.\n        \"\"\"\n        return self._if_sheet_exists\n\n    def __fspath__(self) -> str:\n        return getattr(self._handles.handle, \"name\", \"\")\n\n    def _get_sheet_name(self, sheet_name: str | None) -> str:\n        if sheet_name is None:\n            sheet_name = self._cur_sheet\n        if sheet_name is None:  # pragma: no cover\n            raise ValueError(\"Must pass explicit sheet_name or set _cur_sheet property\")\n        return sheet_name\n\n    def _value_with_fmt(self, val) -> tuple[object, str | None]:\n        \"\"\"\n        Convert numpy types to Python types for the Excel writers.\n\n        Parameters\n        ----------\n        val : object\n            Value to be written into cells\n\n        Returns\n        -------\n        Tuple with the first element being the converted value and the second\n            being an optional format\n        \"\"\"\n        fmt = None\n\n        if is_integer(val):\n            val = int(val)\n        elif is_float(val):\n            val = float(val)\n        elif is_bool(val):\n            val = bool(val)\n        elif isinstance(val, datetime.datetime):\n            fmt = self._datetime_format\n        elif isinstance(val, datetime.date):\n            fmt = self._date_format\n        elif isinstance(val, datetime.timedelta):\n            val = val.total_seconds() / 86400\n            fmt = \"0\"\n        else:\n            val = str(val)\n\n        return val, fmt\n\n    @classmethod\n    def check_extension(cls, ext: str) -> Literal[True]:\n        \"\"\"\n        checks that path's extension against the Writer's supported\n        extensions.  If it isn't supported, raises UnsupportedFiletypeError.\n        \"\"\"\n        if ext.startswith(\".\"):\n            ext = ext[1:]\n        if not any(ext in extension for extension in cls._supported_extensions):\n            raise ValueError(f\"Invalid extension for engine '{cls.engine}': '{ext}'\")\n        return True\n\n    # Allow use as a contextmanager\n    def __enter__(self) -> ExcelWriter:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ) -> None:\n        self.close()\n\n    def close(self) -> None:\n        \"\"\"synonym for save, to make it more file-like\"\"\"\n        self._save()\n        self._handles.close()\n\n\nXLS_SIGNATURES = (\n    b\"\\x09\\x00\\x04\\x00\\x07\\x00\\x10\\x00\",  # BIFF2\n    b\"\\x09\\x02\\x06\\x00\\x00\\x00\\x10\\x00\",  # BIFF3\n    b\"\\x09\\x04\\x06\\x00\\x00\\x00\\x10\\x00\",  # BIFF4\n    b\"\\xD0\\xCF\\x11\\xE0\\xA1\\xB1\\x1A\\xE1\",  # Compound File Binary\n)\nZIP_SIGNATURE = b\"PK\\x03\\x04\"\nPEEK_SIZE = max(map(len, XLS_SIGNATURES + (ZIP_SIGNATURE,)))\n\n\n@doc(storage_options=_shared_docs[\"storage_options\"])\ndef inspect_excel_format(\n    content_or_path: FilePath | ReadBuffer[bytes],\n    storage_options: StorageOptions = None,\n) -> str | None:\n    \"\"\"\n    Inspect the path or content of an excel file and get its format.\n\n    Adopted from xlrd: https://github.com/python-excel/xlrd.\n\n    Parameters\n    ----------\n    content_or_path : str or file-like object\n        Path to file or content of file to inspect. May be a URL.\n    {storage_options}\n\n    Returns\n    -------\n    str or None\n        Format of file if it can be determined.\n\n    Raises\n    ------\n    ValueError\n        If resulting stream is empty.\n    BadZipFile\n        If resulting stream does not have an XLS signature and is not a valid zipfile.\n    \"\"\"\n    if isinstance(content_or_path, bytes):\n        content_or_path = BytesIO(content_or_path)\n\n    with get_handle(\n        content_or_path, \"rb\", storage_options=storage_options, is_text=False\n    ) as handle:\n        stream = handle.handle\n        stream.seek(0)\n        buf = stream.read(PEEK_SIZE)\n        if buf is None:\n            raise ValueError(\"stream is empty\")\n        assert isinstance(buf, bytes)\n        peek = buf\n        stream.seek(0)\n\n        if any(peek.startswith(sig) for sig in XLS_SIGNATURES):\n            return \"xls\"\n        elif not peek.startswith(ZIP_SIGNATURE):\n            return None\n\n        with zipfile.ZipFile(stream) as zf:\n            # Workaround for some third party files that use forward slashes and\n            # lower case names.\n            component_names = [\n                name.replace(\"\\\\\", \"/\").lower() for name in zf.namelist()\n            ]\n\n        if \"xl/workbook.xml\" in component_names:\n            return \"xlsx\"\n        if \"xl/workbook.bin\" in component_names:\n            return \"xlsb\"\n        if \"content.xml\" in component_names:\n            return \"ods\"\n        return \"zip\"\n\n\nclass ExcelFile:\n    \"\"\"\n    Class for parsing tabular Excel sheets into DataFrame objects.\n\n    See read_excel for more documentation.\n\n    Parameters\n    ----------\n    path_or_buffer : str, bytes, path object (pathlib.Path or py._path.local.LocalPath),\n        A file-like object, xlrd workbook or openpyxl workbook.\n        If a string or path object, expected to be a path to a\n        .xls, .xlsx, .xlsb, .xlsm, .odf, .ods, or .odt file.\n    engine : str, default None\n        If io is not a buffer or path, this must be set to identify io.\n        Supported engines: ``xlrd``, ``openpyxl``, ``odf``, ``pyxlsb``\n        Engine compatibility :\n\n        - ``xlrd`` supports old-style Excel files (.xls).\n        - ``openpyxl`` supports newer Excel file formats.\n        - ``odf`` supports OpenDocument file formats (.odf, .ods, .odt).\n        - ``pyxlsb`` supports Binary Excel files.\n\n        .. versionchanged:: 1.2.0\n\n           The engine `xlrd <https://xlrd.readthedocs.io/en/latest/>`_\n           now only supports old-style ``.xls`` files.\n           When ``engine=None``, the following logic will be\n           used to determine the engine:\n\n           - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n             then `odf <https://pypi.org/project/odfpy/>`_ will be used.\n           - Otherwise if ``path_or_buffer`` is an xls format,\n             ``xlrd`` will be used.\n           - Otherwise if ``path_or_buffer`` is in xlsb format,\n             `pyxlsb <https://pypi.org/project/pyxlsb/>`_ will be used.\n\n           .. versionadded:: 1.3.0\n\n           - Otherwise if `openpyxl <https://pypi.org/project/openpyxl/>`_ is installed,\n             then ``openpyxl`` will be used.\n           - Otherwise if ``xlrd >= 2.0`` is installed, a ``ValueError`` will be raised.\n\n           .. warning::\n\n            Please do not report issues when using ``xlrd`` to read ``.xlsx`` files.\n            This is not supported, switch to using ``openpyxl`` instead.\n    \"\"\"\n\n    from pandas.io.excel._odfreader import ODFReader\n    from pandas.io.excel._openpyxl import OpenpyxlReader\n    from pandas.io.excel._pyxlsb import PyxlsbReader\n    from pandas.io.excel._xlrd import XlrdReader\n\n    _engines: Mapping[str, Any] = {\n        \"xlrd\": XlrdReader,\n        \"openpyxl\": OpenpyxlReader,\n        \"odf\": ODFReader,\n        \"pyxlsb\": PyxlsbReader,\n    }\n\n    def __init__(\n        self,\n        path_or_buffer,\n        engine: str | None = None,\n        storage_options: StorageOptions = None,\n    ) -> None:\n        if engine is not None and engine not in self._engines:\n            raise ValueError(f\"Unknown engine: {engine}\")\n\n        # First argument can also be bytes, so create a buffer\n        if isinstance(path_or_buffer, bytes):\n            path_or_buffer = BytesIO(path_or_buffer)\n\n        # Could be a str, ExcelFile, Book, etc.\n        self.io = path_or_buffer\n        # Always a string\n        self._io = stringify_path(path_or_buffer)\n\n        # Determine xlrd version if installed\n        if import_optional_dependency(\"xlrd\", errors=\"ignore\") is None:\n            xlrd_version = None\n        else:\n            import xlrd\n\n            xlrd_version = Version(get_version(xlrd))\n\n        if engine is None:\n            # Only determine ext if it is needed\n            ext: str | None\n            if xlrd_version is not None and isinstance(path_or_buffer, xlrd.Book):\n                ext = \"xls\"\n            else:\n                ext = inspect_excel_format(\n                    content_or_path=path_or_buffer, storage_options=storage_options\n                )\n                if ext is None:\n                    raise ValueError(\n                        \"Excel file format cannot be determined, you must specify \"\n                        \"an engine manually.\"\n                    )\n\n            engine = config.get_option(f\"io.excel.{ext}.reader\", silent=True)\n            if engine == \"auto\":\n                engine = get_default_engine(ext, mode=\"reader\")\n\n        assert engine is not None\n        self.engine = engine\n        self.storage_options = storage_options\n\n        self._reader = self._engines[engine](self._io, storage_options=storage_options)\n\n    def __fspath__(self):\n        return self._io\n\n    def parse(\n        self,\n        sheet_name: str | int | list[int] | list[str] | None = 0,\n        header: int | Sequence[int] | None = 0,\n        names=None,\n        index_col: int | Sequence[int] | None = None,\n        usecols=None,\n        squeeze: bool | None = None,\n        converters=None,\n        true_values: Iterable[Hashable] | None = None,\n        false_values: Iterable[Hashable] | None = None,\n        skiprows: Sequence[int] | int | Callable[[int], object] | None = None,\n        nrows: int | None = None,\n        na_values=None,\n        parse_dates: list | dict | bool = False,\n        date_parser: Callable | lib.NoDefault = lib.no_default,\n        date_format: str | None = None,\n        thousands: str | None = None,\n        comment: str | None = None,\n        skipfooter: int = 0,\n        use_nullable_dtypes: bool = False,\n        **kwds,\n    ) -> DataFrame | dict[str, DataFrame] | dict[int, DataFrame]:\n        \"\"\"\n        Parse specified sheet(s) into a DataFrame.\n\n        Equivalent to read_excel(ExcelFile, ...)  See the read_excel\n        docstring for more info on accepted parameters.\n\n        Returns\n        -------\n        DataFrame or dict of DataFrames\n            DataFrame from the passed in Excel file.\n        \"\"\"\n        return self._reader.parse(\n            sheet_name=sheet_name,\n            header=header,\n            names=names,\n            index_col=index_col,\n            usecols=usecols,\n            squeeze=squeeze,\n            converters=converters,\n            true_values=true_values,\n            false_values=false_values,\n            skiprows=skiprows,\n            nrows=nrows,\n            na_values=na_values,\n            parse_dates=parse_dates,\n            date_parser=date_parser,\n            date_format=date_format,\n            thousands=thousands,\n            comment=comment,\n            skipfooter=skipfooter,\n            use_nullable_dtypes=use_nullable_dtypes,\n            **kwds,\n        )\n\n    @property\n    def book(self):\n        return self._reader.book\n\n    @property\n    def sheet_names(self):\n        return self._reader.sheet_names\n\n    def close(self) -> None:\n        \"\"\"close io if necessary\"\"\"\n        self._reader.close()\n\n    def __enter__(self) -> ExcelFile:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ) -> None:\n        self.close()\n\n    def __del__(self) -> None:\n        # Ensure we don't leak file descriptors, but put in try/except in case\n        # attributes are already deleted\n        try:\n            self.close()\n        except AttributeError:\n            pass\n"
    },
    {
      "filename": "pandas/io/parsers/base_parser.py",
      "content": "from __future__ import annotations\n\nfrom collections import defaultdict\nfrom copy import copy\nimport csv\nimport datetime\nfrom enum import Enum\nimport itertools\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Hashable,\n    Iterable,\n    List,\n    Literal,\n    Mapping,\n    Sequence,\n    Tuple,\n    cast,\n    final,\n    overload,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config.config import get_option\n\nfrom pandas._libs import (\n    lib,\n    parsers,\n)\nimport pandas._libs.ops as libops\nfrom pandas._libs.parsers import STR_NA_VALUES\nfrom pandas._libs.tslibs import parsing\nfrom pandas._typing import (\n    ArrayLike,\n    DtypeArg,\n    DtypeObj,\n    Scalar,\n)\nfrom pandas.compat._optional import import_optional_dependency\nfrom pandas.errors import (\n    ParserError,\n    ParserWarning,\n)\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.astype import astype_array\nfrom pandas.core.dtypes.common import (\n    ensure_object,\n    is_bool_dtype,\n    is_dict_like,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_list_like,\n    is_object_dtype,\n    is_scalar,\n    is_string_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import (\n    CategoricalDtype,\n    ExtensionDtype,\n)\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas import StringDtype\nfrom pandas.core import algorithms\nfrom pandas.core.arrays import (\n    ArrowExtensionArray,\n    BooleanArray,\n    Categorical,\n    ExtensionArray,\n    FloatingArray,\n    IntegerArray,\n)\nfrom pandas.core.indexes.api import (\n    Index,\n    MultiIndex,\n    default_index,\n    ensure_index_from_sequences,\n)\nfrom pandas.core.series import Series\nfrom pandas.core.tools import datetimes as tools\n\nfrom pandas.io.common import is_potential_multi_index\n\nif TYPE_CHECKING:\n    from pandas import DataFrame\n\n\nclass ParserBase:\n    class BadLineHandleMethod(Enum):\n        ERROR = 0\n        WARN = 1\n        SKIP = 2\n\n    _implicit_index: bool = False\n    _first_chunk: bool\n\n    def __init__(self, kwds) -> None:\n        self.names = kwds.get(\"names\")\n        self.orig_names: Sequence[Hashable] | None = None\n\n        self.index_col = kwds.get(\"index_col\", None)\n        self.unnamed_cols: set = set()\n        self.index_names: Sequence[Hashable] | None = None\n        self.col_names: Sequence[Hashable] | None = None\n\n        self.parse_dates = _validate_parse_dates_arg(kwds.pop(\"parse_dates\", False))\n        self._parse_date_cols: Iterable = []\n        self.date_parser = kwds.pop(\"date_parser\", lib.no_default)\n        self.date_format = kwds.pop(\"date_format\", None)\n        self.dayfirst = kwds.pop(\"dayfirst\", False)\n        self.keep_date_col = kwds.pop(\"keep_date_col\", False)\n\n        self.na_values = kwds.get(\"na_values\")\n        self.na_fvalues = kwds.get(\"na_fvalues\")\n        self.na_filter = kwds.get(\"na_filter\", False)\n        self.keep_default_na = kwds.get(\"keep_default_na\", True)\n\n        self.dtype = copy(kwds.get(\"dtype\", None))\n        self.converters = kwds.get(\"converters\")\n        self.use_nullable_dtypes = kwds.get(\"use_nullable_dtypes\", False)\n\n        self.true_values = kwds.get(\"true_values\")\n        self.false_values = kwds.get(\"false_values\")\n        self.cache_dates = kwds.pop(\"cache_dates\", True)\n\n        self._date_conv = _make_date_converter(\n            date_parser=self.date_parser,\n            date_format=self.date_format,\n            dayfirst=self.dayfirst,\n            cache_dates=self.cache_dates,\n        )\n\n        # validate header options for mi\n        self.header = kwds.get(\"header\")\n        if is_list_like(self.header, allow_sets=False):\n            if kwds.get(\"usecols\"):\n                raise ValueError(\n                    \"cannot specify usecols when specifying a multi-index header\"\n                )\n            if kwds.get(\"names\"):\n                raise ValueError(\n                    \"cannot specify names when specifying a multi-index header\"\n                )\n\n            # validate index_col that only contains integers\n            if self.index_col is not None:\n                if not (\n                    is_list_like(self.index_col, allow_sets=False)\n                    and all(map(is_integer, self.index_col))\n                    or is_integer(self.index_col)\n                ):\n                    raise ValueError(\n                        \"index_col must only contain row numbers \"\n                        \"when specifying a multi-index header\"\n                    )\n\n        self._name_processed = False\n\n        self._first_chunk = True\n\n        self.usecols, self.usecols_dtype = self._validate_usecols_arg(kwds[\"usecols\"])\n\n        # Fallback to error to pass a sketchy test(test_override_set_noconvert_columns)\n        # Normally, this arg would get pre-processed earlier on\n        self.on_bad_lines = kwds.get(\"on_bad_lines\", self.BadLineHandleMethod.ERROR)\n\n    def _validate_parse_dates_presence(self, columns: Sequence[Hashable]) -> Iterable:\n        \"\"\"\n        Check if parse_dates are in columns.\n\n        If user has provided names for parse_dates, check if those columns\n        are available.\n\n        Parameters\n        ----------\n        columns : list\n            List of names of the dataframe.\n\n        Returns\n        -------\n        The names of the columns which will get parsed later if a dict or list\n        is given as specification.\n\n        Raises\n        ------\n        ValueError\n            If column to parse_date is not in dataframe.\n\n        \"\"\"\n        cols_needed: Iterable\n        if is_dict_like(self.parse_dates):\n            cols_needed = itertools.chain(*self.parse_dates.values())\n        elif is_list_like(self.parse_dates):\n            # a column in parse_dates could be represented\n            # ColReference = Union[int, str]\n            # DateGroups = List[ColReference]\n            # ParseDates = Union[DateGroups, List[DateGroups],\n            #     Dict[ColReference, DateGroups]]\n            cols_needed = itertools.chain.from_iterable(\n                col if is_list_like(col) and not isinstance(col, tuple) else [col]\n                for col in self.parse_dates\n            )\n        else:\n            cols_needed = []\n\n        cols_needed = list(cols_needed)\n\n        # get only columns that are references using names (str), not by index\n        missing_cols = \", \".join(\n            sorted(\n                {\n                    col\n                    for col in cols_needed\n                    if isinstance(col, str) and col not in columns\n                }\n            )\n        )\n        if missing_cols:\n            raise ValueError(\n                f\"Missing column provided to 'parse_dates': '{missing_cols}'\"\n            )\n        # Convert positions to actual column names\n        return [\n            col if (isinstance(col, str) or col in columns) else columns[col]\n            for col in cols_needed\n        ]\n\n    def close(self) -> None:\n        pass\n\n    @final\n    @property\n    def _has_complex_date_col(self) -> bool:\n        return isinstance(self.parse_dates, dict) or (\n            isinstance(self.parse_dates, list)\n            and len(self.parse_dates) > 0\n            and isinstance(self.parse_dates[0], list)\n        )\n\n    @final\n    def _should_parse_dates(self, i: int) -> bool:\n        if isinstance(self.parse_dates, bool):\n            return self.parse_dates\n        else:\n            if self.index_names is not None:\n                name = self.index_names[i]\n            else:\n                name = None\n            j = i if self.index_col is None else self.index_col[i]\n\n            if is_scalar(self.parse_dates):\n                return (j == self.parse_dates) or (\n                    name is not None and name == self.parse_dates\n                )\n            else:\n                return (j in self.parse_dates) or (\n                    name is not None and name in self.parse_dates\n                )\n\n    @final\n    def _extract_multi_indexer_columns(\n        self,\n        header,\n        index_names: Sequence[Hashable] | None,\n        passed_names: bool = False,\n    ) -> tuple[\n        Sequence[Hashable], Sequence[Hashable] | None, Sequence[Hashable] | None, bool\n    ]:\n        \"\"\"\n        Extract and return the names, index_names, col_names if the column\n        names are a MultiIndex.\n\n        Parameters\n        ----------\n        header: list of lists\n            The header rows\n        index_names: list, optional\n            The names of the future index\n        passed_names: bool, default False\n            A flag specifying if names where passed\n\n        \"\"\"\n        if len(header) < 2:\n            return header[0], index_names, None, passed_names\n\n        # the names are the tuples of the header that are not the index cols\n        # 0 is the name of the index, assuming index_col is a list of column\n        # numbers\n        ic = self.index_col\n        if ic is None:\n            ic = []\n\n        if not isinstance(ic, (list, tuple, np.ndarray)):\n            ic = [ic]\n        sic = set(ic)\n\n        # clean the index_names\n        index_names = header.pop(-1)\n        index_names, _, _ = self._clean_index_names(index_names, self.index_col)\n\n        # extract the columns\n        field_count = len(header[0])\n\n        # check if header lengths are equal\n        if not all(len(header_iter) == field_count for header_iter in header[1:]):\n            raise ParserError(\"Header rows must have an equal number of columns.\")\n\n        def extract(r):\n            return tuple(r[i] for i in range(field_count) if i not in sic)\n\n        columns = list(zip(*(extract(r) for r in header)))\n        names = columns.copy()\n        for single_ic in sorted(ic):\n            names.insert(single_ic, single_ic)\n\n        # Clean the column names (if we have an index_col).\n        if len(ic):\n            col_names = [\n                r[ic[0]]\n                if ((r[ic[0]] is not None) and r[ic[0]] not in self.unnamed_cols)\n                else None\n                for r in header\n            ]\n        else:\n            col_names = [None] * len(header)\n\n        passed_names = True\n\n        return names, index_names, col_names, passed_names\n\n    @final\n    def _maybe_make_multi_index_columns(\n        self,\n        columns: Sequence[Hashable],\n        col_names: Sequence[Hashable] | None = None,\n    ) -> Sequence[Hashable] | MultiIndex:\n        # possibly create a column mi here\n        if is_potential_multi_index(columns):\n            list_columns = cast(List[Tuple], columns)\n            return MultiIndex.from_tuples(list_columns, names=col_names)\n        return columns\n\n    @final\n    def _make_index(\n        self, data, alldata, columns, indexnamerow: list[Scalar] | None = None\n    ) -> tuple[Index | None, Sequence[Hashable] | MultiIndex]:\n        index: Index | None\n        if not is_index_col(self.index_col) or not self.index_col:\n            index = None\n\n        elif not self._has_complex_date_col:\n            simple_index = self._get_simple_index(alldata, columns)\n            index = self._agg_index(simple_index)\n        elif self._has_complex_date_col:\n            if not self._name_processed:\n                (self.index_names, _, self.index_col) = self._clean_index_names(\n                    list(columns), self.index_col\n                )\n                self._name_processed = True\n            date_index = self._get_complex_date_index(data, columns)\n            index = self._agg_index(date_index, try_parse_dates=False)\n\n        # add names for the index\n        if indexnamerow:\n            coffset = len(indexnamerow) - len(columns)\n            assert index is not None\n            index = index.set_names(indexnamerow[:coffset])\n\n        # maybe create a mi on the columns\n        columns = self._maybe_make_multi_index_columns(columns, self.col_names)\n\n        return index, columns\n\n    @final\n    def _get_simple_index(self, data, columns):\n        def ix(col):\n            if not isinstance(col, str):\n                return col\n            raise ValueError(f\"Index {col} invalid\")\n\n        to_remove = []\n        index = []\n        for idx in self.index_col:\n            i = ix(idx)\n            to_remove.append(i)\n            index.append(data[i])\n\n        # remove index items from content and columns, don't pop in\n        # loop\n        for i in sorted(to_remove, reverse=True):\n            data.pop(i)\n            if not self._implicit_index:\n                columns.pop(i)\n\n        return index\n\n    @final\n    def _get_complex_date_index(self, data, col_names):\n        def _get_name(icol):\n            if isinstance(icol, str):\n                return icol\n\n            if col_names is None:\n                raise ValueError(f\"Must supply column order to use {icol!s} as index\")\n\n            for i, c in enumerate(col_names):\n                if i == icol:\n                    return c\n\n        to_remove = []\n        index = []\n        for idx in self.index_col:\n            name = _get_name(idx)\n            to_remove.append(name)\n            index.append(data[name])\n\n        # remove index items from content and columns, don't pop in\n        # loop\n        for c in sorted(to_remove, reverse=True):\n            data.pop(c)\n            col_names.remove(c)\n\n        return index\n\n    def _clean_mapping(self, mapping):\n        \"\"\"converts col numbers to names\"\"\"\n        if not isinstance(mapping, dict):\n            return mapping\n        clean = {}\n        # for mypy\n        assert self.orig_names is not None\n\n        for col, v in mapping.items():\n            if isinstance(col, int) and col not in self.orig_names:\n                col = self.orig_names[col]\n            clean[col] = v\n        if isinstance(mapping, defaultdict):\n            remaining_cols = set(self.orig_names) - set(clean.keys())\n            clean.update({col: mapping[col] for col in remaining_cols})\n        return clean\n\n    @final\n    def _agg_index(self, index, try_parse_dates: bool = True) -> Index:\n        arrays = []\n        converters = self._clean_mapping(self.converters)\n\n        for i, arr in enumerate(index):\n            if try_parse_dates and self._should_parse_dates(i):\n                arr = self._date_conv(arr)\n\n            if self.na_filter:\n                col_na_values = self.na_values\n                col_na_fvalues = self.na_fvalues\n            else:\n                col_na_values = set()\n                col_na_fvalues = set()\n\n            if isinstance(self.na_values, dict):\n                assert self.index_names is not None\n                col_name = self.index_names[i]\n                if col_name is not None:\n                    col_na_values, col_na_fvalues = _get_na_values(\n                        col_name, self.na_values, self.na_fvalues, self.keep_default_na\n                    )\n\n            clean_dtypes = self._clean_mapping(self.dtype)\n\n            cast_type = None\n            index_converter = False\n            if self.index_names is not None:\n                if isinstance(clean_dtypes, dict):\n                    cast_type = clean_dtypes.get(self.index_names[i], None)\n\n                if isinstance(converters, dict):\n                    index_converter = converters.get(self.index_names[i]) is not None\n\n            try_num_bool = not (\n                cast_type and is_string_dtype(cast_type) or index_converter\n            )\n\n            arr, _ = self._infer_types(\n                arr, col_na_values | col_na_fvalues, cast_type is None, try_num_bool\n            )\n            arrays.append(arr)\n\n        names = self.index_names\n        index = ensure_index_from_sequences(arrays, names)\n\n        return index\n\n    @final\n    def _convert_to_ndarrays(\n        self,\n        dct: Mapping,\n        na_values,\n        na_fvalues,\n        verbose: bool = False,\n        converters=None,\n        dtypes=None,\n    ):\n        result = {}\n        for c, values in dct.items():\n            conv_f = None if converters is None else converters.get(c, None)\n            if isinstance(dtypes, dict):\n                cast_type = dtypes.get(c, None)\n            else:\n                # single dtype or None\n                cast_type = dtypes\n\n            if self.na_filter:\n                col_na_values, col_na_fvalues = _get_na_values(\n                    c, na_values, na_fvalues, self.keep_default_na\n                )\n            else:\n                col_na_values, col_na_fvalues = set(), set()\n\n            if c in self._parse_date_cols:\n                # GH#26203 Do not convert columns which get converted to dates\n                # but replace nans to ensure to_datetime works\n                mask = algorithms.isin(values, set(col_na_values) | col_na_fvalues)\n                np.putmask(values, mask, np.nan)\n                result[c] = values\n                continue\n\n            if conv_f is not None:\n                # conv_f applied to data before inference\n                if cast_type is not None:\n                    warnings.warn(\n                        (\n                            \"Both a converter and dtype were specified \"\n                            f\"for column {c} - only the converter will be used.\"\n                        ),\n                        ParserWarning,\n                        stacklevel=find_stack_level(),\n                    )\n\n                try:\n                    values = lib.map_infer(values, conv_f)\n                except ValueError:\n                    # error: Argument 2 to \"isin\" has incompatible type \"List[Any]\";\n                    # expected \"Union[Union[ExtensionArray, ndarray], Index, Series]\"\n                    mask = algorithms.isin(\n                        values, list(na_values)  # type: ignore[arg-type]\n                    ).view(np.uint8)\n                    values = lib.map_infer_mask(values, conv_f, mask)\n\n                cvals, na_count = self._infer_types(\n                    values,\n                    set(col_na_values) | col_na_fvalues,\n                    cast_type is None,\n                    try_num_bool=False,\n                )\n            else:\n                is_ea = is_extension_array_dtype(cast_type)\n                is_str_or_ea_dtype = is_ea or is_string_dtype(cast_type)\n                # skip inference if specified dtype is object\n                # or casting to an EA\n                try_num_bool = not (cast_type and is_str_or_ea_dtype)\n\n                # general type inference and conversion\n                cvals, na_count = self._infer_types(\n                    values,\n                    set(col_na_values) | col_na_fvalues,\n                    cast_type is None,\n                    try_num_bool,\n                )\n\n                # type specified in dtype param or cast_type is an EA\n                if cast_type and (not is_dtype_equal(cvals, cast_type) or is_ea):\n                    if not is_ea and na_count > 0:\n                        if is_bool_dtype(cast_type):\n                            raise ValueError(f\"Bool column has NA values in column {c}\")\n                    cast_type = pandas_dtype(cast_type)\n                    cvals = self._cast_types(cvals, cast_type, c)\n\n            result[c] = cvals\n            if verbose and na_count:\n                print(f\"Filled {na_count} NA values in column {c!s}\")\n        return result\n\n    @final\n    def _set_noconvert_dtype_columns(\n        self, col_indices: list[int], names: Sequence[Hashable]\n    ) -> set[int]:\n        \"\"\"\n        Set the columns that should not undergo dtype conversions.\n\n        Currently, any column that is involved with date parsing will not\n        undergo such conversions. If usecols is specified, the positions of the columns\n        not to cast is relative to the usecols not to all columns.\n\n        Parameters\n        ----------\n        col_indices: The indices specifying order and positions of the columns\n        names: The column names which order is corresponding with the order\n               of col_indices\n\n        Returns\n        -------\n        A set of integers containing the positions of the columns not to convert.\n        \"\"\"\n        usecols: list[int] | list[str] | None\n        noconvert_columns = set()\n        if self.usecols_dtype == \"integer\":\n            # A set of integers will be converted to a list in\n            # the correct order every single time.\n            usecols = sorted(self.usecols)\n        elif callable(self.usecols) or self.usecols_dtype not in (\"empty\", None):\n            # The names attribute should have the correct columns\n            # in the proper order for indexing with parse_dates.\n            usecols = col_indices\n        else:\n            # Usecols is empty.\n            usecols = None\n\n        def _set(x) -> int:\n            if usecols is not None and is_integer(x):\n                x = usecols[x]\n\n            if not is_integer(x):\n                x = col_indices[names.index(x)]\n\n            return x\n\n        if isinstance(self.parse_dates, list):\n            for val in self.parse_dates:\n                if isinstance(val, list):\n                    for k in val:\n                        noconvert_columns.add(_set(k))\n                else:\n                    noconvert_columns.add(_set(val))\n\n        elif isinstance(self.parse_dates, dict):\n            for val in self.parse_dates.values():\n                if isinstance(val, list):\n                    for k in val:\n                        noconvert_columns.add(_set(k))\n                else:\n                    noconvert_columns.add(_set(val))\n\n        elif self.parse_dates:\n            if isinstance(self.index_col, list):\n                for k in self.index_col:\n                    noconvert_columns.add(_set(k))\n            elif self.index_col is not None:\n                noconvert_columns.add(_set(self.index_col))\n\n        return noconvert_columns\n\n    def _infer_types(\n        self, values, na_values, no_dtype_specified, try_num_bool: bool = True\n    ) -> tuple[ArrayLike, int]:\n        \"\"\"\n        Infer types of values, possibly casting\n\n        Parameters\n        ----------\n        values : ndarray\n        na_values : set\n        no_dtype_specified: Specifies if we want to cast explicitly\n        try_num_bool : bool, default try\n           try to cast values to numeric (first preference) or boolean\n\n        Returns\n        -------\n        converted : ndarray or ExtensionArray\n        na_count : int\n        \"\"\"\n        na_count = 0\n        if issubclass(values.dtype.type, (np.number, np.bool_)):\n            # If our array has numeric dtype, we don't have to check for strings in isin\n            na_values = np.array([val for val in na_values if not isinstance(val, str)])\n            mask = algorithms.isin(values, na_values)\n            na_count = mask.astype(\"uint8\", copy=False).sum()\n            if na_count > 0:\n                if is_integer_dtype(values):\n                    values = values.astype(np.float64)\n                np.putmask(values, mask, np.nan)\n            return values, na_count\n\n        use_nullable_dtypes: Literal[True] | Literal[False] = (\n            self.use_nullable_dtypes and no_dtype_specified\n        )\n        dtype_backend = get_option(\"mode.dtype_backend\")\n        result: ArrayLike\n\n        if try_num_bool and is_object_dtype(values.dtype):\n            # exclude e.g DatetimeIndex here\n            try:\n                result, result_mask = lib.maybe_convert_numeric(\n                    values,\n                    na_values,\n                    False,\n                    convert_to_masked_nullable=use_nullable_dtypes,\n                )\n            except (ValueError, TypeError):\n                # e.g. encountering datetime string gets ValueError\n                #  TypeError can be raised in floatify\n                na_count = parsers.sanitize_objects(values, na_values)\n                result = values\n            else:\n                if use_nullable_dtypes:\n                    if result_mask is None:\n                        result_mask = np.zeros(result.shape, dtype=np.bool_)\n\n                    if result_mask.all():\n                        result = IntegerArray(\n                            np.ones(result_mask.shape, dtype=np.int64), result_mask\n                        )\n                    elif is_integer_dtype(result):\n                        result = IntegerArray(result, result_mask)\n                    elif is_bool_dtype(result):\n                        result = BooleanArray(result, result_mask)\n                    elif is_float_dtype(result):\n                        result = FloatingArray(result, result_mask)\n\n                    na_count = result_mask.sum()\n                else:\n                    na_count = isna(result).sum()\n        else:\n            result = values\n            if values.dtype == np.object_:\n                na_count = parsers.sanitize_objects(values, na_values)\n\n        if result.dtype == np.object_ and try_num_bool:\n            result, bool_mask = libops.maybe_convert_bool(\n                np.asarray(values),\n                true_values=self.true_values,\n                false_values=self.false_values,\n                convert_to_masked_nullable=use_nullable_dtypes,\n            )\n            if result.dtype == np.bool_ and use_nullable_dtypes:\n                if bool_mask is None:\n                    bool_mask = np.zeros(result.shape, dtype=np.bool_)\n                result = BooleanArray(result, bool_mask)\n            elif result.dtype == np.object_ and use_nullable_dtypes:\n                # read_excel sends array of datetime objects\n                inferred_type = lib.infer_dtype(result)\n                if inferred_type != \"datetime\":\n                    result = StringDtype().construct_array_type()._from_sequence(values)\n\n        if use_nullable_dtypes and dtype_backend == \"pyarrow\":\n            pa = import_optional_dependency(\"pyarrow\")\n            if isinstance(result, np.ndarray):\n                result = ArrowExtensionArray(pa.array(result, from_pandas=True))\n            else:\n                # ExtensionArray\n                result = ArrowExtensionArray(\n                    pa.array(result.to_numpy(), from_pandas=True)\n                )\n\n        return result, na_count\n\n    def _cast_types(self, values: ArrayLike, cast_type: DtypeObj, column) -> ArrayLike:\n        \"\"\"\n        Cast values to specified type\n\n        Parameters\n        ----------\n        values : ndarray or ExtensionArray\n        cast_type : np.dtype or ExtensionDtype\n           dtype to cast values to\n        column : string\n            column name - used only for error reporting\n\n        Returns\n        -------\n        converted : ndarray or ExtensionArray\n        \"\"\"\n        if isinstance(cast_type, CategoricalDtype):\n            known_cats = cast_type.categories is not None\n\n            if not is_object_dtype(values.dtype) and not known_cats:\n                # TODO: this is for consistency with\n                # c-parser which parses all categories\n                # as strings\n                values = lib.ensure_string_array(\n                    values, skipna=False, convert_na_value=False\n                )\n\n            cats = Index(values).unique().dropna()\n            values = Categorical._from_inferred_categories(\n                cats, cats.get_indexer(values), cast_type, true_values=self.true_values\n            )\n\n        # use the EA's implementation of casting\n        elif isinstance(cast_type, ExtensionDtype):\n            array_type = cast_type.construct_array_type()\n            try:\n                if is_bool_dtype(cast_type):\n                    # error: Unexpected keyword argument \"true_values\" for\n                    # \"_from_sequence_of_strings\" of \"ExtensionArray\"\n                    return array_type._from_sequence_of_strings(  # type: ignore[call-arg]  # noqa:E501\n                        values,\n                        dtype=cast_type,\n                        true_values=self.true_values,\n                        false_values=self.false_values,\n                    )\n                else:\n                    return array_type._from_sequence_of_strings(values, dtype=cast_type)\n            except NotImplementedError as err:\n                raise NotImplementedError(\n                    f\"Extension Array: {array_type} must implement \"\n                    \"_from_sequence_of_strings in order to be used in parser methods\"\n                ) from err\n\n        elif isinstance(values, ExtensionArray):\n            values = values.astype(cast_type, copy=False)\n        elif issubclass(cast_type.type, str):\n            # TODO: why skipna=True here and False above? some tests depend\n            #  on it here, but nothing fails if we change it above\n            #  (as no tests get there as of 2022-12-06)\n            values = lib.ensure_string_array(\n                values, skipna=True, convert_na_value=False\n            )\n        else:\n            try:\n                values = astype_array(values, cast_type, copy=True)\n            except ValueError as err:\n                raise ValueError(\n                    f\"Unable to convert column {column} to type {cast_type}\"\n                ) from err\n        return values\n\n    @overload\n    def _do_date_conversions(\n        self,\n        names: Index,\n        data: DataFrame,\n    ) -> tuple[Sequence[Hashable] | Index, DataFrame]:\n        ...\n\n    @overload\n    def _do_date_conversions(\n        self,\n        names: Sequence[Hashable],\n        data: Mapping[Hashable, ArrayLike],\n    ) -> tuple[Sequence[Hashable], Mapping[Hashable, ArrayLike]]:\n        ...\n\n    def _do_date_conversions(\n        self,\n        names: Sequence[Hashable] | Index,\n        data: Mapping[Hashable, ArrayLike] | DataFrame,\n    ) -> tuple[Sequence[Hashable] | Index, Mapping[Hashable, ArrayLike] | DataFrame]:\n        # returns data, columns\n\n        if self.parse_dates is not None:\n            data, names = _process_date_conversion(\n                data,\n                self._date_conv,\n                self.parse_dates,\n                self.index_col,\n                self.index_names,\n                names,\n                keep_date_col=self.keep_date_col,\n            )\n\n        return names, data\n\n    def _check_data_length(\n        self,\n        columns: Sequence[Hashable],\n        data: Sequence[ArrayLike],\n    ) -> None:\n        \"\"\"Checks if length of data is equal to length of column names.\n\n        One set of trailing commas is allowed. self.index_col not False\n        results in a ParserError previously when lengths do not match.\n\n        Parameters\n        ----------\n        columns: list of column names\n        data: list of array-likes containing the data column-wise.\n        \"\"\"\n        if not self.index_col and len(columns) != len(data) and columns:\n            empty_str = is_object_dtype(data[-1]) and data[-1] == \"\"\n            # error: No overload variant of \"__ror__\" of \"ndarray\" matches\n            # argument type \"ExtensionArray\"\n            empty_str_or_na = empty_str | isna(data[-1])  # type: ignore[operator]\n            if len(columns) == len(data) - 1 and np.all(empty_str_or_na):\n                return\n            warnings.warn(\n                \"Length of header or names does not match length of data. This leads \"\n                \"to a loss of data with index_col=False.\",\n                ParserWarning,\n                stacklevel=find_stack_level(),\n            )\n\n    @overload\n    def _evaluate_usecols(\n        self,\n        usecols: set[int] | Callable[[Hashable], object],\n        names: Sequence[Hashable],\n    ) -> set[int]:\n        ...\n\n    @overload\n    def _evaluate_usecols(\n        self, usecols: set[str], names: Sequence[Hashable]\n    ) -> set[str]:\n        ...\n\n    def _evaluate_usecols(\n        self,\n        usecols: Callable[[Hashable], object] | set[str] | set[int],\n        names: Sequence[Hashable],\n    ) -> set[str] | set[int]:\n        \"\"\"\n        Check whether or not the 'usecols' parameter\n        is a callable.  If so, enumerates the 'names'\n        parameter and returns a set of indices for\n        each entry in 'names' that evaluates to True.\n        If not a callable, returns 'usecols'.\n        \"\"\"\n        if callable(usecols):\n            return {i for i, name in enumerate(names) if usecols(name)}\n        return usecols\n\n    def _validate_usecols_names(self, usecols, names):\n        \"\"\"\n        Validates that all usecols are present in a given\n        list of names. If not, raise a ValueError that\n        shows what usecols are missing.\n\n        Parameters\n        ----------\n        usecols : iterable of usecols\n            The columns to validate are present in names.\n        names : iterable of names\n            The column names to check against.\n\n        Returns\n        -------\n        usecols : iterable of usecols\n            The `usecols` parameter if the validation succeeds.\n\n        Raises\n        ------\n        ValueError : Columns were missing. Error message will list them.\n        \"\"\"\n        missing = [c for c in usecols if c not in names]\n        if len(missing) > 0:\n            raise ValueError(\n                f\"Usecols do not match columns, columns expected but not found: \"\n                f\"{missing}\"\n            )\n\n        return usecols\n\n    def _validate_usecols_arg(self, usecols):\n        \"\"\"\n        Validate the 'usecols' parameter.\n\n        Checks whether or not the 'usecols' parameter contains all integers\n        (column selection by index), strings (column by name) or is a callable.\n        Raises a ValueError if that is not the case.\n\n        Parameters\n        ----------\n        usecols : list-like, callable, or None\n            List of columns to use when parsing or a callable that can be used\n            to filter a list of table columns.\n\n        Returns\n        -------\n        usecols_tuple : tuple\n            A tuple of (verified_usecols, usecols_dtype).\n\n            'verified_usecols' is either a set if an array-like is passed in or\n            'usecols' if a callable or None is passed in.\n\n            'usecols_dtype` is the inferred dtype of 'usecols' if an array-like\n            is passed in or None if a callable or None is passed in.\n        \"\"\"\n        msg = (\n            \"'usecols' must either be list-like of all strings, all unicode, \"\n            \"all integers or a callable.\"\n        )\n        if usecols is not None:\n            if callable(usecols):\n                return usecols, None\n\n            if not is_list_like(usecols):\n                # see gh-20529\n                #\n                # Ensure it is iterable container but not string.\n                raise ValueError(msg)\n\n            usecols_dtype = lib.infer_dtype(usecols, skipna=False)\n\n            if usecols_dtype not in (\"empty\", \"integer\", \"string\"):\n                raise ValueError(msg)\n\n            usecols = set(usecols)\n\n            return usecols, usecols_dtype\n        return usecols, None\n\n    def _clean_index_names(self, columns, index_col) -> tuple[list | None, list, list]:\n        if not is_index_col(index_col):\n            return None, columns, index_col\n\n        columns = list(columns)\n\n        # In case of no rows and multiindex columns we have to set index_names to\n        # list of Nones GH#38292\n        if not columns:\n            return [None] * len(index_col), columns, index_col\n\n        cp_cols = list(columns)\n        index_names: list[str | int | None] = []\n\n        # don't mutate\n        index_col = list(index_col)\n\n        for i, c in enumerate(index_col):\n            if isinstance(c, str):\n                index_names.append(c)\n                for j, name in enumerate(cp_cols):\n                    if name == c:\n                        index_col[i] = j\n                        columns.remove(name)\n                        break\n            else:\n                name = cp_cols[c]\n                columns.remove(name)\n                index_names.append(name)\n\n        # Only clean index names that were placeholders.\n        for i, name in enumerate(index_names):\n            if isinstance(name, str) and name in self.unnamed_cols:\n                index_names[i] = None\n\n        return index_names, columns, index_col\n\n    def _get_empty_meta(\n        self, columns, index_col, index_names, dtype: DtypeArg | None = None\n    ):\n        columns = list(columns)\n\n        # Convert `dtype` to a defaultdict of some kind.\n        # This will enable us to write `dtype[col_name]`\n        # without worrying about KeyError issues later on.\n        dtype_dict: defaultdict[Hashable, Any]\n        if not is_dict_like(dtype):\n            # if dtype == None, default will be object.\n            default_dtype = dtype or object\n            dtype_dict = defaultdict(lambda: default_dtype)\n        else:\n            dtype = cast(dict, dtype)\n            dtype_dict = defaultdict(\n                lambda: object,\n                {columns[k] if is_integer(k) else k: v for k, v in dtype.items()},\n            )\n\n        # Even though we have no data, the \"index\" of the empty DataFrame\n        # could for example still be an empty MultiIndex. Thus, we need to\n        # check whether we have any index columns specified, via either:\n        #\n        # 1) index_col (column indices)\n        # 2) index_names (column names)\n        #\n        # Both must be non-null to ensure a successful construction. Otherwise,\n        # we have to create a generic empty Index.\n        index: Index\n        if (index_col is None or index_col is False) or index_names is None:\n            index = default_index(0)\n        else:\n            data = [Series([], dtype=dtype_dict[name]) for name in index_names]\n            index = ensure_index_from_sequences(data, names=index_names)\n            index_col.sort()\n\n            for i, n in enumerate(index_col):\n                columns.pop(n - i)\n\n        col_dict = {\n            col_name: Series([], dtype=dtype_dict[col_name]) for col_name in columns\n        }\n\n        return index, columns, col_dict\n\n\ndef _make_date_converter(\n    date_parser=lib.no_default,\n    dayfirst: bool = False,\n    cache_dates: bool = True,\n    date_format: str | None = None,\n):\n    if date_parser is not lib.no_default:\n        warnings.warn(\n            \"The argument 'date_parser' is deprecated and will \"\n            \"be removed in a future version. \"\n            \"Please use 'date_format' instead, or read your data in as 'object' dtype \"\n            \"and then call 'to_datetime'.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n    if date_parser is not lib.no_default and date_format is not None:\n        raise TypeError(\"Cannot use both 'date_parser' and 'date_format'\")\n\n    def converter(*date_cols):\n        if date_parser is lib.no_default:\n            strs = parsing.concat_date_cols(date_cols)\n\n            return tools.to_datetime(\n                ensure_object(strs),\n                format=date_format,\n                utc=False,\n                dayfirst=dayfirst,\n                errors=\"ignore\",\n                cache=cache_dates,\n            ).to_numpy()\n        else:\n            try:\n                result = tools.to_datetime(\n                    date_parser(*date_cols), errors=\"ignore\", cache=cache_dates\n                )\n                if isinstance(result, datetime.datetime):\n                    raise Exception(\"scalar parser\")\n                return result\n            except Exception:\n                return tools.to_datetime(\n                    parsing.try_parse_dates(\n                        parsing.concat_date_cols(date_cols),\n                        parser=date_parser,\n                    ),\n                    errors=\"ignore\",\n                )\n\n    return converter\n\n\nparser_defaults = {\n    \"delimiter\": None,\n    \"escapechar\": None,\n    \"quotechar\": '\"',\n    \"quoting\": csv.QUOTE_MINIMAL,\n    \"doublequote\": True,\n    \"skipinitialspace\": False,\n    \"lineterminator\": None,\n    \"header\": \"infer\",\n    \"index_col\": None,\n    \"names\": None,\n    \"skiprows\": None,\n    \"skipfooter\": 0,\n    \"nrows\": None,\n    \"na_values\": None,\n    \"keep_default_na\": True,\n    \"true_values\": None,\n    \"false_values\": None,\n    \"converters\": None,\n    \"dtype\": None,\n    \"cache_dates\": True,\n    \"thousands\": None,\n    \"comment\": None,\n    \"decimal\": \".\",\n    # 'engine': 'c',\n    \"parse_dates\": False,\n    \"keep_date_col\": False,\n    \"dayfirst\": False,\n    \"date_parser\": lib.no_default,\n    \"date_format\": None,\n    \"usecols\": None,\n    # 'iterator': False,\n    \"chunksize\": None,\n    \"verbose\": False,\n    \"encoding\": None,\n    \"compression\": None,\n    \"skip_blank_lines\": True,\n    \"encoding_errors\": \"strict\",\n    \"on_bad_lines\": ParserBase.BadLineHandleMethod.ERROR,\n    \"use_nullable_dtypes\": False,\n}\n\n\ndef _process_date_conversion(\n    data_dict,\n    converter: Callable,\n    parse_spec,\n    index_col,\n    index_names,\n    columns,\n    keep_date_col: bool = False,\n):\n    def _isindex(colspec):\n        return (isinstance(index_col, list) and colspec in index_col) or (\n            isinstance(index_names, list) and colspec in index_names\n        )\n\n    new_cols = []\n    new_data = {}\n\n    orig_names = columns\n    columns = list(columns)\n\n    date_cols = set()\n\n    if parse_spec is None or isinstance(parse_spec, bool):\n        return data_dict, columns\n\n    if isinstance(parse_spec, list):\n        # list of column lists\n        for colspec in parse_spec:\n            if is_scalar(colspec) or isinstance(colspec, tuple):\n                if isinstance(colspec, int) and colspec not in data_dict:\n                    colspec = orig_names[colspec]\n                if _isindex(colspec):\n                    continue\n                # Pyarrow engine returns Series which we need to convert to\n                # numpy array before converter, its a no-op for other parsers\n                data_dict[colspec] = converter(np.asarray(data_dict[colspec]))\n            else:\n                new_name, col, old_names = _try_convert_dates(\n                    converter, colspec, data_dict, orig_names\n                )\n                if new_name in data_dict:\n                    raise ValueError(f\"New date column already in dict {new_name}\")\n                new_data[new_name] = col\n                new_cols.append(new_name)\n                date_cols.update(old_names)\n\n    elif isinstance(parse_spec, dict):\n        # dict of new name to column list\n        for new_name, colspec in parse_spec.items():\n            if new_name in data_dict:\n                raise ValueError(f\"Date column {new_name} already in dict\")\n\n            _, col, old_names = _try_convert_dates(\n                converter, colspec, data_dict, orig_names\n            )\n\n            new_data[new_name] = col\n\n            # If original column can be converted to date we keep the converted values\n            # This can only happen if values are from single column\n            if len(colspec) == 1:\n                new_data[colspec[0]] = col\n\n            new_cols.append(new_name)\n            date_cols.update(old_names)\n\n    data_dict.update(new_data)\n    new_cols.extend(columns)\n\n    if not keep_date_col:\n        for c in list(date_cols):\n            data_dict.pop(c)\n            new_cols.remove(c)\n\n    return data_dict, new_cols\n\n\ndef _try_convert_dates(parser: Callable, colspec, data_dict, columns):\n    colset = set(columns)\n    colnames = []\n\n    for c in colspec:\n        if c in colset:\n            colnames.append(c)\n        elif isinstance(c, int) and c not in columns:\n            colnames.append(columns[c])\n        else:\n            colnames.append(c)\n\n    new_name: tuple | str\n    if all(isinstance(x, tuple) for x in colnames):\n        new_name = tuple(map(\"_\".join, zip(*colnames)))\n    else:\n        new_name = \"_\".join([str(x) for x in colnames])\n    to_parse = [np.asarray(data_dict[c]) for c in colnames if c in data_dict]\n\n    new_col = parser(*to_parse)\n    return new_name, new_col, colnames\n\n\ndef _get_na_values(col, na_values, na_fvalues, keep_default_na):\n    \"\"\"\n    Get the NaN values for a given column.\n\n    Parameters\n    ----------\n    col : str\n        The name of the column.\n    na_values : array-like, dict\n        The object listing the NaN values as strings.\n    na_fvalues : array-like, dict\n        The object listing the NaN values as floats.\n    keep_default_na : bool\n        If `na_values` is a dict, and the column is not mapped in the\n        dictionary, whether to return the default NaN values or the empty set.\n\n    Returns\n    -------\n    nan_tuple : A length-two tuple composed of\n\n        1) na_values : the string NaN values for that column.\n        2) na_fvalues : the float NaN values for that column.\n    \"\"\"\n    if isinstance(na_values, dict):\n        if col in na_values:\n            return na_values[col], na_fvalues[col]\n        else:\n            if keep_default_na:\n                return STR_NA_VALUES, set()\n\n            return set(), set()\n    else:\n        return na_values, na_fvalues\n\n\ndef _validate_parse_dates_arg(parse_dates):\n    \"\"\"\n    Check whether or not the 'parse_dates' parameter\n    is a non-boolean scalar. Raises a ValueError if\n    that is the case.\n    \"\"\"\n    msg = (\n        \"Only booleans, lists, and dictionaries are accepted \"\n        \"for the 'parse_dates' parameter\"\n    )\n\n    if parse_dates is not None:\n        if is_scalar(parse_dates):\n            if not lib.is_bool(parse_dates):\n                raise TypeError(msg)\n\n        elif not isinstance(parse_dates, (list, dict)):\n            raise TypeError(msg)\n\n    return parse_dates\n\n\ndef is_index_col(col) -> bool:\n    return col is not None and col is not False\n"
    },
    {
      "filename": "pandas/io/parsers/readers.py",
      "content": "\"\"\"\nModule contains tools for processing files into DataFrames or other objects\n\nGH#48849 provides a convenient way of deprecating keyword arguments\n\"\"\"\nfrom __future__ import annotations\n\nfrom collections import abc\nimport csv\nimport sys\nfrom textwrap import fill\nfrom types import TracebackType\nfrom typing import (\n    IO,\n    Any,\n    Callable,\n    Hashable,\n    Literal,\n    NamedTuple,\n    Sequence,\n    overload,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config import using_nullable_dtypes\n\nfrom pandas._libs import lib\nfrom pandas._libs.parsers import STR_NA_VALUES\nfrom pandas._typing import (\n    CompressionOptions,\n    CSVEngine,\n    DtypeArg,\n    FilePath,\n    IndexLabel,\n    ReadCsvBuffer,\n    StorageOptions,\n)\nfrom pandas.errors import (\n    AbstractMethodError,\n    ParserWarning,\n)\nfrom pandas.util._decorators import Appender\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.common import (\n    is_file_like,\n    is_float,\n    is_integer,\n    is_list_like,\n)\n\nfrom pandas.core.frame import DataFrame\nfrom pandas.core.indexes.api import RangeIndex\nfrom pandas.core.shared_docs import _shared_docs\n\nfrom pandas.io.common import (\n    IOHandles,\n    get_handle,\n    stringify_path,\n    validate_header_arg,\n)\nfrom pandas.io.parsers.arrow_parser_wrapper import ArrowParserWrapper\nfrom pandas.io.parsers.base_parser import (\n    ParserBase,\n    is_index_col,\n    parser_defaults,\n)\nfrom pandas.io.parsers.c_parser_wrapper import CParserWrapper\nfrom pandas.io.parsers.python_parser import (\n    FixedWidthFieldParser,\n    PythonParser,\n)\n\n_doc_read_csv_and_table = (\n    r\"\"\"\n{summary}\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\nParameters\n----------\nfilepath_or_buffer : str, path object or file-like object\n    Any valid string path is acceptable. The string could be a URL. Valid\n    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n    expected. A local file could be: file://localhost/path/to/table.csv.\n\n    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\n    By file-like object, we refer to objects with a ``read()`` method, such as\n    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\nsep : str, default {_default_sep}\n    Delimiter to use. If sep is None, the C engine cannot automatically detect\n    the separator, but the Python parsing engine can, meaning the latter will\n    be used and automatically detect the separator by Python's builtin sniffer\n    tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n    different from ``'\\s+'`` will be interpreted as regular expressions and\n    will also force the use of the Python parsing engine. Note that regex\n    delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\ndelimiter : str, default ``None``\n    Alias for sep.\nheader : int, list of int, None, default 'infer'\n    Row number(s) to use as the column names, and the start of the\n    data.  Default behavior is to infer the column names: if no names\n    are passed the behavior is identical to ``header=0`` and column\n    names are inferred from the first line of the file, if column\n    names are passed explicitly then the behavior is identical to\n    ``header=None``. Explicitly pass ``header=0`` to be able to\n    replace existing names. The header can be a list of integers that\n    specify row locations for a multi-index on the columns\n    e.g. [0,1,3]. Intervening rows that are not specified will be\n    skipped (e.g. 2 in this example is skipped). Note that this\n    parameter ignores commented lines and empty lines if\n    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n    data rather than the first line of the file.\nnames : array-like, optional\n    List of column names to use. If the file contains a header row,\n    then you should explicitly pass ``header=0`` to override the column names.\n    Duplicates in this list are not allowed.\nindex_col : int, str, sequence of int / str, or False, optional, default ``None``\n  Column(s) to use as the row labels of the ``DataFrame``, either given as\n  string name or column index. If a sequence of int / str is given, a\n  MultiIndex is used.\n\n  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n  column as the index, e.g. when you have a malformed file with delimiters at\n  the end of each line.\nusecols : list-like or callable, optional\n    Return a subset of the columns. If list-like, all elements must either\n    be positional (i.e. integer indices into the document columns) or strings\n    that correspond to column names provided either by the user in `names` or\n    inferred from the document header row(s). If ``names`` are given, the document\n    header row(s) are not taken into account. For example, a valid list-like\n    `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n    To instantiate a DataFrame from ``data`` with element order preserved use\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n    in ``['foo', 'bar']`` order or\n    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n    for ``['bar', 'foo']`` order.\n\n    If callable, the callable function will be evaluated against the column\n    names, returning names where the callable function evaluates to True. An\n    example of a valid callable argument would be ``lambda x: x.upper() in\n    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n    parsing time and lower memory usage.\ndtype : Type name or dict of column -> type, optional\n    Data type for data or columns. E.g. {{'a': np.float64, 'b': np.int32,\n    'c': 'Int64'}}\n    Use `str` or `object` together with suitable `na_values` settings\n    to preserve and not interpret dtype.\n    If converters are specified, they will be applied INSTEAD\n    of dtype conversion.\n\n    .. versionadded:: 1.5.0\n\n        Support for defaultdict was added. Specify a defaultdict as input where\n        the default determines the dtype of the columns which are not explicitly\n        listed.\nengine : {{'c', 'python', 'pyarrow'}}, optional\n    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n    is currently more feature-complete. Multithreading is currently only supported by\n    the pyarrow engine.\n\n    .. versionadded:: 1.4.0\n\n        The \"pyarrow\" engine was added as an *experimental* engine, and some features\n        are unsupported, or may not work correctly, with this engine.\nconverters : dict, optional\n    Dict of functions for converting values in certain columns. Keys can either\n    be integers or column labels.\ntrue_values : list, optional\n    Values to consider as True in addition to case-insensitive variants of \"True\".\nfalse_values : list, optional\n    Values to consider as False in addition to case-insensitive variants of \"False\".\nskipinitialspace : bool, default False\n    Skip spaces after delimiter.\nskiprows : list-like, int or callable, optional\n    Line numbers to skip (0-indexed) or number of lines to skip (int)\n    at the start of the file.\n\n    If callable, the callable function will be evaluated against the row\n    indices, returning True if the row should be skipped and False otherwise.\n    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\nskipfooter : int, default 0\n    Number of lines at bottom of file to skip (Unsupported with engine='c').\nnrows : int, optional\n    Number of rows of file to read. Useful for reading pieces of large files.\nna_values : scalar, str, list-like, or dict, optional\n    Additional strings to recognize as NA/NaN. If dict passed, specific\n    per-column NA values.  By default the following values are interpreted as\n    NaN: '\"\"\"\n    + fill(\"', '\".join(sorted(STR_NA_VALUES)), 70, subsequent_indent=\"    \")\n    + \"\"\"'.\nkeep_default_na : bool, default True\n    Whether or not to include the default NaN values when parsing the data.\n    Depending on whether `na_values` is passed in, the behavior is as follows:\n\n    * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n      is appended to the default NaN values used for parsing.\n    * If `keep_default_na` is True, and `na_values` are not specified, only\n      the default NaN values are used for parsing.\n    * If `keep_default_na` is False, and `na_values` are specified, only\n      the NaN values specified `na_values` are used for parsing.\n    * If `keep_default_na` is False, and `na_values` are not specified, no\n      strings will be parsed as NaN.\n\n    Note that if `na_filter` is passed in as False, the `keep_default_na` and\n    `na_values` parameters will be ignored.\nna_filter : bool, default True\n    Detect missing value markers (empty strings and the value of na_values). In\n    data without any NAs, passing na_filter=False can improve the performance\n    of reading a large file.\nverbose : bool, default False\n    Indicate number of NA values placed in non-numeric columns.\nskip_blank_lines : bool, default True\n    If True, skip over blank lines rather than interpreting as NaN values.\nparse_dates : bool or list of int or names or list of lists or dict, \\\ndefault False\n    The behavior is as follows:\n\n    * boolean. If True -> try parsing the index.\n    * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n      each as a separate date column.\n    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n      a single date column.\n    * dict, e.g. {{'foo' : [1, 3]}} -> parse columns 1, 3 as date and call\n      result 'foo'\n\n    If a column or index cannot be represented as an array of datetimes,\n    say because of an unparsable value or a mixture of timezones, the column\n    or index will be returned unaltered as an object data type. For\n    non-standard datetime parsing, use ``pd.to_datetime`` after\n    ``pd.read_csv``.\n\n    Note: A fast-path exists for iso8601-formatted dates.\ninfer_datetime_format : bool, default False\n    If True and `parse_dates` is enabled, pandas will attempt to infer the\n    format of the datetime strings in the columns, and if it can be inferred,\n    switch to a faster method of parsing them. In some cases this can increase\n    the parsing speed by 5-10x.\n\n    .. deprecated:: 2.0.0\n        A strict version of this argument is now the default, passing it has no effect.\n\nkeep_date_col : bool, default False\n    If True and `parse_dates` specifies combining multiple columns then\n    keep the original columns.\ndate_parser : function, optional\n    Function to use for converting a sequence of string columns to an array of\n    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n    conversion. Pandas will try to call `date_parser` in three different ways,\n    advancing to the next if an exception occurs: 1) Pass one or more arrays\n    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n    string values from the columns defined by `parse_dates` into a single array\n    and pass that; and 3) call `date_parser` once for each row using one or\n    more strings (corresponding to the columns defined by `parse_dates`) as\n    arguments.\n\n  .. deprecated:: 2.0.0\n   Use ``date_format`` instead, or read in as ``object`` and then apply\n   :func:`to_datetime` as-needed.\ndate_format : str, default ``None``\n   If used in conjunction with ``parse_dates``, will parse dates according to this\n   format. For anything more complex (e.g. different formats for different columns),\n   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n\n    .. versionadded:: 2.0.0\ndayfirst : bool, default False\n    DD/MM format dates, international and European format.\ncache_dates : bool, default True\n    If True, use a cache of unique, converted dates to apply the datetime\n    conversion. May produce significant speed-up when parsing duplicate\n    date strings, especially ones with timezone offsets.\n\niterator : bool, default False\n    Return TextFileReader object for iteration or getting chunks with\n    ``get_chunk()``.\n\n    .. versionchanged:: 1.2\n\n       ``TextFileReader`` is a context manager.\nchunksize : int, optional\n    Return TextFileReader object for iteration.\n    See the `IO Tools docs\n    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n    for more information on ``iterator`` and ``chunksize``.\n\n    .. versionchanged:: 1.2\n\n       ``TextFileReader`` is a context manager.\n{decompression_options}\n\n    .. versionchanged:: 1.4.0 Zstandard support.\n\nthousands : str, optional\n    Thousands separator.\ndecimal : str, default '.'\n    Character to recognize as decimal point (e.g. use ',' for European data).\nlineterminator : str (length 1), optional\n    Character to break file into lines. Only valid with C parser.\nquotechar : str (length 1), optional\n    The character used to denote the start and end of a quoted item. Quoted\n    items can include the delimiter and it will be ignored.\nquoting : int or csv.QUOTE_* instance, default 0\n    Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n    QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\ndoublequote : bool, default ``True``\n   When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n   whether or not to interpret two consecutive quotechar elements INSIDE a\n   field as a single ``quotechar`` element.\nescapechar : str (length 1), optional\n    One-character string used to escape other characters.\ncomment : str, optional\n    Indicates remainder of line should not be parsed. If found at the beginning\n    of a line, the line will be ignored altogether. This parameter must be a\n    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n    fully commented lines are ignored by the parameter `header` but not by\n    `skiprows`. For example, if ``comment='#'``, parsing\n    ``#empty\\\\na,b,c\\\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n    treated as the header.\nencoding : str, optional, default \"utf-8\"\n    Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n    standard encodings\n    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n\n    .. versionchanged:: 1.2\n\n       When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n       ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n       This behavior was previously only the case for ``engine=\"python\"``.\n\n    .. versionchanged:: 1.3.0\n\n       ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n       influence on how encoding errors are handled.\n\nencoding_errors : str, optional, default \"strict\"\n    How encoding errors are treated. `List of possible values\n    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n\n    .. versionadded:: 1.3.0\n\ndialect : str or csv.Dialect, optional\n    If provided, this parameter will override values (default or not) for the\n    following parameters: `delimiter`, `doublequote`, `escapechar`,\n    `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n    override values, a ParserWarning will be issued. See csv.Dialect\n    documentation for more details.\non_bad_lines : {{'error', 'warn', 'skip'}} or callable, default 'error'\n    Specifies what to do upon encountering a bad line (a line with too many fields).\n    Allowed values are :\n\n        - 'error', raise an Exception when a bad line is encountered.\n        - 'warn', raise a warning when a bad line is encountered and skip that line.\n        - 'skip', skip bad lines without raising or warning when they are encountered.\n\n    .. versionadded:: 1.3.0\n\n    .. versionadded:: 1.4.0\n\n        - callable, function with signature\n          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n          If the function returns ``None``, the bad line will be ignored.\n          If the function returns a new list of strings with more elements than\n          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n          Only supported when ``engine=\"python\"``\n\ndelim_whitespace : bool, default False\n    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n    used as the sep. Equivalent to setting ``sep='\\\\s+'``. If this option\n    is set to True, nothing should be passed in for the ``delimiter``\n    parameter.\nlow_memory : bool, default True\n    Internally process the file in chunks, resulting in lower memory use\n    while parsing, but possibly mixed type inference.  To ensure no mixed\n    types either set False, or specify the type with the `dtype` parameter.\n    Note that the entire file is read into a single DataFrame regardless,\n    use the `chunksize` or `iterator` parameter to return the data in chunks.\n    (Only valid with C parser).\nmemory_map : bool, default False\n    If a filepath is provided for `filepath_or_buffer`, map the file object\n    directly onto memory and access the data directly from there. Using this\n    option can improve performance because there is no longer any I/O overhead.\nfloat_precision : str, optional\n    Specifies which converter the C engine should use for floating-point\n    values. The options are ``None`` or 'high' for the ordinary converter,\n    'legacy' for the original lower precision pandas converter, and\n    'round_trip' for the round-trip converter.\n\n    .. versionchanged:: 1.2\n\n{storage_options}\n\n    .. versionadded:: 1.2\n\nuse_nullable_dtypes : bool = False\n    Whether or not to use nullable dtypes as default when reading data. If\n    set to True, nullable dtypes are used for all dtypes that have a nullable\n    implementation, even if no nulls are present.\n\n    .. note::\n\n        The nullable dtype implementation can be configured by calling\n        ``pd.set_option(\"mode.dtype_backend\", \"pandas\")`` to use\n        numpy-backed nullable dtypes or\n        ``pd.set_option(\"mode.dtype_backend\", \"pyarrow\")`` to use\n        pyarrow-backed nullable dtypes (using ``pd.ArrowDtype``).\n\n    .. versionadded:: 2.0\n\nReturns\n-------\nDataFrame or TextFileReader\n    A comma-separated values (csv) file is returned as two-dimensional\n    data structure with labeled axes.\n\nSee Also\n--------\nDataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_fwf : Read a table of fixed-width formatted lines into DataFrame.\n\nExamples\n--------\n>>> pd.{func_name}('data.csv')  # doctest: +SKIP\n\"\"\"\n)\n\n\n_c_parser_defaults = {\n    \"delim_whitespace\": False,\n    \"na_filter\": True,\n    \"low_memory\": True,\n    \"memory_map\": False,\n    \"float_precision\": None,\n}\n\n_fwf_defaults = {\"colspecs\": \"infer\", \"infer_nrows\": 100, \"widths\": None}\n\n_c_unsupported = {\"skipfooter\"}\n_python_unsupported = {\"low_memory\", \"float_precision\"}\n_pyarrow_unsupported = {\n    \"skipfooter\",\n    \"float_precision\",\n    \"chunksize\",\n    \"comment\",\n    \"nrows\",\n    \"thousands\",\n    \"memory_map\",\n    \"dialect\",\n    \"on_bad_lines\",\n    \"delim_whitespace\",\n    \"quoting\",\n    \"lineterminator\",\n    \"converters\",\n    \"decimal\",\n    \"iterator\",\n    \"dayfirst\",\n    \"verbose\",\n    \"skipinitialspace\",\n    \"low_memory\",\n}\n\n\nclass _DeprecationConfig(NamedTuple):\n    default_value: Any\n    msg: str | None\n\n\n@overload\ndef validate_integer(name, val: None, min_val: int = ...) -> None:\n    ...\n\n\n@overload\ndef validate_integer(name, val: float, min_val: int = ...) -> int:\n    ...\n\n\n@overload\ndef validate_integer(name, val: int | None, min_val: int = ...) -> int | None:\n    ...\n\n\ndef validate_integer(name, val: int | float | None, min_val: int = 0) -> int | None:\n    \"\"\"\n    Checks whether the 'name' parameter for parsing is either\n    an integer OR float that can SAFELY be cast to an integer\n    without losing accuracy. Raises a ValueError if that is\n    not the case.\n\n    Parameters\n    ----------\n    name : str\n        Parameter name (used for error reporting)\n    val : int or float\n        The value to check\n    min_val : int\n        Minimum allowed value (val < min_val will result in a ValueError)\n    \"\"\"\n    if val is None:\n        return val\n\n    msg = f\"'{name:s}' must be an integer >={min_val:d}\"\n    if is_float(val):\n        if int(val) != val:\n            raise ValueError(msg)\n        val = int(val)\n    elif not (is_integer(val) and val >= min_val):\n        raise ValueError(msg)\n\n    return int(val)\n\n\ndef _validate_names(names: Sequence[Hashable] | None) -> None:\n    \"\"\"\n    Raise ValueError if the `names` parameter contains duplicates or has an\n    invalid data type.\n\n    Parameters\n    ----------\n    names : array-like or None\n        An array containing a list of the names used for the output DataFrame.\n\n    Raises\n    ------\n    ValueError\n        If names are not unique or are not ordered (e.g. set).\n    \"\"\"\n    if names is not None:\n        if len(names) != len(set(names)):\n            raise ValueError(\"Duplicate names are not allowed.\")\n        if not (\n            is_list_like(names, allow_sets=False) or isinstance(names, abc.KeysView)\n        ):\n            raise ValueError(\"Names should be an ordered collection.\")\n\n\ndef _read(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str], kwds\n) -> DataFrame | TextFileReader:\n    \"\"\"Generic reader of line files.\"\"\"\n    # if we pass a date_parser and parse_dates=False, we should not parse the\n    # dates GH#44366\n    if kwds.get(\"parse_dates\", None) is None:\n        if (\n            kwds.get(\"date_parser\", lib.no_default) is lib.no_default\n            and kwds.get(\"date_format\", None) is None\n        ):\n            kwds[\"parse_dates\"] = False\n        else:\n            kwds[\"parse_dates\"] = True\n\n    # Extract some of the arguments (pass chunksize on).\n    iterator = kwds.get(\"iterator\", False)\n    chunksize = kwds.get(\"chunksize\", None)\n    if kwds.get(\"engine\") == \"pyarrow\":\n        if iterator:\n            raise ValueError(\n                \"The 'iterator' option is not supported with the 'pyarrow' engine\"\n            )\n\n        if chunksize is not None:\n            raise ValueError(\n                \"The 'chunksize' option is not supported with the 'pyarrow' engine\"\n            )\n    else:\n        chunksize = validate_integer(\"chunksize\", chunksize, 1)\n\n    nrows = kwds.get(\"nrows\", None)\n\n    # Check for duplicates in names.\n    _validate_names(kwds.get(\"names\", None))\n\n    # Create the parser.\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n\n    if chunksize or iterator:\n        return parser\n\n    with parser:\n        return parser.read(nrows)\n\n\n# iterator=True -> TextFileReader\n@overload\ndef read_csv(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = ...,\n    delimiter: str | None | lib.NoDefault = ...,\n    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n    index_col: IndexLabel | Literal[False] | None = ...,\n    usecols=...,\n    dtype: DtypeArg | None = ...,\n    engine: CSVEngine | None = ...,\n    converters=...,\n    true_values=...,\n    false_values=...,\n    skipinitialspace: bool = ...,\n    skiprows=...,\n    skipfooter: int = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    skip_blank_lines: bool = ...,\n    parse_dates: bool | Sequence[Hashable] | None = ...,\n    infer_datetime_format: bool | lib.NoDefault = ...,\n    keep_date_col: bool = ...,\n    date_parser=...,\n    date_format: str | None = ...,\n    dayfirst: bool = ...,\n    cache_dates: bool = ...,\n    iterator: Literal[True],\n    chunksize: int | None = ...,\n    compression: CompressionOptions = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    lineterminator: str | None = ...,\n    quotechar: str = ...,\n    quoting: int = ...,\n    doublequote: bool = ...,\n    escapechar: str | None = ...,\n    comment: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    dialect: str | csv.Dialect | None = ...,\n    on_bad_lines=...,\n    delim_whitespace: bool = ...,\n    low_memory=...,\n    memory_map: bool = ...,\n    float_precision: Literal[\"high\", \"legacy\"] | None = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> TextFileReader:\n    ...\n\n\n# chunksize=int -> TextFileReader\n@overload\ndef read_csv(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = ...,\n    delimiter: str | None | lib.NoDefault = ...,\n    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n    index_col: IndexLabel | Literal[False] | None = ...,\n    usecols=...,\n    dtype: DtypeArg | None = ...,\n    engine: CSVEngine | None = ...,\n    converters=...,\n    true_values=...,\n    false_values=...,\n    skipinitialspace: bool = ...,\n    skiprows=...,\n    skipfooter: int = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    skip_blank_lines: bool = ...,\n    parse_dates: bool | Sequence[Hashable] | None = ...,\n    infer_datetime_format: bool | lib.NoDefault = ...,\n    keep_date_col: bool = ...,\n    date_parser=...,\n    date_format: str | None = ...,\n    dayfirst: bool = ...,\n    cache_dates: bool = ...,\n    iterator: bool = ...,\n    chunksize: int,\n    compression: CompressionOptions = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    lineterminator: str | None = ...,\n    quotechar: str = ...,\n    quoting: int = ...,\n    doublequote: bool = ...,\n    escapechar: str | None = ...,\n    comment: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    dialect: str | csv.Dialect | None = ...,\n    on_bad_lines=...,\n    delim_whitespace: bool = ...,\n    low_memory=...,\n    memory_map: bool = ...,\n    float_precision: Literal[\"high\", \"legacy\"] | None = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> TextFileReader:\n    ...\n\n\n# default case -> DataFrame\n@overload\ndef read_csv(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = ...,\n    delimiter: str | None | lib.NoDefault = ...,\n    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n    index_col: IndexLabel | Literal[False] | None = ...,\n    usecols=...,\n    dtype: DtypeArg | None = ...,\n    engine: CSVEngine | None = ...,\n    converters=...,\n    true_values=...,\n    false_values=...,\n    skipinitialspace: bool = ...,\n    skiprows=...,\n    skipfooter: int = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    skip_blank_lines: bool = ...,\n    parse_dates: bool | Sequence[Hashable] | None = ...,\n    infer_datetime_format: bool | lib.NoDefault = ...,\n    keep_date_col: bool = ...,\n    date_parser=...,\n    date_format: str | None = ...,\n    dayfirst: bool = ...,\n    cache_dates: bool = ...,\n    iterator: Literal[False] = ...,\n    chunksize: None = ...,\n    compression: CompressionOptions = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    lineterminator: str | None = ...,\n    quotechar: str = ...,\n    quoting: int = ...,\n    doublequote: bool = ...,\n    escapechar: str | None = ...,\n    comment: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    dialect: str | csv.Dialect | None = ...,\n    on_bad_lines=...,\n    delim_whitespace: bool = ...,\n    low_memory=...,\n    memory_map: bool = ...,\n    float_precision: Literal[\"high\", \"legacy\"] | None = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> DataFrame:\n    ...\n\n\n# Unions -> DataFrame | TextFileReader\n@overload\ndef read_csv(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = ...,\n    delimiter: str | None | lib.NoDefault = ...,\n    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n    index_col: IndexLabel | Literal[False] | None = ...,\n    usecols=...,\n    dtype: DtypeArg | None = ...,\n    engine: CSVEngine | None = ...,\n    converters=...,\n    true_values=...,\n    false_values=...,\n    skipinitialspace: bool = ...,\n    skiprows=...,\n    skipfooter: int = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    skip_blank_lines: bool = ...,\n    parse_dates: bool | Sequence[Hashable] | None = ...,\n    infer_datetime_format: bool | lib.NoDefault = ...,\n    keep_date_col: bool = ...,\n    date_parser=...,\n    date_format: str | None = ...,\n    dayfirst: bool = ...,\n    cache_dates: bool = ...,\n    iterator: bool = ...,\n    chunksize: int | None = ...,\n    compression: CompressionOptions = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    lineterminator: str | None = ...,\n    quotechar: str = ...,\n    quoting: int = ...,\n    doublequote: bool = ...,\n    escapechar: str | None = ...,\n    comment: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    dialect: str | csv.Dialect | None = ...,\n    on_bad_lines=...,\n    delim_whitespace: bool = ...,\n    low_memory=...,\n    memory_map: bool = ...,\n    float_precision: Literal[\"high\", \"legacy\"] | None = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> DataFrame | TextFileReader:\n    ...\n\n\n@Appender(\n    _doc_read_csv_and_table.format(\n        func_name=\"read_csv\",\n        summary=\"Read a comma-separated values (csv) file into DataFrame.\",\n        _default_sep=\"','\",\n        storage_options=_shared_docs[\"storage_options\"],\n        decompression_options=_shared_docs[\"decompression_options\"]\n        % \"filepath_or_buffer\",\n    )\n)\ndef read_csv(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = lib.no_default,\n    delimiter: str | None | lib.NoDefault = None,\n    # Column and Index Locations and Names\n    header: int | Sequence[int] | None | Literal[\"infer\"] = \"infer\",\n    names: Sequence[Hashable] | None | lib.NoDefault = lib.no_default,\n    index_col: IndexLabel | Literal[False] | None = None,\n    usecols=None,\n    # General Parsing Configuration\n    dtype: DtypeArg | None = None,\n    engine: CSVEngine | None = None,\n    converters=None,\n    true_values=None,\n    false_values=None,\n    skipinitialspace: bool = False,\n    skiprows=None,\n    skipfooter: int = 0,\n    nrows: int | None = None,\n    # NA and Missing Data Handling\n    na_values=None,\n    keep_default_na: bool = True,\n    na_filter: bool = True,\n    verbose: bool = False,\n    skip_blank_lines: bool = True,\n    # Datetime Handling\n    parse_dates: bool | Sequence[Hashable] | None = None,\n    infer_datetime_format: bool | lib.NoDefault = lib.no_default,\n    keep_date_col: bool = False,\n    date_parser=lib.no_default,\n    date_format: str | None = None,\n    dayfirst: bool = False,\n    cache_dates: bool = True,\n    # Iteration\n    iterator: bool = False,\n    chunksize: int | None = None,\n    # Quoting, Compression, and File Format\n    compression: CompressionOptions = \"infer\",\n    thousands: str | None = None,\n    decimal: str = \".\",\n    lineterminator: str | None = None,\n    quotechar: str = '\"',\n    quoting: int = csv.QUOTE_MINIMAL,\n    doublequote: bool = True,\n    escapechar: str | None = None,\n    comment: str | None = None,\n    encoding: str | None = None,\n    encoding_errors: str | None = \"strict\",\n    dialect: str | csv.Dialect | None = None,\n    # Error Handling\n    on_bad_lines: str = \"error\",\n    # Internal\n    delim_whitespace: bool = False,\n    low_memory=_c_parser_defaults[\"low_memory\"],\n    memory_map: bool = False,\n    float_precision: Literal[\"high\", \"legacy\"] | None = None,\n    storage_options: StorageOptions = None,\n    use_nullable_dtypes: bool | lib.NoDefault = lib.no_default,\n) -> DataFrame | TextFileReader:\n    if infer_datetime_format is not lib.no_default:\n        warnings.warn(\n            \"The argument 'infer_datetime_format' is deprecated and will \"\n            \"be removed in a future version. \"\n            \"A strict version of it is now the default, see \"\n            \"https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. \"\n            \"You can safely remove this argument.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n    # locals() should never be modified\n    kwds = locals().copy()\n    del kwds[\"filepath_or_buffer\"]\n    del kwds[\"sep\"]\n\n    kwds_defaults = _refine_defaults_read(\n        dialect,\n        delimiter,\n        delim_whitespace,\n        engine,\n        sep,\n        on_bad_lines,\n        names,\n        defaults={\"delimiter\": \",\"},\n        use_nullable_dtypes=use_nullable_dtypes,\n    )\n    kwds.update(kwds_defaults)\n\n    return _read(filepath_or_buffer, kwds)\n\n\n# iterator=True -> TextFileReader\n@overload\ndef read_table(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = ...,\n    delimiter: str | None | lib.NoDefault = ...,\n    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n    index_col: IndexLabel | Literal[False] | None = ...,\n    usecols=...,\n    dtype: DtypeArg | None = ...,\n    engine: CSVEngine | None = ...,\n    converters=...,\n    true_values=...,\n    false_values=...,\n    skipinitialspace: bool = ...,\n    skiprows=...,\n    skipfooter: int = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    skip_blank_lines: bool = ...,\n    parse_dates: bool | Sequence[Hashable] = ...,\n    infer_datetime_format: bool | lib.NoDefault = ...,\n    keep_date_col: bool = ...,\n    date_parser=...,\n    date_format: str | None = ...,\n    dayfirst: bool = ...,\n    cache_dates: bool = ...,\n    iterator: Literal[True],\n    chunksize: int | None = ...,\n    compression: CompressionOptions = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    lineterminator: str | None = ...,\n    quotechar: str = ...,\n    quoting: int = ...,\n    doublequote: bool = ...,\n    escapechar: str | None = ...,\n    comment: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    dialect: str | csv.Dialect | None = ...,\n    on_bad_lines=...,\n    delim_whitespace: bool = ...,\n    low_memory=...,\n    memory_map: bool = ...,\n    float_precision: str | None = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> TextFileReader:\n    ...\n\n\n# chunksize=int -> TextFileReader\n@overload\ndef read_table(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = ...,\n    delimiter: str | None | lib.NoDefault = ...,\n    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n    index_col: IndexLabel | Literal[False] | None = ...,\n    usecols=...,\n    dtype: DtypeArg | None = ...,\n    engine: CSVEngine | None = ...,\n    converters=...,\n    true_values=...,\n    false_values=...,\n    skipinitialspace: bool = ...,\n    skiprows=...,\n    skipfooter: int = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    skip_blank_lines: bool = ...,\n    parse_dates: bool | Sequence[Hashable] = ...,\n    infer_datetime_format: bool | lib.NoDefault = ...,\n    keep_date_col: bool = ...,\n    date_parser=...,\n    date_format: str | None = ...,\n    dayfirst: bool = ...,\n    cache_dates: bool = ...,\n    iterator: bool = ...,\n    chunksize: int,\n    compression: CompressionOptions = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    lineterminator: str | None = ...,\n    quotechar: str = ...,\n    quoting: int = ...,\n    doublequote: bool = ...,\n    escapechar: str | None = ...,\n    comment: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    dialect: str | csv.Dialect | None = ...,\n    on_bad_lines=...,\n    delim_whitespace: bool = ...,\n    low_memory=...,\n    memory_map: bool = ...,\n    float_precision: str | None = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> TextFileReader:\n    ...\n\n\n# default -> DataFrame\n@overload\ndef read_table(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = ...,\n    delimiter: str | None | lib.NoDefault = ...,\n    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n    index_col: IndexLabel | Literal[False] | None = ...,\n    usecols=...,\n    dtype: DtypeArg | None = ...,\n    engine: CSVEngine | None = ...,\n    converters=...,\n    true_values=...,\n    false_values=...,\n    skipinitialspace: bool = ...,\n    skiprows=...,\n    skipfooter: int = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    skip_blank_lines: bool = ...,\n    parse_dates: bool | Sequence[Hashable] = ...,\n    infer_datetime_format: bool | lib.NoDefault = ...,\n    keep_date_col: bool = ...,\n    date_parser=...,\n    date_format: str | None = ...,\n    dayfirst: bool = ...,\n    cache_dates: bool = ...,\n    iterator: Literal[False] = ...,\n    chunksize: None = ...,\n    compression: CompressionOptions = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    lineterminator: str | None = ...,\n    quotechar: str = ...,\n    quoting: int = ...,\n    doublequote: bool = ...,\n    escapechar: str | None = ...,\n    comment: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    dialect: str | csv.Dialect | None = ...,\n    on_bad_lines=...,\n    delim_whitespace: bool = ...,\n    low_memory=...,\n    memory_map: bool = ...,\n    float_precision: str | None = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> DataFrame:\n    ...\n\n\n# Unions -> DataFrame | TextFileReader\n@overload\ndef read_table(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = ...,\n    delimiter: str | None | lib.NoDefault = ...,\n    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n    index_col: IndexLabel | Literal[False] | None = ...,\n    usecols=...,\n    dtype: DtypeArg | None = ...,\n    engine: CSVEngine | None = ...,\n    converters=...,\n    true_values=...,\n    false_values=...,\n    skipinitialspace: bool = ...,\n    skiprows=...,\n    skipfooter: int = ...,\n    nrows: int | None = ...,\n    na_values=...,\n    keep_default_na: bool = ...,\n    na_filter: bool = ...,\n    verbose: bool = ...,\n    skip_blank_lines: bool = ...,\n    parse_dates: bool | Sequence[Hashable] = ...,\n    infer_datetime_format: bool | lib.NoDefault = ...,\n    keep_date_col: bool = ...,\n    date_parser=...,\n    date_format: str | None = ...,\n    dayfirst: bool = ...,\n    cache_dates: bool = ...,\n    iterator: bool = ...,\n    chunksize: int | None = ...,\n    compression: CompressionOptions = ...,\n    thousands: str | None = ...,\n    decimal: str = ...,\n    lineterminator: str | None = ...,\n    quotechar: str = ...,\n    quoting: int = ...,\n    doublequote: bool = ...,\n    escapechar: str | None = ...,\n    comment: str | None = ...,\n    encoding: str | None = ...,\n    encoding_errors: str | None = ...,\n    dialect: str | csv.Dialect | None = ...,\n    on_bad_lines=...,\n    delim_whitespace: bool = ...,\n    low_memory=...,\n    memory_map: bool = ...,\n    float_precision: str | None = ...,\n    storage_options: StorageOptions = ...,\n    use_nullable_dtypes: bool | lib.NoDefault = ...,\n) -> DataFrame | TextFileReader:\n    ...\n\n\n@Appender(\n    _doc_read_csv_and_table.format(\n        func_name=\"read_table\",\n        summary=\"Read general delimited file into DataFrame.\",\n        _default_sep=r\"'\\\\t' (tab-stop)\",\n        storage_options=_shared_docs[\"storage_options\"],\n        decompression_options=_shared_docs[\"decompression_options\"]\n        % \"filepath_or_buffer\",\n    )\n)\ndef read_table(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    sep: str | None | lib.NoDefault = lib.no_default,\n    delimiter: str | None | lib.NoDefault = None,\n    # Column and Index Locations and Names\n    header: int | Sequence[int] | None | Literal[\"infer\"] = \"infer\",\n    names: Sequence[Hashable] | None | lib.NoDefault = lib.no_default,\n    index_col: IndexLabel | Literal[False] | None = None,\n    usecols=None,\n    # General Parsing Configuration\n    dtype: DtypeArg | None = None,\n    engine: CSVEngine | None = None,\n    converters=None,\n    true_values=None,\n    false_values=None,\n    skipinitialspace: bool = False,\n    skiprows=None,\n    skipfooter: int = 0,\n    nrows: int | None = None,\n    # NA and Missing Data Handling\n    na_values=None,\n    keep_default_na: bool = True,\n    na_filter: bool = True,\n    verbose: bool = False,\n    skip_blank_lines: bool = True,\n    # Datetime Handling\n    parse_dates: bool | Sequence[Hashable] = False,\n    infer_datetime_format: bool | lib.NoDefault = lib.no_default,\n    keep_date_col: bool = False,\n    date_parser=lib.no_default,\n    date_format: str | None = None,\n    dayfirst: bool = False,\n    cache_dates: bool = True,\n    # Iteration\n    iterator: bool = False,\n    chunksize: int | None = None,\n    # Quoting, Compression, and File Format\n    compression: CompressionOptions = \"infer\",\n    thousands: str | None = None,\n    decimal: str = \".\",\n    lineterminator: str | None = None,\n    quotechar: str = '\"',\n    quoting: int = csv.QUOTE_MINIMAL,\n    doublequote: bool = True,\n    escapechar: str | None = None,\n    comment: str | None = None,\n    encoding: str | None = None,\n    encoding_errors: str | None = \"strict\",\n    dialect: str | csv.Dialect | None = None,\n    # Error Handling\n    on_bad_lines: str = \"error\",\n    # Internal\n    delim_whitespace: bool = False,\n    low_memory=_c_parser_defaults[\"low_memory\"],\n    memory_map: bool = False,\n    float_precision: str | None = None,\n    storage_options: StorageOptions = None,\n    use_nullable_dtypes: bool | lib.NoDefault = lib.no_default,\n) -> DataFrame | TextFileReader:\n    if infer_datetime_format is not lib.no_default:\n        warnings.warn(\n            \"The argument 'infer_datetime_format' is deprecated and will \"\n            \"be removed in a future version. \"\n            \"A strict version of it is now the default, see \"\n            \"https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. \"\n            \"You can safely remove this argument.\",\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n\n    # locals() should never be modified\n    kwds = locals().copy()\n    del kwds[\"filepath_or_buffer\"]\n    del kwds[\"sep\"]\n\n    kwds_defaults = _refine_defaults_read(\n        dialect,\n        delimiter,\n        delim_whitespace,\n        engine,\n        sep,\n        on_bad_lines,\n        names,\n        defaults={\"delimiter\": \"\\t\"},\n        use_nullable_dtypes=use_nullable_dtypes,\n    )\n    kwds.update(kwds_defaults)\n\n    return _read(filepath_or_buffer, kwds)\n\n\ndef read_fwf(\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    *,\n    colspecs: Sequence[tuple[int, int]] | str | None = \"infer\",\n    widths: Sequence[int] | None = None,\n    infer_nrows: int = 100,\n    use_nullable_dtypes: bool | lib.NoDefault = lib.no_default,\n    **kwds,\n) -> DataFrame | TextFileReader:\n    r\"\"\"\n    Read a table of fixed-width formatted lines into DataFrame.\n\n    Also supports optionally iterating or breaking of the file\n    into chunks.\n\n    Additional help can be found in the `online docs for IO Tools\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n\n    Parameters\n    ----------\n    filepath_or_buffer : str, path object, or file-like object\n        String, path object (implementing ``os.PathLike[str]``), or file-like\n        object implementing a text ``read()`` function.The string could be a URL.\n        Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is\n        expected. A local file could be:\n        ``file://localhost/path/to/table.csv``.\n    colspecs : list of tuple (int, int) or 'infer'. optional\n        A list of tuples giving the extents of the fixed-width\n        fields of each line as half-open intervals (i.e.,  [from, to[ ).\n        String value 'infer' can be used to instruct the parser to try\n        detecting the column specifications from the first 100 rows of\n        the data which are not being skipped via skiprows (default='infer').\n    widths : list of int, optional\n        A list of field widths which can be used instead of 'colspecs' if\n        the intervals are contiguous.\n    infer_nrows : int, default 100\n        The number of rows to consider when letting the parser determine the\n        `colspecs`.\n    use_nullable_dtypes : bool = False\n        Whether or not to use nullable dtypes as default when reading data. If\n        set to True, nullable dtypes are used for all dtypes that have a nullable\n        implementation, even if no nulls are present.\n\n        .. note::\n\n            The nullable dtype implementation can be configured by calling\n            ``pd.set_option(\"mode.dtype_backend\", \"pandas\")`` to use\n            numpy-backed nullable dtypes or\n            ``pd.set_option(\"mode.dtype_backend\", \"pyarrow\")`` to use\n            pyarrow-backed nullable dtypes (using ``pd.ArrowDtype``).\n            This is only implemented for the ``pyarrow`` or ``python``\n            engines.\n\n        .. versionadded:: 2.0\n\n    **kwds : optional\n        Optional keyword arguments can be passed to ``TextFileReader``.\n\n    Returns\n    -------\n    DataFrame or TextFileReader\n        A comma-separated values (csv) file is returned as two-dimensional\n        data structure with labeled axes.\n\n    See Also\n    --------\n    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n\n    Examples\n    --------\n    >>> pd.read_fwf('data.csv')  # doctest: +SKIP\n    \"\"\"\n    # Check input arguments.\n    if colspecs is None and widths is None:\n        raise ValueError(\"Must specify either colspecs or widths\")\n    if colspecs not in (None, \"infer\") and widths is not None:\n        raise ValueError(\"You must specify only one of 'widths' and 'colspecs'\")\n\n    use_nullable_dtypes = (\n        use_nullable_dtypes\n        if use_nullable_dtypes is not lib.no_default\n        else using_nullable_dtypes()\n    )\n\n    # Compute 'colspecs' from 'widths', if specified.\n    if widths is not None:\n        colspecs, col = [], 0\n        for w in widths:\n            colspecs.append((col, col + w))\n            col += w\n\n    # for mypy\n    assert colspecs is not None\n\n    # GH#40830\n    # Ensure length of `colspecs` matches length of `names`\n    names = kwds.get(\"names\")\n    if names is not None:\n        if len(names) != len(colspecs) and colspecs != \"infer\":\n            # need to check len(index_col) as it might contain\n            # unnamed indices, in which case it's name is not required\n            len_index = 0\n            if kwds.get(\"index_col\") is not None:\n                index_col: Any = kwds.get(\"index_col\")\n                if index_col is not False:\n                    if not is_list_like(index_col):\n                        len_index = 1\n                    else:\n                        len_index = len(index_col)\n            if kwds.get(\"usecols\") is None and len(names) + len_index != len(colspecs):\n                # If usecols is used colspec may be longer than names\n                raise ValueError(\"Length of colspecs must match length of names\")\n\n    kwds[\"colspecs\"] = colspecs\n    kwds[\"infer_nrows\"] = infer_nrows\n    kwds[\"engine\"] = \"python-fwf\"\n    kwds[\"use_nullable_dtypes\"] = use_nullable_dtypes\n    return _read(filepath_or_buffer, kwds)\n\n\nclass TextFileReader(abc.Iterator):\n    \"\"\"\n\n    Passed dialect overrides any of the related parser options\n\n    \"\"\"\n\n    def __init__(\n        self,\n        f: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str] | list,\n        engine: CSVEngine | None = None,\n        **kwds,\n    ) -> None:\n        if engine is not None:\n            engine_specified = True\n        else:\n            engine = \"python\"\n            engine_specified = False\n        self.engine = engine\n        self._engine_specified = kwds.get(\"engine_specified\", engine_specified)\n\n        _validate_skipfooter(kwds)\n\n        dialect = _extract_dialect(kwds)\n        if dialect is not None:\n            if engine == \"pyarrow\":\n                raise ValueError(\n                    \"The 'dialect' option is not supported with the 'pyarrow' engine\"\n                )\n            kwds = _merge_with_dialect_properties(dialect, kwds)\n\n        if kwds.get(\"header\", \"infer\") == \"infer\":\n            kwds[\"header\"] = 0 if kwds.get(\"names\") is None else None\n\n        self.orig_options = kwds\n\n        # miscellanea\n        self._currow = 0\n\n        options = self._get_options_with_defaults(engine)\n        options[\"storage_options\"] = kwds.get(\"storage_options\", None)\n\n        self.chunksize = options.pop(\"chunksize\", None)\n        self.nrows = options.pop(\"nrows\", None)\n\n        self._check_file_or_buffer(f, engine)\n        self.options, self.engine = self._clean_options(options, engine)\n\n        if \"has_index_names\" in kwds:\n            self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n\n        self.handles: IOHandles | None = None\n        self._engine = self._make_engine(f, self.engine)\n\n    def close(self) -> None:\n        if self.handles is not None:\n            self.handles.close()\n        self._engine.close()\n\n    def _get_options_with_defaults(self, engine: CSVEngine) -> dict[str, Any]:\n        kwds = self.orig_options\n\n        options = {}\n        default: object | None\n\n        for argname, default in parser_defaults.items():\n            value = kwds.get(argname, default)\n\n            # see gh-12935\n            if (\n                engine == \"pyarrow\"\n                and argname in _pyarrow_unsupported\n                and value != default\n                and value != getattr(value, \"value\", default)\n            ):\n                raise ValueError(\n                    f\"The {repr(argname)} option is not supported with the \"\n                    f\"'pyarrow' engine\"\n                )\n            options[argname] = value\n\n        for argname, default in _c_parser_defaults.items():\n            if argname in kwds:\n                value = kwds[argname]\n\n                if engine != \"c\" and value != default:\n                    if \"python\" in engine and argname not in _python_unsupported:\n                        pass\n                    else:\n                        raise ValueError(\n                            f\"The {repr(argname)} option is not supported with the \"\n                            f\"{repr(engine)} engine\"\n                        )\n            else:\n                value = default\n            options[argname] = value\n\n        if engine == \"python-fwf\":\n            for argname, default in _fwf_defaults.items():\n                options[argname] = kwds.get(argname, default)\n\n        return options\n\n    def _check_file_or_buffer(self, f, engine: CSVEngine) -> None:\n        # see gh-16530\n        if is_file_like(f) and engine != \"c\" and not hasattr(f, \"__iter__\"):\n            # The C engine doesn't need the file-like to have the \"__iter__\"\n            # attribute. However, the Python engine needs \"__iter__(...)\"\n            # when iterating through such an object, meaning it\n            # needs to have that attribute\n            raise ValueError(\n                \"The 'python' engine cannot iterate through this file buffer.\"\n            )\n\n    def _clean_options(\n        self, options: dict[str, Any], engine: CSVEngine\n    ) -> tuple[dict[str, Any], CSVEngine]:\n        result = options.copy()\n\n        fallback_reason = None\n\n        # C engine not supported yet\n        if engine == \"c\":\n            if options[\"skipfooter\"] > 0:\n                fallback_reason = \"the 'c' engine does not support skipfooter\"\n                engine = \"python\"\n\n        sep = options[\"delimiter\"]\n        delim_whitespace = options[\"delim_whitespace\"]\n\n        if sep is None and not delim_whitespace:\n            if engine in (\"c\", \"pyarrow\"):\n                fallback_reason = (\n                    f\"the '{engine}' engine does not support \"\n                    \"sep=None with delim_whitespace=False\"\n                )\n                engine = \"python\"\n        elif sep is not None and len(sep) > 1:\n            if engine == \"c\" and sep == r\"\\s+\":\n                result[\"delim_whitespace\"] = True\n                del result[\"delimiter\"]\n            elif engine not in (\"python\", \"python-fwf\"):\n                # wait until regex engine integrated\n                fallback_reason = (\n                    f\"the '{engine}' engine does not support \"\n                    \"regex separators (separators > 1 char and \"\n                    r\"different from '\\s+' are interpreted as regex)\"\n                )\n                engine = \"python\"\n        elif delim_whitespace:\n            if \"python\" in engine:\n                result[\"delimiter\"] = r\"\\s+\"\n        elif sep is not None:\n            encodeable = True\n            encoding = sys.getfilesystemencoding() or \"utf-8\"\n            try:\n                if len(sep.encode(encoding)) > 1:\n                    encodeable = False\n            except UnicodeDecodeError:\n                encodeable = False\n            if not encodeable and engine not in (\"python\", \"python-fwf\"):\n                fallback_reason = (\n                    f\"the separator encoded in {encoding} \"\n                    f\"is > 1 char long, and the '{engine}' engine \"\n                    \"does not support such separators\"\n                )\n                engine = \"python\"\n\n        quotechar = options[\"quotechar\"]\n        if quotechar is not None and isinstance(quotechar, (str, bytes)):\n            if (\n                len(quotechar) == 1\n                and ord(quotechar) > 127\n                and engine not in (\"python\", \"python-fwf\")\n            ):\n                fallback_reason = (\n                    \"ord(quotechar) > 127, meaning the \"\n                    \"quotechar is larger than one byte, \"\n                    f\"and the '{engine}' engine does not support such quotechars\"\n                )\n                engine = \"python\"\n\n        if fallback_reason and self._engine_specified:\n            raise ValueError(fallback_reason)\n\n        if engine == \"c\":\n            for arg in _c_unsupported:\n                del result[arg]\n\n        if \"python\" in engine:\n            for arg in _python_unsupported:\n                if fallback_reason and result[arg] != _c_parser_defaults[arg]:\n                    raise ValueError(\n                        \"Falling back to the 'python' engine because \"\n                        f\"{fallback_reason}, but this causes {repr(arg)} to be \"\n                        \"ignored as it is not supported by the 'python' engine.\"\n                    )\n                del result[arg]\n\n        if fallback_reason:\n            warnings.warn(\n                (\n                    \"Falling back to the 'python' engine because \"\n                    f\"{fallback_reason}; you can avoid this warning by specifying \"\n                    \"engine='python'.\"\n                ),\n                ParserWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        index_col = options[\"index_col\"]\n        names = options[\"names\"]\n        converters = options[\"converters\"]\n        na_values = options[\"na_values\"]\n        skiprows = options[\"skiprows\"]\n\n        validate_header_arg(options[\"header\"])\n\n        if index_col is True:\n            raise ValueError(\"The value of index_col couldn't be 'True'\")\n        if is_index_col(index_col):\n            if not isinstance(index_col, (list, tuple, np.ndarray)):\n                index_col = [index_col]\n        result[\"index_col\"] = index_col\n\n        names = list(names) if names is not None else names\n\n        # type conversion-related\n        if converters is not None:\n            if not isinstance(converters, dict):\n                raise TypeError(\n                    \"Type converters must be a dict or subclass, \"\n                    f\"input was a {type(converters).__name__}\"\n                )\n        else:\n            converters = {}\n\n        # Converting values to NA\n        keep_default_na = options[\"keep_default_na\"]\n        na_values, na_fvalues = _clean_na_values(na_values, keep_default_na)\n\n        # handle skiprows; this is internally handled by the\n        # c-engine, so only need for python and pyarrow parsers\n        if engine == \"pyarrow\":\n            if not is_integer(skiprows) and skiprows is not None:\n                # pyarrow expects skiprows to be passed as an integer\n                raise ValueError(\n                    \"skiprows argument must be an integer when using \"\n                    \"engine='pyarrow'\"\n                )\n        else:\n            if is_integer(skiprows):\n                skiprows = list(range(skiprows))\n            if skiprows is None:\n                skiprows = set()\n            elif not callable(skiprows):\n                skiprows = set(skiprows)\n\n        # put stuff back\n        result[\"names\"] = names\n        result[\"converters\"] = converters\n        result[\"na_values\"] = na_values\n        result[\"na_fvalues\"] = na_fvalues\n        result[\"skiprows\"] = skiprows\n\n        return result, engine\n\n    def __next__(self) -> DataFrame:\n        try:\n            return self.get_chunk()\n        except StopIteration:\n            self.close()\n            raise\n\n    def _make_engine(\n        self,\n        f: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str] | list | IO,\n        engine: CSVEngine = \"c\",\n    ) -> ParserBase:\n        mapping: dict[str, type[ParserBase]] = {\n            \"c\": CParserWrapper,\n            \"python\": PythonParser,\n            \"pyarrow\": ArrowParserWrapper,\n            \"python-fwf\": FixedWidthFieldParser,\n        }\n        if engine not in mapping:\n            raise ValueError(\n                f\"Unknown engine: {engine} (valid options are {mapping.keys()})\"\n            )\n        if not isinstance(f, list):\n            # open file here\n            is_text = True\n            mode = \"r\"\n            if engine == \"pyarrow\":\n                is_text = False\n                mode = \"rb\"\n            elif (\n                engine == \"c\"\n                and self.options.get(\"encoding\", \"utf-8\") == \"utf-8\"\n                and isinstance(stringify_path(f), str)\n            ):\n                # c engine can decode utf-8 bytes, adding TextIOWrapper makes\n                # the c-engine especially for memory_map=True far slower\n                is_text = False\n                if \"b\" not in mode:\n                    mode += \"b\"\n            self.handles = get_handle(\n                f,\n                mode,\n                encoding=self.options.get(\"encoding\", None),\n                compression=self.options.get(\"compression\", None),\n                memory_map=self.options.get(\"memory_map\", False),\n                is_text=is_text,\n                errors=self.options.get(\"encoding_errors\", \"strict\"),\n                storage_options=self.options.get(\"storage_options\", None),\n            )\n            assert self.handles is not None\n            f = self.handles.handle\n\n        elif engine != \"python\":\n            msg = f\"Invalid file path or buffer object type: {type(f)}\"\n            raise ValueError(msg)\n\n        try:\n            return mapping[engine](f, **self.options)\n        except Exception:\n            if self.handles is not None:\n                self.handles.close()\n            raise\n\n    def _failover_to_python(self) -> None:\n        raise AbstractMethodError(self)\n\n    def read(self, nrows: int | None = None) -> DataFrame:\n        if self.engine == \"pyarrow\":\n            try:\n                # error: \"ParserBase\" has no attribute \"read\"\n                df = self._engine.read()  # type: ignore[attr-defined]\n            except Exception:\n                self.close()\n                raise\n        else:\n            nrows = validate_integer(\"nrows\", nrows)\n            try:\n                # error: \"ParserBase\" has no attribute \"read\"\n                (\n                    index,\n                    columns,\n                    col_dict,\n                ) = self._engine.read(  # type: ignore[attr-defined]\n                    nrows\n                )\n            except Exception:\n                self.close()\n                raise\n\n            if index is None:\n                if col_dict:\n                    # Any column is actually fine:\n                    new_rows = len(next(iter(col_dict.values())))\n                    index = RangeIndex(self._currow, self._currow + new_rows)\n                else:\n                    new_rows = 0\n            else:\n                new_rows = len(index)\n\n            df = DataFrame(col_dict, columns=columns, index=index)\n\n            self._currow += new_rows\n        return df\n\n    def get_chunk(self, size: int | None = None) -> DataFrame:\n        if size is None:\n            size = self.chunksize\n        if self.nrows is not None:\n            if self._currow >= self.nrows:\n                raise StopIteration\n            size = min(size, self.nrows - self._currow)\n        return self.read(nrows=size)\n\n    def __enter__(self) -> TextFileReader:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ) -> None:\n        self.close()\n\n\ndef TextParser(*args, **kwds) -> TextFileReader:\n    \"\"\"\n    Converts lists of lists/tuples into DataFrames with proper type inference\n    and optional (e.g. string to datetime) conversion. Also enables iterating\n    lazily over chunks of large files\n\n    Parameters\n    ----------\n    data : file-like object or list\n    delimiter : separator character to use\n    dialect : str or csv.Dialect instance, optional\n        Ignored if delimiter is longer than 1 character\n    names : sequence, default\n    header : int, default 0\n        Row to use to parse column labels. Defaults to the first row. Prior\n        rows will be discarded\n    index_col : int or list, optional\n        Column or columns to use as the (possibly hierarchical) index\n    has_index_names: bool, default False\n        True if the cols defined in index_col have an index name and are\n        not in the header.\n    na_values : scalar, str, list-like, or dict, optional\n        Additional strings to recognize as NA/NaN.\n    keep_default_na : bool, default True\n    thousands : str, optional\n        Thousands separator\n    comment : str, optional\n        Comment out remainder of line\n    parse_dates : bool, default False\n    keep_date_col : bool, default False\n    date_parser : function, optional\n\n        .. deprecated:: 2.0.0\n    date_format : str, default ``None``\n\n        .. versionadded:: 2.0.0\n    skiprows : list of integers\n        Row numbers to skip\n    skipfooter : int\n        Number of line at bottom of file to skip\n    converters : dict, optional\n        Dict of functions for converting values in certain columns. Keys can\n        either be integers or column labels, values are functions that take one\n        input argument, the cell (not column) content, and return the\n        transformed content.\n    encoding : str, optional\n        Encoding to use for UTF when reading/writing (ex. 'utf-8')\n    float_precision : str, optional\n        Specifies which converter the C engine should use for floating-point\n        values. The options are `None` or `high` for the ordinary converter,\n        `legacy` for the original lower precision pandas converter, and\n        `round_trip` for the round-trip converter.\n\n        .. versionchanged:: 1.2\n    \"\"\"\n    kwds[\"engine\"] = \"python\"\n    return TextFileReader(*args, **kwds)\n\n\ndef _clean_na_values(na_values, keep_default_na: bool = True):\n    na_fvalues: set | dict\n    if na_values is None:\n        if keep_default_na:\n            na_values = STR_NA_VALUES\n        else:\n            na_values = set()\n        na_fvalues = set()\n    elif isinstance(na_values, dict):\n        old_na_values = na_values.copy()\n        na_values = {}  # Prevent aliasing.\n\n        # Convert the values in the na_values dictionary\n        # into array-likes for further use. This is also\n        # where we append the default NaN values, provided\n        # that `keep_default_na=True`.\n        for k, v in old_na_values.items():\n            if not is_list_like(v):\n                v = [v]\n\n            if keep_default_na:\n                v = set(v) | STR_NA_VALUES\n\n            na_values[k] = v\n        na_fvalues = {k: _floatify_na_values(v) for k, v in na_values.items()}\n    else:\n        if not is_list_like(na_values):\n            na_values = [na_values]\n        na_values = _stringify_na_values(na_values)\n        if keep_default_na:\n            na_values = na_values | STR_NA_VALUES\n\n        na_fvalues = _floatify_na_values(na_values)\n\n    return na_values, na_fvalues\n\n\ndef _floatify_na_values(na_values):\n    # create float versions of the na_values\n    result = set()\n    for v in na_values:\n        try:\n            v = float(v)\n            if not np.isnan(v):\n                result.add(v)\n        except (TypeError, ValueError, OverflowError):\n            pass\n    return result\n\n\ndef _stringify_na_values(na_values):\n    \"\"\"return a stringified and numeric for these values\"\"\"\n    result: list[str | float] = []\n    for x in na_values:\n        result.append(str(x))\n        result.append(x)\n        try:\n            v = float(x)\n\n            # we are like 999 here\n            if v == int(v):\n                v = int(v)\n                result.append(f\"{v}.0\")\n                result.append(str(v))\n\n            result.append(v)\n        except (TypeError, ValueError, OverflowError):\n            pass\n        try:\n            result.append(int(x))\n        except (TypeError, ValueError, OverflowError):\n            pass\n    return set(result)\n\n\ndef _refine_defaults_read(\n    dialect: str | csv.Dialect | None,\n    delimiter: str | None | lib.NoDefault,\n    delim_whitespace: bool,\n    engine: CSVEngine | None,\n    sep: str | None | lib.NoDefault,\n    on_bad_lines: str | Callable,\n    names: Sequence[Hashable] | None | lib.NoDefault,\n    defaults: dict[str, Any],\n    use_nullable_dtypes: bool | lib.NoDefault,\n):\n    \"\"\"Validate/refine default values of input parameters of read_csv, read_table.\n\n    Parameters\n    ----------\n    dialect : str or csv.Dialect\n        If provided, this parameter will override values (default or not) for the\n        following parameters: `delimiter`, `doublequote`, `escapechar`,\n        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n        override values, a ParserWarning will be issued. See csv.Dialect\n        documentation for more details.\n    delimiter : str or object\n        Alias for sep.\n    delim_whitespace : bool\n        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n        used as the sep. Equivalent to setting ``sep='\\\\s+'``. If this option\n        is set to True, nothing should be passed in for the ``delimiter``\n        parameter.\n    engine : {{'c', 'python'}}\n        Parser engine to use. The C engine is faster while the python engine is\n        currently more feature-complete.\n    sep : str or object\n        A delimiter provided by the user (str) or a sentinel value, i.e.\n        pandas._libs.lib.no_default.\n    on_bad_lines : str, callable\n        An option for handling bad lines or a sentinel value(None).\n    names : array-like, optional\n        List of column names to use. If the file contains a header row,\n        then you should explicitly pass ``header=0`` to override the column names.\n        Duplicates in this list are not allowed.\n    defaults: dict\n        Default values of input parameters.\n\n    Returns\n    -------\n    kwds : dict\n        Input parameters with correct values.\n\n    Raises\n    ------\n    ValueError :\n        If a delimiter was specified with ``sep`` (or ``delimiter``) and\n        ``delim_whitespace=True``.\n    \"\"\"\n    # fix types for sep, delimiter to Union(str, Any)\n    delim_default = defaults[\"delimiter\"]\n    kwds: dict[str, Any] = {}\n    # gh-23761\n    #\n    # When a dialect is passed, it overrides any of the overlapping\n    # parameters passed in directly. We don't want to warn if the\n    # default parameters were passed in (since it probably means\n    # that the user didn't pass them in explicitly in the first place).\n    #\n    # \"delimiter\" is the annoying corner case because we alias it to\n    # \"sep\" before doing comparison to the dialect values later on.\n    # Thus, we need a flag to indicate that we need to \"override\"\n    # the comparison to dialect values by checking if default values\n    # for BOTH \"delimiter\" and \"sep\" were provided.\n    if dialect is not None:\n        kwds[\"sep_override\"] = delimiter is None and (\n            sep is lib.no_default or sep == delim_default\n        )\n\n    if delimiter and (sep is not lib.no_default):\n        raise ValueError(\"Specified a sep and a delimiter; you can only specify one.\")\n\n    kwds[\"names\"] = None if names is lib.no_default else names\n\n    # Alias sep -> delimiter.\n    if delimiter is None:\n        delimiter = sep\n\n    if delim_whitespace and (delimiter is not lib.no_default):\n        raise ValueError(\n            \"Specified a delimiter with both sep and \"\n            \"delim_whitespace=True; you can only specify one.\"\n        )\n\n    if delimiter == \"\\n\":\n        raise ValueError(\n            r\"Specified \\n as separator or delimiter. This forces the python engine \"\n            \"which does not accept a line terminator. Hence it is not allowed to use \"\n            \"the line terminator as separator.\",\n        )\n\n    if delimiter is lib.no_default:\n        # assign default separator value\n        kwds[\"delimiter\"] = delim_default\n    else:\n        kwds[\"delimiter\"] = delimiter\n\n    if engine is not None:\n        kwds[\"engine_specified\"] = True\n    else:\n        kwds[\"engine\"] = \"c\"\n        kwds[\"engine_specified\"] = False\n\n    if on_bad_lines == \"error\":\n        kwds[\"on_bad_lines\"] = ParserBase.BadLineHandleMethod.ERROR\n    elif on_bad_lines == \"warn\":\n        kwds[\"on_bad_lines\"] = ParserBase.BadLineHandleMethod.WARN\n    elif on_bad_lines == \"skip\":\n        kwds[\"on_bad_lines\"] = ParserBase.BadLineHandleMethod.SKIP\n    elif callable(on_bad_lines):\n        if engine != \"python\":\n            raise ValueError(\n                \"on_bad_line can only be a callable function if engine='python'\"\n            )\n        kwds[\"on_bad_lines\"] = on_bad_lines\n    else:\n        raise ValueError(f\"Argument {on_bad_lines} is invalid for on_bad_lines\")\n\n    use_nullable_dtypes = (\n        use_nullable_dtypes\n        if use_nullable_dtypes is not lib.no_default\n        else using_nullable_dtypes()\n    )\n    kwds[\"use_nullable_dtypes\"] = use_nullable_dtypes\n\n    return kwds\n\n\ndef _extract_dialect(kwds: dict[str, Any]) -> csv.Dialect | None:\n    \"\"\"\n    Extract concrete csv dialect instance.\n\n    Returns\n    -------\n    csv.Dialect or None\n    \"\"\"\n    if kwds.get(\"dialect\") is None:\n        return None\n\n    dialect = kwds[\"dialect\"]\n    if dialect in csv.list_dialects():\n        dialect = csv.get_dialect(dialect)\n\n    _validate_dialect(dialect)\n\n    return dialect\n\n\nMANDATORY_DIALECT_ATTRS = (\n    \"delimiter\",\n    \"doublequote\",\n    \"escapechar\",\n    \"skipinitialspace\",\n    \"quotechar\",\n    \"quoting\",\n)\n\n\ndef _validate_dialect(dialect: csv.Dialect) -> None:\n    \"\"\"\n    Validate csv dialect instance.\n\n    Raises\n    ------\n    ValueError\n        If incorrect dialect is provided.\n    \"\"\"\n    for param in MANDATORY_DIALECT_ATTRS:\n        if not hasattr(dialect, param):\n            raise ValueError(f\"Invalid dialect {dialect} provided\")\n\n\ndef _merge_with_dialect_properties(\n    dialect: csv.Dialect,\n    defaults: dict[str, Any],\n) -> dict[str, Any]:\n    \"\"\"\n    Merge default kwargs in TextFileReader with dialect parameters.\n\n    Parameters\n    ----------\n    dialect : csv.Dialect\n        Concrete csv dialect. See csv.Dialect documentation for more details.\n    defaults : dict\n        Keyword arguments passed to TextFileReader.\n\n    Returns\n    -------\n    kwds : dict\n        Updated keyword arguments, merged with dialect parameters.\n    \"\"\"\n    kwds = defaults.copy()\n\n    for param in MANDATORY_DIALECT_ATTRS:\n        dialect_val = getattr(dialect, param)\n\n        parser_default = parser_defaults[param]\n        provided = kwds.get(param, parser_default)\n\n        # Messages for conflicting values between the dialect\n        # instance and the actual parameters provided.\n        conflict_msgs = []\n\n        # Don't warn if the default parameter was passed in,\n        # even if it conflicts with the dialect (gh-23761).\n        if provided not in (parser_default, dialect_val):\n            msg = (\n                f\"Conflicting values for '{param}': '{provided}' was \"\n                f\"provided, but the dialect specifies '{dialect_val}'. \"\n                \"Using the dialect-specified value.\"\n            )\n\n            # Annoying corner case for not warning about\n            # conflicts between dialect and delimiter parameter.\n            # Refer to the outer \"_read_\" function for more info.\n            if not (param == \"delimiter\" and kwds.pop(\"sep_override\", False)):\n                conflict_msgs.append(msg)\n\n        if conflict_msgs:\n            warnings.warn(\n                \"\\n\\n\".join(conflict_msgs), ParserWarning, stacklevel=find_stack_level()\n            )\n        kwds[param] = dialect_val\n    return kwds\n\n\ndef _validate_skipfooter(kwds: dict[str, Any]) -> None:\n    \"\"\"\n    Check whether skipfooter is compatible with other kwargs in TextFileReader.\n\n    Parameters\n    ----------\n    kwds : dict\n        Keyword arguments passed to TextFileReader.\n\n    Raises\n    ------\n    ValueError\n        If skipfooter is not compatible with other parameters.\n    \"\"\"\n    if kwds.get(\"skipfooter\"):\n        if kwds.get(\"iterator\") or kwds.get(\"chunksize\"):\n            raise ValueError(\"'skipfooter' not supported for iteration\")\n        if kwds.get(\"nrows\"):\n            raise ValueError(\"'skipfooter' not supported with 'nrows'\")\n"
    },
    {
      "filename": "pandas/tests/io/excel/test_writers.py",
      "content": "from datetime import (\n    date,\n    datetime,\n    timedelta,\n)\nfrom functools import partial\nfrom io import BytesIO\nimport os\nimport re\n\nimport numpy as np\nimport pytest\n\nimport pandas.util._test_decorators as td\n\nimport pandas as pd\nfrom pandas import (\n    DataFrame,\n    Index,\n    MultiIndex,\n    option_context,\n)\nimport pandas._testing as tm\n\nfrom pandas.io.excel import (\n    ExcelFile,\n    ExcelWriter,\n    _OpenpyxlWriter,\n    _XlsxWriter,\n    register_writer,\n)\nfrom pandas.io.excel._util import _writers\n\n\n@pytest.fixture\ndef path(ext):\n    \"\"\"\n    Fixture to open file for use in each test case.\n    \"\"\"\n    with tm.ensure_clean(ext) as file_path:\n        yield file_path\n\n\n@pytest.fixture\ndef set_engine(engine, ext):\n    \"\"\"\n    Fixture to set engine for use in each test case.\n\n    Rather than requiring `engine=...` to be provided explicitly as an\n    argument in each test, this fixture sets a global option to dictate\n    which engine should be used to write Excel files. After executing\n    the test it rolls back said change to the global option.\n    \"\"\"\n    option_name = f\"io.excel.{ext.strip('.')}.writer\"\n    with option_context(option_name, engine):\n        yield\n\n\n@pytest.mark.parametrize(\n    \"ext\",\n    [\n        pytest.param(\".xlsx\", marks=[td.skip_if_no(\"openpyxl\"), td.skip_if_no(\"xlrd\")]),\n        pytest.param(\".xlsm\", marks=[td.skip_if_no(\"openpyxl\"), td.skip_if_no(\"xlrd\")]),\n        pytest.param(\n            \".xlsx\", marks=[td.skip_if_no(\"xlsxwriter\"), td.skip_if_no(\"xlrd\")]\n        ),\n        pytest.param(\".ods\", marks=td.skip_if_no(\"odf\")),\n    ],\n)\nclass TestRoundTrip:\n    @pytest.mark.parametrize(\n        \"header,expected\",\n        [(None, DataFrame([np.nan] * 4)), (0, DataFrame({\"Unnamed: 0\": [np.nan] * 3}))],\n    )\n    def test_read_one_empty_col_no_header(self, ext, header, expected):\n        # xref gh-12292\n        filename = \"no_header\"\n        df = DataFrame([[\"\", 1, 100], [\"\", 2, 200], [\"\", 3, 300], [\"\", 4, 400]])\n\n        with tm.ensure_clean(ext) as path:\n            df.to_excel(path, filename, index=False, header=False)\n            result = pd.read_excel(\n                path, sheet_name=filename, usecols=[0], header=header\n            )\n\n        tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"header,expected\",\n        [(None, DataFrame([0] + [np.nan] * 4)), (0, DataFrame([np.nan] * 4))],\n    )\n    def test_read_one_empty_col_with_header(self, ext, header, expected):\n        filename = \"with_header\"\n        df = DataFrame([[\"\", 1, 100], [\"\", 2, 200], [\"\", 3, 300], [\"\", 4, 400]])\n\n        with tm.ensure_clean(ext) as path:\n            df.to_excel(path, \"with_header\", index=False, header=True)\n            result = pd.read_excel(\n                path, sheet_name=filename, usecols=[0], header=header\n            )\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_set_column_names_in_parameter(self, ext):\n        # GH 12870 : pass down column names associated with\n        # keyword argument names\n        refdf = DataFrame([[1, \"foo\"], [2, \"bar\"], [3, \"baz\"]], columns=[\"a\", \"b\"])\n\n        with tm.ensure_clean(ext) as pth:\n            with ExcelWriter(pth) as writer:\n                refdf.to_excel(writer, \"Data_no_head\", header=False, index=False)\n                refdf.to_excel(writer, \"Data_with_head\", index=False)\n\n            refdf.columns = [\"A\", \"B\"]\n\n            with ExcelFile(pth) as reader:\n                xlsdf_no_head = pd.read_excel(\n                    reader, sheet_name=\"Data_no_head\", header=None, names=[\"A\", \"B\"]\n                )\n                xlsdf_with_head = pd.read_excel(\n                    reader,\n                    sheet_name=\"Data_with_head\",\n                    index_col=None,\n                    names=[\"A\", \"B\"],\n                )\n\n            tm.assert_frame_equal(xlsdf_no_head, refdf)\n            tm.assert_frame_equal(xlsdf_with_head, refdf)\n\n    def test_creating_and_reading_multiple_sheets(self, ext):\n        # see gh-9450\n        #\n        # Test reading multiple sheets, from a runtime\n        # created Excel file with multiple sheets.\n        def tdf(col_sheet_name):\n            d, i = [11, 22, 33], [1, 2, 3]\n            return DataFrame(d, i, columns=[col_sheet_name])\n\n        sheets = [\"AAA\", \"BBB\", \"CCC\"]\n\n        dfs = [tdf(s) for s in sheets]\n        dfs = dict(zip(sheets, dfs))\n\n        with tm.ensure_clean(ext) as pth:\n            with ExcelWriter(pth) as ew:\n                for sheetname, df in dfs.items():\n                    df.to_excel(ew, sheetname)\n\n            dfs_returned = pd.read_excel(pth, sheet_name=sheets, index_col=0)\n\n            for s in sheets:\n                tm.assert_frame_equal(dfs[s], dfs_returned[s])\n\n    def test_read_excel_multiindex_empty_level(self, ext):\n        # see gh-12453\n        with tm.ensure_clean(ext) as path:\n            df = DataFrame(\n                {\n                    (\"One\", \"x\"): {0: 1},\n                    (\"Two\", \"X\"): {0: 3},\n                    (\"Two\", \"Y\"): {0: 7},\n                    (\"Zero\", \"\"): {0: 0},\n                }\n            )\n\n            expected = DataFrame(\n                {\n                    (\"One\", \"x\"): {0: 1},\n                    (\"Two\", \"X\"): {0: 3},\n                    (\"Two\", \"Y\"): {0: 7},\n                    (\"Zero\", \"Unnamed: 4_level_1\"): {0: 0},\n                }\n            )\n\n            df.to_excel(path)\n            actual = pd.read_excel(path, header=[0, 1], index_col=0)\n            tm.assert_frame_equal(actual, expected)\n\n            df = DataFrame(\n                {\n                    (\"Beg\", \"\"): {0: 0},\n                    (\"Middle\", \"x\"): {0: 1},\n                    (\"Tail\", \"X\"): {0: 3},\n                    (\"Tail\", \"Y\"): {0: 7},\n                }\n            )\n\n            expected = DataFrame(\n                {\n                    (\"Beg\", \"Unnamed: 1_level_1\"): {0: 0},\n                    (\"Middle\", \"x\"): {0: 1},\n                    (\"Tail\", \"X\"): {0: 3},\n                    (\"Tail\", \"Y\"): {0: 7},\n                }\n            )\n\n            df.to_excel(path)\n            actual = pd.read_excel(path, header=[0, 1], index_col=0)\n            tm.assert_frame_equal(actual, expected)\n\n    @pytest.mark.parametrize(\"c_idx_names\", [True, False])\n    @pytest.mark.parametrize(\"r_idx_names\", [True, False])\n    @pytest.mark.parametrize(\"c_idx_levels\", [1, 3])\n    @pytest.mark.parametrize(\"r_idx_levels\", [1, 3])\n    def test_excel_multindex_roundtrip(\n        self, ext, c_idx_names, r_idx_names, c_idx_levels, r_idx_levels, request\n    ):\n        # see gh-4679\n        with tm.ensure_clean(ext) as pth:\n            if (c_idx_levels == 1 and c_idx_names) and not (\n                r_idx_levels == 3 and not r_idx_names\n            ):\n                mark = pytest.mark.xfail(\n                    reason=\"Column index name cannot be serialized unless \"\n                    \"it's a MultiIndex\"\n                )\n                request.node.add_marker(mark)\n\n            # Empty name case current read in as\n            # unnamed levels, not Nones.\n            check_names = r_idx_names or r_idx_levels <= 1\n\n            df = tm.makeCustomDataframe(\n                5, 5, c_idx_names, r_idx_names, c_idx_levels, r_idx_levels\n            )\n            df.to_excel(pth)\n\n            act = pd.read_excel(\n                pth,\n                index_col=list(range(r_idx_levels)),\n                header=list(range(c_idx_levels)),\n            )\n            tm.assert_frame_equal(df, act, check_names=check_names)\n\n            df.iloc[0, :] = np.nan\n            df.to_excel(pth)\n\n            act = pd.read_excel(\n                pth,\n                index_col=list(range(r_idx_levels)),\n                header=list(range(c_idx_levels)),\n            )\n            tm.assert_frame_equal(df, act, check_names=check_names)\n\n            df.iloc[-1, :] = np.nan\n            df.to_excel(pth)\n            act = pd.read_excel(\n                pth,\n                index_col=list(range(r_idx_levels)),\n                header=list(range(c_idx_levels)),\n            )\n            tm.assert_frame_equal(df, act, check_names=check_names)\n\n    def test_read_excel_parse_dates(self, ext):\n        # see gh-11544, gh-12051\n        df = DataFrame(\n            {\"col\": [1, 2, 3], \"date_strings\": pd.date_range(\"2012-01-01\", periods=3)}\n        )\n        df2 = df.copy()\n        df2[\"date_strings\"] = df2[\"date_strings\"].dt.strftime(\"%m/%d/%Y\")\n\n        with tm.ensure_clean(ext) as pth:\n            df2.to_excel(pth)\n\n            res = pd.read_excel(pth, index_col=0)\n            tm.assert_frame_equal(df2, res)\n\n            res = pd.read_excel(pth, parse_dates=[\"date_strings\"], index_col=0)\n            tm.assert_frame_equal(df, res)\n\n            date_parser = lambda x: datetime.strptime(x, \"%m/%d/%Y\")\n            with tm.assert_produces_warning(\n                FutureWarning, match=\"use 'date_format' instead\"\n            ):\n                res = pd.read_excel(\n                    pth,\n                    parse_dates=[\"date_strings\"],\n                    date_parser=date_parser,\n                    index_col=0,\n                )\n            tm.assert_frame_equal(df, res)\n            res = pd.read_excel(\n                pth, parse_dates=[\"date_strings\"], date_format=\"%m/%d/%Y\", index_col=0\n            )\n            tm.assert_frame_equal(df, res)\n\n    def test_multiindex_interval_datetimes(self, ext):\n        # GH 30986\n        midx = MultiIndex.from_arrays(\n            [\n                range(4),\n                pd.interval_range(\n                    start=pd.Timestamp(\"2020-01-01\"), periods=4, freq=\"6M\"\n                ),\n            ]\n        )\n        df = DataFrame(range(4), index=midx)\n        with tm.ensure_clean(ext) as pth:\n            df.to_excel(pth)\n            result = pd.read_excel(pth, index_col=[0, 1])\n        expected = DataFrame(\n            range(4),\n            MultiIndex.from_arrays(\n                [\n                    range(4),\n                    [\n                        \"(2020-01-31, 2020-07-31]\",\n                        \"(2020-07-31, 2021-01-31]\",\n                        \"(2021-01-31, 2021-07-31]\",\n                        \"(2021-07-31, 2022-01-31]\",\n                    ],\n                ]\n            ),\n        )\n        tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"engine,ext\",\n    [\n        pytest.param(\n            \"openpyxl\",\n            \".xlsx\",\n            marks=[td.skip_if_no(\"openpyxl\"), td.skip_if_no(\"xlrd\")],\n        ),\n        pytest.param(\n            \"openpyxl\",\n            \".xlsm\",\n            marks=[td.skip_if_no(\"openpyxl\"), td.skip_if_no(\"xlrd\")],\n        ),\n        pytest.param(\n            \"xlsxwriter\",\n            \".xlsx\",\n            marks=[td.skip_if_no(\"xlsxwriter\"), td.skip_if_no(\"xlrd\")],\n        ),\n        pytest.param(\"odf\", \".ods\", marks=td.skip_if_no(\"odf\")),\n    ],\n)\n@pytest.mark.usefixtures(\"set_engine\")\nclass TestExcelWriter:\n    def test_excel_sheet_size(self, path):\n        # GH 26080\n        breaking_row_count = 2**20 + 1\n        breaking_col_count = 2**14 + 1\n        # purposely using two arrays to prevent memory issues while testing\n        row_arr = np.zeros(shape=(breaking_row_count, 1))\n        col_arr = np.zeros(shape=(1, breaking_col_count))\n        row_df = DataFrame(row_arr)\n        col_df = DataFrame(col_arr)\n\n        msg = \"sheet is too large\"\n        with pytest.raises(ValueError, match=msg):\n            row_df.to_excel(path)\n\n        with pytest.raises(ValueError, match=msg):\n            col_df.to_excel(path)\n\n    def test_excel_sheet_by_name_raise(self, path):\n        gt = DataFrame(np.random.randn(10, 2))\n        gt.to_excel(path)\n\n        with ExcelFile(path) as xl:\n            df = pd.read_excel(xl, sheet_name=0, index_col=0)\n\n        tm.assert_frame_equal(gt, df)\n\n        msg = \"Worksheet named '0' not found\"\n        with pytest.raises(ValueError, match=msg):\n            pd.read_excel(xl, \"0\")\n\n    def test_excel_writer_context_manager(self, frame, path):\n        with ExcelWriter(path) as writer:\n            frame.to_excel(writer, \"Data1\")\n            frame2 = frame.copy()\n            frame2.columns = frame.columns[::-1]\n            frame2.to_excel(writer, \"Data2\")\n\n        with ExcelFile(path) as reader:\n            found_df = pd.read_excel(reader, sheet_name=\"Data1\", index_col=0)\n            found_df2 = pd.read_excel(reader, sheet_name=\"Data2\", index_col=0)\n\n            tm.assert_frame_equal(found_df, frame)\n            tm.assert_frame_equal(found_df2, frame2)\n\n    def test_roundtrip(self, frame, path):\n        frame = frame.copy()\n        frame.iloc[:5, frame.columns.get_loc(\"A\")] = np.nan\n\n        frame.to_excel(path, \"test1\")\n        frame.to_excel(path, \"test1\", columns=[\"A\", \"B\"])\n        frame.to_excel(path, \"test1\", header=False)\n        frame.to_excel(path, \"test1\", index=False)\n\n        # test roundtrip\n        frame.to_excel(path, \"test1\")\n        recons = pd.read_excel(path, sheet_name=\"test1\", index_col=0)\n        tm.assert_frame_equal(frame, recons)\n\n        frame.to_excel(path, \"test1\", index=False)\n        recons = pd.read_excel(path, sheet_name=\"test1\", index_col=None)\n        recons.index = frame.index\n        tm.assert_frame_equal(frame, recons)\n\n        frame.to_excel(path, \"test1\", na_rep=\"NA\")\n        recons = pd.read_excel(path, sheet_name=\"test1\", index_col=0, na_values=[\"NA\"])\n        tm.assert_frame_equal(frame, recons)\n\n        # GH 3611\n        frame.to_excel(path, \"test1\", na_rep=\"88\")\n        recons = pd.read_excel(path, sheet_name=\"test1\", index_col=0, na_values=[\"88\"])\n        tm.assert_frame_equal(frame, recons)\n\n        frame.to_excel(path, \"test1\", na_rep=\"88\")\n        recons = pd.read_excel(\n            path, sheet_name=\"test1\", index_col=0, na_values=[88, 88.0]\n        )\n        tm.assert_frame_equal(frame, recons)\n\n        # GH 6573\n        frame.to_excel(path, \"Sheet1\")\n        recons = pd.read_excel(path, index_col=0)\n        tm.assert_frame_equal(frame, recons)\n\n        frame.to_excel(path, \"0\")\n        recons = pd.read_excel(path, index_col=0)\n        tm.assert_frame_equal(frame, recons)\n\n        # GH 8825 Pandas Series should provide to_excel method\n        s = frame[\"A\"]\n        s.to_excel(path)\n        recons = pd.read_excel(path, index_col=0)\n        tm.assert_frame_equal(s.to_frame(), recons)\n\n    def test_mixed(self, frame, path):\n        mixed_frame = frame.copy()\n        mixed_frame[\"foo\"] = \"bar\"\n\n        mixed_frame.to_excel(path, \"test1\")\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n        tm.assert_frame_equal(mixed_frame, recons)\n\n    def test_ts_frame(self, tsframe, path):\n        df = tsframe\n\n        # freq doesn't round-trip\n        index = pd.DatetimeIndex(np.asarray(df.index), freq=None)\n        df.index = index\n\n        df.to_excel(path, \"test1\")\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n        tm.assert_frame_equal(df, recons)\n\n    def test_basics_with_nan(self, frame, path):\n        frame = frame.copy()\n        frame.iloc[:5, frame.columns.get_loc(\"A\")] = np.nan\n        frame.to_excel(path, \"test1\")\n        frame.to_excel(path, \"test1\", columns=[\"A\", \"B\"])\n        frame.to_excel(path, \"test1\", header=False)\n        frame.to_excel(path, \"test1\", index=False)\n\n    @pytest.mark.parametrize(\"np_type\", [np.int8, np.int16, np.int32, np.int64])\n    def test_int_types(self, np_type, path):\n        # Test np.int values read come back as int\n        # (rather than float which is Excel's format).\n        df = DataFrame(np.random.randint(-10, 10, size=(10, 2)), dtype=np_type)\n        df.to_excel(path, \"test1\")\n\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n\n        int_frame = df.astype(np.int64)\n        tm.assert_frame_equal(int_frame, recons)\n\n        recons2 = pd.read_excel(path, sheet_name=\"test1\", index_col=0)\n        tm.assert_frame_equal(int_frame, recons2)\n\n    @pytest.mark.parametrize(\"np_type\", [np.float16, np.float32, np.float64])\n    def test_float_types(self, np_type, path):\n        # Test np.float values read come back as float.\n        df = DataFrame(np.random.random_sample(10), dtype=np_type)\n        df.to_excel(path, \"test1\")\n\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0).astype(\n                np_type\n            )\n\n        tm.assert_frame_equal(df, recons)\n\n    def test_bool_types(self, path):\n        # Test np.bool_ values read come back as float.\n        df = DataFrame([1, 0, True, False], dtype=np.bool_)\n        df.to_excel(path, \"test1\")\n\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0).astype(\n                np.bool_\n            )\n\n        tm.assert_frame_equal(df, recons)\n\n    def test_inf_roundtrip(self, path):\n        df = DataFrame([(1, np.inf), (2, 3), (5, -np.inf)])\n        df.to_excel(path, \"test1\")\n\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n\n        tm.assert_frame_equal(df, recons)\n\n    def test_sheets(self, frame, tsframe, path):\n        # freq doesn't round-trip\n        index = pd.DatetimeIndex(np.asarray(tsframe.index), freq=None)\n        tsframe.index = index\n\n        frame = frame.copy()\n        frame.iloc[:5, frame.columns.get_loc(\"A\")] = np.nan\n\n        frame.to_excel(path, \"test1\")\n        frame.to_excel(path, \"test1\", columns=[\"A\", \"B\"])\n        frame.to_excel(path, \"test1\", header=False)\n        frame.to_excel(path, \"test1\", index=False)\n\n        # Test writing to separate sheets\n        with ExcelWriter(path) as writer:\n            frame.to_excel(writer, \"test1\")\n            tsframe.to_excel(writer, \"test2\")\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n            tm.assert_frame_equal(frame, recons)\n            recons = pd.read_excel(reader, sheet_name=\"test2\", index_col=0)\n            tm.assert_frame_equal(tsframe, recons)\n        assert 2 == len(reader.sheet_names)\n        assert \"test1\" == reader.sheet_names[0]\n        assert \"test2\" == reader.sheet_names[1]\n\n    def test_colaliases(self, frame, path):\n        frame = frame.copy()\n        frame.iloc[:5, frame.columns.get_loc(\"A\")] = np.nan\n\n        frame.to_excel(path, \"test1\")\n        frame.to_excel(path, \"test1\", columns=[\"A\", \"B\"])\n        frame.to_excel(path, \"test1\", header=False)\n        frame.to_excel(path, \"test1\", index=False)\n\n        # column aliases\n        col_aliases = Index([\"AA\", \"X\", \"Y\", \"Z\"])\n        frame.to_excel(path, \"test1\", header=col_aliases)\n        with ExcelFile(path) as reader:\n            rs = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n        xp = frame.copy()\n        xp.columns = col_aliases\n        tm.assert_frame_equal(xp, rs)\n\n    def test_roundtrip_indexlabels(self, merge_cells, frame, path):\n        frame = frame.copy()\n        frame.iloc[:5, frame.columns.get_loc(\"A\")] = np.nan\n\n        frame.to_excel(path, \"test1\")\n        frame.to_excel(path, \"test1\", columns=[\"A\", \"B\"])\n        frame.to_excel(path, \"test1\", header=False)\n        frame.to_excel(path, \"test1\", index=False)\n\n        # test index_label\n        df = DataFrame(np.random.randn(10, 2)) >= 0\n        df.to_excel(path, \"test1\", index_label=[\"test\"], merge_cells=merge_cells)\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0).astype(\n                np.int64\n            )\n        df.index.names = [\"test\"]\n        assert df.index.names == recons.index.names\n\n        df = DataFrame(np.random.randn(10, 2)) >= 0\n        df.to_excel(\n            path,\n            \"test1\",\n            index_label=[\"test\", \"dummy\", \"dummy2\"],\n            merge_cells=merge_cells,\n        )\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0).astype(\n                np.int64\n            )\n        df.index.names = [\"test\"]\n        assert df.index.names == recons.index.names\n\n        df = DataFrame(np.random.randn(10, 2)) >= 0\n        df.to_excel(path, \"test1\", index_label=\"test\", merge_cells=merge_cells)\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0).astype(\n                np.int64\n            )\n        df.index.names = [\"test\"]\n        tm.assert_frame_equal(df, recons.astype(bool))\n\n        frame.to_excel(\n            path,\n            \"test1\",\n            columns=[\"A\", \"B\", \"C\", \"D\"],\n            index=False,\n            merge_cells=merge_cells,\n        )\n        # take 'A' and 'B' as indexes (same row as cols 'C', 'D')\n        df = frame.copy()\n        df = df.set_index([\"A\", \"B\"])\n\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=[0, 1])\n        tm.assert_frame_equal(df, recons)\n\n    def test_excel_roundtrip_indexname(self, merge_cells, path):\n        df = DataFrame(np.random.randn(10, 4))\n        df.index.name = \"foo\"\n\n        df.to_excel(path, merge_cells=merge_cells)\n\n        with ExcelFile(path) as xf:\n            result = pd.read_excel(xf, sheet_name=xf.sheet_names[0], index_col=0)\n\n        tm.assert_frame_equal(result, df)\n        assert result.index.name == \"foo\"\n\n    def test_excel_roundtrip_datetime(self, merge_cells, tsframe, path):\n        # datetime.date, not sure what to test here exactly\n\n        # freq does not round-trip\n        index = pd.DatetimeIndex(np.asarray(tsframe.index), freq=None)\n        tsframe.index = index\n\n        tsf = tsframe.copy()\n\n        tsf.index = [x.date() for x in tsframe.index]\n        tsf.to_excel(path, \"test1\", merge_cells=merge_cells)\n\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n\n        tm.assert_frame_equal(tsframe, recons)\n\n    def test_excel_date_datetime_format(self, ext, path):\n        # see gh-4133\n        #\n        # Excel output format strings\n        df = DataFrame(\n            [\n                [date(2014, 1, 31), date(1999, 9, 24)],\n                [datetime(1998, 5, 26, 23, 33, 4), datetime(2014, 2, 28, 13, 5, 13)],\n            ],\n            index=[\"DATE\", \"DATETIME\"],\n            columns=[\"X\", \"Y\"],\n        )\n        df_expected = DataFrame(\n            [\n                [datetime(2014, 1, 31), datetime(1999, 9, 24)],\n                [datetime(1998, 5, 26, 23, 33, 4), datetime(2014, 2, 28, 13, 5, 13)],\n            ],\n            index=[\"DATE\", \"DATETIME\"],\n            columns=[\"X\", \"Y\"],\n        )\n\n        with tm.ensure_clean(ext) as filename2:\n            with ExcelWriter(path) as writer1:\n                df.to_excel(writer1, \"test1\")\n\n            with ExcelWriter(\n                filename2,\n                date_format=\"DD.MM.YYYY\",\n                datetime_format=\"DD.MM.YYYY HH-MM-SS\",\n            ) as writer2:\n                df.to_excel(writer2, \"test1\")\n\n            with ExcelFile(path) as reader1:\n                rs1 = pd.read_excel(reader1, sheet_name=\"test1\", index_col=0)\n\n            with ExcelFile(filename2) as reader2:\n                rs2 = pd.read_excel(reader2, sheet_name=\"test1\", index_col=0)\n\n        tm.assert_frame_equal(rs1, rs2)\n\n        # Since the reader returns a datetime object for dates,\n        # we need to use df_expected to check the result.\n        tm.assert_frame_equal(rs2, df_expected)\n\n    def test_to_excel_interval_no_labels(self, path):\n        # see gh-19242\n        #\n        # Test writing Interval without labels.\n        df = DataFrame(np.random.randint(-10, 10, size=(20, 1)), dtype=np.int64)\n        expected = df.copy()\n\n        df[\"new\"] = pd.cut(df[0], 10)\n        expected[\"new\"] = pd.cut(expected[0], 10).astype(str)\n\n        df.to_excel(path, \"test1\")\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n        tm.assert_frame_equal(expected, recons)\n\n    def test_to_excel_interval_labels(self, path):\n        # see gh-19242\n        #\n        # Test writing Interval with labels.\n        df = DataFrame(np.random.randint(-10, 10, size=(20, 1)), dtype=np.int64)\n        expected = df.copy()\n        intervals = pd.cut(\n            df[0], 10, labels=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\n        )\n        df[\"new\"] = intervals\n        expected[\"new\"] = pd.Series(list(intervals))\n\n        df.to_excel(path, \"test1\")\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n        tm.assert_frame_equal(expected, recons)\n\n    def test_to_excel_timedelta(self, path):\n        # see gh-19242, gh-9155\n        #\n        # Test writing timedelta to xls.\n        df = DataFrame(\n            np.random.randint(-10, 10, size=(20, 1)), columns=[\"A\"], dtype=np.int64\n        )\n        expected = df.copy()\n\n        df[\"new\"] = df[\"A\"].apply(lambda x: timedelta(seconds=x))\n        expected[\"new\"] = expected[\"A\"].apply(\n            lambda x: timedelta(seconds=x).total_seconds() / 86400\n        )\n\n        df.to_excel(path, \"test1\")\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n        tm.assert_frame_equal(expected, recons)\n\n    def test_to_excel_periodindex(self, tsframe, path):\n        xp = tsframe.resample(\"M\", kind=\"period\").mean()\n\n        xp.to_excel(path, \"sht1\")\n\n        with ExcelFile(path) as reader:\n            rs = pd.read_excel(reader, sheet_name=\"sht1\", index_col=0)\n        tm.assert_frame_equal(xp, rs.to_period(\"M\"))\n\n    def test_to_excel_multiindex(self, merge_cells, frame, path):\n        arrays = np.arange(len(frame.index) * 2, dtype=np.int64).reshape(2, -1)\n        new_index = MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\n        frame.index = new_index\n\n        frame.to_excel(path, \"test1\", header=False)\n        frame.to_excel(path, \"test1\", columns=[\"A\", \"B\"])\n\n        # round trip\n        frame.to_excel(path, \"test1\", merge_cells=merge_cells)\n        with ExcelFile(path) as reader:\n            df = pd.read_excel(reader, sheet_name=\"test1\", index_col=[0, 1])\n        tm.assert_frame_equal(frame, df)\n\n    # GH13511\n    def test_to_excel_multiindex_nan_label(self, merge_cells, path):\n        df = DataFrame({\"A\": [None, 2, 3], \"B\": [10, 20, 30], \"C\": np.random.sample(3)})\n        df = df.set_index([\"A\", \"B\"])\n\n        df.to_excel(path, merge_cells=merge_cells)\n        df1 = pd.read_excel(path, index_col=[0, 1])\n        tm.assert_frame_equal(df, df1)\n\n    # Test for Issue 11328. If column indices are integers, make\n    # sure they are handled correctly for either setting of\n    # merge_cells\n    def test_to_excel_multiindex_cols(self, merge_cells, frame, path):\n        arrays = np.arange(len(frame.index) * 2, dtype=np.int64).reshape(2, -1)\n        new_index = MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\n        frame.index = new_index\n\n        new_cols_index = MultiIndex.from_tuples([(40, 1), (40, 2), (50, 1), (50, 2)])\n        frame.columns = new_cols_index\n        header = [0, 1]\n        if not merge_cells:\n            header = 0\n\n        # round trip\n        frame.to_excel(path, \"test1\", merge_cells=merge_cells)\n        with ExcelFile(path) as reader:\n            df = pd.read_excel(\n                reader, sheet_name=\"test1\", header=header, index_col=[0, 1]\n            )\n        if not merge_cells:\n            fm = frame.columns.format(sparsify=False, adjoin=False, names=False)\n            frame.columns = [\".\".join(map(str, q)) for q in zip(*fm)]\n        tm.assert_frame_equal(frame, df)\n\n    def test_to_excel_multiindex_dates(self, merge_cells, tsframe, path):\n        # try multiindex with dates\n        new_index = [tsframe.index, np.arange(len(tsframe.index), dtype=np.int64)]\n        tsframe.index = MultiIndex.from_arrays(new_index)\n\n        tsframe.index.names = [\"time\", \"foo\"]\n        tsframe.to_excel(path, \"test1\", merge_cells=merge_cells)\n        with ExcelFile(path) as reader:\n            recons = pd.read_excel(reader, sheet_name=\"test1\", index_col=[0, 1])\n\n        tm.assert_frame_equal(tsframe, recons)\n        assert recons.index.names == (\"time\", \"foo\")\n\n    def test_to_excel_multiindex_no_write_index(self, path):\n        # Test writing and re-reading a MI without the index. GH 5616.\n\n        # Initial non-MI frame.\n        frame1 = DataFrame({\"a\": [10, 20], \"b\": [30, 40], \"c\": [50, 60]})\n\n        # Add a MI.\n        frame2 = frame1.copy()\n        multi_index = MultiIndex.from_tuples([(70, 80), (90, 100)])\n        frame2.index = multi_index\n\n        # Write out to Excel without the index.\n        frame2.to_excel(path, \"test1\", index=False)\n\n        # Read it back in.\n        with ExcelFile(path) as reader:\n            frame3 = pd.read_excel(reader, sheet_name=\"test1\")\n\n        # Test that it is the same as the initial frame.\n        tm.assert_frame_equal(frame1, frame3)\n\n    def test_to_excel_empty_multiindex(self, path):\n        # GH 19543.\n        expected = DataFrame([], columns=[0, 1, 2])\n\n        df = DataFrame([], index=MultiIndex.from_tuples([], names=[0, 1]), columns=[2])\n        df.to_excel(path, \"test1\")\n\n        with ExcelFile(path) as reader:\n            result = pd.read_excel(reader, sheet_name=\"test1\")\n        tm.assert_frame_equal(\n            result, expected, check_index_type=False, check_dtype=False\n        )\n\n    def test_to_excel_float_format(self, path):\n        df = DataFrame(\n            [[0.123456, 0.234567, 0.567567], [12.32112, 123123.2, 321321.2]],\n            index=[\"A\", \"B\"],\n            columns=[\"X\", \"Y\", \"Z\"],\n        )\n        df.to_excel(path, \"test1\", float_format=\"%.2f\")\n\n        with ExcelFile(path) as reader:\n            result = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n\n        expected = DataFrame(\n            [[0.12, 0.23, 0.57], [12.32, 123123.20, 321321.20]],\n            index=[\"A\", \"B\"],\n            columns=[\"X\", \"Y\", \"Z\"],\n        )\n        tm.assert_frame_equal(result, expected)\n\n    def test_to_excel_output_encoding(self, ext):\n        # Avoid mixed inferred_type.\n        df = DataFrame(\n            [[\"\\u0192\", \"\\u0193\", \"\\u0194\"], [\"\\u0195\", \"\\u0196\", \"\\u0197\"]],\n            index=[\"A\\u0192\", \"B\"],\n            columns=[\"X\\u0193\", \"Y\", \"Z\"],\n        )\n\n        with tm.ensure_clean(\"__tmp_to_excel_float_format__.\" + ext) as filename:\n            df.to_excel(filename, sheet_name=\"TestSheet\")\n            result = pd.read_excel(filename, sheet_name=\"TestSheet\", index_col=0)\n            tm.assert_frame_equal(result, df)\n\n    def test_to_excel_unicode_filename(self, ext):\n        with tm.ensure_clean(\"\\u0192u.\" + ext) as filename:\n            try:\n                with open(filename, \"wb\"):\n                    pass\n            except UnicodeEncodeError:\n                pytest.skip(\"No unicode file names on this system\")\n\n            df = DataFrame(\n                [[0.123456, 0.234567, 0.567567], [12.32112, 123123.2, 321321.2]],\n                index=[\"A\", \"B\"],\n                columns=[\"X\", \"Y\", \"Z\"],\n            )\n            df.to_excel(filename, \"test1\", float_format=\"%.2f\")\n\n            with ExcelFile(filename) as reader:\n                result = pd.read_excel(reader, sheet_name=\"test1\", index_col=0)\n\n        expected = DataFrame(\n            [[0.12, 0.23, 0.57], [12.32, 123123.20, 321321.20]],\n            index=[\"A\", \"B\"],\n            columns=[\"X\", \"Y\", \"Z\"],\n        )\n        tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\"use_headers\", [True, False])\n    @pytest.mark.parametrize(\"r_idx_nlevels\", [1, 2, 3])\n    @pytest.mark.parametrize(\"c_idx_nlevels\", [1, 2, 3])\n    def test_excel_010_hemstring(\n        self, merge_cells, c_idx_nlevels, r_idx_nlevels, use_headers, path\n    ):\n        def roundtrip(data, header=True, parser_hdr=0, index=True):\n            data.to_excel(path, header=header, merge_cells=merge_cells, index=index)\n\n            with ExcelFile(path) as xf:\n                return pd.read_excel(\n                    xf, sheet_name=xf.sheet_names[0], header=parser_hdr\n                )\n\n        # Basic test.\n        parser_header = 0 if use_headers else None\n        res = roundtrip(DataFrame([0]), use_headers, parser_header)\n\n        assert res.shape == (1, 2)\n        assert res.iloc[0, 0] is not np.nan\n\n        # More complex tests with multi-index.\n        nrows = 5\n        ncols = 3\n\n        # ensure limited functionality in 0.10\n        # override of gh-2370 until sorted out in 0.11\n\n        df = tm.makeCustomDataframe(\n            nrows, ncols, r_idx_nlevels=r_idx_nlevels, c_idx_nlevels=c_idx_nlevels\n        )\n\n        # This if will be removed once multi-column Excel writing\n        # is implemented. For now fixing gh-9794.\n        if c_idx_nlevels > 1:\n            msg = (\n                \"Writing to Excel with MultiIndex columns and no index \"\n                \"\\\\('index'=False\\\\) is not yet implemented.\"\n            )\n            with pytest.raises(NotImplementedError, match=msg):\n                roundtrip(df, use_headers, index=False)\n        else:\n            res = roundtrip(df, use_headers)\n\n            if use_headers:\n                assert res.shape == (nrows, ncols + r_idx_nlevels)\n            else:\n                # First row taken as columns.\n                assert res.shape == (nrows - 1, ncols + r_idx_nlevels)\n\n            # No NaNs.\n            for r in range(len(res.index)):\n                for c in range(len(res.columns)):\n                    assert res.iloc[r, c] is not np.nan\n\n    def test_duplicated_columns(self, path):\n        # see gh-5235\n        df = DataFrame([[1, 2, 3], [1, 2, 3], [1, 2, 3]], columns=[\"A\", \"B\", \"B\"])\n        df.to_excel(path, \"test1\")\n        expected = DataFrame(\n            [[1, 2, 3], [1, 2, 3], [1, 2, 3]], columns=[\"A\", \"B\", \"B.1\"]\n        )\n\n        # By default, we mangle.\n        result = pd.read_excel(path, sheet_name=\"test1\", index_col=0)\n        tm.assert_frame_equal(result, expected)\n\n        # see gh-11007, gh-10970\n        df = DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], columns=[\"A\", \"B\", \"A\", \"B\"])\n        df.to_excel(path, \"test1\")\n\n        result = pd.read_excel(path, sheet_name=\"test1\", index_col=0)\n        expected = DataFrame(\n            [[1, 2, 3, 4], [5, 6, 7, 8]], columns=[\"A\", \"B\", \"A.1\", \"B.1\"]\n        )\n        tm.assert_frame_equal(result, expected)\n\n        # see gh-10982\n        df.to_excel(path, \"test1\", index=False, header=False)\n        result = pd.read_excel(path, sheet_name=\"test1\", header=None)\n\n        expected = DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]])\n        tm.assert_frame_equal(result, expected)\n\n    def test_swapped_columns(self, path):\n        # Test for issue #5427.\n        write_frame = DataFrame({\"A\": [1, 1, 1], \"B\": [2, 2, 2]})\n        write_frame.to_excel(path, \"test1\", columns=[\"B\", \"A\"])\n\n        read_frame = pd.read_excel(path, sheet_name=\"test1\", header=0)\n\n        tm.assert_series_equal(write_frame[\"A\"], read_frame[\"A\"])\n        tm.assert_series_equal(write_frame[\"B\"], read_frame[\"B\"])\n\n    def test_invalid_columns(self, path):\n        # see gh-10982\n        write_frame = DataFrame({\"A\": [1, 1, 1], \"B\": [2, 2, 2]})\n\n        with pytest.raises(KeyError, match=\"Not all names specified\"):\n            write_frame.to_excel(path, \"test1\", columns=[\"B\", \"C\"])\n\n        with pytest.raises(\n            KeyError, match=\"'passes columns are not ALL present dataframe'\"\n        ):\n            write_frame.to_excel(path, \"test1\", columns=[\"C\", \"D\"])\n\n    @pytest.mark.parametrize(\n        \"to_excel_index,read_excel_index_col\",\n        [\n            (True, 0),  # Include index in write to file\n            (False, None),  # Dont include index in write to file\n        ],\n    )\n    def test_write_subset_columns(self, path, to_excel_index, read_excel_index_col):\n        # GH 31677\n        write_frame = DataFrame({\"A\": [1, 1, 1], \"B\": [2, 2, 2], \"C\": [3, 3, 3]})\n        write_frame.to_excel(\n            path, \"col_subset_bug\", columns=[\"A\", \"B\"], index=to_excel_index\n        )\n\n        expected = write_frame[[\"A\", \"B\"]]\n        read_frame = pd.read_excel(\n            path, sheet_name=\"col_subset_bug\", index_col=read_excel_index_col\n        )\n\n        tm.assert_frame_equal(expected, read_frame)\n\n    def test_comment_arg(self, path):\n        # see gh-18735\n        #\n        # Test the comment argument functionality to pd.read_excel.\n\n        # Create file to read in.\n        df = DataFrame({\"A\": [\"one\", \"#one\", \"one\"], \"B\": [\"two\", \"two\", \"#two\"]})\n        df.to_excel(path, \"test_c\")\n\n        # Read file without comment arg.\n        result1 = pd.read_excel(path, sheet_name=\"test_c\", index_col=0)\n\n        result1.iloc[1, 0] = None\n        result1.iloc[1, 1] = None\n        result1.iloc[2, 1] = None\n\n        result2 = pd.read_excel(path, sheet_name=\"test_c\", comment=\"#\", index_col=0)\n        tm.assert_frame_equal(result1, result2)\n\n    def test_comment_default(self, path):\n        # Re issue #18735\n        # Test the comment argument default to pd.read_excel\n\n        # Create file to read in\n        df = DataFrame({\"A\": [\"one\", \"#one\", \"one\"], \"B\": [\"two\", \"two\", \"#two\"]})\n        df.to_excel(path, \"test_c\")\n\n        # Read file with default and explicit comment=None\n        result1 = pd.read_excel(path, sheet_name=\"test_c\")\n        result2 = pd.read_excel(path, sheet_name=\"test_c\", comment=None)\n        tm.assert_frame_equal(result1, result2)\n\n    def test_comment_used(self, path):\n        # see gh-18735\n        #\n        # Test the comment argument is working as expected when used.\n\n        # Create file to read in.\n        df = DataFrame({\"A\": [\"one\", \"#one\", \"one\"], \"B\": [\"two\", \"two\", \"#two\"]})\n        df.to_excel(path, \"test_c\")\n\n        # Test read_frame_comment against manually produced expected output.\n        expected = DataFrame({\"A\": [\"one\", None, \"one\"], \"B\": [\"two\", None, None]})\n        result = pd.read_excel(path, sheet_name=\"test_c\", comment=\"#\", index_col=0)\n        tm.assert_frame_equal(result, expected)\n\n    def test_comment_empty_line(self, path):\n        # Re issue #18735\n        # Test that pd.read_excel ignores commented lines at the end of file\n\n        df = DataFrame({\"a\": [\"1\", \"#2\"], \"b\": [\"2\", \"3\"]})\n        df.to_excel(path, index=False)\n\n        # Test that all-comment lines at EoF are ignored\n        expected = DataFrame({\"a\": [1], \"b\": [2]})\n        result = pd.read_excel(path, comment=\"#\")\n        tm.assert_frame_equal(result, expected)\n\n    def test_datetimes(self, path):\n        # Test writing and reading datetimes. For issue #9139. (xref #9185)\n        datetimes = [\n            datetime(2013, 1, 13, 1, 2, 3),\n            datetime(2013, 1, 13, 2, 45, 56),\n            datetime(2013, 1, 13, 4, 29, 49),\n            datetime(2013, 1, 13, 6, 13, 42),\n            datetime(2013, 1, 13, 7, 57, 35),\n            datetime(2013, 1, 13, 9, 41, 28),\n            datetime(2013, 1, 13, 11, 25, 21),\n            datetime(2013, 1, 13, 13, 9, 14),\n            datetime(2013, 1, 13, 14, 53, 7),\n            datetime(2013, 1, 13, 16, 37, 0),\n            datetime(2013, 1, 13, 18, 20, 52),\n        ]\n\n        write_frame = DataFrame({\"A\": datetimes})\n        write_frame.to_excel(path, \"Sheet1\")\n        read_frame = pd.read_excel(path, sheet_name=\"Sheet1\", header=0)\n\n        tm.assert_series_equal(write_frame[\"A\"], read_frame[\"A\"])\n\n    def test_bytes_io(self, engine):\n        # see gh-7074\n        with BytesIO() as bio:\n            df = DataFrame(np.random.randn(10, 2))\n\n            # Pass engine explicitly, as there is no file path to infer from.\n            with ExcelWriter(bio, engine=engine) as writer:\n                df.to_excel(writer)\n\n            bio.seek(0)\n            reread_df = pd.read_excel(bio, index_col=0)\n            tm.assert_frame_equal(df, reread_df)\n\n    def test_write_lists_dict(self, path):\n        # see gh-8188.\n        df = DataFrame(\n            {\n                \"mixed\": [\"a\", [\"b\", \"c\"], {\"d\": \"e\", \"f\": 2}],\n                \"numeric\": [1, 2, 3.0],\n                \"str\": [\"apple\", \"banana\", \"cherry\"],\n            }\n        )\n        df.to_excel(path, \"Sheet1\")\n        read = pd.read_excel(path, sheet_name=\"Sheet1\", header=0, index_col=0)\n\n        expected = df.copy()\n        expected.mixed = expected.mixed.apply(str)\n        expected.numeric = expected.numeric.astype(\"int64\")\n\n        tm.assert_frame_equal(read, expected)\n\n    def test_render_as_column_name(self, path):\n        # see gh-34331\n        df = DataFrame({\"render\": [1, 2], \"data\": [3, 4]})\n        df.to_excel(path, \"Sheet1\")\n        read = pd.read_excel(path, \"Sheet1\", index_col=0)\n        expected = df\n        tm.assert_frame_equal(read, expected)\n\n    def test_true_and_false_value_options(self, path):\n        # see gh-13347\n        df = DataFrame([[\"foo\", \"bar\"]], columns=[\"col1\", \"col2\"])\n        expected = df.replace({\"foo\": True, \"bar\": False})\n\n        df.to_excel(path)\n        read_frame = pd.read_excel(\n            path, true_values=[\"foo\"], false_values=[\"bar\"], index_col=0\n        )\n        tm.assert_frame_equal(read_frame, expected)\n\n    def test_freeze_panes(self, path):\n        # see gh-15160\n        expected = DataFrame([[1, 2], [3, 4]], columns=[\"col1\", \"col2\"])\n        expected.to_excel(path, \"Sheet1\", freeze_panes=(1, 1))\n\n        result = pd.read_excel(path, index_col=0)\n        tm.assert_frame_equal(result, expected)\n\n    def test_path_path_lib(self, engine, ext):\n        df = tm.makeDataFrame()\n        writer = partial(df.to_excel, engine=engine)\n\n        reader = partial(pd.read_excel, index_col=0)\n        result = tm.round_trip_pathlib(writer, reader, path=f\"foo{ext}\")\n        tm.assert_frame_equal(result, df)\n\n    def test_path_local_path(self, engine, ext):\n        df = tm.makeDataFrame()\n        writer = partial(df.to_excel, engine=engine)\n\n        reader = partial(pd.read_excel, index_col=0)\n        result = tm.round_trip_localpath(writer, reader, path=f\"foo{ext}\")\n        tm.assert_frame_equal(result, df)\n\n    def test_merged_cell_custom_objects(self, path):\n        # see GH-27006\n        mi = MultiIndex.from_tuples(\n            [\n                (pd.Period(\"2018\"), pd.Period(\"2018Q1\")),\n                (pd.Period(\"2018\"), pd.Period(\"2018Q2\")),\n            ]\n        )\n        expected = DataFrame(np.ones((2, 2), dtype=\"int64\"), columns=mi)\n        expected.to_excel(path)\n        result = pd.read_excel(path, header=[0, 1], index_col=0)\n        # need to convert PeriodIndexes to standard Indexes for assert equal\n        expected.columns = expected.columns.set_levels(\n            [[str(i) for i in mi.levels[0]], [str(i) for i in mi.levels[1]]],\n            level=[0, 1],\n        )\n        tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\"dtype\", [None, object])\n    def test_raise_when_saving_timezones(self, dtype, tz_aware_fixture, path):\n        # GH 27008, GH 7056\n        tz = tz_aware_fixture\n        data = pd.Timestamp(\"2019\", tz=tz)\n        df = DataFrame([data], dtype=dtype)\n        with pytest.raises(ValueError, match=\"Excel does not support\"):\n            df.to_excel(path)\n\n        data = data.to_pydatetime()\n        df = DataFrame([data], dtype=dtype)\n        with pytest.raises(ValueError, match=\"Excel does not support\"):\n            df.to_excel(path)\n\n    def test_excel_duplicate_columns_with_names(self, path):\n        # GH#39695\n        df = DataFrame({\"A\": [0, 1], \"B\": [10, 11]})\n        df.to_excel(path, columns=[\"A\", \"B\", \"A\"], index=False)\n\n        result = pd.read_excel(path)\n        expected = DataFrame([[0, 10, 0], [1, 11, 1]], columns=[\"A\", \"B\", \"A.1\"])\n        tm.assert_frame_equal(result, expected)\n\n    def test_if_sheet_exists_raises(self, ext):\n        # GH 40230\n        msg = \"if_sheet_exists is only valid in append mode (mode='a')\"\n\n        with tm.ensure_clean(ext) as f:\n            with pytest.raises(ValueError, match=re.escape(msg)):\n                ExcelWriter(f, if_sheet_exists=\"replace\")\n\n    def test_excel_writer_empty_frame(self, engine, ext):\n        # GH#45793\n        with tm.ensure_clean(ext) as path:\n            with ExcelWriter(path, engine=engine) as writer:\n                DataFrame().to_excel(writer)\n            result = pd.read_excel(path)\n            expected = DataFrame()\n            tm.assert_frame_equal(result, expected)\n\n    def test_to_excel_empty_frame(self, engine, ext):\n        # GH#45793\n        with tm.ensure_clean(ext) as path:\n            DataFrame().to_excel(path, engine=engine)\n            result = pd.read_excel(path)\n            expected = DataFrame()\n            tm.assert_frame_equal(result, expected)\n\n\nclass TestExcelWriterEngineTests:\n    @pytest.mark.parametrize(\n        \"klass,ext\",\n        [\n            pytest.param(_XlsxWriter, \".xlsx\", marks=td.skip_if_no(\"xlsxwriter\")),\n            pytest.param(_OpenpyxlWriter, \".xlsx\", marks=td.skip_if_no(\"openpyxl\")),\n        ],\n    )\n    def test_ExcelWriter_dispatch(self, klass, ext):\n        with tm.ensure_clean(ext) as path:\n            with ExcelWriter(path) as writer:\n                if ext == \".xlsx\" and td.safe_import(\"xlsxwriter\"):\n                    # xlsxwriter has preference over openpyxl if both installed\n                    assert isinstance(writer, _XlsxWriter)\n                else:\n                    assert isinstance(writer, klass)\n\n    def test_ExcelWriter_dispatch_raises(self):\n        with pytest.raises(ValueError, match=\"No engine\"):\n            ExcelWriter(\"nothing\")\n\n    def test_register_writer(self):\n        class DummyClass(ExcelWriter):\n            called_save = False\n            called_write_cells = False\n            called_sheets = False\n            _supported_extensions = (\"xlsx\", \"xls\")\n            _engine = \"dummy\"\n\n            def book(self):\n                pass\n\n            def _save(self):\n                type(self).called_save = True\n\n            def _write_cells(self, *args, **kwargs):\n                type(self).called_write_cells = True\n\n            @property\n            def sheets(self):\n                type(self).called_sheets = True\n\n            @classmethod\n            def assert_called_and_reset(cls):\n                assert cls.called_save\n                assert cls.called_write_cells\n                assert not cls.called_sheets\n                cls.called_save = False\n                cls.called_write_cells = False\n\n        register_writer(DummyClass)\n\n        with option_context(\"io.excel.xlsx.writer\", \"dummy\"):\n            path = \"something.xlsx\"\n            with tm.ensure_clean(path) as filepath:\n                with ExcelWriter(filepath) as writer:\n                    assert isinstance(writer, DummyClass)\n                df = tm.makeCustomDataframe(1, 1)\n                df.to_excel(filepath)\n            DummyClass.assert_called_and_reset()\n\n        with tm.ensure_clean(\"something.xls\") as filepath:\n            df.to_excel(filepath, engine=\"dummy\")\n        DummyClass.assert_called_and_reset()\n\n\n@td.skip_if_no(\"xlrd\")\n@td.skip_if_no(\"openpyxl\")\nclass TestFSPath:\n    def test_excelfile_fspath(self):\n        with tm.ensure_clean(\"foo.xlsx\") as path:\n            df = DataFrame({\"A\": [1, 2]})\n            df.to_excel(path)\n            with ExcelFile(path) as xl:\n                result = os.fspath(xl)\n            assert result == path\n\n    def test_excelwriter_fspath(self):\n        with tm.ensure_clean(\"foo.xlsx\") as path:\n            with ExcelWriter(path) as writer:\n                assert os.fspath(writer) == str(path)\n\n\n@pytest.mark.parametrize(\"klass\", _writers.values())\ndef test_subclass_attr(klass):\n    # testing that subclasses of ExcelWriter don't have public attributes (issue 49602)\n    attrs_base = {name for name in dir(ExcelWriter) if not name.startswith(\"_\")}\n    attrs_klass = {name for name in dir(klass) if not name.startswith(\"_\")}\n    assert not attrs_base.symmetric_difference(attrs_klass)\n"
    },
    {
      "filename": "pandas/tests/io/parser/test_parse_dates.py",
      "content": "\"\"\"\nTests date parsing functionality for all of the\nparsers defined in parsers.py\n\"\"\"\n\nfrom datetime import (\n    date,\n    datetime,\n    timedelta,\n    timezone,\n)\nfrom io import StringIO\n\nfrom dateutil.parser import parse as du_parse\nfrom hypothesis import given\nimport numpy as np\nimport pytest\nimport pytz\n\nfrom pandas._libs.tslibs import parsing\nfrom pandas._libs.tslibs.parsing import py_parse_datetime_string\nfrom pandas.compat.pyarrow import pa_version_under7p0\n\nimport pandas as pd\nfrom pandas import (\n    DataFrame,\n    DatetimeIndex,\n    Index,\n    MultiIndex,\n    Series,\n    Timestamp,\n)\nimport pandas._testing as tm\nfrom pandas._testing._hypothesis import DATETIME_NO_TZ\nfrom pandas.core.indexes.datetimes import date_range\n\nfrom pandas.io.parsers import read_csv\n\nxfail_pyarrow = pytest.mark.usefixtures(\"pyarrow_xfail\")\n\n# GH#43650: Some expected failures with the pyarrow engine can occasionally\n# cause a deadlock instead, so we skip these instead of xfailing\nskip_pyarrow = pytest.mark.usefixtures(\"pyarrow_skip\")\n\n\n@xfail_pyarrow\ndef test_read_csv_with_custom_date_parser(all_parsers):\n    # GH36111\n    def __custom_date_parser(time):\n        time = time.astype(np.float_)\n        time = time.astype(np.int_)  # convert float seconds to int type\n        return pd.to_timedelta(time, unit=\"s\")\n\n    testdata = StringIO(\n        \"\"\"time e n h\n        41047.00 -98573.7297 871458.0640 389.0089\n        41048.00 -98573.7299 871458.0640 389.0089\n        41049.00 -98573.7300 871458.0642 389.0088\n        41050.00 -98573.7299 871458.0643 389.0088\n        41051.00 -98573.7302 871458.0640 389.0086\n        \"\"\"\n    )\n    result = all_parsers.read_csv_check_warnings(\n        FutureWarning,\n        \"Please use 'date_format' instead\",\n        testdata,\n        delim_whitespace=True,\n        parse_dates=True,\n        date_parser=__custom_date_parser,\n        index_col=\"time\",\n    )\n    time = [41047, 41048, 41049, 41050, 41051]\n    time = pd.TimedeltaIndex([pd.to_timedelta(i, unit=\"s\") for i in time], name=\"time\")\n    expected = DataFrame(\n        {\n            \"e\": [-98573.7297, -98573.7299, -98573.7300, -98573.7299, -98573.7302],\n            \"n\": [871458.0640, 871458.0640, 871458.0642, 871458.0643, 871458.0640],\n            \"h\": [389.0089, 389.0089, 389.0088, 389.0088, 389.0086],\n        },\n        index=time,\n    )\n\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_read_csv_with_custom_date_parser_parse_dates_false(all_parsers):\n    # GH44366\n    def __custom_date_parser(time):\n        time = time.astype(np.float_)\n        time = time.astype(np.int_)  # convert float seconds to int type\n        return pd.to_timedelta(time, unit=\"s\")\n\n    testdata = StringIO(\n        \"\"\"time e\n        41047.00 -93.77\n        41048.00 -95.79\n        41049.00 -98.73\n        41050.00 -93.99\n        41051.00 -97.72\n        \"\"\"\n    )\n    result = all_parsers.read_csv_check_warnings(\n        FutureWarning,\n        \"Please use 'date_format' instead\",\n        testdata,\n        delim_whitespace=True,\n        parse_dates=False,\n        date_parser=__custom_date_parser,\n        index_col=\"time\",\n    )\n    time = Series([41047.00, 41048.00, 41049.00, 41050.00, 41051.00], name=\"time\")\n    expected = DataFrame(\n        {\"e\": [-93.77, -95.79, -98.73, -93.99, -97.72]},\n        index=time,\n    )\n\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_separator_date_conflict(all_parsers):\n    # Regression test for gh-4678\n    #\n    # Make sure thousands separator and\n    # date parsing do not conflict.\n    parser = all_parsers\n    data = \"06-02-2013;13:00;1-000.215\"\n    expected = DataFrame(\n        [[datetime(2013, 6, 2, 13, 0, 0), 1000.215]], columns=[\"Date\", 2]\n    )\n\n    df = parser.read_csv(\n        StringIO(data),\n        sep=\";\",\n        thousands=\"-\",\n        parse_dates={\"Date\": [0, 1]},\n        header=None,\n    )\n    tm.assert_frame_equal(df, expected)\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\"keep_date_col\", [True, False])\ndef test_multiple_date_col_custom(all_parsers, keep_date_col):\n    data = \"\"\"\\\nKORD,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\nKORD,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\nKORD,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\nKORD,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\nKORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\nKORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000\n\"\"\"\n    parser = all_parsers\n\n    def date_parser(*date_cols):\n        \"\"\"\n        Test date parser.\n\n        Parameters\n        ----------\n        date_cols : args\n            The list of data columns to parse.\n\n        Returns\n        -------\n        parsed : Series\n        \"\"\"\n        return parsing.try_parse_dates(\n            parsing.concat_date_cols(date_cols), parser=du_parse\n        )\n\n    kwds = {\n        \"header\": None,\n        \"date_parser\": date_parser,\n        \"parse_dates\": {\"actual\": [1, 2], \"nominal\": [1, 3]},\n        \"keep_date_col\": keep_date_col,\n        \"names\": [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\"],\n    }\n    result = parser.read_csv_check_warnings(\n        FutureWarning,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        **kwds,\n    )\n\n    expected = DataFrame(\n        [\n            [\n                datetime(1999, 1, 27, 19, 0),\n                datetime(1999, 1, 27, 18, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 19:00:00\",\n                \" 18:56:00\",\n                0.81,\n                2.81,\n                7.2,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 20, 0),\n                datetime(1999, 1, 27, 19, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 20:00:00\",\n                \" 19:56:00\",\n                0.01,\n                2.21,\n                7.2,\n                0.0,\n                260.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                datetime(1999, 1, 27, 20, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 21:00:00\",\n                \" 20:56:00\",\n                -0.59,\n                2.21,\n                5.7,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                datetime(1999, 1, 27, 21, 18),\n                \"KORD\",\n                \"19990127\",\n                \" 21:00:00\",\n                \" 21:18:00\",\n                -0.99,\n                2.01,\n                3.6,\n                0.0,\n                270.0,\n            ],\n            [\n                datetime(1999, 1, 27, 22, 0),\n                datetime(1999, 1, 27, 21, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 22:00:00\",\n                \" 21:56:00\",\n                -0.59,\n                1.71,\n                5.1,\n                0.0,\n                290.0,\n            ],\n            [\n                datetime(1999, 1, 27, 23, 0),\n                datetime(1999, 1, 27, 22, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 23:00:00\",\n                \" 22:56:00\",\n                -0.59,\n                1.71,\n                4.6,\n                0.0,\n                280.0,\n            ],\n        ],\n        columns=[\n            \"actual\",\n            \"nominal\",\n            \"X0\",\n            \"X1\",\n            \"X2\",\n            \"X3\",\n            \"X4\",\n            \"X5\",\n            \"X6\",\n            \"X7\",\n            \"X8\",\n        ],\n    )\n\n    if not keep_date_col:\n        expected = expected.drop([\"X1\", \"X2\", \"X3\"], axis=1)\n\n    # Python can sometimes be flaky about how\n    # the aggregated columns are entered, so\n    # this standardizes the order.\n    result = result[expected.columns]\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"container\", [list, tuple, Index, Series])\n@pytest.mark.parametrize(\"dim\", [1, 2])\ndef test_concat_date_col_fail(container, dim):\n    msg = \"not all elements from date_cols are numpy arrays\"\n    value = \"19990127\"\n\n    date_cols = tuple(container([value]) for _ in range(dim))\n\n    with pytest.raises(ValueError, match=msg):\n        parsing.concat_date_cols(date_cols)\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\"keep_date_col\", [True, False])\ndef test_multiple_date_col(all_parsers, keep_date_col):\n    data = \"\"\"\\\nKORD,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\nKORD,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\nKORD,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\nKORD,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\nKORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\nKORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000\n\"\"\"\n    parser = all_parsers\n    kwds = {\n        \"header\": None,\n        \"parse_dates\": [[1, 2], [1, 3]],\n        \"keep_date_col\": keep_date_col,\n        \"names\": [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\"],\n    }\n    result = parser.read_csv(StringIO(data), **kwds)\n\n    expected = DataFrame(\n        [\n            [\n                datetime(1999, 1, 27, 19, 0),\n                datetime(1999, 1, 27, 18, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 19:00:00\",\n                \" 18:56:00\",\n                0.81,\n                2.81,\n                7.2,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 20, 0),\n                datetime(1999, 1, 27, 19, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 20:00:00\",\n                \" 19:56:00\",\n                0.01,\n                2.21,\n                7.2,\n                0.0,\n                260.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                datetime(1999, 1, 27, 20, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 21:00:00\",\n                \" 20:56:00\",\n                -0.59,\n                2.21,\n                5.7,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                datetime(1999, 1, 27, 21, 18),\n                \"KORD\",\n                \"19990127\",\n                \" 21:00:00\",\n                \" 21:18:00\",\n                -0.99,\n                2.01,\n                3.6,\n                0.0,\n                270.0,\n            ],\n            [\n                datetime(1999, 1, 27, 22, 0),\n                datetime(1999, 1, 27, 21, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 22:00:00\",\n                \" 21:56:00\",\n                -0.59,\n                1.71,\n                5.1,\n                0.0,\n                290.0,\n            ],\n            [\n                datetime(1999, 1, 27, 23, 0),\n                datetime(1999, 1, 27, 22, 56),\n                \"KORD\",\n                \"19990127\",\n                \" 23:00:00\",\n                \" 22:56:00\",\n                -0.59,\n                1.71,\n                4.6,\n                0.0,\n                280.0,\n            ],\n        ],\n        columns=[\n            \"X1_X2\",\n            \"X1_X3\",\n            \"X0\",\n            \"X1\",\n            \"X2\",\n            \"X3\",\n            \"X4\",\n            \"X5\",\n            \"X6\",\n            \"X7\",\n            \"X8\",\n        ],\n    )\n\n    if not keep_date_col:\n        expected = expected.drop([\"X1\", \"X2\", \"X3\"], axis=1)\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_date_col_as_index_col(all_parsers):\n    data = \"\"\"\\\nKORD,19990127 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\nKORD,19990127 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\nKORD,19990127 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\nKORD,19990127 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\nKORD,19990127 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\n\"\"\"\n    parser = all_parsers\n    kwds = {\n        \"header\": None,\n        \"parse_dates\": [1],\n        \"index_col\": 1,\n        \"names\": [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\"],\n    }\n    result = parser.read_csv(StringIO(data), **kwds)\n\n    index = Index(\n        [\n            datetime(1999, 1, 27, 19, 0),\n            datetime(1999, 1, 27, 20, 0),\n            datetime(1999, 1, 27, 21, 0),\n            datetime(1999, 1, 27, 21, 0),\n            datetime(1999, 1, 27, 22, 0),\n        ],\n        name=\"X1\",\n    )\n    expected = DataFrame(\n        [\n            [\"KORD\", \" 18:56:00\", 0.81, 2.81, 7.2, 0.0, 280.0],\n            [\"KORD\", \" 19:56:00\", 0.01, 2.21, 7.2, 0.0, 260.0],\n            [\"KORD\", \" 20:56:00\", -0.59, 2.21, 5.7, 0.0, 280.0],\n            [\"KORD\", \" 21:18:00\", -0.99, 2.01, 3.6, 0.0, 270.0],\n            [\"KORD\", \" 21:56:00\", -0.59, 1.71, 5.1, 0.0, 290.0],\n        ],\n        columns=[\"X0\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\"],\n        index=index,\n    )\n    if parser.engine == \"pyarrow\" and not pa_version_under7p0:\n        # https://github.com/pandas-dev/pandas/issues/44231\n        # pyarrow 6.0 starts to infer time type\n        expected[\"X2\"] = pd.to_datetime(\"1970-01-01\" + expected[\"X2\"]).dt.time\n\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_multiple_date_cols_int_cast(all_parsers):\n    data = (\n        \"KORD,19990127, 19:00:00, 18:56:00, 0.8100\\n\"\n        \"KORD,19990127, 20:00:00, 19:56:00, 0.0100\\n\"\n        \"KORD,19990127, 21:00:00, 20:56:00, -0.5900\\n\"\n        \"KORD,19990127, 21:00:00, 21:18:00, -0.9900\\n\"\n        \"KORD,19990127, 22:00:00, 21:56:00, -0.5900\\n\"\n        \"KORD,19990127, 23:00:00, 22:56:00, -0.5900\"\n    )\n    parse_dates = {\"actual\": [1, 2], \"nominal\": [1, 3]}\n    parser = all_parsers\n\n    kwds = {\n        \"header\": None,\n        \"parse_dates\": parse_dates,\n        \"date_parser\": pd.to_datetime,\n    }\n    result = parser.read_csv_check_warnings(\n        FutureWarning, \"use 'date_format' instead\", StringIO(data), **kwds\n    )\n\n    expected = DataFrame(\n        [\n            [datetime(1999, 1, 27, 19, 0), datetime(1999, 1, 27, 18, 56), \"KORD\", 0.81],\n            [datetime(1999, 1, 27, 20, 0), datetime(1999, 1, 27, 19, 56), \"KORD\", 0.01],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                datetime(1999, 1, 27, 20, 56),\n                \"KORD\",\n                -0.59,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                datetime(1999, 1, 27, 21, 18),\n                \"KORD\",\n                -0.99,\n            ],\n            [\n                datetime(1999, 1, 27, 22, 0),\n                datetime(1999, 1, 27, 21, 56),\n                \"KORD\",\n                -0.59,\n            ],\n            [\n                datetime(1999, 1, 27, 23, 0),\n                datetime(1999, 1, 27, 22, 56),\n                \"KORD\",\n                -0.59,\n            ],\n        ],\n        columns=[\"actual\", \"nominal\", 0, 4],\n    )\n\n    # Python can sometimes be flaky about how\n    # the aggregated columns are entered, so\n    # this standardizes the order.\n    result = result[expected.columns]\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_multiple_date_col_timestamp_parse(all_parsers):\n    parser = all_parsers\n    data = \"\"\"05/31/2012,15:30:00.029,1306.25,1,E,0,,1306.25\n05/31/2012,15:30:00.029,1306.25,8,E,0,,1306.25\"\"\"\n\n    result = parser.read_csv_check_warnings(\n        FutureWarning,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        parse_dates=[[0, 1]],\n        header=None,\n        date_parser=Timestamp,\n    )\n    expected = DataFrame(\n        [\n            [\n                Timestamp(\"05/31/2012, 15:30:00.029\"),\n                1306.25,\n                1,\n                \"E\",\n                0,\n                np.nan,\n                1306.25,\n            ],\n            [\n                Timestamp(\"05/31/2012, 15:30:00.029\"),\n                1306.25,\n                8,\n                \"E\",\n                0,\n                np.nan,\n                1306.25,\n            ],\n        ],\n        columns=[\"0_1\", 2, 3, 4, 5, 6, 7],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_multiple_date_cols_with_header(all_parsers):\n    parser = all_parsers\n    data = \"\"\"\\\nID,date,NominalTime,ActualTime,TDew,TAir,Windspeed,Precip,WindDir\nKORD,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\nKORD,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\nKORD,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\nKORD,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\nKORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\nKORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000\"\"\"\n\n    result = parser.read_csv(StringIO(data), parse_dates={\"nominal\": [1, 2]})\n    expected = DataFrame(\n        [\n            [\n                datetime(1999, 1, 27, 19, 0),\n                \"KORD\",\n                \" 18:56:00\",\n                0.81,\n                2.81,\n                7.2,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 20, 0),\n                \"KORD\",\n                \" 19:56:00\",\n                0.01,\n                2.21,\n                7.2,\n                0.0,\n                260.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                \"KORD\",\n                \" 20:56:00\",\n                -0.59,\n                2.21,\n                5.7,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                \"KORD\",\n                \" 21:18:00\",\n                -0.99,\n                2.01,\n                3.6,\n                0.0,\n                270.0,\n            ],\n            [\n                datetime(1999, 1, 27, 22, 0),\n                \"KORD\",\n                \" 21:56:00\",\n                -0.59,\n                1.71,\n                5.1,\n                0.0,\n                290.0,\n            ],\n            [\n                datetime(1999, 1, 27, 23, 0),\n                \"KORD\",\n                \" 22:56:00\",\n                -0.59,\n                1.71,\n                4.6,\n                0.0,\n                280.0,\n            ],\n        ],\n        columns=[\n            \"nominal\",\n            \"ID\",\n            \"ActualTime\",\n            \"TDew\",\n            \"TAir\",\n            \"Windspeed\",\n            \"Precip\",\n            \"WindDir\",\n        ],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"data,parse_dates,msg\",\n    [\n        (\n            \"\"\"\\\ndate_NominalTime,date,NominalTime\nKORD1,19990127, 19:00:00\nKORD2,19990127, 20:00:00\"\"\",\n            [[1, 2]],\n            (\"New date column already in dict date_NominalTime\"),\n        ),\n        (\n            \"\"\"\\\nID,date,nominalTime\nKORD,19990127, 19:00:00\nKORD,19990127, 20:00:00\"\"\",\n            {\"ID\": [1, 2]},\n            \"Date column ID already in dict\",\n        ),\n    ],\n)\ndef test_multiple_date_col_name_collision(all_parsers, data, parse_dates, msg):\n    parser = all_parsers\n\n    with pytest.raises(ValueError, match=msg):\n        parser.read_csv(StringIO(data), parse_dates=parse_dates)\n\n\ndef test_date_parser_int_bug(all_parsers):\n    # see gh-3071\n    parser = all_parsers\n    data = (\n        \"posix_timestamp,elapsed,sys,user,queries,query_time,rows,\"\n        \"accountid,userid,contactid,level,silo,method\\n\"\n        \"1343103150,0.062353,0,4,6,0.01690,3,\"\n        \"12345,1,-1,3,invoice_InvoiceResource,search\\n\"\n    )\n\n    result = parser.read_csv_check_warnings(\n        FutureWarning,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        index_col=0,\n        parse_dates=[0],\n        date_parser=lambda x: datetime.utcfromtimestamp(int(x)),\n    )\n    expected = DataFrame(\n        [\n            [\n                0.062353,\n                0,\n                4,\n                6,\n                0.01690,\n                3,\n                12345,\n                1,\n                -1,\n                3,\n                \"invoice_InvoiceResource\",\n                \"search\",\n            ]\n        ],\n        columns=[\n            \"elapsed\",\n            \"sys\",\n            \"user\",\n            \"queries\",\n            \"query_time\",\n            \"rows\",\n            \"accountid\",\n            \"userid\",\n            \"contactid\",\n            \"level\",\n            \"silo\",\n            \"method\",\n        ],\n        index=Index([Timestamp(\"2012-07-24 04:12:30\")], name=\"posix_timestamp\"),\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_nat_parse(all_parsers):\n    # see gh-3062\n    parser = all_parsers\n    df = DataFrame(\n        dict({\"A\": np.arange(10, dtype=\"float64\"), \"B\": Timestamp(\"20010101\")})\n    )\n    df.iloc[3:6, :] = np.nan\n\n    with tm.ensure_clean(\"__nat_parse_.csv\") as path:\n        df.to_csv(path)\n\n        result = parser.read_csv(path, index_col=0, parse_dates=[\"B\"])\n        tm.assert_frame_equal(result, df)\n\n\n@xfail_pyarrow\ndef test_csv_custom_parser(all_parsers):\n    data = \"\"\"A,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5\n\"\"\"\n    parser = all_parsers\n    result = parser.read_csv_check_warnings(\n        FutureWarning,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        date_parser=lambda x: datetime.strptime(x, \"%Y%m%d\"),\n    )\n    expected = parser.read_csv(StringIO(data), parse_dates=True)\n    tm.assert_frame_equal(result, expected)\n    result = parser.read_csv(StringIO(data), date_format=\"%Y%m%d\")\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_parse_dates_implicit_first_col(all_parsers):\n    data = \"\"\"A,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5\n\"\"\"\n    parser = all_parsers\n    result = parser.read_csv(StringIO(data), parse_dates=True)\n\n    expected = parser.read_csv(StringIO(data), index_col=0, parse_dates=True)\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_parse_dates_string(all_parsers):\n    data = \"\"\"date,A,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5\n\"\"\"\n    parser = all_parsers\n    result = parser.read_csv(StringIO(data), index_col=\"date\", parse_dates=[\"date\"])\n    # freq doesn't round-trip\n    index = DatetimeIndex(\n        list(date_range(\"1/1/2009\", periods=3)), name=\"date\", freq=None\n    )\n\n    expected = DataFrame(\n        {\"A\": [\"a\", \"b\", \"c\"], \"B\": [1, 3, 4], \"C\": [2, 4, 5]}, index=index\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n# Bug in https://github.com/dateutil/dateutil/issues/217\n# has been addressed, but we just don't pass in the `yearfirst`\n@pytest.mark.xfail(reason=\"yearfirst is not surfaced in read_*\")\n@pytest.mark.parametrize(\"parse_dates\", [[[\"date\", \"time\"]], [[0, 1]]])\ndef test_yy_format_with_year_first(all_parsers, parse_dates):\n    data = \"\"\"date,time,B,C\n090131,0010,1,2\n090228,1020,3,4\n090331,0830,5,6\n\"\"\"\n    parser = all_parsers\n    result = parser.read_csv_check_warnings(\n        UserWarning,\n        \"Could not infer format\",\n        StringIO(data),\n        index_col=0,\n        parse_dates=parse_dates,\n    )\n    index = DatetimeIndex(\n        [\n            datetime(2009, 1, 31, 0, 10, 0),\n            datetime(2009, 2, 28, 10, 20, 0),\n            datetime(2009, 3, 31, 8, 30, 0),\n        ],\n        dtype=object,\n        name=\"date_time\",\n    )\n    expected = DataFrame({\"B\": [1, 3, 5], \"C\": [2, 4, 6]}, index=index)\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\"parse_dates\", [[0, 2], [\"a\", \"c\"]])\ndef test_parse_dates_column_list(all_parsers, parse_dates):\n    data = \"a,b,c\\n01/01/2010,1,15/02/2010\"\n    parser = all_parsers\n\n    expected = DataFrame(\n        {\"a\": [datetime(2010, 1, 1)], \"b\": [1], \"c\": [datetime(2010, 2, 15)]}\n    )\n    expected = expected.set_index([\"a\", \"b\"])\n\n    result = parser.read_csv(\n        StringIO(data), index_col=[0, 1], parse_dates=parse_dates, dayfirst=True\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\"index_col\", [[0, 1], [1, 0]])\ndef test_multi_index_parse_dates(all_parsers, index_col):\n    data = \"\"\"index1,index2,A,B,C\n20090101,one,a,1,2\n20090101,two,b,3,4\n20090101,three,c,4,5\n20090102,one,a,1,2\n20090102,two,b,3,4\n20090102,three,c,4,5\n20090103,one,a,1,2\n20090103,two,b,3,4\n20090103,three,c,4,5\n\"\"\"\n    parser = all_parsers\n    index = MultiIndex.from_product(\n        [\n            (datetime(2009, 1, 1), datetime(2009, 1, 2), datetime(2009, 1, 3)),\n            (\"one\", \"two\", \"three\"),\n        ],\n        names=[\"index1\", \"index2\"],\n    )\n\n    # Out of order.\n    if index_col == [1, 0]:\n        index = index.swaplevel(0, 1)\n\n    expected = DataFrame(\n        [\n            [\"a\", 1, 2],\n            [\"b\", 3, 4],\n            [\"c\", 4, 5],\n            [\"a\", 1, 2],\n            [\"b\", 3, 4],\n            [\"c\", 4, 5],\n            [\"a\", 1, 2],\n            [\"b\", 3, 4],\n            [\"c\", 4, 5],\n        ],\n        columns=[\"A\", \"B\", \"C\"],\n        index=index,\n    )\n    result = parser.read_csv_check_warnings(\n        UserWarning,\n        \"Could not infer format\",\n        StringIO(data),\n        index_col=index_col,\n        parse_dates=True,\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\"kwargs\", [{\"dayfirst\": True}, {\"day_first\": True}])\ndef test_parse_dates_custom_euro_format(all_parsers, kwargs):\n    parser = all_parsers\n    data = \"\"\"foo,bar,baz\n31/01/2010,1,2\n01/02/2010,1,NA\n02/02/2010,1,2\n\"\"\"\n    if \"dayfirst\" in kwargs:\n        df = parser.read_csv_check_warnings(\n            FutureWarning,\n            \"use 'date_format' instead\",\n            StringIO(data),\n            names=[\"time\", \"Q\", \"NTU\"],\n            date_parser=lambda d: du_parse(d, **kwargs),\n            header=0,\n            index_col=0,\n            parse_dates=True,\n            na_values=[\"NA\"],\n        )\n        exp_index = Index(\n            [datetime(2010, 1, 31), datetime(2010, 2, 1), datetime(2010, 2, 2)],\n            name=\"time\",\n        )\n        expected = DataFrame(\n            {\"Q\": [1, 1, 1], \"NTU\": [2, np.nan, 2]},\n            index=exp_index,\n            columns=[\"Q\", \"NTU\"],\n        )\n        tm.assert_frame_equal(df, expected)\n    else:\n        msg = \"got an unexpected keyword argument 'day_first'\"\n        with pytest.raises(TypeError, match=msg):\n            parser.read_csv_check_warnings(\n                FutureWarning,\n                \"use 'date_format' instead\",\n                StringIO(data),\n                names=[\"time\", \"Q\", \"NTU\"],\n                date_parser=lambda d: du_parse(d, **kwargs),\n                skiprows=[0],\n                index_col=0,\n                parse_dates=True,\n                na_values=[\"NA\"],\n            )\n\n\ndef test_parse_tz_aware(all_parsers, request):\n    # See gh-1693\n    parser = all_parsers\n    if parser.engine == \"pyarrow\" and pa_version_under7p0:\n        request.node.add_marker(pytest.mark.xfail(reason=\"Fails for pyarrow < 7.0\"))\n    data = \"Date,x\\n2012-06-13T01:39:00Z,0.5\"\n\n    result = parser.read_csv(StringIO(data), index_col=0, parse_dates=True)\n    expected = DataFrame(\n        {\"x\": [0.5]}, index=Index([Timestamp(\"2012-06-13 01:39:00+00:00\")], name=\"Date\")\n    )\n    tm.assert_frame_equal(result, expected)\n    if parser.engine == \"pyarrow\":\n        expected_tz = pytz.utc\n    else:\n        expected_tz = timezone.utc\n    assert result.index.tz is expected_tz\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\n    \"parse_dates,index_col\",\n    [({\"nominal\": [1, 2]}, \"nominal\"), ({\"nominal\": [1, 2]}, 0), ([[1, 2]], 0)],\n)\ndef test_multiple_date_cols_index(all_parsers, parse_dates, index_col):\n    parser = all_parsers\n    data = \"\"\"\nID,date,NominalTime,ActualTime,TDew,TAir,Windspeed,Precip,WindDir\nKORD1,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\nKORD2,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\nKORD3,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\nKORD4,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\nKORD5,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\nKORD6,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000\n\"\"\"\n    expected = DataFrame(\n        [\n            [\n                datetime(1999, 1, 27, 19, 0),\n                \"KORD1\",\n                \" 18:56:00\",\n                0.81,\n                2.81,\n                7.2,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 20, 0),\n                \"KORD2\",\n                \" 19:56:00\",\n                0.01,\n                2.21,\n                7.2,\n                0.0,\n                260.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                \"KORD3\",\n                \" 20:56:00\",\n                -0.59,\n                2.21,\n                5.7,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                \"KORD4\",\n                \" 21:18:00\",\n                -0.99,\n                2.01,\n                3.6,\n                0.0,\n                270.0,\n            ],\n            [\n                datetime(1999, 1, 27, 22, 0),\n                \"KORD5\",\n                \" 21:56:00\",\n                -0.59,\n                1.71,\n                5.1,\n                0.0,\n                290.0,\n            ],\n            [\n                datetime(1999, 1, 27, 23, 0),\n                \"KORD6\",\n                \" 22:56:00\",\n                -0.59,\n                1.71,\n                4.6,\n                0.0,\n                280.0,\n            ],\n        ],\n        columns=[\n            \"nominal\",\n            \"ID\",\n            \"ActualTime\",\n            \"TDew\",\n            \"TAir\",\n            \"Windspeed\",\n            \"Precip\",\n            \"WindDir\",\n        ],\n    )\n    expected = expected.set_index(\"nominal\")\n\n    if not isinstance(parse_dates, dict):\n        expected.index.name = \"date_NominalTime\"\n\n    result = parser.read_csv(\n        StringIO(data), parse_dates=parse_dates, index_col=index_col\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_multiple_date_cols_chunked(all_parsers):\n    parser = all_parsers\n    data = \"\"\"\\\nID,date,nominalTime,actualTime,A,B,C,D,E\nKORD,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\nKORD,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\nKORD,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\nKORD,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\nKORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\nKORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000\n\"\"\"\n\n    expected = DataFrame(\n        [\n            [\n                datetime(1999, 1, 27, 19, 0),\n                \"KORD\",\n                \" 18:56:00\",\n                0.81,\n                2.81,\n                7.2,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 20, 0),\n                \"KORD\",\n                \" 19:56:00\",\n                0.01,\n                2.21,\n                7.2,\n                0.0,\n                260.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                \"KORD\",\n                \" 20:56:00\",\n                -0.59,\n                2.21,\n                5.7,\n                0.0,\n                280.0,\n            ],\n            [\n                datetime(1999, 1, 27, 21, 0),\n                \"KORD\",\n                \" 21:18:00\",\n                -0.99,\n                2.01,\n                3.6,\n                0.0,\n                270.0,\n            ],\n            [\n                datetime(1999, 1, 27, 22, 0),\n                \"KORD\",\n                \" 21:56:00\",\n                -0.59,\n                1.71,\n                5.1,\n                0.0,\n                290.0,\n            ],\n            [\n                datetime(1999, 1, 27, 23, 0),\n                \"KORD\",\n                \" 22:56:00\",\n                -0.59,\n                1.71,\n                4.6,\n                0.0,\n                280.0,\n            ],\n        ],\n        columns=[\"nominal\", \"ID\", \"actualTime\", \"A\", \"B\", \"C\", \"D\", \"E\"],\n    )\n    expected = expected.set_index(\"nominal\")\n\n    with parser.read_csv(\n        StringIO(data),\n        parse_dates={\"nominal\": [1, 2]},\n        index_col=\"nominal\",\n        chunksize=2,\n    ) as reader:\n        chunks = list(reader)\n\n    tm.assert_frame_equal(chunks[0], expected[:2])\n    tm.assert_frame_equal(chunks[1], expected[2:4])\n    tm.assert_frame_equal(chunks[2], expected[4:])\n\n\n@xfail_pyarrow\ndef test_multiple_date_col_named_index_compat(all_parsers):\n    parser = all_parsers\n    data = \"\"\"\\\nID,date,nominalTime,actualTime,A,B,C,D,E\nKORD,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\nKORD,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\nKORD,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\nKORD,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\nKORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\nKORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000\n\"\"\"\n\n    with_indices = parser.read_csv(\n        StringIO(data), parse_dates={\"nominal\": [1, 2]}, index_col=\"nominal\"\n    )\n    with_names = parser.read_csv(\n        StringIO(data),\n        index_col=\"nominal\",\n        parse_dates={\"nominal\": [\"date\", \"nominalTime\"]},\n    )\n    tm.assert_frame_equal(with_indices, with_names)\n\n\n@xfail_pyarrow\ndef test_multiple_date_col_multiple_index_compat(all_parsers):\n    parser = all_parsers\n    data = \"\"\"\\\nID,date,nominalTime,actualTime,A,B,C,D,E\nKORD,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\nKORD,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\nKORD,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\nKORD,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\nKORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\nKORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000\n\"\"\"\n    result = parser.read_csv(\n        StringIO(data), index_col=[\"nominal\", \"ID\"], parse_dates={\"nominal\": [1, 2]}\n    )\n    expected = parser.read_csv(StringIO(data), parse_dates={\"nominal\": [1, 2]})\n\n    expected = expected.set_index([\"nominal\", \"ID\"])\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"kwargs\", [{}, {\"index_col\": \"C\"}])\ndef test_read_with_parse_dates_scalar_non_bool(all_parsers, kwargs):\n    # see gh-5636\n    parser = all_parsers\n    msg = (\n        \"Only booleans, lists, and dictionaries \"\n        \"are accepted for the 'parse_dates' parameter\"\n    )\n    data = \"\"\"A,B,C\n    1,2,2003-11-1\"\"\"\n\n    with pytest.raises(TypeError, match=msg):\n        parser.read_csv(StringIO(data), parse_dates=\"C\", **kwargs)\n\n\n@pytest.mark.parametrize(\"parse_dates\", [(1,), np.array([4, 5]), {1, 3}])\ndef test_read_with_parse_dates_invalid_type(all_parsers, parse_dates):\n    parser = all_parsers\n    msg = (\n        \"Only booleans, lists, and dictionaries \"\n        \"are accepted for the 'parse_dates' parameter\"\n    )\n    data = \"\"\"A,B,C\n    1,2,2003-11-1\"\"\"\n\n    with pytest.raises(TypeError, match=msg):\n        parser.read_csv(StringIO(data), parse_dates=(1,))\n\n\n@pytest.mark.parametrize(\"cache_dates\", [True, False])\n@pytest.mark.parametrize(\"value\", [\"nan\", \"\"])\ndef test_bad_date_parse(all_parsers, cache_dates, value):\n    # if we have an invalid date make sure that we handle this with\n    # and w/o the cache properly\n    parser = all_parsers\n    s = StringIO((f\"{value},\\n\") * 50000)\n\n    if parser.engine == \"pyarrow\":\n        # None in input gets converted to 'None', for which\n        # pandas tries to guess the datetime format, triggering\n        # the warning. TODO: parse dates directly in pyarrow, see\n        # https://github.com/pandas-dev/pandas/issues/48017\n        warn = UserWarning\n    else:\n        warn = None\n    parser.read_csv_check_warnings(\n        warn,\n        \"Could not infer format\",\n        s,\n        header=None,\n        names=[\"foo\", \"bar\"],\n        parse_dates=[\"foo\"],\n        cache_dates=cache_dates,\n    )\n\n\n@pytest.mark.parametrize(\"cache_dates\", [True, False])\n@pytest.mark.parametrize(\"value\", [\"0\"])\ndef test_bad_date_parse_with_warning(all_parsers, cache_dates, value):\n    # if we have an invalid date make sure that we handle this with\n    # and w/o the cache properly.\n    parser = all_parsers\n    s = StringIO((f\"{value},\\n\") * 50000)\n\n    if parser.engine == \"pyarrow\":\n        # pyarrow reads \"0\" as 0 (of type int64), and so\n        # pandas doesn't try to guess the datetime format\n        # TODO: parse dates directly in pyarrow, see\n        # https://github.com/pandas-dev/pandas/issues/48017\n        warn = None\n    else:\n        warn = UserWarning\n    parser.read_csv_check_warnings(\n        warn,\n        \"Could not infer format\",\n        s,\n        header=None,\n        names=[\"foo\", \"bar\"],\n        parse_dates=[\"foo\"],\n        cache_dates=cache_dates,\n    )\n\n\n@xfail_pyarrow\ndef test_parse_dates_empty_string(all_parsers):\n    # see gh-2263\n    parser = all_parsers\n    data = \"Date,test\\n2012-01-01,1\\n,2\"\n    result = parser.read_csv(StringIO(data), parse_dates=[\"Date\"], na_filter=False)\n\n    expected = DataFrame(\n        [[datetime(2012, 1, 1), 1], [pd.NaT, 2]], columns=[\"Date\", \"test\"]\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"reader\", [\"read_csv_check_warnings\", \"read_table_check_warnings\"]\n)\ndef test_parse_dates_infer_datetime_format_warning(all_parsers, reader):\n    # GH 49024, 51017\n    parser = all_parsers\n    data = \"Date,test\\n2012-01-01,1\\n,2\"\n\n    getattr(parser, reader)(\n        FutureWarning,\n        \"The argument 'infer_datetime_format' is deprecated\",\n        StringIO(data),\n        parse_dates=[\"Date\"],\n        infer_datetime_format=True,\n        sep=\",\",\n    )\n\n\n@pytest.mark.parametrize(\n    \"reader\", [\"read_csv_check_warnings\", \"read_table_check_warnings\"]\n)\ndef test_parse_dates_date_parser_and_date_format(all_parsers, reader):\n    # GH 50601\n    parser = all_parsers\n    data = \"Date,test\\n2012-01-01,1\\n,2\"\n    msg = \"Cannot use both 'date_parser' and 'date_format'\"\n    with pytest.raises(TypeError, match=msg):\n        getattr(parser, reader)(\n            FutureWarning,\n            \"use 'date_format' instead\",\n            StringIO(data),\n            parse_dates=[\"Date\"],\n            date_parser=pd.to_datetime,\n            date_format=\"ISO8601\",\n            sep=\",\",\n        )\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\n    \"data,kwargs,expected\",\n    [\n        (\n            \"a\\n04.15.2016\",\n            {\"parse_dates\": [\"a\"]},\n            DataFrame([datetime(2016, 4, 15)], columns=[\"a\"]),\n        ),\n        (\n            \"a\\n04.15.2016\",\n            {\"parse_dates\": True, \"index_col\": 0},\n            DataFrame(index=DatetimeIndex([\"2016-04-15\"], name=\"a\"), columns=[]),\n        ),\n        (\n            \"a,b\\n04.15.2016,09.16.2013\",\n            {\"parse_dates\": [\"a\", \"b\"]},\n            DataFrame(\n                [[datetime(2016, 4, 15), datetime(2013, 9, 16)]], columns=[\"a\", \"b\"]\n            ),\n        ),\n        (\n            \"a,b\\n04.15.2016,09.16.2013\",\n            {\"parse_dates\": True, \"index_col\": [0, 1]},\n            DataFrame(\n                index=MultiIndex.from_tuples(\n                    [(datetime(2016, 4, 15), datetime(2013, 9, 16))], names=[\"a\", \"b\"]\n                ),\n                columns=[],\n            ),\n        ),\n    ],\n)\ndef test_parse_dates_no_convert_thousands(all_parsers, data, kwargs, expected):\n    # see gh-14066\n    parser = all_parsers\n\n    result = parser.read_csv(StringIO(data), thousands=\".\", **kwargs)\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_parse_date_time_multi_level_column_name(all_parsers):\n    data = \"\"\"\\\nD,T,A,B\ndate, time,a,b\n2001-01-05, 09:00:00, 0.0, 10.\n2001-01-06, 00:00:00, 1.0, 11.\n\"\"\"\n    parser = all_parsers\n    result = parser.read_csv_check_warnings(\n        FutureWarning,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        header=[0, 1],\n        parse_dates={\"date_time\": [0, 1]},\n        date_parser=pd.to_datetime,\n    )\n\n    expected_data = [\n        [datetime(2001, 1, 5, 9, 0, 0), 0.0, 10.0],\n        [datetime(2001, 1, 6, 0, 0, 0), 1.0, 11.0],\n    ]\n    expected = DataFrame(expected_data, columns=[\"date_time\", (\"A\", \"a\"), (\"B\", \"b\")])\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\n    \"data,kwargs,expected\",\n    [\n        (\n            \"\"\"\\\ndate,time,a,b\n2001-01-05, 10:00:00, 0.0, 10.\n2001-01-05, 00:00:00, 1., 11.\n\"\"\",\n            {\"header\": 0, \"parse_dates\": {\"date_time\": [0, 1]}},\n            DataFrame(\n                [\n                    [datetime(2001, 1, 5, 10, 0, 0), 0.0, 10],\n                    [datetime(2001, 1, 5, 0, 0, 0), 1.0, 11.0],\n                ],\n                columns=[\"date_time\", \"a\", \"b\"],\n            ),\n        ),\n        (\n            (\n                \"KORD,19990127, 19:00:00, 18:56:00, 0.8100\\n\"\n                \"KORD,19990127, 20:00:00, 19:56:00, 0.0100\\n\"\n                \"KORD,19990127, 21:00:00, 20:56:00, -0.5900\\n\"\n                \"KORD,19990127, 21:00:00, 21:18:00, -0.9900\\n\"\n                \"KORD,19990127, 22:00:00, 21:56:00, -0.5900\\n\"\n                \"KORD,19990127, 23:00:00, 22:56:00, -0.5900\"\n            ),\n            {\"header\": None, \"parse_dates\": {\"actual\": [1, 2], \"nominal\": [1, 3]}},\n            DataFrame(\n                [\n                    [\n                        datetime(1999, 1, 27, 19, 0),\n                        datetime(1999, 1, 27, 18, 56),\n                        \"KORD\",\n                        0.81,\n                    ],\n                    [\n                        datetime(1999, 1, 27, 20, 0),\n                        datetime(1999, 1, 27, 19, 56),\n                        \"KORD\",\n                        0.01,\n                    ],\n                    [\n                        datetime(1999, 1, 27, 21, 0),\n                        datetime(1999, 1, 27, 20, 56),\n                        \"KORD\",\n                        -0.59,\n                    ],\n                    [\n                        datetime(1999, 1, 27, 21, 0),\n                        datetime(1999, 1, 27, 21, 18),\n                        \"KORD\",\n                        -0.99,\n                    ],\n                    [\n                        datetime(1999, 1, 27, 22, 0),\n                        datetime(1999, 1, 27, 21, 56),\n                        \"KORD\",\n                        -0.59,\n                    ],\n                    [\n                        datetime(1999, 1, 27, 23, 0),\n                        datetime(1999, 1, 27, 22, 56),\n                        \"KORD\",\n                        -0.59,\n                    ],\n                ],\n                columns=[\"actual\", \"nominal\", 0, 4],\n            ),\n        ),\n    ],\n)\ndef test_parse_date_time(all_parsers, data, kwargs, expected):\n    parser = all_parsers\n    result = parser.read_csv_check_warnings(\n        FutureWarning,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        date_parser=pd.to_datetime,\n        **kwargs,\n    )\n\n    # Python can sometimes be flaky about how\n    # the aggregated columns are entered, so\n    # this standardizes the order.\n    result = result[expected.columns]\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\n# From date_parser fallback behavior\n@pytest.mark.filterwarnings(\"ignore:elementwise comparison:FutureWarning\")\ndef test_parse_date_fields(all_parsers):\n    parser = all_parsers\n    data = \"year,month,day,a\\n2001,01,10,10.\\n2001,02,1,11.\"\n    result = parser.read_csv_check_warnings(\n        FutureWarning,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        header=0,\n        parse_dates={\"ymd\": [0, 1, 2]},\n        date_parser=pd.to_datetime,\n    )\n\n    expected = DataFrame(\n        [[datetime(2001, 1, 10), 10.0], [datetime(2001, 2, 1), 11.0]],\n        columns=[\"ymd\", \"a\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\n    (\"key\", \"value\", \"warn\"),\n    [\n        (\n            \"date_parser\",\n            lambda x: pd.to_datetime(x, format=\"%Y %m %d %H %M %S\"),\n            FutureWarning,\n        ),\n        (\"date_format\", \"%Y %m %d %H %M %S\", None),\n    ],\n)\ndef test_parse_date_all_fields(all_parsers, key, value, warn):\n    parser = all_parsers\n    data = \"\"\"\\\nyear,month,day,hour,minute,second,a,b\n2001,01,05,10,00,0,0.0,10.\n2001,01,5,10,0,00,1.,11.\n\"\"\"\n    result = parser.read_csv_check_warnings(\n        warn,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        header=0,\n        parse_dates={\"ymdHMS\": [0, 1, 2, 3, 4, 5]},\n        **{key: value},\n    )\n    expected = DataFrame(\n        [\n            [datetime(2001, 1, 5, 10, 0, 0), 0.0, 10.0],\n            [datetime(2001, 1, 5, 10, 0, 0), 1.0, 11.0],\n        ],\n        columns=[\"ymdHMS\", \"a\", \"b\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\n@pytest.mark.parametrize(\n    (\"key\", \"value\", \"warn\"),\n    [\n        (\n            \"date_parser\",\n            lambda x: pd.to_datetime(x, format=\"%Y %m %d %H %M %S.%f\"),\n            FutureWarning,\n        ),\n        (\"date_format\", \"%Y %m %d %H %M %S.%f\", None),\n    ],\n)\ndef test_datetime_fractional_seconds(all_parsers, key, value, warn):\n    parser = all_parsers\n    data = \"\"\"\\\nyear,month,day,hour,minute,second,a,b\n2001,01,05,10,00,0.123456,0.0,10.\n2001,01,5,10,0,0.500000,1.,11.\n\"\"\"\n    result = parser.read_csv_check_warnings(\n        warn,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        header=0,\n        parse_dates={\"ymdHMS\": [0, 1, 2, 3, 4, 5]},\n        **{key: value},\n    )\n    expected = DataFrame(\n        [\n            [datetime(2001, 1, 5, 10, 0, 0, microsecond=123456), 0.0, 10.0],\n            [datetime(2001, 1, 5, 10, 0, 0, microsecond=500000), 1.0, 11.0],\n        ],\n        columns=[\"ymdHMS\", \"a\", \"b\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_generic(all_parsers):\n    parser = all_parsers\n    data = \"year,month,day,a\\n2001,01,10,10.\\n2001,02,1,11.\"\n\n    def parse_function(yy, mm):\n        return [date(year=int(y), month=int(m), day=1) for y, m in zip(yy, mm)]\n\n    result = parser.read_csv_check_warnings(\n        FutureWarning,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        header=0,\n        parse_dates={\"ym\": [0, 1]},\n        date_parser=parse_function,\n    )\n    expected = DataFrame(\n        [[date(2001, 1, 1), 10, 10.0], [date(2001, 2, 1), 1, 11.0]],\n        columns=[\"ym\", \"day\", \"a\"],\n    )\n    expected[\"ym\"] = expected[\"ym\"].astype(\"datetime64[ns]\")\n    tm.assert_frame_equal(result, expected)\n\n\n@xfail_pyarrow\ndef test_date_parser_resolution_if_not_ns(all_parsers):\n    # see gh-10245\n    parser = all_parsers\n    data = \"\"\"\\\ndate,time,prn,rxstatus\n2013-11-03,19:00:00,126,00E80000\n2013-11-03,19:00:00,23,00E80000\n2013-11-03,19:00:00,13,00E80000\n\"\"\"\n\n    def date_parser(dt, time):\n        try:\n            arr = dt + \"T\" + time\n        except TypeError:\n            # dt & time are date/time objects\n            arr = [datetime.combine(d, t) for d, t in zip(dt, time)]\n        return np.array(arr, dtype=\"datetime64[s]\")\n\n    result = parser.read_csv_check_warnings(\n        FutureWarning,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        date_parser=date_parser,\n        parse_dates={\"datetime\": [\"date\", \"time\"]},\n        index_col=[\"datetime\", \"prn\"],\n    )\n\n    datetimes = np.array([\"2013-11-03T19:00:00\"] * 3, dtype=\"datetime64[s]\")\n    expected = DataFrame(\n        data={\"rxstatus\": [\"00E80000\"] * 3},\n        index=MultiIndex.from_tuples(\n            [(datetimes[0], 126), (datetimes[1], 23), (datetimes[2], 13)],\n            names=[\"datetime\", \"prn\"],\n        ),\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_parse_date_column_with_empty_string(all_parsers):\n    # see gh-6428\n    parser = all_parsers\n    data = \"case,opdate\\n7,10/18/2006\\n7,10/18/2008\\n621, \"\n    result = parser.read_csv(StringIO(data), parse_dates=[\"opdate\"])\n\n    expected_data = [[7, \"10/18/2006\"], [7, \"10/18/2008\"], [621, \" \"]]\n    expected = DataFrame(expected_data, columns=[\"case\", \"opdate\"])\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"data,expected\",\n    [\n        (\n            \"a\\n135217135789158401\\n1352171357E+5\",\n            DataFrame({\"a\": [135217135789158401, 135217135700000]}, dtype=\"float64\"),\n        ),\n        (\n            \"a\\n99999999999\\n123456789012345\\n1234E+0\",\n            DataFrame({\"a\": [99999999999, 123456789012345, 1234]}, dtype=\"float64\"),\n        ),\n    ],\n)\n@pytest.mark.parametrize(\"parse_dates\", [True, False])\ndef test_parse_date_float(all_parsers, data, expected, parse_dates):\n    # see gh-2697\n    #\n    # Date parsing should fail, so we leave the data untouched\n    # (i.e. float precision should remain unchanged).\n    parser = all_parsers\n\n    result = parser.read_csv(StringIO(data), parse_dates=parse_dates)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_parse_timezone(all_parsers):\n    # see gh-22256\n    parser = all_parsers\n    data = \"\"\"dt,val\n              2018-01-04 09:01:00+09:00,23350\n              2018-01-04 09:02:00+09:00,23400\n              2018-01-04 09:03:00+09:00,23400\n              2018-01-04 09:04:00+09:00,23400\n              2018-01-04 09:05:00+09:00,23400\"\"\"\n    result = parser.read_csv(StringIO(data), parse_dates=[\"dt\"])\n\n    dti = DatetimeIndex(\n        list(\n            date_range(\n                start=\"2018-01-04 09:01:00\",\n                end=\"2018-01-04 09:05:00\",\n                freq=\"1min\",\n                tz=timezone(timedelta(minutes=540)),\n            )\n        ),\n        freq=None,\n    )\n    expected_data = {\"dt\": dti, \"val\": [23350, 23400, 23400, 23400, 23400]}\n\n    expected = DataFrame(expected_data)\n    tm.assert_frame_equal(result, expected)\n\n\n@skip_pyarrow\n@pytest.mark.parametrize(\n    \"date_string\",\n    [\"32/32/2019\", \"02/30/2019\", \"13/13/2019\", \"13/2019\", \"a3/11/2018\", \"10/11/2o17\"],\n)\ndef test_invalid_parse_delimited_date(all_parsers, date_string):\n    parser = all_parsers\n    expected = DataFrame({0: [date_string]}, dtype=\"object\")\n    result = parser.read_csv_check_warnings(\n        UserWarning,\n        \"Could not infer format\",\n        StringIO(date_string),\n        header=None,\n        parse_dates=[0],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@skip_pyarrow\n@pytest.mark.parametrize(\n    \"date_string,dayfirst,expected\",\n    [\n        # %d/%m/%Y; month > 12 thus replacement\n        (\"13/02/2019\", True, datetime(2019, 2, 13)),\n        # %m/%d/%Y; day > 12 thus there will be no replacement\n        (\"02/13/2019\", False, datetime(2019, 2, 13)),\n        # %d/%m/%Y; dayfirst==True thus replacement\n        (\"04/02/2019\", True, datetime(2019, 2, 4)),\n    ],\n)\ndef test_parse_delimited_date_swap_no_warning(\n    all_parsers, date_string, dayfirst, expected\n):\n    parser = all_parsers\n    expected = DataFrame({0: [expected]}, dtype=\"datetime64[ns]\")\n    result = parser.read_csv(\n        StringIO(date_string), header=None, dayfirst=dayfirst, parse_dates=[0]\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@skip_pyarrow\n@pytest.mark.parametrize(\n    \"date_string,dayfirst,expected\",\n    [\n        # %d/%m/%Y; month > 12\n        (\"13/02/2019\", False, datetime(2019, 2, 13)),\n        # %m/%d/%Y; day > 12\n        (\"02/13/2019\", True, datetime(2019, 2, 13)),\n    ],\n)\ndef test_parse_delimited_date_swap_with_warning(\n    all_parsers, date_string, dayfirst, expected\n):\n    parser = all_parsers\n    expected = DataFrame({0: [expected]}, dtype=\"datetime64[ns]\")\n    warning_msg = (\n        \"Parsing dates in .* format when dayfirst=.* was specified. \"\n        \"Pass `dayfirst=.*` or specify a format to silence this warning.\"\n    )\n    result = parser.read_csv_check_warnings(\n        UserWarning,\n        warning_msg,\n        StringIO(date_string),\n        header=None,\n        dayfirst=dayfirst,\n        parse_dates=[0],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_parse_multiple_delimited_dates_with_swap_warnings():\n    # GH46210\n    with pytest.raises(\n        ValueError,\n        match=(\n            r'^time data \"31/05/2000\" doesn\\'t match format \"%m/%d/%Y\", '\n            r\"at position 1. You might want to try:\"\n        ),\n    ):\n        pd.to_datetime([\"01/01/2000\", \"31/05/2000\", \"31/05/2001\", \"01/02/2000\"])\n\n\ndef _helper_hypothesis_delimited_date(call, date_string, **kwargs):\n    msg, result = None, None\n    try:\n        result = call(date_string, **kwargs)\n    except ValueError as er:\n        msg = str(er)\n    return msg, result\n\n\n@skip_pyarrow\n@given(DATETIME_NO_TZ)\n@pytest.mark.parametrize(\"delimiter\", list(\" -./\"))\n@pytest.mark.parametrize(\"dayfirst\", [True, False])\n@pytest.mark.parametrize(\n    \"date_format\",\n    [\"%d %m %Y\", \"%m %d %Y\", \"%m %Y\", \"%Y %m %d\", \"%y %m %d\", \"%Y%m%d\", \"%y%m%d\"],\n)\ndef test_hypothesis_delimited_date(\n    request, date_format, dayfirst, delimiter, test_datetime\n):\n    if date_format == \"%m %Y\" and delimiter == \".\":\n        request.node.add_marker(\n            pytest.mark.xfail(\n                reason=\"parse_datetime_string cannot reliably tell whether \"\n                \"e.g. %m.%Y is a float or a date\"\n            )\n        )\n    date_string = test_datetime.strftime(date_format.replace(\" \", delimiter))\n\n    except_out_dateutil, result = _helper_hypothesis_delimited_date(\n        py_parse_datetime_string, date_string, dayfirst=dayfirst\n    )\n    except_in_dateutil, expected = _helper_hypothesis_delimited_date(\n        du_parse,\n        date_string,\n        default=datetime(1, 1, 1),\n        dayfirst=dayfirst,\n        yearfirst=False,\n    )\n\n    assert except_out_dateutil == except_in_dateutil\n    assert result == expected\n\n\n@skip_pyarrow\n@pytest.mark.parametrize(\n    \"names, usecols, parse_dates, missing_cols\",\n    [\n        (None, [\"val\"], [\"date\", \"time\"], \"date, time\"),\n        (None, [\"val\"], [0, \"time\"], \"time\"),\n        (None, [\"val\"], [[\"date\", \"time\"]], \"date, time\"),\n        (None, [\"val\"], [[0, \"time\"]], \"time\"),\n        (None, [\"val\"], {\"date\": [0, \"time\"]}, \"time\"),\n        (None, [\"val\"], {\"date\": [\"date\", \"time\"]}, \"date, time\"),\n        (None, [\"val\"], [[\"date\", \"time\"], \"date\"], \"date, time\"),\n        ([\"date1\", \"time1\", \"temperature\"], None, [\"date\", \"time\"], \"date, time\"),\n        (\n            [\"date1\", \"time1\", \"temperature\"],\n            [\"date1\", \"temperature\"],\n            [\"date1\", \"time\"],\n            \"time\",\n        ),\n    ],\n)\ndef test_missing_parse_dates_column_raises(\n    all_parsers, names, usecols, parse_dates, missing_cols\n):\n    # gh-31251 column names provided in parse_dates could be missing.\n    parser = all_parsers\n    content = StringIO(\"date,time,val\\n2020-01-31,04:20:32,32\\n\")\n    msg = f\"Missing column provided to 'parse_dates': '{missing_cols}'\"\n    with pytest.raises(ValueError, match=msg):\n        parser.read_csv(\n            content, sep=\",\", names=names, usecols=usecols, parse_dates=parse_dates\n        )\n\n\n@skip_pyarrow\ndef test_date_parser_and_names(all_parsers):\n    # GH#33699\n    parser = all_parsers\n    data = StringIO(\"\"\"x,y\\n1,2\"\"\")\n    result = parser.read_csv_check_warnings(\n        UserWarning,\n        \"Could not infer format\",\n        data,\n        parse_dates=[\"B\"],\n        names=[\"B\"],\n    )\n    expected = DataFrame({\"B\": [\"y\", \"2\"]}, index=[\"x\", \"1\"])\n    tm.assert_frame_equal(result, expected)\n\n\n@skip_pyarrow\ndef test_date_parser_multiindex_columns(all_parsers):\n    parser = all_parsers\n    data = \"\"\"a,b\n1,2\n2019-12-31,6\"\"\"\n    result = parser.read_csv(StringIO(data), parse_dates=[(\"a\", \"1\")], header=[0, 1])\n    expected = DataFrame({(\"a\", \"1\"): Timestamp(\"2019-12-31\"), (\"b\", \"2\"): [6]})\n    tm.assert_frame_equal(result, expected)\n\n\n@skip_pyarrow\n@pytest.mark.parametrize(\n    \"parse_spec, col_name\",\n    [\n        ([[(\"a\", \"1\"), (\"b\", \"2\")]], (\"a_b\", \"1_2\")),\n        ({(\"foo\", \"1\"): [(\"a\", \"1\"), (\"b\", \"2\")]}, (\"foo\", \"1\")),\n    ],\n)\ndef test_date_parser_multiindex_columns_combine_cols(all_parsers, parse_spec, col_name):\n    parser = all_parsers\n    data = \"\"\"a,b,c\n1,2,3\n2019-12,-31,6\"\"\"\n    result = parser.read_csv(\n        StringIO(data),\n        parse_dates=parse_spec,\n        header=[0, 1],\n    )\n    expected = DataFrame({col_name: Timestamp(\"2019-12-31\"), (\"c\", \"3\"): [6]})\n    tm.assert_frame_equal(result, expected)\n\n\n@skip_pyarrow\ndef test_date_parser_usecols_thousands(all_parsers):\n    # GH#39365\n    data = \"\"\"A,B,C\n    1,3,20-09-01-01\n    2,4,20-09-01-01\n    \"\"\"\n\n    parser = all_parsers\n    result = parser.read_csv_check_warnings(\n        UserWarning,\n        \"Could not infer format\",\n        StringIO(data),\n        parse_dates=[1],\n        usecols=[1, 2],\n        thousands=\"-\",\n    )\n    expected = DataFrame({\"B\": [3, 4], \"C\": [Timestamp(\"20-09-2001 01:00:00\")] * 2})\n    tm.assert_frame_equal(result, expected)\n\n\n@skip_pyarrow\ndef test_parse_dates_and_keep_orgin_column(all_parsers):\n    # GH#13378\n    parser = all_parsers\n    data = \"\"\"A\n20150908\n20150909\n\"\"\"\n    result = parser.read_csv(\n        StringIO(data), parse_dates={\"date\": [\"A\"]}, keep_date_col=True\n    )\n    expected_data = [Timestamp(\"2015-09-08\"), Timestamp(\"2015-09-09\")]\n    expected = DataFrame({\"date\": expected_data, \"A\": expected_data})\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_dayfirst_warnings():\n    # GH 12585\n\n    # CASE 1: valid input\n    input = \"date\\n31/12/2014\\n10/03/2011\"\n    expected = DatetimeIndex(\n        [\"2014-12-31\", \"2011-03-10\"], dtype=\"datetime64[ns]\", freq=None, name=\"date\"\n    )\n    warning_msg = (\n        \"Parsing dates in .* format when dayfirst=.* was specified. \"\n        \"Pass `dayfirst=.*` or specify a format to silence this warning.\"\n    )\n\n    # A. dayfirst arg correct, no warning\n    res1 = read_csv(\n        StringIO(input), parse_dates=[\"date\"], dayfirst=True, index_col=\"date\"\n    ).index\n    tm.assert_index_equal(expected, res1)\n\n    # B. dayfirst arg incorrect, warning\n    with tm.assert_produces_warning(UserWarning, match=warning_msg):\n        res2 = read_csv(\n            StringIO(input), parse_dates=[\"date\"], dayfirst=False, index_col=\"date\"\n        ).index\n    tm.assert_index_equal(expected, res2)\n\n    # CASE 2: invalid input\n    # cannot consistently process with single format\n    # return to user unaltered\n\n    # first in DD/MM/YYYY, second in MM/DD/YYYY\n    input = \"date\\n31/12/2014\\n03/30/2011\"\n    expected = Index([\"31/12/2014\", \"03/30/2011\"], dtype=\"object\", name=\"date\")\n\n    # A. use dayfirst=True\n    res5 = read_csv(\n        StringIO(input), parse_dates=[\"date\"], dayfirst=True, index_col=\"date\"\n    ).index\n    tm.assert_index_equal(expected, res5)\n\n    # B. use dayfirst=False\n    with tm.assert_produces_warning(UserWarning, match=warning_msg):\n        res6 = read_csv(\n            StringIO(input), parse_dates=[\"date\"], dayfirst=False, index_col=\"date\"\n        ).index\n    tm.assert_index_equal(expected, res6)\n\n\n@pytest.mark.parametrize(\n    \"date_string, dayfirst\",\n    [\n        pytest.param(\n            \"31/1/2014\",\n            False,\n            id=\"second date is single-digit\",\n        ),\n        pytest.param(\n            \"1/31/2014\",\n            True,\n            id=\"first date is single-digit\",\n        ),\n    ],\n)\ndef test_dayfirst_warnings_no_leading_zero(date_string, dayfirst):\n    # GH47880\n    initial_value = f\"date\\n{date_string}\"\n    expected = DatetimeIndex(\n        [\"2014-01-31\"], dtype=\"datetime64[ns]\", freq=None, name=\"date\"\n    )\n    warning_msg = (\n        \"Parsing dates in .* format when dayfirst=.* was specified. \"\n        \"Pass `dayfirst=.*` or specify a format to silence this warning.\"\n    )\n    with tm.assert_produces_warning(UserWarning, match=warning_msg):\n        res = read_csv(\n            StringIO(initial_value),\n            parse_dates=[\"date\"],\n            index_col=\"date\",\n            dayfirst=dayfirst,\n        ).index\n    tm.assert_index_equal(expected, res)\n\n\n@skip_pyarrow\ndef test_infer_first_column_as_index(all_parsers):\n    # GH#11019\n    parser = all_parsers\n    data = \"a,b,c\\n1970-01-01,2,3,4\"\n    result = parser.read_csv_check_warnings(\n        UserWarning,\n        \"Could not infer format\",\n        StringIO(data),\n        parse_dates=[\"a\"],\n    )\n    expected = DataFrame({\"a\": \"2\", \"b\": 3, \"c\": 4}, index=[\"1970-01-01\"])\n    tm.assert_frame_equal(result, expected)\n\n\n@skip_pyarrow\n@pytest.mark.parametrize(\n    (\"key\", \"value\", \"warn\"),\n    [\n        (\"date_parser\", lambda x: pd.to_datetime(x, format=\"%Y-%m-%d\"), FutureWarning),\n        (\"date_format\", \"%Y-%m-%d\", None),\n    ],\n)\ndef test_replace_nans_before_parsing_dates(all_parsers, key, value, warn):\n    # GH#26203\n    parser = all_parsers\n    data = \"\"\"Test\n2012-10-01\n0\n2015-05-15\n#\n2017-09-09\n\"\"\"\n    result = parser.read_csv_check_warnings(\n        warn,\n        \"use 'date_format' instead\",\n        StringIO(data),\n        na_values={\"Test\": [\"#\", \"0\"]},\n        parse_dates=[\"Test\"],\n        **{key: value},\n    )\n    expected = DataFrame(\n        {\n            \"Test\": [\n                Timestamp(\"2012-10-01\"),\n                pd.NaT,\n                Timestamp(\"2015-05-15\"),\n                pd.NaT,\n                Timestamp(\"2017-09-09\"),\n            ]\n        }\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@skip_pyarrow\ndef test_parse_dates_and_string_dtype(all_parsers):\n    # GH#34066\n    parser = all_parsers\n    data = \"\"\"a,b\n1,2019-12-31\n\"\"\"\n    result = parser.read_csv(StringIO(data), dtype=\"string\", parse_dates=[\"b\"])\n    expected = DataFrame({\"a\": [\"1\"], \"b\": [Timestamp(\"2019-12-31\")]})\n    expected[\"a\"] = expected[\"a\"].astype(\"string\")\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_parse_dot_separated_dates(all_parsers):\n    # https://github.com/pandas-dev/pandas/issues/2586\n    parser = all_parsers\n    data = \"\"\"a,b\n27.03.2003 14:55:00.000,1\n03.08.2003 15:20:00.000,2\"\"\"\n    if parser.engine == \"pyarrow\":\n        expected_index = Index(\n            [\"27.03.2003 14:55:00.000\", \"03.08.2003 15:20:00.000\"],\n            dtype=\"object\",\n            name=\"a\",\n        )\n        warn = None\n    else:\n        expected_index = DatetimeIndex(\n            [\"2003-03-27 14:55:00\", \"2003-08-03 15:20:00\"],\n            dtype=\"datetime64[ns]\",\n            name=\"a\",\n        )\n        warn = UserWarning\n    msg = r\"when dayfirst=False \\(the default\\) was specified\"\n    result = parser.read_csv_check_warnings(\n        warn, msg, StringIO(data), parse_dates=True, index_col=0\n    )\n    expected = DataFrame({\"b\": [1, 2]}, index=expected_index)\n    tm.assert_frame_equal(result, expected)\n"
    },
    {
      "filename": "pandas/tests/io/parser/test_read_fwf.py",
      "content": "\"\"\"\nTests the 'read_fwf' function in parsers.py. This\ntest suite is independent of the others because the\nengine is set to 'python-fwf' internally.\n\"\"\"\n\nfrom datetime import datetime\nfrom io import (\n    BytesIO,\n    StringIO,\n)\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\n\nfrom pandas.errors import EmptyDataError\n\nimport pandas as pd\nfrom pandas import (\n    DataFrame,\n    DatetimeIndex,\n)\nimport pandas._testing as tm\nfrom pandas.core.arrays import (\n    ArrowStringArray,\n    StringArray,\n)\nfrom pandas.tests.io.test_compression import _compression_to_extension\n\nfrom pandas.io.parsers import (\n    read_csv,\n    read_fwf,\n)\n\n\ndef test_basic():\n    data = \"\"\"\\\nA         B            C            D\n201158    360.242940   149.910199   11950.7\n201159    444.953632   166.985655   11788.4\n201160    364.136849   183.628767   11806.2\n201161    413.836124   184.375703   11916.8\n201162    502.953953   173.237159   12468.3\n\"\"\"\n    result = read_fwf(StringIO(data))\n    expected = DataFrame(\n        [\n            [201158, 360.242940, 149.910199, 11950.7],\n            [201159, 444.953632, 166.985655, 11788.4],\n            [201160, 364.136849, 183.628767, 11806.2],\n            [201161, 413.836124, 184.375703, 11916.8],\n            [201162, 502.953953, 173.237159, 12468.3],\n        ],\n        columns=[\"A\", \"B\", \"C\", \"D\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_colspecs():\n    data = \"\"\"\\\nA   B     C            D            E\n201158    360.242940   149.910199   11950.7\n201159    444.953632   166.985655   11788.4\n201160    364.136849   183.628767   11806.2\n201161    413.836124   184.375703   11916.8\n201162    502.953953   173.237159   12468.3\n\"\"\"\n    colspecs = [(0, 4), (4, 8), (8, 20), (21, 33), (34, 43)]\n    result = read_fwf(StringIO(data), colspecs=colspecs)\n\n    expected = DataFrame(\n        [\n            [2011, 58, 360.242940, 149.910199, 11950.7],\n            [2011, 59, 444.953632, 166.985655, 11788.4],\n            [2011, 60, 364.136849, 183.628767, 11806.2],\n            [2011, 61, 413.836124, 184.375703, 11916.8],\n            [2011, 62, 502.953953, 173.237159, 12468.3],\n        ],\n        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_widths():\n    data = \"\"\"\\\nA    B    C            D            E\n2011 58   360.242940   149.910199   11950.7\n2011 59   444.953632   166.985655   11788.4\n2011 60   364.136849   183.628767   11806.2\n2011 61   413.836124   184.375703   11916.8\n2011 62   502.953953   173.237159   12468.3\n\"\"\"\n    result = read_fwf(StringIO(data), widths=[5, 5, 13, 13, 7])\n\n    expected = DataFrame(\n        [\n            [2011, 58, 360.242940, 149.910199, 11950.7],\n            [2011, 59, 444.953632, 166.985655, 11788.4],\n            [2011, 60, 364.136849, 183.628767, 11806.2],\n            [2011, 61, 413.836124, 184.375703, 11916.8],\n            [2011, 62, 502.953953, 173.237159, 12468.3],\n        ],\n        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_non_space_filler():\n    # From Thomas Kluyver:\n    #\n    # Apparently, some non-space filler characters can be seen, this is\n    # supported by specifying the 'delimiter' character:\n    #\n    # http://publib.boulder.ibm.com/infocenter/dmndhelp/v6r1mx/index.jsp?topic=/com.ibm.wbit.612.help.config.doc/topics/rfixwidth.html\n    data = \"\"\"\\\nA~~~~B~~~~C~~~~~~~~~~~~D~~~~~~~~~~~~E\n201158~~~~360.242940~~~149.910199~~~11950.7\n201159~~~~444.953632~~~166.985655~~~11788.4\n201160~~~~364.136849~~~183.628767~~~11806.2\n201161~~~~413.836124~~~184.375703~~~11916.8\n201162~~~~502.953953~~~173.237159~~~12468.3\n\"\"\"\n    colspecs = [(0, 4), (4, 8), (8, 20), (21, 33), (34, 43)]\n    result = read_fwf(StringIO(data), colspecs=colspecs, delimiter=\"~\")\n\n    expected = DataFrame(\n        [\n            [2011, 58, 360.242940, 149.910199, 11950.7],\n            [2011, 59, 444.953632, 166.985655, 11788.4],\n            [2011, 60, 364.136849, 183.628767, 11806.2],\n            [2011, 61, 413.836124, 184.375703, 11916.8],\n            [2011, 62, 502.953953, 173.237159, 12468.3],\n        ],\n        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_over_specified():\n    data = \"\"\"\\\nA   B     C            D            E\n201158    360.242940   149.910199   11950.7\n201159    444.953632   166.985655   11788.4\n201160    364.136849   183.628767   11806.2\n201161    413.836124   184.375703   11916.8\n201162    502.953953   173.237159   12468.3\n\"\"\"\n    colspecs = [(0, 4), (4, 8), (8, 20), (21, 33), (34, 43)]\n\n    with pytest.raises(ValueError, match=\"must specify only one of\"):\n        read_fwf(StringIO(data), colspecs=colspecs, widths=[6, 10, 10, 7])\n\n\ndef test_under_specified():\n    data = \"\"\"\\\nA   B     C            D            E\n201158    360.242940   149.910199   11950.7\n201159    444.953632   166.985655   11788.4\n201160    364.136849   183.628767   11806.2\n201161    413.836124   184.375703   11916.8\n201162    502.953953   173.237159   12468.3\n\"\"\"\n    with pytest.raises(ValueError, match=\"Must specify either\"):\n        read_fwf(StringIO(data), colspecs=None, widths=None)\n\n\ndef test_read_csv_compat():\n    csv_data = \"\"\"\\\nA,B,C,D,E\n2011,58,360.242940,149.910199,11950.7\n2011,59,444.953632,166.985655,11788.4\n2011,60,364.136849,183.628767,11806.2\n2011,61,413.836124,184.375703,11916.8\n2011,62,502.953953,173.237159,12468.3\n\"\"\"\n    expected = read_csv(StringIO(csv_data), engine=\"python\")\n\n    fwf_data = \"\"\"\\\nA   B     C            D            E\n201158    360.242940   149.910199   11950.7\n201159    444.953632   166.985655   11788.4\n201160    364.136849   183.628767   11806.2\n201161    413.836124   184.375703   11916.8\n201162    502.953953   173.237159   12468.3\n\"\"\"\n    colspecs = [(0, 4), (4, 8), (8, 20), (21, 33), (34, 43)]\n    result = read_fwf(StringIO(fwf_data), colspecs=colspecs)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_bytes_io_input():\n    result = read_fwf(BytesIO(\"שלום\\nשלום\".encode()), widths=[2, 2], encoding=\"utf8\")\n    expected = DataFrame([[\"של\", \"ום\"]], columns=[\"של\", \"ום\"])\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_fwf_colspecs_is_list_or_tuple():\n    data = \"\"\"index,A,B,C,D\nfoo,2,3,4,5\nbar,7,8,9,10\nbaz,12,13,14,15\nqux,12,13,14,15\nfoo2,12,13,14,15\nbar2,12,13,14,15\n\"\"\"\n\n    msg = \"column specifications must be a list or tuple.+\"\n\n    with pytest.raises(TypeError, match=msg):\n        read_fwf(StringIO(data), colspecs={\"a\": 1}, delimiter=\",\")\n\n\ndef test_fwf_colspecs_is_list_or_tuple_of_two_element_tuples():\n    data = \"\"\"index,A,B,C,D\nfoo,2,3,4,5\nbar,7,8,9,10\nbaz,12,13,14,15\nqux,12,13,14,15\nfoo2,12,13,14,15\nbar2,12,13,14,15\n\"\"\"\n\n    msg = \"Each column specification must be.+\"\n\n    with pytest.raises(TypeError, match=msg):\n        read_fwf(StringIO(data), colspecs=[(\"a\", 1)])\n\n\n@pytest.mark.parametrize(\n    \"colspecs,exp_data\",\n    [\n        ([(0, 3), (3, None)], [[123, 456], [456, 789]]),\n        ([(None, 3), (3, 6)], [[123, 456], [456, 789]]),\n        ([(0, None), (3, None)], [[123456, 456], [456789, 789]]),\n        ([(None, None), (3, 6)], [[123456, 456], [456789, 789]]),\n    ],\n)\ndef test_fwf_colspecs_none(colspecs, exp_data):\n    # see gh-7079\n    data = \"\"\"\\\n123456\n456789\n\"\"\"\n    expected = DataFrame(exp_data)\n\n    result = read_fwf(StringIO(data), colspecs=colspecs, header=None)\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\n    \"infer_nrows,exp_data\",\n    [\n        # infer_nrows --> colspec == [(2, 3), (5, 6)]\n        (1, [[1, 2], [3, 8]]),\n        # infer_nrows > number of rows\n        (10, [[1, 2], [123, 98]]),\n    ],\n)\ndef test_fwf_colspecs_infer_nrows(infer_nrows, exp_data):\n    # see gh-15138\n    data = \"\"\"\\\n  1  2\n123 98\n\"\"\"\n    expected = DataFrame(exp_data)\n\n    result = read_fwf(StringIO(data), infer_nrows=infer_nrows, header=None)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_fwf_regression():\n    # see gh-3594\n    #\n    # Turns out \"T060\" is parsable as a datetime slice!\n    tz_list = [1, 10, 20, 30, 60, 80, 100]\n    widths = [16] + [8] * len(tz_list)\n    names = [\"SST\"] + [f\"T{z:03d}\" for z in tz_list[1:]]\n\n    data = \"\"\"  2009164202000   9.5403  9.4105  8.6571  7.8372  6.0612  5.8843  5.5192\n2009164203000   9.5435  9.2010  8.6167  7.8176  6.0804  5.8728  5.4869\n2009164204000   9.5873  9.1326  8.4694  7.5889  6.0422  5.8526  5.4657\n2009164205000   9.5810  9.0896  8.4009  7.4652  6.0322  5.8189  5.4379\n2009164210000   9.6034  9.0897  8.3822  7.4905  6.0908  5.7904  5.4039\n\"\"\"\n\n    with tm.assert_produces_warning(FutureWarning, match=\"use 'date_format' instead\"):\n        result = read_fwf(\n            StringIO(data),\n            index_col=0,\n            header=None,\n            names=names,\n            widths=widths,\n            parse_dates=True,\n            date_parser=lambda s: datetime.strptime(s, \"%Y%j%H%M%S\"),\n        )\n    expected = DataFrame(\n        [\n            [9.5403, 9.4105, 8.6571, 7.8372, 6.0612, 5.8843, 5.5192],\n            [9.5435, 9.2010, 8.6167, 7.8176, 6.0804, 5.8728, 5.4869],\n            [9.5873, 9.1326, 8.4694, 7.5889, 6.0422, 5.8526, 5.4657],\n            [9.5810, 9.0896, 8.4009, 7.4652, 6.0322, 5.8189, 5.4379],\n            [9.6034, 9.0897, 8.3822, 7.4905, 6.0908, 5.7904, 5.4039],\n        ],\n        index=DatetimeIndex(\n            [\n                \"2009-06-13 20:20:00\",\n                \"2009-06-13 20:30:00\",\n                \"2009-06-13 20:40:00\",\n                \"2009-06-13 20:50:00\",\n                \"2009-06-13 21:00:00\",\n            ]\n        ),\n        columns=[\"SST\", \"T010\", \"T020\", \"T030\", \"T060\", \"T080\", \"T100\"],\n    )\n    tm.assert_frame_equal(result, expected)\n    result = read_fwf(\n        StringIO(data),\n        index_col=0,\n        header=None,\n        names=names,\n        widths=widths,\n        parse_dates=True,\n        date_format=\"%Y%j%H%M%S\",\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_fwf_for_uint8():\n    data = \"\"\"1421302965.213420    PRI=3 PGN=0xef00      DST=0x17 SRC=0x28    04 154 00 00 00 00 00 127\n1421302964.226776    PRI=6 PGN=0xf002               SRC=0x47    243 00 00 255 247 00 00 71\"\"\"  # noqa:E501\n    df = read_fwf(\n        StringIO(data),\n        colspecs=[(0, 17), (25, 26), (33, 37), (49, 51), (58, 62), (63, 1000)],\n        names=[\"time\", \"pri\", \"pgn\", \"dst\", \"src\", \"data\"],\n        converters={\n            \"pgn\": lambda x: int(x, 16),\n            \"src\": lambda x: int(x, 16),\n            \"dst\": lambda x: int(x, 16),\n            \"data\": lambda x: len(x.split(\" \")),\n        },\n    )\n\n    expected = DataFrame(\n        [\n            [1421302965.213420, 3, 61184, 23, 40, 8],\n            [1421302964.226776, 6, 61442, None, 71, 8],\n        ],\n        columns=[\"time\", \"pri\", \"pgn\", \"dst\", \"src\", \"data\"],\n    )\n    expected[\"dst\"] = expected[\"dst\"].astype(object)\n    tm.assert_frame_equal(df, expected)\n\n\n@pytest.mark.parametrize(\"comment\", [\"#\", \"~\", \"!\"])\ndef test_fwf_comment(comment):\n    data = \"\"\"\\\n  1   2.   4  #hello world\n  5  NaN  10.0\n\"\"\"\n    data = data.replace(\"#\", comment)\n\n    colspecs = [(0, 3), (4, 9), (9, 25)]\n    expected = DataFrame([[1, 2.0, 4], [5, np.nan, 10.0]])\n\n    result = read_fwf(StringIO(data), colspecs=colspecs, header=None, comment=comment)\n    tm.assert_almost_equal(result, expected)\n\n\ndef test_fwf_skip_blank_lines():\n    data = \"\"\"\n\nA         B            C            D\n\n201158    360.242940   149.910199   11950.7\n201159    444.953632   166.985655   11788.4\n\n\n201162    502.953953   173.237159   12468.3\n\n\"\"\"\n    result = read_fwf(StringIO(data), skip_blank_lines=True)\n    expected = DataFrame(\n        [\n            [201158, 360.242940, 149.910199, 11950.7],\n            [201159, 444.953632, 166.985655, 11788.4],\n            [201162, 502.953953, 173.237159, 12468.3],\n        ],\n        columns=[\"A\", \"B\", \"C\", \"D\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n    data = \"\"\"\\\nA         B            C            D\n201158    360.242940   149.910199   11950.7\n201159    444.953632   166.985655   11788.4\n\n\n201162    502.953953   173.237159   12468.3\n\"\"\"\n    result = read_fwf(StringIO(data), skip_blank_lines=False)\n    expected = DataFrame(\n        [\n            [201158, 360.242940, 149.910199, 11950.7],\n            [201159, 444.953632, 166.985655, 11788.4],\n            [np.nan, np.nan, np.nan, np.nan],\n            [np.nan, np.nan, np.nan, np.nan],\n            [201162, 502.953953, 173.237159, 12468.3],\n        ],\n        columns=[\"A\", \"B\", \"C\", \"D\"],\n    )\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"thousands\", [\",\", \"#\", \"~\"])\ndef test_fwf_thousands(thousands):\n    data = \"\"\"\\\n 1 2,334.0    5\n10   13     10.\n\"\"\"\n    data = data.replace(\",\", thousands)\n\n    colspecs = [(0, 3), (3, 11), (12, 16)]\n    expected = DataFrame([[1, 2334.0, 5], [10, 13, 10.0]])\n\n    result = read_fwf(\n        StringIO(data), header=None, colspecs=colspecs, thousands=thousands\n    )\n    tm.assert_almost_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"header\", [True, False])\ndef test_bool_header_arg(header):\n    # see gh-6114\n    data = \"\"\"\\\nMyColumn\n   a\n   b\n   a\n   b\"\"\"\n\n    msg = \"Passing a bool to header is invalid\"\n    with pytest.raises(TypeError, match=msg):\n        read_fwf(StringIO(data), header=header)\n\n\ndef test_full_file():\n    # File with all values.\n    test = \"\"\"index                             A    B    C\n2000-01-03T00:00:00  0.980268513777    3  foo\n2000-01-04T00:00:00  1.04791624281    -4  bar\n2000-01-05T00:00:00  0.498580885705   73  baz\n2000-01-06T00:00:00  1.12020151869     1  foo\n2000-01-07T00:00:00  0.487094399463    0  bar\n2000-01-10T00:00:00  0.836648671666    2  baz\n2000-01-11T00:00:00  0.157160753327   34  foo\"\"\"\n    colspecs = ((0, 19), (21, 35), (38, 40), (42, 45))\n    expected = read_fwf(StringIO(test), colspecs=colspecs)\n\n    result = read_fwf(StringIO(test))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_full_file_with_missing():\n    # File with missing values.\n    test = \"\"\"index                             A    B    C\n2000-01-03T00:00:00  0.980268513777    3  foo\n2000-01-04T00:00:00  1.04791624281    -4  bar\n                     0.498580885705   73  baz\n2000-01-06T00:00:00  1.12020151869     1  foo\n2000-01-07T00:00:00                    0  bar\n2000-01-10T00:00:00  0.836648671666    2  baz\n                                      34\"\"\"\n    colspecs = ((0, 19), (21, 35), (38, 40), (42, 45))\n    expected = read_fwf(StringIO(test), colspecs=colspecs)\n\n    result = read_fwf(StringIO(test))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_full_file_with_spaces():\n    # File with spaces in columns.\n    test = \"\"\"\nAccount                 Name  Balance     CreditLimit   AccountCreated\n101     Keanu Reeves          9315.45     10000.00           1/17/1998\n312     Gerard Butler         90.00       1000.00             8/6/2003\n868     Jennifer Love Hewitt  0           17000.00           5/25/1985\n761     Jada Pinkett-Smith    49654.87    100000.00          12/5/2006\n317     Bill Murray           789.65      5000.00             2/5/2007\n\"\"\".strip(\n        \"\\r\\n\"\n    )\n    colspecs = ((0, 7), (8, 28), (30, 38), (42, 53), (56, 70))\n    expected = read_fwf(StringIO(test), colspecs=colspecs)\n\n    result = read_fwf(StringIO(test))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_full_file_with_spaces_and_missing():\n    # File with spaces and missing values in columns.\n    test = \"\"\"\nAccount               Name    Balance     CreditLimit   AccountCreated\n101                           10000.00                       1/17/1998\n312     Gerard Butler         90.00       1000.00             8/6/2003\n868                                                          5/25/1985\n761     Jada Pinkett-Smith    49654.87    100000.00          12/5/2006\n317     Bill Murray           789.65\n\"\"\".strip(\n        \"\\r\\n\"\n    )\n    colspecs = ((0, 7), (8, 28), (30, 38), (42, 53), (56, 70))\n    expected = read_fwf(StringIO(test), colspecs=colspecs)\n\n    result = read_fwf(StringIO(test))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_messed_up_data():\n    # Completely messed up file.\n    test = \"\"\"\n   Account          Name             Balance     Credit Limit   Account Created\n       101                           10000.00                       1/17/1998\n       312     Gerard Butler         90.00       1000.00\n\n       761     Jada Pinkett-Smith    49654.87    100000.00          12/5/2006\n  317          Bill Murray           789.65\n\"\"\".strip(\n        \"\\r\\n\"\n    )\n    colspecs = ((2, 10), (15, 33), (37, 45), (49, 61), (64, 79))\n    expected = read_fwf(StringIO(test), colspecs=colspecs)\n\n    result = read_fwf(StringIO(test))\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_multiple_delimiters():\n    test = r\"\"\"\ncol1~~~~~col2  col3++++++++++++++++++col4\n~~22.....11.0+++foo~~~~~~~~~~Keanu Reeves\n  33+++122.33\\\\\\bar.........Gerard Butler\n++44~~~~12.01   baz~~Jennifer Love Hewitt\n~~55       11+++foo++++Jada Pinkett-Smith\n..66++++++.03~~~bar           Bill Murray\n\"\"\".strip(\n        \"\\r\\n\"\n    )\n    delimiter = \" +~.\\\\\"\n    colspecs = ((0, 4), (7, 13), (15, 19), (21, 41))\n    expected = read_fwf(StringIO(test), colspecs=colspecs, delimiter=delimiter)\n\n    result = read_fwf(StringIO(test), delimiter=delimiter)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_variable_width_unicode():\n    data = \"\"\"\nשלום שלום\nום   שלל\nשל   ום\n\"\"\".strip(\n        \"\\r\\n\"\n    )\n    encoding = \"utf8\"\n    kwargs = {\"header\": None, \"encoding\": encoding}\n\n    expected = read_fwf(\n        BytesIO(data.encode(encoding)), colspecs=[(0, 4), (5, 9)], **kwargs\n    )\n    result = read_fwf(BytesIO(data.encode(encoding)), **kwargs)\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"dtype\", [{}, {\"a\": \"float64\", \"b\": str, \"c\": \"int32\"}])\ndef test_dtype(dtype):\n    data = \"\"\" a    b    c\n1    2    3.2\n3    4    5.2\n\"\"\"\n    colspecs = [(0, 5), (5, 10), (10, None)]\n    result = read_fwf(StringIO(data), colspecs=colspecs, dtype=dtype)\n\n    expected = DataFrame(\n        {\"a\": [1, 3], \"b\": [2, 4], \"c\": [3.2, 5.2]}, columns=[\"a\", \"b\", \"c\"]\n    )\n\n    for col, dt in dtype.items():\n        expected[col] = expected[col].astype(dt)\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_skiprows_inference():\n    # see gh-11256\n    data = \"\"\"\nText contained in the file header\n\nDataCol1   DataCol2\n     0.0        1.0\n   101.6      956.1\n\"\"\".strip()\n    skiprows = 2\n    expected = read_csv(StringIO(data), skiprows=skiprows, delim_whitespace=True)\n\n    result = read_fwf(StringIO(data), skiprows=skiprows)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_skiprows_by_index_inference():\n    data = \"\"\"\nTo be skipped\nNot  To  Be  Skipped\nOnce more to be skipped\n123  34   8      123\n456  78   9      456\n\"\"\".strip()\n    skiprows = [0, 2]\n    expected = read_csv(StringIO(data), skiprows=skiprows, delim_whitespace=True)\n\n    result = read_fwf(StringIO(data), skiprows=skiprows)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_skiprows_inference_empty():\n    data = \"\"\"\nAA   BBB  C\n12   345  6\n78   901  2\n\"\"\".strip()\n\n    msg = \"No rows from which to infer column width\"\n    with pytest.raises(EmptyDataError, match=msg):\n        read_fwf(StringIO(data), skiprows=3)\n\n\ndef test_whitespace_preservation():\n    # see gh-16772\n    header = None\n    csv_data = \"\"\"\n a ,bbb\n cc,dd \"\"\"\n\n    fwf_data = \"\"\"\n a bbb\n ccdd \"\"\"\n    result = read_fwf(\n        StringIO(fwf_data), widths=[3, 3], header=header, skiprows=[0], delimiter=\"\\n\\t\"\n    )\n    expected = read_csv(StringIO(csv_data), header=header)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_default_delimiter():\n    header = None\n    csv_data = \"\"\"\na,bbb\ncc,dd\"\"\"\n\n    fwf_data = \"\"\"\na \\tbbb\ncc\\tdd \"\"\"\n    result = read_fwf(StringIO(fwf_data), widths=[3, 3], header=header, skiprows=[0])\n    expected = read_csv(StringIO(csv_data), header=header)\n    tm.assert_frame_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"infer\", [True, False])\ndef test_fwf_compression(compression_only, infer):\n    data = \"\"\"1111111111\n    2222222222\n    3333333333\"\"\".strip()\n\n    compression = compression_only\n    extension = _compression_to_extension[compression]\n\n    kwargs = {\"widths\": [5, 5], \"names\": [\"one\", \"two\"]}\n    expected = read_fwf(StringIO(data), **kwargs)\n\n    data = bytes(data, encoding=\"utf-8\")\n\n    with tm.ensure_clean(filename=\"tmp.\" + extension) as path:\n        tm.write_to_compressed(compression, path, data)\n\n        if infer is not None:\n            kwargs[\"compression\"] = \"infer\" if infer else compression\n\n        result = read_fwf(path, **kwargs)\n        tm.assert_frame_equal(result, expected)\n\n\ndef test_binary_mode():\n    \"\"\"\n    read_fwf supports opening files in binary mode.\n\n    GH 18035.\n    \"\"\"\n    data = \"\"\"aas aas aas\nbba bab b a\"\"\"\n    df_reference = DataFrame(\n        [[\"bba\", \"bab\", \"b a\"]], columns=[\"aas\", \"aas.1\", \"aas.2\"], index=[0]\n    )\n    with tm.ensure_clean() as path:\n        Path(path).write_text(data)\n        with open(path, \"rb\") as file:\n            df = read_fwf(file)\n            file.seek(0)\n            tm.assert_frame_equal(df, df_reference)\n\n\n@pytest.mark.parametrize(\"memory_map\", [True, False])\ndef test_encoding_mmap(memory_map):\n    \"\"\"\n    encoding should be working, even when using a memory-mapped file.\n\n    GH 23254.\n    \"\"\"\n    encoding = \"iso8859_1\"\n    with tm.ensure_clean() as path:\n        Path(path).write_bytes(\" 1 A Ä 2\\n\".encode(encoding))\n        df = read_fwf(\n            path,\n            header=None,\n            widths=[2, 2, 2, 2],\n            encoding=encoding,\n            memory_map=memory_map,\n        )\n    df_reference = DataFrame([[1, \"A\", \"Ä\", 2]])\n    tm.assert_frame_equal(df, df_reference)\n\n\n@pytest.mark.parametrize(\n    \"colspecs, names, widths, index_col\",\n    [\n        (\n            [(0, 6), (6, 12), (12, 18), (18, None)],\n            list(\"abcde\"),\n            None,\n            None,\n        ),\n        (\n            None,\n            list(\"abcde\"),\n            [6] * 4,\n            None,\n        ),\n        (\n            [(0, 6), (6, 12), (12, 18), (18, None)],\n            list(\"abcde\"),\n            None,\n            True,\n        ),\n        (\n            None,\n            list(\"abcde\"),\n            [6] * 4,\n            False,\n        ),\n        (\n            None,\n            list(\"abcde\"),\n            [6] * 4,\n            True,\n        ),\n        (\n            [(0, 6), (6, 12), (12, 18), (18, None)],\n            list(\"abcde\"),\n            None,\n            False,\n        ),\n    ],\n)\ndef test_len_colspecs_len_names(colspecs, names, widths, index_col):\n    # GH#40830\n    data = \"\"\"col1  col2  col3  col4\n    bab   ba    2\"\"\"\n    msg = \"Length of colspecs must match length of names\"\n    with pytest.raises(ValueError, match=msg):\n        read_fwf(\n            StringIO(data),\n            colspecs=colspecs,\n            names=names,\n            widths=widths,\n            index_col=index_col,\n        )\n\n\n@pytest.mark.parametrize(\n    \"colspecs, names, widths, index_col, expected\",\n    [\n        (\n            [(0, 6), (6, 12), (12, 18), (18, None)],\n            list(\"abc\"),\n            None,\n            0,\n            DataFrame(\n                index=[\"col1\", \"ba\"],\n                columns=[\"a\", \"b\", \"c\"],\n                data=[[\"col2\", \"col3\", \"col4\"], [\"b   ba\", \"2\", np.nan]],\n            ),\n        ),\n        (\n            [(0, 6), (6, 12), (12, 18), (18, None)],\n            list(\"ab\"),\n            None,\n            [0, 1],\n            DataFrame(\n                index=[[\"col1\", \"ba\"], [\"col2\", \"b   ba\"]],\n                columns=[\"a\", \"b\"],\n                data=[[\"col3\", \"col4\"], [\"2\", np.nan]],\n            ),\n        ),\n        (\n            [(0, 6), (6, 12), (12, 18), (18, None)],\n            list(\"a\"),\n            None,\n            [0, 1, 2],\n            DataFrame(\n                index=[[\"col1\", \"ba\"], [\"col2\", \"b   ba\"], [\"col3\", \"2\"]],\n                columns=[\"a\"],\n                data=[[\"col4\"], [np.nan]],\n            ),\n        ),\n        (\n            None,\n            list(\"abc\"),\n            [6] * 4,\n            0,\n            DataFrame(\n                index=[\"col1\", \"ba\"],\n                columns=[\"a\", \"b\", \"c\"],\n                data=[[\"col2\", \"col3\", \"col4\"], [\"b   ba\", \"2\", np.nan]],\n            ),\n        ),\n        (\n            None,\n            list(\"ab\"),\n            [6] * 4,\n            [0, 1],\n            DataFrame(\n                index=[[\"col1\", \"ba\"], [\"col2\", \"b   ba\"]],\n                columns=[\"a\", \"b\"],\n                data=[[\"col3\", \"col4\"], [\"2\", np.nan]],\n            ),\n        ),\n        (\n            None,\n            list(\"a\"),\n            [6] * 4,\n            [0, 1, 2],\n            DataFrame(\n                index=[[\"col1\", \"ba\"], [\"col2\", \"b   ba\"], [\"col3\", \"2\"]],\n                columns=[\"a\"],\n                data=[[\"col4\"], [np.nan]],\n            ),\n        ),\n    ],\n)\ndef test_len_colspecs_len_names_with_index_col(\n    colspecs, names, widths, index_col, expected\n):\n    # GH#40830\n    data = \"\"\"col1  col2  col3  col4\n    bab   ba    2\"\"\"\n    result = read_fwf(\n        StringIO(data),\n        colspecs=colspecs,\n        names=names,\n        widths=widths,\n        index_col=index_col,\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_colspecs_with_comment():\n    # GH 14135\n    result = read_fwf(\n        StringIO(\"#\\nA1K\\n\"), colspecs=[(1, 2), (2, 3)], comment=\"#\", header=None\n    )\n    expected = DataFrame([[1, \"K\"]], columns=[0, 1])\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_skip_rows_and_n_rows():\n    # GH#44021\n    data = \"\"\"a\\tb\n1\\t a\n2\\t b\n3\\t c\n4\\t d\n5\\t e\n6\\t f\n    \"\"\"\n    result = read_fwf(StringIO(data), nrows=4, skiprows=[2, 4])\n    expected = DataFrame({\"a\": [1, 3, 5, 6], \"b\": [\"a\", \"c\", \"e\", \"f\"]})\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_skiprows_with_iterator():\n    # GH#10261\n    data = \"\"\"0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n    \"\"\"\n    df_iter = read_fwf(\n        StringIO(data),\n        colspecs=[(0, 2)],\n        names=[\"a\"],\n        iterator=True,\n        chunksize=2,\n        skiprows=[0, 1, 2, 6, 9],\n    )\n    expected_frames = [\n        DataFrame({\"a\": [3, 4]}),\n        DataFrame({\"a\": [5, 7, 8]}, index=[2, 3, 4]),\n        DataFrame({\"a\": []}, dtype=\"object\"),\n    ]\n    for i, result in enumerate(df_iter):\n        tm.assert_frame_equal(result, expected_frames[i])\n\n\ndef test_names_and_infer_colspecs():\n    # GH#45337\n    data = \"\"\"X   Y   Z\n      959.0    345   22.2\n    \"\"\"\n    result = read_fwf(StringIO(data), skiprows=1, usecols=[0, 2], names=[\"a\", \"b\"])\n    expected = DataFrame({\"a\": [959.0], \"b\": 22.2})\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_widths_and_usecols():\n    # GH#46580\n    data = \"\"\"0  1    n -0.4100.1\n0  2    p  0.2 90.1\n0  3    n -0.3140.4\"\"\"\n    result = read_fwf(\n        StringIO(data),\n        header=None,\n        usecols=(0, 1, 3),\n        widths=(3, 5, 1, 5, 5),\n        index_col=False,\n        names=(\"c0\", \"c1\", \"c3\"),\n    )\n    expected = DataFrame(\n        {\n            \"c0\": 0,\n            \"c1\": [1, 2, 3],\n            \"c3\": [-0.4, 0.2, -0.3],\n        }\n    )\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_use_nullable_dtypes(string_storage, dtype_backend):\n    # GH#50289\n    if string_storage == \"python\":\n        arr = StringArray(np.array([\"a\", \"b\"], dtype=np.object_))\n        arr_na = StringArray(np.array([pd.NA, \"a\"], dtype=np.object_))\n    else:\n        pa = pytest.importorskip(\"pyarrow\")\n        arr = ArrowStringArray(pa.array([\"a\", \"b\"]))\n        arr_na = ArrowStringArray(pa.array([None, \"a\"]))\n\n    data = \"\"\"a  b    c      d  e     f  g    h  i\n1  2.5  True  a\n3  4.5  False b  True  6  7.5  a\"\"\"\n    with pd.option_context(\"mode.string_storage\", string_storage):\n        with pd.option_context(\"mode.dtype_backend\", dtype_backend):\n            result = read_fwf(StringIO(data), use_nullable_dtypes=True)\n\n    expected = DataFrame(\n        {\n            \"a\": pd.Series([1, 3], dtype=\"Int64\"),\n            \"b\": pd.Series([2.5, 4.5], dtype=\"Float64\"),\n            \"c\": pd.Series([True, False], dtype=\"boolean\"),\n            \"d\": arr,\n            \"e\": pd.Series([pd.NA, True], dtype=\"boolean\"),\n            \"f\": pd.Series([pd.NA, 6], dtype=\"Int64\"),\n            \"g\": pd.Series([pd.NA, 7.5], dtype=\"Float64\"),\n            \"h\": arr_na,\n            \"i\": pd.Series([pd.NA, pd.NA], dtype=\"Int64\"),\n        }\n    )\n    if dtype_backend == \"pyarrow\":\n        pa = pytest.importorskip(\"pyarrow\")\n        from pandas.arrays import ArrowExtensionArray\n\n        expected = DataFrame(\n            {\n                col: ArrowExtensionArray(pa.array(expected[col], from_pandas=True))\n                for col in expected.columns\n            }\n        )\n        expected[\"i\"] = ArrowExtensionArray(pa.array([None, None]))\n\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_use_nullable_dtypes_option():\n    # GH#50748\n\n    data = \"\"\"a\n1\n3\"\"\"\n    with pd.option_context(\"mode.nullable_dtypes\", True):\n        result = read_fwf(StringIO(data))\n\n    expected = DataFrame({\"a\": pd.Series([1, 3], dtype=\"Int64\")})\n    tm.assert_frame_equal(result, expected)\n"
    }
  ]
}