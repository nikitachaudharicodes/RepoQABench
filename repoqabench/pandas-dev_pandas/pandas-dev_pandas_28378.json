{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "28378",
  "issue_description": "# tm.makeObjectSeries returns a Series with dtype=\"datetime64[ns]\"\n\n```python-traceback\r\n>>> import pandas.util.testing as tm\r\n>>> tm.makeObjectSeries()\r\n...\r\nt1Q4jcW3nW   2000-02-09\r\nElZCr3WTz2   2000-02-10\r\np43Nqg7u4D   2000-02-11\r\ndtype: datetime64[ns]\r\n```\r\n\r\n@jbrockmendel and I were both surprised that calling `makeObjectSeries` returns a Series with a datetime dtype. I think it would be more logical for this function to actually return an object dtype Series",
  "issue_comments": [
    {
      "id": 530128141,
      "user": "WillAyd",
      "body": "xref https://github.com/pandas-dev/pandas/pull/27838/files#r322964878"
    },
    {
      "id": 530850947,
      "user": "jmg7173",
      "body": "Hi, I want to contribute to this issue.\r\n\r\nIs it okay just add `dtype=object` at Series generation for this issue?\r\nOr is there more deep modification about this issue? (I'm not meaning about modification of test code related to makeObjectSeries)\r\n"
    },
    {
      "id": 530878679,
      "user": "WillAyd",
      "body": "Yea I think OK to start with that and see what breaks. Ideally may want to have it contain more than date time objects but not sure what effort that would take; just getting it to object dtype in the first place will be a good start"
    },
    {
      "id": 530884657,
      "user": "jmg7173",
      "body": "I modified it to object. But performing add at Series, it modifies dates and dtype as datetime64.\r\n\r\nHow can I forcibly prevent add/sub operator from modifying data and dtype?\r\nIs it natural that automatically changing the dtype when some modification occurs?\r\nI think data modification should not be happend if it modifies dtype as datetime64. (In case of datetime64, it raises exception)"
    },
    {
      "id": 530903214,
      "user": "jbrockmendel",
      "body": "> How can I forcibly prevent add/sub operator from modifying data and dtype?\r\n\r\nCan you be more specific about what you mean by \"performing add at Series\"?"
    },
    {
      "id": 530907066,
      "user": "jmg7173",
      "body": "I mean, at `tests/arithmetic/test_object.py test_object_arr_invalid`, it performs add operation.\r\n\r\nI simulated it manually, it manipulates Series with `dtype=object` to `dtype=datetime64[ns]` and also, it also manipulates data also.\r\n\r\n```\r\n>>> obj_ser = tm.makeObjectSeries()\r\n>>> obj_ser \r\nYipVeLYwc8   2000-01-03\r\n...\r\nwHtkS7TlO2    2000-02-08 00:00:00\r\ntTYkAjpXUN    2000-02-09 00:00:00\r\n5OGw3PNMfM    2000-02-10 00:00:00\r\n14tcE5tlIl    2000-02-11 00:00:00\r\ndtype: object\r\n\r\n>>> operator.add(obj_ser, 1)\r\nYipVeLYwc8   2000-01-04\r\n...\r\ntTYkAjpXUN   2000-02-10\r\n5OGw3PNMfM   2000-02-11\r\n14tcE5tlIl   2000-02-14\r\ndtype: datetime64[ns]\r\n```\r\n"
    },
    {
      "id": 531031584,
      "user": "jbrockmendel",
      "body": "what happens if you forget the datetimes altogether and fill it with strings?"
    },
    {
      "id": 531093633,
      "user": "jmg7173",
      "body": "It passes tests.\r\n\r\nIf I change `1` to `\"1\"`, it appends `\"1\"` to all the data.\r\nAnd it doesn't convert dtype.\r\n```\r\n>>> tm.makeObjectSeries() + \"1\"\r\n\r\nsb7X5CxFoh    Zn6Ur8Sd6Z1\r\n...\r\n5ixyCT4dWC    ECQrisl5Ln1\r\nvAvu2yKqsG    HN5LTCpT0V1\r\nDor4uHXigH    Mx1u5ukW141\r\ndtype: object\r\n```"
    },
    {
      "id": 531236909,
      "user": "jmg7173",
      "body": "Is it okay to change filling data as `datetime64[ns]` to `String` with `dtype: object`?"
    },
    {
      "id": 531294816,
      "user": "jbrockmendel",
      "body": "> Is it okay to change filling data as datetime64[ns] to String with dtype: object?\r\n\r\nProbably, yes.  The one thing I want to double-check is that we aren't losing coverage for cases where we actually _do_ want a datetime64 Series, which I think might be the case in the arithmetic test.\r\n\r\n@WillAyd thoughts on how to track this down more generally?  grepping shows only 11 usages of makeObjectSeries, but some of those are in fixtures."
    },
    {
      "id": 531313980,
      "user": "jmg7173",
      "body": "As I searched, there is no usecase that `objectSeries` used just for `datetime` excluding arithmetic test.\r\n\r\nI searched it including `object_series` fixture and instance of `TestData`(`TestData.objSeries`).\r\nI hope I found everything :)"
    },
    {
      "id": 531377840,
      "user": "WillAyd",
      "body": "> @WillAyd thoughts on how to track this down more generally? grepping shows only 11 usages of makeObjectSeries, but some of those are in fixtures.\r\n\r\nYea I think @jmg7173 if you can search for the fixture where this is used and check that as well would be helpful.\r\n\r\nMostly could just change and see what breaks though. The naming of this obviously means the intent is to deal with object data, so I don't think should break too much actually making this return an object dtype"
    },
    {
      "id": 531443939,
      "user": "jmg7173",
      "body": ">  Yea I think @jmg7173 if you can search for the fixture where this is used and check that as well would be helpful.\r\n\r\nHere's list where this is used:\r\n* As function `makeObjectSeries` itself:\r\n```\r\npandas/tests/arithmetic/test_object.py L144 test_objarr_add_invalid\r\npandas/tests/dtypes/test_missing.py L94-101 test_isna_isnull\r\npandas/tests/dtypes/test_missing.py L51-59 test_notna_notnull\r\npandas/tests/generic/test_generic.py L703-704 test_squeeze\r\npandas/tests/generic/test_generic.py L746 test_transpose\r\npandas/tests/generic/test_generic.py L768 test_take\r\npandas/tests/io/test_packers.py L459 test_basic at TestSeries (defined as self.d[\"object\"])\r\npandas/tests/io/json/test_pandas.py L885-890 test_series_from_json_to_json at TestPandasContainer (defined as self.objSeries)\r\n```\r\n\r\n* As return of method `objSeries` at class `TestData`:\r\n```\r\nDefined:\r\npandas/tests/series/common.py  L23 as method objSeries of class TestData\r\n(TestData fixture defined at pandas/tests/series/indexing/conftest.py)\r\n\r\nUsed:\r\npandas/pandas/tests/series/test_repr.py L74 (in test_repr)\r\npandas/tests/series/indexing/test_indexing.py L92 test_getitem_get\r\npandas/tests/series/indexing/test_indexing.py L121 test_getitem_fancy\r\npandas/tests/series/indexing/test_indexing.py L559 test_slice\r\n```\r\n\r\n* As fixture `object_series`:\r\n```\r\nDefined:\r\npandas/tests/series/conftest.py L26 as object_series\r\n\r\nUsed:\r\npandas/tests/series/test_combine_concat.py L14 test_append as object_series fixture\r\n\r\n```\r\n\r\n> Mostly could just change and see what breaks though. The naming of this obviously means the intent is to deal with object data, so I don't think should break too much actually making this return an object dtype\r\n\r\nAs you say, there is no breaks at test.\r\n\r\nEven though tests are passed, I think `test_objarr_add_invalid` should be checked whether it used as right way.\r\n\r\nI'll make PR about this issue!"
    },
    {
      "id": 531446471,
      "user": "WillAyd",
      "body": "Awesome - thanks for digging into this"
    }
  ],
  "text_context": "# tm.makeObjectSeries returns a Series with dtype=\"datetime64[ns]\"\n\n```python-traceback\r\n>>> import pandas.util.testing as tm\r\n>>> tm.makeObjectSeries()\r\n...\r\nt1Q4jcW3nW   2000-02-09\r\nElZCr3WTz2   2000-02-10\r\np43Nqg7u4D   2000-02-11\r\ndtype: datetime64[ns]\r\n```\r\n\r\n@jbrockmendel and I were both surprised that calling `makeObjectSeries` returns a Series with a datetime dtype. I think it would be more logical for this function to actually return an object dtype Series\n\nxref https://github.com/pandas-dev/pandas/pull/27838/files#r322964878\n\nHi, I want to contribute to this issue.\r\n\r\nIs it okay just add `dtype=object` at Series generation for this issue?\r\nOr is there more deep modification about this issue? (I'm not meaning about modification of test code related to makeObjectSeries)\r\n\n\nYea I think OK to start with that and see what breaks. Ideally may want to have it contain more than date time objects but not sure what effort that would take; just getting it to object dtype in the first place will be a good start\n\nI modified it to object. But performing add at Series, it modifies dates and dtype as datetime64.\r\n\r\nHow can I forcibly prevent add/sub operator from modifying data and dtype?\r\nIs it natural that automatically changing the dtype when some modification occurs?\r\nI think data modification should not be happend if it modifies dtype as datetime64. (In case of datetime64, it raises exception)\n\n> How can I forcibly prevent add/sub operator from modifying data and dtype?\r\n\r\nCan you be more specific about what you mean by \"performing add at Series\"?\n\nI mean, at `tests/arithmetic/test_object.py test_object_arr_invalid`, it performs add operation.\r\n\r\nI simulated it manually, it manipulates Series with `dtype=object` to `dtype=datetime64[ns]` and also, it also manipulates data also.\r\n\r\n```\r\n>>> obj_ser = tm.makeObjectSeries()\r\n>>> obj_ser \r\nYipVeLYwc8   2000-01-03\r\n...\r\nwHtkS7TlO2    2000-02-08 00:00:00\r\ntTYkAjpXUN    2000-02-09 00:00:00\r\n5OGw3PNMfM    2000-02-10 00:00:00\r\n14tcE5tlIl    2000-02-11 00:00:00\r\ndtype: object\r\n\r\n>>> operator.add(obj_ser, 1)\r\nYipVeLYwc8   2000-01-04\r\n...\r\ntTYkAjpXUN   2000-02-10\r\n5OGw3PNMfM   2000-02-11\r\n14tcE5tlIl   2000-02-14\r\ndtype: datetime64[ns]\r\n```\r\n\n\nwhat happens if you forget the datetimes altogether and fill it with strings?\n\nIt passes tests.\r\n\r\nIf I change `1` to `\"1\"`, it appends `\"1\"` to all the data.\r\nAnd it doesn't convert dtype.\r\n```\r\n>>> tm.makeObjectSeries() + \"1\"\r\n\r\nsb7X5CxFoh    Zn6Ur8Sd6Z1\r\n...\r\n5ixyCT4dWC    ECQrisl5Ln1\r\nvAvu2yKqsG    HN5LTCpT0V1\r\nDor4uHXigH    Mx1u5ukW141\r\ndtype: object\r\n```\n\nIs it okay to change filling data as `datetime64[ns]` to `String` with `dtype: object`?\n\n> Is it okay to change filling data as datetime64[ns] to String with dtype: object?\r\n\r\nProbably, yes.  The one thing I want to double-check is that we aren't losing coverage for cases where we actually _do_ want a datetime64 Series, which I think might be the case in the arithmetic test.\r\n\r\n@WillAyd thoughts on how to track this down more generally?  grepping shows only 11 usages of makeObjectSeries, but some of those are in fixtures.\n\nAs I searched, there is no usecase that `objectSeries` used just for `datetime` excluding arithmetic test.\r\n\r\nI searched it including `object_series` fixture and instance of `TestData`(`TestData.objSeries`).\r\nI hope I found everything :)\n\n> @WillAyd thoughts on how to track this down more generally? grepping shows only 11 usages of makeObjectSeries, but some of those are in fixtures.\r\n\r\nYea I think @jmg7173 if you can search for the fixture where this is used and check that as well would be helpful.\r\n\r\nMostly could just change and see what breaks though. The naming of this obviously means the intent is to deal with object data, so I don't think should break too much actually making this return an object dtype\n\n>  Yea I think @jmg7173 if you can search for the fixture where this is used and check that as well would be helpful.\r\n\r\nHere's list where this is used:\r\n* As function `makeObjectSeries` itself:\r\n```\r\npandas/tests/arithmetic/test_object.py L144 test_objarr_add_invalid\r\npandas/tests/dtypes/test_missing.py L94-101 test_isna_isnull\r\npandas/tests/dtypes/test_missing.py L51-59 test_notna_notnull\r\npandas/tests/generic/test_generic.py L703-704 test_squeeze\r\npandas/tests/generic/test_generic.py L746 test_transpose\r\npandas/tests/generic/test_generic.py L768 test_take\r\npandas/tests/io/test_packers.py L459 test_basic at TestSeries (defined as self.d[\"object\"])\r\npandas/tests/io/json/test_pandas.py L885-890 test_series_from_json_to_json at TestPandasContainer (defined as self.objSeries)\r\n```\r\n\r\n* As return of method `objSeries` at class `TestData`:\r\n```\r\nDefined:\r\npandas/tests/series/common.py  L23 as method objSeries of class TestData\r\n(TestData fixture defined at pandas/tests/series/indexing/conftest.py)\r\n\r\nUsed:\r\npandas/pandas/tests/series/test_repr.py L74 (in test_repr)\r\npandas/tests/series/indexing/test_indexing.py L92 test_getitem_get\r\npandas/tests/series/indexing/test_indexing.py L121 test_getitem_fancy\r\npandas/tests/series/indexing/test_indexing.py L559 test_slice\r\n```\r\n\r\n* As fixture `object_series`:\r\n```\r\nDefined:\r\npandas/tests/series/conftest.py L26 as object_series\r\n\r\nUsed:\r\npandas/tests/series/test_combine_concat.py L14 test_append as object_series fixture\r\n\r\n```\r\n\r\n> Mostly could just change and see what breaks though. The naming of this obviously means the intent is to deal with object data, so I don't think should break too much actually making this return an object dtype\r\n\r\nAs you say, there is no breaks at test.\r\n\r\nEven though tests are passed, I think `test_objarr_add_invalid` should be checked whether it used as right way.\r\n\r\nI'll make PR about this issue!\n\nAwesome - thanks for digging into this",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/27838",
  "code_context": [
    {
      "filename": "pandas/tests/io/json/conftest.py",
      "content": "import pytest\n\n\n@pytest.fixture(params=[\"split\", \"records\", \"index\", \"columns\", \"values\"])\ndef orient(request):\n    \"\"\"\n    Fixture for orients excluding the table format.\n    \"\"\"\n    return request.param\n"
    },
    {
      "filename": "pandas/tests/io/json/test_pandas.py",
      "content": "from collections import OrderedDict\nfrom datetime import timedelta\nfrom io import StringIO\nimport json\nimport os\n\nimport numpy as np\nimport pytest\n\nfrom pandas.compat import PY35, is_platform_32bit\nimport pandas.util._test_decorators as td\n\nimport pandas as pd\nfrom pandas import DataFrame, DatetimeIndex, Series, Timestamp, read_json\nimport pandas.util.testing as tm\nfrom pandas.util.testing import (\n    assert_frame_equal,\n    assert_series_equal,\n    ensure_clean,\n    network,\n)\n\n_seriesd = tm.getSeriesData()\n_tsd = tm.getTimeSeriesData()\n\n_frame = DataFrame(_seriesd)\n_frame2 = DataFrame(_seriesd, columns=[\"D\", \"C\", \"B\", \"A\"])\n_intframe = DataFrame({k: v.astype(np.int64) for k, v in _seriesd.items()})\n\n_tsframe = DataFrame(_tsd)\n_cat_frame = _frame.copy()\ncat = [\"bah\"] * 5 + [\"bar\"] * 5 + [\"baz\"] * 5 + [\"foo\"] * (len(_cat_frame) - 15)\n_cat_frame.index = pd.CategoricalIndex(cat, name=\"E\")\n_cat_frame[\"E\"] = list(reversed(cat))\n_cat_frame[\"sort\"] = np.arange(len(_cat_frame), dtype=\"int64\")\n\n_mixed_frame = _frame.copy()\n\n\nclass TestPandasContainer:\n    @pytest.fixture(scope=\"function\", autouse=True)\n    def setup(self, datapath):\n        self.dirpath = datapath(\"io\", \"json\", \"data\")\n\n        self.ts = tm.makeTimeSeries()\n        self.ts.name = \"ts\"\n\n        self.series = tm.makeStringSeries()\n        self.series.name = \"series\"\n\n        self.objSeries = tm.makeObjectSeries()\n        self.objSeries.name = \"objects\"\n\n        self.empty_series = Series([], index=[])\n        self.empty_frame = DataFrame()\n\n        self.frame = _frame.copy()\n        self.frame2 = _frame2.copy()\n        self.intframe = _intframe.copy()\n        self.tsframe = _tsframe.copy()\n        self.mixed_frame = _mixed_frame.copy()\n        self.categorical = _cat_frame.copy()\n\n        yield\n\n        del self.dirpath\n\n        del self.ts\n\n        del self.series\n\n        del self.objSeries\n\n        del self.empty_series\n        del self.empty_frame\n\n        del self.frame\n        del self.frame2\n        del self.intframe\n        del self.tsframe\n        del self.mixed_frame\n\n    def test_frame_double_encoded_labels(self, orient):\n        df = DataFrame(\n            [[\"a\", \"b\"], [\"c\", \"d\"]],\n            index=['index \" 1', \"index / 2\"],\n            columns=[\"a \\\\ b\", \"y / z\"],\n        )\n\n        result = read_json(df.to_json(orient=orient), orient=orient)\n        expected = df.copy()\n\n        if orient == \"records\" or orient == \"values\":\n            expected = expected.reset_index(drop=True)\n        if orient == \"values\":\n            expected.columns = range(len(expected.columns))\n\n        assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\"orient\", [\"split\", \"records\", \"values\"])\n    def test_frame_non_unique_index(self, orient):\n        df = DataFrame([[\"a\", \"b\"], [\"c\", \"d\"]], index=[1, 1], columns=[\"x\", \"y\"])\n        result = read_json(df.to_json(orient=orient), orient=orient)\n        expected = df.copy()\n\n        if orient == \"records\" or orient == \"values\":\n            expected = expected.reset_index(drop=True)\n        if orient == \"values\":\n            expected.columns = range(len(expected.columns))\n\n        assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\"orient\", [\"index\", \"columns\"])\n    def test_frame_non_unique_index_raises(self, orient):\n        df = DataFrame([[\"a\", \"b\"], [\"c\", \"d\"]], index=[1, 1], columns=[\"x\", \"y\"])\n        msg = \"DataFrame index must be unique for orient='{}'\".format(orient)\n        with pytest.raises(ValueError, match=msg):\n            df.to_json(orient=orient)\n\n    @pytest.mark.parametrize(\"orient\", [\"split\", \"values\"])\n    @pytest.mark.parametrize(\n        \"data\",\n        [\n            [[\"a\", \"b\"], [\"c\", \"d\"]],\n            [[1.5, 2.5], [3.5, 4.5]],\n            [[1, 2.5], [3, 4.5]],\n            [[Timestamp(\"20130101\"), 3.5], [Timestamp(\"20130102\"), 4.5]],\n        ],\n    )\n    def test_frame_non_unique_columns(self, orient, data):\n        df = DataFrame(data, index=[1, 2], columns=[\"x\", \"x\"])\n\n        result = read_json(\n            df.to_json(orient=orient), orient=orient, convert_dates=[\"x\"]\n        )\n        if orient == \"values\":\n            expected = pd.DataFrame(data)\n            if expected.iloc[:, 0].dtype == \"datetime64[ns]\":\n                # orient == \"values\" by default will write Timestamp objects out\n                # in milliseconds; these are internally stored in nanosecond,\n                # so divide to get where we need\n                # TODO: a to_epoch method would also solve; see GH 14772\n                expected.iloc[:, 0] = expected.iloc[:, 0].astype(np.int64) // 1000000\n        elif orient == \"split\":\n            expected = df\n\n        assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\"orient\", [\"index\", \"columns\", \"records\"])\n    def test_frame_non_unique_columns_raises(self, orient):\n        df = DataFrame([[\"a\", \"b\"], [\"c\", \"d\"]], index=[1, 2], columns=[\"x\", \"x\"])\n\n        msg = \"DataFrame columns must be unique for orient='{}'\".format(orient)\n        with pytest.raises(ValueError, match=msg):\n            df.to_json(orient=orient)\n\n    def test_frame_from_json_to_json(self):\n        def _check_orient(\n            df,\n            orient,\n            dtype=None,\n            numpy=False,\n            convert_axes=True,\n            check_dtype=True,\n            raise_ok=None,\n            sort=None,\n            check_index_type=True,\n            check_column_type=True,\n            check_numpy_dtype=False,\n        ):\n            if sort is not None:\n                df = df.sort_values(sort)\n            else:\n                df = df.sort_index()\n\n            # if we are not unique, then check that we are raising ValueError\n            # for the appropriate orients\n            if not df.index.is_unique and orient in [\"index\", \"columns\"]:\n                msg = \"DataFrame index must be unique for orient='{}'\".format(orient)\n                with pytest.raises(ValueError, match=msg):\n                    df.to_json(orient=orient)\n                return\n            if not df.columns.is_unique and orient in [\"index\", \"columns\", \"records\"]:\n                # TODO: not executed. fix this.\n                with pytest.raises(ValueError, match=\"ksjkajksfjksjfkjs\"):\n                    df.to_json(orient=orient)\n                return\n\n            dfjson = df.to_json(orient=orient)\n\n            try:\n                unser = read_json(\n                    dfjson,\n                    orient=orient,\n                    dtype=dtype,\n                    numpy=numpy,\n                    convert_axes=convert_axes,\n                )\n            except Exception as detail:\n                if raise_ok is not None:\n                    if isinstance(detail, raise_ok):\n                        return\n                raise\n\n            if sort is not None and sort in unser.columns:\n                unser = unser.sort_values(sort)\n            else:\n                unser = unser.sort_index()\n\n            if not dtype:\n                check_dtype = False\n\n            if not convert_axes and df.index.dtype.type == np.datetime64:\n                unser.index = DatetimeIndex(unser.index.values.astype(\"i8\") * 1e6)\n            if orient == \"records\":\n                # index is not captured in this orientation\n                tm.assert_almost_equal(\n                    df.values, unser.values, check_dtype=check_numpy_dtype\n                )\n                tm.assert_index_equal(\n                    df.columns, unser.columns, exact=check_column_type\n                )\n            elif orient == \"values\":\n                # index and cols are not captured in this orientation\n                if numpy is True and df.shape == (0, 0):\n                    assert unser.shape[0] == 0\n                else:\n                    tm.assert_almost_equal(\n                        df.values, unser.values, check_dtype=check_numpy_dtype\n                    )\n            elif orient == \"split\":\n                # index and col labels might not be strings\n                unser.index = [str(i) for i in unser.index]\n                unser.columns = [str(i) for i in unser.columns]\n\n                if sort is None:\n                    unser = unser.sort_index()\n                tm.assert_almost_equal(\n                    df.values, unser.values, check_dtype=check_numpy_dtype\n                )\n            else:\n                if convert_axes:\n                    tm.assert_frame_equal(\n                        df,\n                        unser,\n                        check_dtype=check_dtype,\n                        check_index_type=check_index_type,\n                        check_column_type=check_column_type,\n                    )\n                else:\n                    tm.assert_frame_equal(\n                        df, unser, check_less_precise=False, check_dtype=check_dtype\n                    )\n\n        def _check_all_orients(\n            df,\n            dtype=None,\n            convert_axes=True,\n            raise_ok=None,\n            sort=None,\n            check_index_type=True,\n            check_column_type=True,\n        ):\n\n            # numpy=False\n            if convert_axes:\n                _check_orient(\n                    df,\n                    \"columns\",\n                    dtype=dtype,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n                _check_orient(\n                    df,\n                    \"records\",\n                    dtype=dtype,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n                _check_orient(\n                    df,\n                    \"split\",\n                    dtype=dtype,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n                _check_orient(\n                    df,\n                    \"index\",\n                    dtype=dtype,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n                _check_orient(\n                    df,\n                    \"values\",\n                    dtype=dtype,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n\n            _check_orient(df, \"columns\", dtype=dtype, convert_axes=False, sort=sort)\n            _check_orient(df, \"records\", dtype=dtype, convert_axes=False, sort=sort)\n            _check_orient(df, \"split\", dtype=dtype, convert_axes=False, sort=sort)\n            _check_orient(df, \"index\", dtype=dtype, convert_axes=False, sort=sort)\n            _check_orient(df, \"values\", dtype=dtype, convert_axes=False, sort=sort)\n\n            # numpy=True and raise_ok might be not None, so ignore the error\n            if convert_axes:\n                _check_orient(\n                    df,\n                    \"columns\",\n                    dtype=dtype,\n                    numpy=True,\n                    raise_ok=raise_ok,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n                _check_orient(\n                    df,\n                    \"records\",\n                    dtype=dtype,\n                    numpy=True,\n                    raise_ok=raise_ok,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n                _check_orient(\n                    df,\n                    \"split\",\n                    dtype=dtype,\n                    numpy=True,\n                    raise_ok=raise_ok,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n                _check_orient(\n                    df,\n                    \"index\",\n                    dtype=dtype,\n                    numpy=True,\n                    raise_ok=raise_ok,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n                _check_orient(\n                    df,\n                    \"values\",\n                    dtype=dtype,\n                    numpy=True,\n                    raise_ok=raise_ok,\n                    sort=sort,\n                    check_index_type=False,\n                    check_column_type=False,\n                )\n\n            _check_orient(\n                df,\n                \"columns\",\n                dtype=dtype,\n                numpy=True,\n                convert_axes=False,\n                raise_ok=raise_ok,\n                sort=sort,\n            )\n            _check_orient(\n                df,\n                \"records\",\n                dtype=dtype,\n                numpy=True,\n                convert_axes=False,\n                raise_ok=raise_ok,\n                sort=sort,\n            )\n            _check_orient(\n                df,\n                \"split\",\n                dtype=dtype,\n                numpy=True,\n                convert_axes=False,\n                raise_ok=raise_ok,\n                sort=sort,\n            )\n            _check_orient(\n                df,\n                \"index\",\n                dtype=dtype,\n                numpy=True,\n                convert_axes=False,\n                raise_ok=raise_ok,\n                sort=sort,\n            )\n            _check_orient(\n                df,\n                \"values\",\n                dtype=dtype,\n                numpy=True,\n                convert_axes=False,\n                raise_ok=raise_ok,\n                sort=sort,\n            )\n\n        # basic\n        _check_all_orients(self.frame)\n        assert self.frame.to_json() == self.frame.to_json(orient=\"columns\")\n\n        _check_all_orients(self.intframe, dtype=self.intframe.values.dtype)\n        _check_all_orients(self.intframe, dtype=False)\n\n        # big one\n        # index and columns are strings as all unserialised JSON object keys\n        # are assumed to be strings\n        biggie = DataFrame(\n            np.zeros((200, 4)),\n            columns=[str(i) for i in range(4)],\n            index=[str(i) for i in range(200)],\n        )\n        _check_all_orients(biggie, dtype=False, convert_axes=False)\n\n        # dtypes\n        _check_all_orients(\n            DataFrame(biggie, dtype=np.float64), dtype=np.float64, convert_axes=False\n        )\n        _check_all_orients(\n            DataFrame(biggie, dtype=np.int), dtype=np.int, convert_axes=False\n        )\n        _check_all_orients(\n            DataFrame(biggie, dtype=\"U3\"),\n            dtype=\"U3\",\n            convert_axes=False,\n            raise_ok=ValueError,\n        )\n\n        # categorical\n        _check_all_orients(self.categorical, sort=\"sort\", raise_ok=ValueError)\n\n        # empty\n        _check_all_orients(\n            self.empty_frame, check_index_type=False, check_column_type=False\n        )\n\n        # time series data\n        _check_all_orients(self.tsframe)\n\n        # mixed data\n        index = pd.Index([\"a\", \"b\", \"c\", \"d\", \"e\"])\n        data = {\n            \"A\": [0.0, 1.0, 2.0, 3.0, 4.0],\n            \"B\": [0.0, 1.0, 0.0, 1.0, 0.0],\n            \"C\": [\"foo1\", \"foo2\", \"foo3\", \"foo4\", \"foo5\"],\n            \"D\": [True, False, True, False, True],\n        }\n        df = DataFrame(data=data, index=index)\n        _check_orient(df, \"split\", check_dtype=False)\n        _check_orient(df, \"records\", check_dtype=False)\n        _check_orient(df, \"values\", check_dtype=False)\n        _check_orient(df, \"columns\", check_dtype=False)\n        # index oriented is problematic as it is read back in in a transposed\n        # state, so the columns are interpreted as having mixed data and\n        # given object dtypes.\n        # force everything to have object dtype beforehand\n        _check_orient(df.transpose().transpose(), \"index\", dtype=False)\n\n    @pytest.mark.parametrize(\n        \"data,msg,orient\",\n        [\n            ('{\"key\":b:a:d}', \"Expected object or value\", \"columns\"),\n            # too few indices\n            (\n                '{\"columns\":[\"A\",\"B\"],'\n                '\"index\":[\"2\",\"3\"],'\n                '\"data\":[[1.0,\"1\"],[2.0,\"2\"],[null,\"3\"]]}',\n                r\"Shape of passed values is \\(3, 2\\), indices imply \\(2, 2\\)\",\n                \"split\",\n            ),\n            # too many columns\n            (\n                '{\"columns\":[\"A\",\"B\",\"C\"],'\n                '\"index\":[\"1\",\"2\",\"3\"],'\n                '\"data\":[[1.0,\"1\"],[2.0,\"2\"],[null,\"3\"]]}',\n                \"3 columns passed, passed data had 2 columns\",\n                \"split\",\n            ),\n            # bad key\n            (\n                '{\"badkey\":[\"A\",\"B\"],'\n                '\"index\":[\"2\",\"3\"],'\n                '\"data\":[[1.0,\"1\"],[2.0,\"2\"],[null,\"3\"]]}',\n                r\"unexpected key\\(s\\): badkey\",\n                \"split\",\n            ),\n        ],\n    )\n    def test_frame_from_json_bad_data_raises(self, data, msg, orient):\n        with pytest.raises(ValueError, match=msg):\n            read_json(StringIO(data), orient=orient)\n\n    @pytest.mark.parametrize(\"dtype\", [True, False])\n    @pytest.mark.parametrize(\"convert_axes\", [True, False])\n    @pytest.mark.parametrize(\"numpy\", [True, False])\n    def test_frame_from_json_missing_data(self, orient, convert_axes, numpy, dtype):\n        num_df = DataFrame([[1, 2], [4, 5, 6]])\n        result = read_json(\n            num_df.to_json(orient=orient),\n            orient=orient,\n            convert_axes=convert_axes,\n            dtype=dtype,\n        )\n        assert np.isnan(result.iloc[0, 2])\n\n        obj_df = DataFrame([[\"1\", \"2\"], [\"4\", \"5\", \"6\"]])\n        result = read_json(\n            obj_df.to_json(orient=orient),\n            orient=orient,\n            convert_axes=convert_axes,\n            dtype=dtype,\n        )\n        if not dtype:  # TODO: Special case for object data; maybe a bug?\n            assert result.iloc[0, 2] is None\n        else:\n            assert np.isnan(result.iloc[0, 2])\n\n    @pytest.mark.parametrize(\"inf\", [np.inf, np.NINF])\n    @pytest.mark.parametrize(\"dtype\", [True, False])\n    def test_frame_infinity(self, orient, inf, dtype):\n        # infinities get mapped to nulls which get mapped to NaNs during\n        # deserialisation\n        df = DataFrame([[1, 2], [4, 5, 6]])\n        df.loc[0, 2] = inf\n        result = read_json(df.to_json(), dtype=dtype)\n        assert np.isnan(result.iloc[0, 2])\n\n    @pytest.mark.skipif(\n        is_platform_32bit(), reason=\"not compliant on 32-bit, xref #15865\"\n    )\n    @pytest.mark.parametrize(\n        \"value,precision,expected_val\",\n        [\n            (0.95, 1, 1.0),\n            (1.95, 1, 2.0),\n            (-1.95, 1, -2.0),\n            (0.995, 2, 1.0),\n            (0.9995, 3, 1.0),\n            (0.99999999999999944, 15, 1.0),\n        ],\n    )\n    def test_frame_to_json_float_precision(self, value, precision, expected_val):\n        df = pd.DataFrame([dict(a_float=value)])\n        encoded = df.to_json(double_precision=precision)\n        assert encoded == '{{\"a_float\":{{\"0\":{}}}}}'.format(expected_val)\n\n    def test_frame_to_json_except(self):\n        df = DataFrame([1, 2, 3])\n        msg = \"Invalid value 'garbage' for option 'orient'\"\n        with pytest.raises(ValueError, match=msg):\n            df.to_json(orient=\"garbage\")\n\n    def test_frame_empty(self):\n        df = DataFrame(columns=[\"jim\", \"joe\"])\n        assert not df._is_mixed_type\n        assert_frame_equal(\n            read_json(df.to_json(), dtype=dict(df.dtypes)), df, check_index_type=False\n        )\n        # GH 7445\n        result = pd.DataFrame({\"test\": []}, index=[]).to_json(orient=\"columns\")\n        expected = '{\"test\":{}}'\n        assert result == expected\n\n    def test_frame_empty_mixedtype(self):\n        # mixed type\n        df = DataFrame(columns=[\"jim\", \"joe\"])\n        df[\"joe\"] = df[\"joe\"].astype(\"i8\")\n        assert df._is_mixed_type\n        assert_frame_equal(\n            read_json(df.to_json(), dtype=dict(df.dtypes)), df, check_index_type=False\n        )\n\n    def test_frame_mixedtype_orient(self):  # GH10289\n        vals = [\n            [10, 1, \"foo\", 0.1, 0.01],\n            [20, 2, \"bar\", 0.2, 0.02],\n            [30, 3, \"baz\", 0.3, 0.03],\n            [40, 4, \"qux\", 0.4, 0.04],\n        ]\n\n        df = DataFrame(\n            vals, index=list(\"abcd\"), columns=[\"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\"]\n        )\n\n        assert df._is_mixed_type\n        right = df.copy()\n\n        for orient in [\"split\", \"index\", \"columns\"]:\n            inp = df.to_json(orient=orient)\n            left = read_json(inp, orient=orient, convert_axes=False)\n            assert_frame_equal(left, right)\n\n        right.index = np.arange(len(df))\n        inp = df.to_json(orient=\"records\")\n        left = read_json(inp, orient=\"records\", convert_axes=False)\n        assert_frame_equal(left, right)\n\n        right.columns = np.arange(df.shape[1])\n        inp = df.to_json(orient=\"values\")\n        left = read_json(inp, orient=\"values\", convert_axes=False)\n        assert_frame_equal(left, right)\n\n    def test_v12_compat(self):\n        df = DataFrame(\n            [\n                [1.56808523, 0.65727391, 1.81021139, -0.17251653],\n                [-0.2550111, -0.08072427, -0.03202878, -0.17581665],\n                [1.51493992, 0.11805825, 1.629455, -1.31506612],\n                [-0.02765498, 0.44679743, 0.33192641, -0.27885413],\n                [0.05951614, -2.69652057, 1.28163262, 0.34703478],\n            ],\n            columns=[\"A\", \"B\", \"C\", \"D\"],\n            index=pd.date_range(\"2000-01-03\", \"2000-01-07\"),\n        )\n        df[\"date\"] = pd.Timestamp(\"19920106 18:21:32.12\")\n        df.iloc[3, df.columns.get_loc(\"date\")] = pd.Timestamp(\"20130101\")\n        df[\"modified\"] = df[\"date\"]\n        df.iloc[1, df.columns.get_loc(\"modified\")] = pd.NaT\n\n        v12_json = os.path.join(self.dirpath, \"tsframe_v012.json\")\n        df_unser = pd.read_json(v12_json)\n        assert_frame_equal(df, df_unser)\n\n        df_iso = df.drop([\"modified\"], axis=1)\n        v12_iso_json = os.path.join(self.dirpath, \"tsframe_iso_v012.json\")\n        df_unser_iso = pd.read_json(v12_iso_json)\n        assert_frame_equal(df_iso, df_unser_iso)\n\n    def test_blocks_compat_GH9037(self):\n        index = pd.date_range(\"20000101\", periods=10, freq=\"H\")\n        df_mixed = DataFrame(\n            OrderedDict(\n                float_1=[\n                    -0.92077639,\n                    0.77434435,\n                    1.25234727,\n                    0.61485564,\n                    -0.60316077,\n                    0.24653374,\n                    0.28668979,\n                    -2.51969012,\n                    0.95748401,\n                    -1.02970536,\n                ],\n                int_1=[\n                    19680418,\n                    75337055,\n                    99973684,\n                    65103179,\n                    79373900,\n                    40314334,\n                    21290235,\n                    4991321,\n                    41903419,\n                    16008365,\n                ],\n                str_1=[\n                    \"78c608f1\",\n                    \"64a99743\",\n                    \"13d2ff52\",\n                    \"ca7f4af2\",\n                    \"97236474\",\n                    \"bde7e214\",\n                    \"1a6bde47\",\n                    \"b1190be5\",\n                    \"7a669144\",\n                    \"8d64d068\",\n                ],\n                float_2=[\n                    -0.0428278,\n                    -1.80872357,\n                    3.36042349,\n                    -0.7573685,\n                    -0.48217572,\n                    0.86229683,\n                    1.08935819,\n                    0.93898739,\n                    -0.03030452,\n                    1.43366348,\n                ],\n                str_2=[\n                    \"14f04af9\",\n                    \"d085da90\",\n                    \"4bcfac83\",\n                    \"81504caf\",\n                    \"2ffef4a9\",\n                    \"08e2f5c4\",\n                    \"07e1af03\",\n                    \"addbd4a7\",\n                    \"1f6a09ba\",\n                    \"4bfc4d87\",\n                ],\n                int_2=[\n                    86967717,\n                    98098830,\n                    51927505,\n                    20372254,\n                    12601730,\n                    20884027,\n                    34193846,\n                    10561746,\n                    24867120,\n                    76131025,\n                ],\n            ),\n            index=index,\n        )\n\n        # JSON deserialisation always creates unicode strings\n        df_mixed.columns = df_mixed.columns.astype(\"unicode\")\n\n        df_roundtrip = pd.read_json(df_mixed.to_json(orient=\"split\"), orient=\"split\")\n        assert_frame_equal(\n            df_mixed,\n            df_roundtrip,\n            check_index_type=True,\n            check_column_type=True,\n            check_frame_type=True,\n            by_blocks=True,\n            check_exact=True,\n        )\n\n    def test_frame_nonprintable_bytes(self):\n        # GH14256: failing column caused segfaults, if it is not the last one\n\n        class BinaryThing:\n            def __init__(self, hexed):\n                self.hexed = hexed\n                self.binary = bytes.fromhex(hexed)\n\n            def __str__(self):\n                return self.hexed\n\n        hexed = \"574b4454ba8c5eb4f98a8f45\"\n        binthing = BinaryThing(hexed)\n\n        # verify the proper conversion of printable content\n        df_printable = DataFrame({\"A\": [binthing.hexed]})\n        assert df_printable.to_json() == '{{\"A\":{{\"0\":\"{hex}\"}}}}'.format(hex=hexed)\n\n        # check if non-printable content throws appropriate Exception\n        df_nonprintable = DataFrame({\"A\": [binthing]})\n        msg = \"Unsupported UTF-8 sequence length when encoding string\"\n        with pytest.raises(OverflowError, match=msg):\n            df_nonprintable.to_json()\n\n        # the same with multiple columns threw segfaults\n        df_mixed = DataFrame({\"A\": [binthing], \"B\": [1]}, columns=[\"A\", \"B\"])\n        with pytest.raises(OverflowError):\n            df_mixed.to_json()\n\n        # default_handler should resolve exceptions for non-string types\n        assert df_nonprintable.to_json(\n            default_handler=str\n        ) == '{{\"A\":{{\"0\":\"{hex}\"}}}}'.format(hex=hexed)\n        assert df_mixed.to_json(\n            default_handler=str\n        ) == '{{\"A\":{{\"0\":\"{hex}\"}},\"B\":{{\"0\":1}}}}'.format(hex=hexed)\n\n    def test_label_overflow(self):\n        # GH14256: buffer length not checked when writing label\n        df = pd.DataFrame({\"bar\" * 100000: [1], \"foo\": [1337]})\n        assert df.to_json() == '{{\"{bar}\":{{\"0\":1}},\"foo\":{{\"0\":1337}}}}'.format(\n            bar=(\"bar\" * 100000)\n        )\n\n    def test_series_non_unique_index(self):\n        s = Series([\"a\", \"b\"], index=[1, 1])\n\n        msg = \"Series index must be unique for orient='index'\"\n        with pytest.raises(ValueError, match=msg):\n            s.to_json(orient=\"index\")\n\n        assert_series_equal(\n            s, read_json(s.to_json(orient=\"split\"), orient=\"split\", typ=\"series\")\n        )\n        unser = read_json(s.to_json(orient=\"records\"), orient=\"records\", typ=\"series\")\n        tm.assert_numpy_array_equal(s.values, unser.values)\n\n    def test_series_default_orient(self):\n        assert self.series.to_json() == self.series.to_json(orient=\"index\")\n\n    @pytest.mark.parametrize(\"numpy\", [True, False])\n    def test_series_roundtrip_simple(self, orient, numpy):\n        data = self.series.to_json(orient=orient)\n        result = pd.read_json(data, typ=\"series\", orient=orient, numpy=numpy)\n        expected = self.series.copy()\n\n        if not numpy and PY35 and orient in (\"index\", \"columns\"):\n            expected = expected.sort_index()\n        if orient in (\"values\", \"records\"):\n            expected = expected.reset_index(drop=True)\n        if orient != \"split\":\n            expected.name = None\n\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"dtype\", [False, None])\n    @pytest.mark.parametrize(\"numpy\", [True, False])\n    def test_series_roundtrip_object(self, orient, numpy, dtype):\n        # TODO: see why tm.makeObjectSeries provides back DTA\n        dtSeries = Series(\n            [str(d) for d in self.objSeries],\n            index=self.objSeries.index,\n            name=self.objSeries.name,\n        )\n        data = dtSeries.to_json(orient=orient)\n        result = pd.read_json(\n            data, typ=\"series\", orient=orient, numpy=numpy, dtype=dtype\n        )\n        if dtype is False:\n            expected = dtSeries.copy()\n        else:\n            expected = self.objSeries.copy()\n\n        if not numpy and PY35 and orient in (\"index\", \"columns\"):\n            expected = expected.sort_index()\n        if orient in (\"values\", \"records\"):\n            expected = expected.reset_index(drop=True)\n        if orient != \"split\":\n            expected.name = None\n\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"numpy\", [True, False])\n    def test_series_roundtrip_empty(self, orient, numpy):\n        data = self.empty_series.to_json(orient=orient)\n        result = pd.read_json(data, typ=\"series\", orient=orient, numpy=numpy)\n        expected = self.empty_series.copy()\n\n        # TODO: see what causes inconsistency\n        if not numpy and PY35 and orient == \"index\":\n            expected = expected.sort_index()\n        if orient in (\"values\", \"records\"):\n            expected = expected.reset_index(drop=True)\n        else:\n            expected.index = expected.index.astype(float)\n\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"numpy\", [True, False])\n    def test_series_roundtrip_timeseries(self, orient, numpy):\n        data = self.ts.to_json(orient=orient)\n        result = pd.read_json(data, typ=\"series\", orient=orient, numpy=numpy)\n        expected = self.ts.copy()\n\n        if orient in (\"values\", \"records\"):\n            expected = expected.reset_index(drop=True)\n        if orient != \"split\":\n            expected.name = None\n\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"dtype\", [np.float64, np.int])\n    @pytest.mark.parametrize(\"numpy\", [True, False])\n    def test_series_roundtrip_numeric(self, orient, numpy, dtype):\n        s = Series(range(6), index=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\n        data = s.to_json(orient=orient)\n        result = pd.read_json(data, typ=\"series\", orient=orient, numpy=numpy)\n\n        expected = s.copy()\n        if orient in (\"values\", \"records\"):\n            expected = expected.reset_index(drop=True)\n\n        tm.assert_series_equal(result, expected)\n\n    def test_series_to_json_except(self):\n        s = Series([1, 2, 3])\n        msg = \"Invalid value 'garbage' for option 'orient'\"\n        with pytest.raises(ValueError, match=msg):\n            s.to_json(orient=\"garbage\")\n\n    def test_series_from_json_precise_float(self):\n        s = Series([4.56, 4.56, 4.56])\n        result = read_json(s.to_json(), typ=\"series\", precise_float=True)\n        assert_series_equal(result, s, check_index_type=False)\n\n    def test_series_with_dtype(self):\n        # GH 21986\n        s = Series([4.56, 4.56, 4.56])\n        result = read_json(s.to_json(), typ=\"series\", dtype=np.int64)\n        expected = Series([4] * 3)\n        assert_series_equal(result, expected)\n\n    def test_frame_from_json_precise_float(self):\n        df = DataFrame([[4.56, 4.56, 4.56], [4.56, 4.56, 4.56]])\n        result = read_json(df.to_json(), precise_float=True)\n        assert_frame_equal(result, df, check_index_type=False, check_column_type=False)\n\n    def test_typ(self):\n\n        s = Series(range(6), index=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], dtype=\"int64\")\n        result = read_json(s.to_json(), typ=None)\n        assert_series_equal(result, s)\n\n    def test_reconstruction_index(self):\n\n        df = DataFrame([[1, 2, 3], [4, 5, 6]])\n        result = read_json(df.to_json())\n\n        assert_frame_equal(result, df)\n\n        df = DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}, index=[\"A\", \"B\", \"C\"])\n        result = read_json(df.to_json())\n        assert_frame_equal(result, df)\n\n    def test_path(self):\n        with ensure_clean(\"test.json\") as path:\n            for df in [\n                self.frame,\n                self.frame2,\n                self.intframe,\n                self.tsframe,\n                self.mixed_frame,\n            ]:\n                df.to_json(path)\n                read_json(path)\n\n    def test_axis_dates(self):\n\n        # frame\n        json = self.tsframe.to_json()\n        result = read_json(json)\n        assert_frame_equal(result, self.tsframe)\n\n        # series\n        json = self.ts.to_json()\n        result = read_json(json, typ=\"series\")\n        assert_series_equal(result, self.ts, check_names=False)\n        assert result.name is None\n\n    def test_convert_dates(self):\n\n        # frame\n        df = self.tsframe.copy()\n        df[\"date\"] = Timestamp(\"20130101\")\n\n        json = df.to_json()\n        result = read_json(json)\n        assert_frame_equal(result, df)\n\n        df[\"foo\"] = 1.0\n        json = df.to_json(date_unit=\"ns\")\n\n        result = read_json(json, convert_dates=False)\n        expected = df.copy()\n        expected[\"date\"] = expected[\"date\"].values.view(\"i8\")\n        expected[\"foo\"] = expected[\"foo\"].astype(\"int64\")\n        assert_frame_equal(result, expected)\n\n        # series\n        ts = Series(Timestamp(\"20130101\"), index=self.ts.index)\n        json = ts.to_json()\n        result = read_json(json, typ=\"series\")\n        assert_series_equal(result, ts)\n\n    @pytest.mark.parametrize(\n        \"infer_word\",\n        [\n            \"trade_time\",\n            \"date\",\n            \"datetime\",\n            \"sold_at\",\n            \"modified\",\n            \"timestamp\",\n            \"timestamps\",\n        ],\n    )\n    def test_convert_dates_infer(self, infer_word):\n        # GH10747\n        from pandas.io.json import dumps\n\n        data = [{\"id\": 1, infer_word: 1036713600000}, {\"id\": 2}]\n        expected = DataFrame(\n            [[1, Timestamp(\"2002-11-08\")], [2, pd.NaT]], columns=[\"id\", infer_word]\n        )\n        result = read_json(dumps(data))[[\"id\", infer_word]]\n        assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"date,date_unit\",\n        [\n            (\"20130101 20:43:42.123\", None),\n            (\"20130101 20:43:42\", \"s\"),\n            (\"20130101 20:43:42.123\", \"ms\"),\n            (\"20130101 20:43:42.123456\", \"us\"),\n            (\"20130101 20:43:42.123456789\", \"ns\"),\n        ],\n    )\n    def test_date_format_frame(self, date, date_unit):\n        df = self.tsframe.copy()\n\n        df[\"date\"] = Timestamp(date)\n        df.iloc[1, df.columns.get_loc(\"date\")] = pd.NaT\n        df.iloc[5, df.columns.get_loc(\"date\")] = pd.NaT\n        if date_unit:\n            json = df.to_json(date_format=\"iso\", date_unit=date_unit)\n        else:\n            json = df.to_json(date_format=\"iso\")\n        result = read_json(json)\n        expected = df.copy()\n        # expected.index = expected.index.tz_localize(\"UTC\")\n        expected[\"date\"] = expected[\"date\"].dt.tz_localize(\"UTC\")\n        assert_frame_equal(result, expected)\n\n    def test_date_format_frame_raises(self):\n        df = self.tsframe.copy()\n        msg = \"Invalid value 'foo' for option 'date_unit'\"\n        with pytest.raises(ValueError, match=msg):\n            df.to_json(date_format=\"iso\", date_unit=\"foo\")\n\n    @pytest.mark.parametrize(\n        \"date,date_unit\",\n        [\n            (\"20130101 20:43:42.123\", None),\n            (\"20130101 20:43:42\", \"s\"),\n            (\"20130101 20:43:42.123\", \"ms\"),\n            (\"20130101 20:43:42.123456\", \"us\"),\n            (\"20130101 20:43:42.123456789\", \"ns\"),\n        ],\n    )\n    def test_date_format_series(self, date, date_unit):\n        ts = Series(Timestamp(date), index=self.ts.index)\n        ts.iloc[1] = pd.NaT\n        ts.iloc[5] = pd.NaT\n        if date_unit:\n            json = ts.to_json(date_format=\"iso\", date_unit=date_unit)\n        else:\n            json = ts.to_json(date_format=\"iso\")\n        result = read_json(json, typ=\"series\")\n        expected = ts.copy()\n        # expected.index = expected.index.tz_localize(\"UTC\")\n        expected = expected.dt.tz_localize(\"UTC\")\n        assert_series_equal(result, expected)\n\n    def test_date_format_series_raises(self):\n        ts = Series(Timestamp(\"20130101 20:43:42.123\"), index=self.ts.index)\n        msg = \"Invalid value 'foo' for option 'date_unit'\"\n        with pytest.raises(ValueError, match=msg):\n            ts.to_json(date_format=\"iso\", date_unit=\"foo\")\n\n    @pytest.mark.parametrize(\"unit\", [\"s\", \"ms\", \"us\", \"ns\"])\n    def test_date_unit(self, unit):\n        df = self.tsframe.copy()\n        df[\"date\"] = Timestamp(\"20130101 20:43:42\")\n        dl = df.columns.get_loc(\"date\")\n        df.iloc[1, dl] = Timestamp(\"19710101 20:43:42\")\n        df.iloc[2, dl] = Timestamp(\"21460101 20:43:42\")\n        df.iloc[4, dl] = pd.NaT\n\n        json = df.to_json(date_format=\"epoch\", date_unit=unit)\n\n        # force date unit\n        result = read_json(json, date_unit=unit)\n        assert_frame_equal(result, df)\n\n        # detect date unit\n        result = read_json(json, date_unit=None)\n        assert_frame_equal(result, df)\n\n    def test_weird_nested_json(self):\n        # this used to core dump the parser\n        s = r\"\"\"{\n        \"status\": \"success\",\n        \"data\": {\n        \"posts\": [\n            {\n            \"id\": 1,\n            \"title\": \"A blog post\",\n            \"body\": \"Some useful content\"\n            },\n            {\n            \"id\": 2,\n            \"title\": \"Another blog post\",\n            \"body\": \"More content\"\n            }\n           ]\n          }\n        }\"\"\"\n\n        read_json(s)\n\n    def test_doc_example(self):\n        dfj2 = DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n        dfj2[\"date\"] = Timestamp(\"20130101\")\n        dfj2[\"ints\"] = range(5)\n        dfj2[\"bools\"] = True\n        dfj2.index = pd.date_range(\"20130101\", periods=5)\n\n        json = dfj2.to_json()\n        result = read_json(json, dtype={\"ints\": np.int64, \"bools\": np.bool_})\n        assert_frame_equal(result, result)\n\n    def test_misc_example(self):\n\n        # parsing unordered input fails\n        result = read_json('[{\"a\": 1, \"b\": 2}, {\"b\":2, \"a\" :1}]', numpy=True)\n        expected = DataFrame([[1, 2], [1, 2]], columns=[\"a\", \"b\"])\n\n        error_msg = \"\"\"DataFrame\\\\.index are different\n\nDataFrame\\\\.index values are different \\\\(100\\\\.0 %\\\\)\n\\\\[left\\\\]:  Index\\\\(\\\\['a', 'b'\\\\], dtype='object'\\\\)\n\\\\[right\\\\]: RangeIndex\\\\(start=0, stop=2, step=1\\\\)\"\"\"\n        with pytest.raises(AssertionError, match=error_msg):\n            assert_frame_equal(result, expected, check_index_type=False)\n\n        result = read_json('[{\"a\": 1, \"b\": 2}, {\"b\":2, \"a\" :1}]')\n        expected = DataFrame([[1, 2], [1, 2]], columns=[\"a\", \"b\"])\n        assert_frame_equal(result, expected)\n\n    @network\n    @pytest.mark.single\n    def test_round_trip_exception_(self):\n        # GH 3867\n        csv = \"https://raw.github.com/hayd/lahman2012/master/csvs/Teams.csv\"\n        df = pd.read_csv(csv)\n        s = df.to_json()\n        result = pd.read_json(s)\n        assert_frame_equal(result.reindex(index=df.index, columns=df.columns), df)\n\n    @network\n    @pytest.mark.single\n    @pytest.mark.parametrize(\n        \"field,dtype\",\n        [\n            [\"created_at\", pd.DatetimeTZDtype(tz=\"UTC\")],\n            [\"closed_at\", \"datetime64[ns]\"],\n            [\"updated_at\", pd.DatetimeTZDtype(tz=\"UTC\")],\n        ],\n    )\n    def test_url(self, field, dtype):\n        url = \"https://api.github.com/repos/pandas-dev/pandas/issues?per_page=5\"  # noqa\n        result = read_json(url, convert_dates=True)\n        assert result[field].dtype == dtype\n\n    def test_timedelta(self):\n        converter = lambda x: pd.to_timedelta(x, unit=\"ms\")\n\n        s = Series([timedelta(23), timedelta(seconds=5)])\n        assert s.dtype == \"timedelta64[ns]\"\n\n        result = pd.read_json(s.to_json(), typ=\"series\").apply(converter)\n        assert_series_equal(result, s)\n\n        s = Series([timedelta(23), timedelta(seconds=5)], index=pd.Index([0, 1]))\n        assert s.dtype == \"timedelta64[ns]\"\n        result = pd.read_json(s.to_json(), typ=\"series\").apply(converter)\n        assert_series_equal(result, s)\n\n        frame = DataFrame([timedelta(23), timedelta(seconds=5)])\n        assert frame[0].dtype == \"timedelta64[ns]\"\n        assert_frame_equal(frame, pd.read_json(frame.to_json()).apply(converter))\n\n        frame = DataFrame(\n            {\n                \"a\": [timedelta(days=23), timedelta(seconds=5)],\n                \"b\": [1, 2],\n                \"c\": pd.date_range(start=\"20130101\", periods=2),\n            }\n        )\n\n        result = pd.read_json(frame.to_json(date_unit=\"ns\"))\n        result[\"a\"] = pd.to_timedelta(result.a, unit=\"ns\")\n        result[\"c\"] = pd.to_datetime(result.c)\n        assert_frame_equal(frame, result)\n\n    def test_mixed_timedelta_datetime(self):\n        frame = DataFrame(\n            {\"a\": [timedelta(23), pd.Timestamp(\"20130101\")]}, dtype=object\n        )\n\n        expected = DataFrame(\n            {\"a\": [pd.Timedelta(frame.a[0]).value, pd.Timestamp(frame.a[1]).value]}\n        )\n        result = pd.read_json(frame.to_json(date_unit=\"ns\"), dtype={\"a\": \"int64\"})\n        assert_frame_equal(result, expected, check_index_type=False)\n\n    def test_default_handler(self):\n        value = object()\n        frame = DataFrame({\"a\": [7, value]})\n        expected = DataFrame({\"a\": [7, str(value)]})\n        result = pd.read_json(frame.to_json(default_handler=str))\n        assert_frame_equal(expected, result, check_index_type=False)\n\n    def test_default_handler_indirect(self):\n        from pandas.io.json import dumps\n\n        def default(obj):\n            if isinstance(obj, complex):\n                return [(\"mathjs\", \"Complex\"), (\"re\", obj.real), (\"im\", obj.imag)]\n            return str(obj)\n\n        df_list = [\n            9,\n            DataFrame(\n                {\"a\": [1, \"STR\", complex(4, -5)], \"b\": [float(\"nan\"), None, \"N/A\"]},\n                columns=[\"a\", \"b\"],\n            ),\n        ]\n        expected = (\n            '[9,[[1,null],[\"STR\",null],[[[\"mathjs\",\"Complex\"],'\n            '[\"re\",4.0],[\"im\",-5.0]],\"N\\\\/A\"]]]'\n        )\n        assert dumps(df_list, default_handler=default, orient=\"values\") == expected\n\n    def test_default_handler_numpy_unsupported_dtype(self):\n        # GH12554 to_json raises 'Unhandled numpy dtype 15'\n        df = DataFrame(\n            {\"a\": [1, 2.3, complex(4, -5)], \"b\": [float(\"nan\"), None, complex(1.2, 0)]},\n            columns=[\"a\", \"b\"],\n        )\n        expected = (\n            '[[\"(1+0j)\",\"(nan+0j)\"],'\n            '[\"(2.3+0j)\",\"(nan+0j)\"],'\n            '[\"(4-5j)\",\"(1.2+0j)\"]]'\n        )\n        assert df.to_json(default_handler=str, orient=\"values\") == expected\n\n    def test_default_handler_raises(self):\n        msg = \"raisin\"\n\n        def my_handler_raises(obj):\n            raise TypeError(msg)\n\n        with pytest.raises(TypeError, match=msg):\n            DataFrame({\"a\": [1, 2, object()]}).to_json(\n                default_handler=my_handler_raises\n            )\n        with pytest.raises(TypeError, match=msg):\n            DataFrame({\"a\": [1, 2, complex(4, -5)]}).to_json(\n                default_handler=my_handler_raises\n            )\n\n    def test_categorical(self):\n        # GH4377 df.to_json segfaults with non-ndarray blocks\n        df = DataFrame({\"A\": [\"a\", \"b\", \"c\", \"a\", \"b\", \"b\", \"a\"]})\n        df[\"B\"] = df[\"A\"]\n        expected = df.to_json()\n\n        df[\"B\"] = df[\"A\"].astype(\"category\")\n        assert expected == df.to_json()\n\n        s = df[\"A\"]\n        sc = df[\"B\"]\n        assert s.to_json() == sc.to_json()\n\n    def test_datetime_tz(self):\n        # GH4377 df.to_json segfaults with non-ndarray blocks\n        tz_range = pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\")\n        tz_naive = tz_range.tz_convert(\"utc\").tz_localize(None)\n\n        df = DataFrame({\"A\": tz_range, \"B\": pd.date_range(\"20130101\", periods=3)})\n\n        df_naive = df.copy()\n        df_naive[\"A\"] = tz_naive\n        expected = df_naive.to_json()\n        assert expected == df.to_json()\n\n        stz = Series(tz_range)\n        s_naive = Series(tz_naive)\n        assert stz.to_json() == s_naive.to_json()\n\n    @pytest.mark.filterwarnings(\"ignore:Sparse:FutureWarning\")\n    @pytest.mark.filterwarnings(\"ignore:DataFrame.to_sparse:FutureWarning\")\n    @pytest.mark.filterwarnings(\"ignore:Series.to_sparse:FutureWarning\")\n    def test_sparse(self):\n        # GH4377 df.to_json segfaults with non-ndarray blocks\n        df = pd.DataFrame(np.random.randn(10, 4))\n        df.loc[:8] = np.nan\n\n        sdf = df.to_sparse()\n        expected = df.to_json()\n        assert expected == sdf.to_json()\n\n        s = pd.Series(np.random.randn(10))\n        s.loc[:8] = np.nan\n        ss = s.to_sparse()\n\n        expected = s.to_json()\n        assert expected == ss.to_json()\n\n    @pytest.mark.parametrize(\n        \"ts\",\n        [\n            Timestamp(\"2013-01-10 05:00:00Z\"),\n            Timestamp(\"2013-01-10 00:00:00\", tz=\"US/Eastern\"),\n            Timestamp(\"2013-01-10 00:00:00-0500\"),\n        ],\n    )\n    def test_tz_is_utc(self, ts):\n        from pandas.io.json import dumps\n\n        exp = '\"2013-01-10T05:00:00.000Z\"'\n\n        assert dumps(ts, iso_dates=True) == exp\n        dt = ts.to_pydatetime()\n        assert dumps(dt, iso_dates=True) == exp\n\n    @pytest.mark.parametrize(\n        \"tz_range\",\n        [\n            pd.date_range(\"2013-01-01 05:00:00Z\", periods=2),\n            pd.date_range(\"2013-01-01 00:00:00\", periods=2, tz=\"US/Eastern\"),\n            pd.date_range(\"2013-01-01 00:00:00-0500\", periods=2),\n        ],\n    )\n    def test_tz_range_is_utc(self, tz_range):\n        from pandas.io.json import dumps\n\n        exp = '[\"2013-01-01T05:00:00.000Z\",\"2013-01-02T05:00:00.000Z\"]'\n        dfexp = (\n            '{\"DT\":{'\n            '\"0\":\"2013-01-01T05:00:00.000Z\",'\n            '\"1\":\"2013-01-02T05:00:00.000Z\"}}'\n        )\n\n        assert dumps(tz_range, iso_dates=True) == exp\n        dti = pd.DatetimeIndex(tz_range)\n        assert dumps(dti, iso_dates=True) == exp\n        df = DataFrame({\"DT\": dti})\n        result = dumps(df, iso_dates=True)\n        assert result == dfexp\n\n    def test_read_inline_jsonl(self):\n        # GH9180\n        result = read_json('{\"a\": 1, \"b\": 2}\\n{\"b\":2, \"a\" :1}\\n', lines=True)\n        expected = DataFrame([[1, 2], [1, 2]], columns=[\"a\", \"b\"])\n        assert_frame_equal(result, expected)\n\n    @td.skip_if_not_us_locale\n    def test_read_s3_jsonl(self, s3_resource):\n        # GH17200\n\n        result = read_json(\"s3n://pandas-test/items.jsonl\", lines=True)\n        expected = DataFrame([[1, 2], [1, 2]], columns=[\"a\", \"b\"])\n        assert_frame_equal(result, expected)\n\n    def test_read_local_jsonl(self):\n        # GH17200\n        with ensure_clean(\"tmp_items.json\") as path:\n            with open(path, \"w\") as infile:\n                infile.write('{\"a\": 1, \"b\": 2}\\n{\"b\":2, \"a\" :1}\\n')\n            result = read_json(path, lines=True)\n            expected = DataFrame([[1, 2], [1, 2]], columns=[\"a\", \"b\"])\n            assert_frame_equal(result, expected)\n\n    def test_read_jsonl_unicode_chars(self):\n        # GH15132: non-ascii unicode characters\n        # \\u201d == RIGHT DOUBLE QUOTATION MARK\n\n        # simulate file handle\n        json = '{\"a\": \"foo”\", \"b\": \"bar\"}\\n{\"a\": \"foo\", \"b\": \"bar\"}\\n'\n        json = StringIO(json)\n        result = read_json(json, lines=True)\n        expected = DataFrame([[\"foo\\u201d\", \"bar\"], [\"foo\", \"bar\"]], columns=[\"a\", \"b\"])\n        assert_frame_equal(result, expected)\n\n        # simulate string\n        json = '{\"a\": \"foo”\", \"b\": \"bar\"}\\n{\"a\": \"foo\", \"b\": \"bar\"}\\n'\n        result = read_json(json, lines=True)\n        expected = DataFrame([[\"foo\\u201d\", \"bar\"], [\"foo\", \"bar\"]], columns=[\"a\", \"b\"])\n        assert_frame_equal(result, expected)\n\n    def test_read_json_large_numbers(self):\n        # GH18842\n        json = '{\"articleId\": \"1404366058080022500245\"}'\n        json = StringIO(json)\n        result = read_json(json, typ=\"series\")\n        expected = Series(1.404366e21, index=[\"articleId\"])\n        assert_series_equal(result, expected)\n\n        json = '{\"0\": {\"articleId\": \"1404366058080022500245\"}}'\n        json = StringIO(json)\n        result = read_json(json)\n        expected = DataFrame(1.404366e21, index=[\"articleId\"], columns=[0])\n        assert_frame_equal(result, expected)\n\n    def test_to_jsonl(self):\n        # GH9180\n        df = DataFrame([[1, 2], [1, 2]], columns=[\"a\", \"b\"])\n        result = df.to_json(orient=\"records\", lines=True)\n        expected = '{\"a\":1,\"b\":2}\\n{\"a\":1,\"b\":2}'\n        assert result == expected\n\n        df = DataFrame([[\"foo}\", \"bar\"], ['foo\"', \"bar\"]], columns=[\"a\", \"b\"])\n        result = df.to_json(orient=\"records\", lines=True)\n        expected = '{\"a\":\"foo}\",\"b\":\"bar\"}\\n{\"a\":\"foo\\\\\"\",\"b\":\"bar\"}'\n        assert result == expected\n        assert_frame_equal(pd.read_json(result, lines=True), df)\n\n        # GH15096: escaped characters in columns and data\n        df = DataFrame([[\"foo\\\\\", \"bar\"], ['foo\"', \"bar\"]], columns=[\"a\\\\\", \"b\"])\n        result = df.to_json(orient=\"records\", lines=True)\n        expected = '{\"a\\\\\\\\\":\"foo\\\\\\\\\",\"b\":\"bar\"}\\n' '{\"a\\\\\\\\\":\"foo\\\\\"\",\"b\":\"bar\"}'\n        assert result == expected\n        assert_frame_equal(pd.read_json(result, lines=True), df)\n\n    # TODO: there is a near-identical test for pytables; can we share?\n    def test_latin_encoding(self):\n        # GH 13774\n        pytest.skip(\"encoding not implemented in .to_json(), xref #13774\")\n\n        values = [\n            [b\"E\\xc9, 17\", b\"\", b\"a\", b\"b\", b\"c\"],\n            [b\"E\\xc9, 17\", b\"a\", b\"b\", b\"c\"],\n            [b\"EE, 17\", b\"\", b\"a\", b\"b\", b\"c\"],\n            [b\"E\\xc9, 17\", b\"\\xf8\\xfc\", b\"a\", b\"b\", b\"c\"],\n            [b\"\", b\"a\", b\"b\", b\"c\"],\n            [b\"\\xf8\\xfc\", b\"a\", b\"b\", b\"c\"],\n            [b\"A\\xf8\\xfc\", b\"\", b\"a\", b\"b\", b\"c\"],\n            [np.nan, b\"\", b\"b\", b\"c\"],\n            [b\"A\\xf8\\xfc\", np.nan, b\"\", b\"b\", b\"c\"],\n        ]\n\n        values = [\n            [x.decode(\"latin-1\") if isinstance(x, bytes) else x for x in y]\n            for y in values\n        ]\n\n        examples = []\n        for dtype in [\"category\", object]:\n            for val in values:\n                examples.append(Series(val, dtype=dtype))\n\n        def roundtrip(s, encoding=\"latin-1\"):\n            with ensure_clean(\"test.json\") as path:\n                s.to_json(path, encoding=encoding)\n                retr = read_json(path, encoding=encoding)\n                assert_series_equal(s, retr, check_categorical=False)\n\n        for s in examples:\n            roundtrip(s)\n\n    def test_data_frame_size_after_to_json(self):\n        # GH15344\n        df = DataFrame({\"a\": [str(1)]})\n\n        size_before = df.memory_usage(index=True, deep=True).sum()\n        df.to_json()\n        size_after = df.memory_usage(index=True, deep=True).sum()\n\n        assert size_before == size_after\n\n    @pytest.mark.parametrize(\n        \"index\", [None, [1, 2], [1.0, 2.0], [\"a\", \"b\"], [\"1\", \"2\"], [\"1.\", \"2.\"]]\n    )\n    @pytest.mark.parametrize(\"columns\", [[\"a\", \"b\"], [\"1\", \"2\"], [\"1.\", \"2.\"]])\n    def test_from_json_to_json_table_index_and_columns(self, index, columns):\n        # GH25433 GH25435\n        expected = DataFrame([[1, 2], [3, 4]], index=index, columns=columns)\n        dfjson = expected.to_json(orient=\"table\")\n        result = pd.read_json(dfjson, orient=\"table\")\n        assert_frame_equal(result, expected)\n\n    def test_from_json_to_json_table_dtypes(self):\n        # GH21345\n        expected = pd.DataFrame({\"a\": [1, 2], \"b\": [3.0, 4.0], \"c\": [\"5\", \"6\"]})\n        dfjson = expected.to_json(orient=\"table\")\n        result = pd.read_json(dfjson, orient=\"table\")\n        assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\"dtype\", [True, {\"b\": int, \"c\": int}])\n    def test_read_json_table_dtype_raises(self, dtype):\n        # GH21345\n        df = pd.DataFrame({\"a\": [1, 2], \"b\": [3.0, 4.0], \"c\": [\"5\", \"6\"]})\n        dfjson = df.to_json(orient=\"table\")\n        msg = \"cannot pass both dtype and orient='table'\"\n        with pytest.raises(ValueError, match=msg):\n            pd.read_json(dfjson, orient=\"table\", dtype=dtype)\n\n    def test_read_json_table_convert_axes_raises(self):\n        # GH25433 GH25435\n        df = DataFrame([[1, 2], [3, 4]], index=[1.0, 2.0], columns=[\"1.\", \"2.\"])\n        dfjson = df.to_json(orient=\"table\")\n        msg = \"cannot pass both convert_axes and orient='table'\"\n        with pytest.raises(ValueError, match=msg):\n            pd.read_json(dfjson, orient=\"table\", convert_axes=True)\n\n    @pytest.mark.parametrize(\n        \"data, expected\",\n        [\n            (\n                DataFrame([[1, 2], [4, 5]], columns=[\"a\", \"b\"]),\n                {\"columns\": [\"a\", \"b\"], \"data\": [[1, 2], [4, 5]]},\n            ),\n            (\n                DataFrame([[1, 2], [4, 5]], columns=[\"a\", \"b\"]).rename_axis(\"foo\"),\n                {\"columns\": [\"a\", \"b\"], \"data\": [[1, 2], [4, 5]]},\n            ),\n            (\n                DataFrame(\n                    [[1, 2], [4, 5]], columns=[\"a\", \"b\"], index=[[\"a\", \"b\"], [\"c\", \"d\"]]\n                ),\n                {\"columns\": [\"a\", \"b\"], \"data\": [[1, 2], [4, 5]]},\n            ),\n            (Series([1, 2, 3], name=\"A\"), {\"name\": \"A\", \"data\": [1, 2, 3]}),\n            (\n                Series([1, 2, 3], name=\"A\").rename_axis(\"foo\"),\n                {\"name\": \"A\", \"data\": [1, 2, 3]},\n            ),\n            (\n                Series([1, 2], name=\"A\", index=[[\"a\", \"b\"], [\"c\", \"d\"]]),\n                {\"name\": \"A\", \"data\": [1, 2]},\n            ),\n        ],\n    )\n    def test_index_false_to_json_split(self, data, expected):\n        # GH 17394\n        # Testing index=False in to_json with orient='split'\n\n        result = data.to_json(orient=\"split\", index=False)\n        result = json.loads(result)\n\n        assert result == expected\n\n    @pytest.mark.parametrize(\n        \"data\",\n        [\n            (DataFrame([[1, 2], [4, 5]], columns=[\"a\", \"b\"])),\n            (DataFrame([[1, 2], [4, 5]], columns=[\"a\", \"b\"]).rename_axis(\"foo\")),\n            (\n                DataFrame(\n                    [[1, 2], [4, 5]], columns=[\"a\", \"b\"], index=[[\"a\", \"b\"], [\"c\", \"d\"]]\n                )\n            ),\n            (Series([1, 2, 3], name=\"A\")),\n            (Series([1, 2, 3], name=\"A\").rename_axis(\"foo\")),\n            (Series([1, 2], name=\"A\", index=[[\"a\", \"b\"], [\"c\", \"d\"]])),\n        ],\n    )\n    def test_index_false_to_json_table(self, data):\n        # GH 17394\n        # Testing index=False in to_json with orient='table'\n\n        result = data.to_json(orient=\"table\", index=False)\n        result = json.loads(result)\n\n        expected = {\n            \"schema\": pd.io.json.build_table_schema(data, index=False),\n            \"data\": DataFrame(data).to_dict(orient=\"records\"),\n        }\n\n        assert result == expected\n\n    @pytest.mark.parametrize(\"orient\", [\"records\", \"index\", \"columns\", \"values\"])\n    def test_index_false_error_to_json(self, orient):\n        # GH 17394\n        # Testing error message from to_json with index=False\n\n        df = pd.DataFrame([[1, 2], [4, 5]], columns=[\"a\", \"b\"])\n\n        msg = \"'index=False' is only valid when 'orient' is 'split' or 'table'\"\n        with pytest.raises(ValueError, match=msg):\n            df.to_json(orient=orient, index=False)\n\n    @pytest.mark.parametrize(\"orient\", [\"split\", \"table\"])\n    @pytest.mark.parametrize(\"index\", [True, False])\n    def test_index_false_from_json_to_json(self, orient, index):\n        # GH25170\n        # Test index=False in from_json to_json\n        expected = DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n        dfjson = expected.to_json(orient=orient, index=index)\n        result = read_json(dfjson, orient=orient)\n        assert_frame_equal(result, expected)\n\n    def test_read_timezone_information(self):\n        # GH 25546\n        result = read_json(\n            '{\"2019-01-01T11:00:00.000Z\":88}', typ=\"series\", orient=\"index\"\n        )\n        expected = Series([88], index=DatetimeIndex([\"2019-01-01 11:00:00\"], tz=\"UTC\"))\n        assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"date_format,key\", [(\"epoch\", 86400000), (\"iso\", \"P1DT0H0M0S\")]\n    )\n    def test_timedelta_as_label(self, date_format, key):\n        df = pd.DataFrame([[1]], columns=[pd.Timedelta(\"1D\")])\n        expected = '{{\"{key}\":{{\"0\":1}}}}'.format(key=key)\n        result = df.to_json(date_format=date_format)\n\n        assert result == expected\n\n    @pytest.mark.parametrize(\n        \"orient,expected\",\n        [\n            (\"index\", \"{\\\"('a', 'b')\\\":{\\\"('c', 'd')\\\":1}}\"),\n            (\"columns\", \"{\\\"('c', 'd')\\\":{\\\"('a', 'b')\\\":1}}\"),\n            # TODO: the below have separate encoding procedures\n            # They produce JSON but not in a consistent manner\n            pytest.param(\"split\", \"\", marks=pytest.mark.skip),\n            pytest.param(\"table\", \"\", marks=pytest.mark.skip),\n        ],\n    )\n    def test_tuple_labels(self, orient, expected):\n        # GH 20500\n        df = pd.DataFrame([[1]], index=[(\"a\", \"b\")], columns=[(\"c\", \"d\")])\n        result = df.to_json(orient=orient)\n        assert result == expected\n"
    }
  ],
  "questions": [
    "I modified it to object. But performing add at Series, it modifies dates and dtype as datetime64.\r\n\r\nHow can I forcibly prevent add/sub operator from modifying data and dtype?\r\nIs it natural that automatically changing the dtype when some modification occurs?\r\nI think data modification should not be happend if it modifies dtype as datetime64. (In case of datetime64, it raises exception)",
    "> How can I forcibly prevent add/sub operator from modifying data and dtype?\r\n\r\nCan you be more specific about what you mean by \"performing add at Series\"?",
    "Is it okay to change filling data as `datetime64[ns]` to `String` with `dtype: object`?",
    "> Is it okay to change filling data as datetime64[ns] to String with dtype: object?\r\n\r\nProbably, yes.  The one thing I want to double-check is that we aren't losing coverage for cases where we actually _do_ want a datetime64 Series, which I think might be the case in the arithmetic test.\r\n\r\n@WillAyd thoughts on how to track this down more generally?  grepping shows only 11 usages of makeObjectSeries, but some of those are in fixtures.",
    "> @WillAyd thoughts on how to track this down more generally? grepping shows only 11 usages of makeObjectSeries, but some of those are in fixtures.\r\n\r\nYea I think @jmg7173 if you can search for the fixture where this is used and check that as well would be helpful.\r\n\r\nMostly could just change and see what breaks though. The naming of this obviously means the intent is to deal with object data, so I don't think should break too much actually making this return an object dtype"
  ],
  "golden_answers": [
    "I mean, at `tests/arithmetic/test_object.py test_object_arr_invalid`, it performs add operation.\r\n\r\nI simulated it manually, it manipulates Series with `dtype=object` to `dtype=datetime64[ns]` and also, it also manipulates data also.\r\n\r\n```\r\n>>> obj_ser = tm.makeObjectSeries()\r\n>>> obj_ser \r\nYipVeLYwc8   2000-01-03\r\n...\r\nwHtkS7TlO2    2000-02-08 00:00:00\r\ntTYkAjpXUN    2000-02-09 00:00:00\r\n5OGw3PNMfM    2000-02-10 00:00:00\r\n14tcE5tlIl    2000-02-11 00:00:00\r\ndtype: object\r\n\r\n>>> operator.add(obj_ser, 1)\r\nYipVeLYwc8   2000-01-04\r\n...\r\ntTYkAjpXUN   2000-02-10\r\n5OGw3PNMfM   2000-02-11\r\n14tcE5tlIl   2000-02-14\r\ndtype: datetime64[ns]\r\n```",
    "I mean, at `tests/arithmetic/test_object.py test_object_arr_invalid`, it performs add operation.\r\n\r\nI simulated it manually, it manipulates Series with `dtype=object` to `dtype=datetime64[ns]` and also, it also manipulates data also.\r\n\r\n```\r\n>>> obj_ser = tm.makeObjectSeries()\r\n>>> obj_ser \r\nYipVeLYwc8   2000-01-03\r\n...\r\nwHtkS7TlO2    2000-02-08 00:00:00\r\ntTYkAjpXUN    2000-02-09 00:00:00\r\n5OGw3PNMfM    2000-02-10 00:00:00\r\n14tcE5tlIl    2000-02-11 00:00:00\r\ndtype: object\r\n\r\n>>> operator.add(obj_ser, 1)\r\nYipVeLYwc8   2000-01-04\r\n...\r\ntTYkAjpXUN   2000-02-10\r\n5OGw3PNMfM   2000-02-11\r\n14tcE5tlIl   2000-02-14\r\ndtype: datetime64[ns]\r\n```",
    "As I searched, there is no usecase that `objectSeries` used just for `datetime` excluding arithmetic test.\r\n\r\nI searched it including `object_series` fixture and instance of `TestData`(`TestData.objSeries`).\r\nI hope I found everything :)",
    "As I searched, there is no usecase that `objectSeries` used just for `datetime` excluding arithmetic test.\r\n\r\nI searched it including `object_series` fixture and instance of `TestData`(`TestData.objSeries`).\r\nI hope I found everything :)",
    ">  Yea I think @jmg7173 if you can search for the fixture where this is used and check that as well would be helpful.\r\n\r\nHere's list where this is used:\r\n* As function `makeObjectSeries` itself:\r\n```\r\npandas/tests/arithmetic/test_object.py L144 test_objarr_add_invalid\r\npandas/tests/dtypes/test_missing.py L94-101 test_isna_isnull\r\npandas/tests/dtypes/test_missing.py L51-59 test_notna_notnull\r\npandas/tests/generic/test_generic.py L703-704 test_squeeze\r\npandas/tests/generic/test_generic.py L746 test_transpose\r\npandas/tests/generic/test_generic.py L768 test_take\r\npandas/tests/io/test_packers.py L459 test_basic at TestSeries (defined as self.d[\"object\"])\r\npandas/tests/io/json/test_pandas.py L885-890 test_series_from_json_to_json at TestPandasContainer (defined as self.objSeries)\r\n```\r\n\r\n* As return of method `objSeries` at class `TestData`:\r\n```\r\nDefined:\r\npandas/tests/series/common.py  L23 as method objSeries of class TestData\r\n(TestData fixture defined at pandas/tests/series/indexing/conftest.py)\r\n\r\nUsed:\r\npandas/pandas/tests/series/test_repr.py L74 (in test_repr)\r\npandas/tests/series/indexing/test_indexing.py L92 test_getitem_get\r\npandas/tests/series/indexing/test_indexing.py L121 test_getitem_fancy\r\npandas/tests/series/indexing/test_indexing.py L559 test_slice\r\n```\r\n\r\n* As fixture `object_series`:\r\n```\r\nDefined:\r\npandas/tests/series/conftest.py L26 as object_series\r\n\r\nUsed:\r\npandas/tests/series/test_combine_concat.py L14 test_append as object_series fixture\r\n\r\n```\r\n\r\n> Mostly could just change and see what breaks though. The naming of this obviously means the intent is to deal with object data, so I don't think should break too much actually making this return an object dtype\r\n\r\nAs you say, there is no breaks at test.\r\n\r\nEven though tests are passed, I think `test_objarr_add_invalid` should be checked whether it used as right way.\r\n\r\nI'll make PR about this issue!"
  ],
  "questions_generated": [
    "What is the expected behavior of the `tm.makeObjectSeries` function in the pandas-dev/pandas repository, and why is there a suggestion to change its return type?",
    "In the context of the pandas-dev/pandas issue discussion, why might it be problematic for the `makeObjectSeries` to return a Series with datetime dtype when performing arithmetic operations?",
    "How does changing the fill data from `datetime64[ns]` to strings with `dtype: object` affect the behavior of the `makeObjectSeries` function during testing?",
    "What are the potential concerns with losing coverage for cases where a `datetime64` Series is needed, and how might this relate to the arithmetic tests discussed in the issue?",
    "How does the use of `tm.makeObjectSeries` in the `TestPandasContainer` class setup function relate to the issue discussed, and what impact might changes have on this test setup?"
  ],
  "golden_answers_generated": [
    "The `tm.makeObjectSeries` function is expected to return a Series with an object dtype. However, it currently returns a Series with dtype 'datetime64[ns]', which was unexpected by the developers. The suggestion to change its return type to object dtype is to align the function's behavior with its name and logical expectations, as it is supposed to generate a Series containing generic Python objects, not specifically datetime objects.",
    "Returning a Series with datetime dtype when performing arithmetic operations can lead to unintended type conversions and data modifications. For instance, adding an integer to a datetime Series will result in a new datetime value, which might not be the intended behavior if the Series was expected to hold generic objects. This automatic type conversion can cause issues in tests and logic that expect the Series to remain with object dtype and unchanged data.",
    "Changing the fill data to strings with `dtype: object` ensures that the Series retains its object dtype during operations, such as addition, thereby preventing automatic type conversion to datetime64[ns]. This change helps maintain the integrity of the data and the expected dtype throughout testing, ensuring that operations like addition append strings rather than altering the dtype or the nature of the data.",
    "Losing coverage for cases where a `datetime64` Series is needed could result in gaps in testing for functionality that specifically relies on datetime operations, such as date arithmetic or time series analysis. This is relevant to the arithmetic tests because they might be designed to validate behavior specific to datetime Series. If the `makeObjectSeries` function is changed to avoid datetime data, it may inadvertently skip tests that are crucial for ensuring correct datetime handling, thus necessitating a careful review of the test suite to ensure all necessary cases are still covered.",
    "The `tm.makeObjectSeries` function is used to create an object Series for testing within the `TestPandasContainer` class. Changes to its return type could affect tests that depend on its current behavior, potentially requiring updates to the test expectations or the logic being tested. If the Series changes from containing datetime64 objects to strings or other types, any tests assuming datetime behavior would need to be adjusted to align with the new expected data types and operations, thus maintaining the robustness of the test cases."
  ]
}