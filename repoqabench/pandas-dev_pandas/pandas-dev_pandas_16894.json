{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "16894",
  "issue_description": "# DataFrame._init_dict handles columns with nan incorrectly if columns passed separately\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```py\r\n>>> df = pd.DataFrame({np.nan: [1, 2]})\r\n>>> df[np.nan]   # Arguably expectedly, nan matches nan\r\n0    1\r\n1    2\r\nName: nan, dtype: int64\r\n\r\n>>> df = pd.DataFrame({np.nan: [1, 2], 2: [2, 3]}, columns=[np.nan, 2])\r\n>>> df   # nan from dict didn't match nan from ensured Float64Index\r\n  NaN    2.0\r\n0  NaN     2\r\n1  NaN     3\r\n```\r\n#### Problem description\r\nWhen DataFrame is initialized from dict, if columns are passed, nan isn't recognized and retrieved from dict correctly. The problem is in [loops like](https://github.com/pandas-dev/pandas/blob/3e20eab7ad5639810b4824790cd559367b326b0b/pandas/core/frame.py#L427-L428):\r\n```py\r\ncolumns = _ensure_index(columns)  # Float64Index\r\nfor c in columns:  # c = np.float64(np.nan)  (is not np.nan)\r\n    if c in data_dict:  # c is not in dict\r\n        ....\r\n```\r\nIf `columns` aren't passed separately, initialization works as expected.\r\n```py\r\n>>> pd.DataFrame({np.nan: [1, 2], 2: [2, 3]})\r\n   NaN    2.0\r\n0     1     2\r\n1     2     3\r\n```\r\n\r\nConsistentcy would be nice.\r\n\r\n#### Expected Output\r\n```py\r\n>>> df = pd.DataFrame({np.nan: [1, 2], 2: [2, 3]}, columns=[np.nan, 2])\r\n>>> df   # nan from dict matches nan from Float64Index\r\n  NaN    2.0\r\n0  1     2\r\n1  2     3\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\npandas 0.21.0.dev+225.gb55b1a2fe\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 315282082,
      "user": "gfyoung",
      "body": "You can actually replicate this using `dict` !\r\n\r\n~~~python\r\n>>> import numpy as np\r\n>>> s = {np.nan : 2}\r\n>>> np.nan in s\r\nTrue\r\n>>>\r\n>>> s = {np.float64(np.nan) : 2}\r\n>>> np.nan in s\r\nFalse\r\n~~~\r\n\r\nHonestly, I blame `numpy` for this :smile:, as this is annoying to patch.  On the one hand, ensuring `Index` makes sense, but then you can't check `np.nan` anymore.\r\n\r\nThe obvious *candidate* workaround is to write `columns=[np.float64(np.nan), 2]`.  Perhaps what we could do is call `ensure_index` (or some form of it) on the columns to perform the same casting?  Not really sure at this point, but let us know if the workaround works for you at least."
    },
    {
      "id": 315372948,
      "user": "kernc",
      "body": "`np.float64(np.nan)` wouldn't work, because it's an _instance_ of NaN, and nans don't equate to one another. `dict` probably uses (or falls back to) operator-is equality, which a **singleton** like `np.nan` (`pd.NaT`, ...) responds positive to. Compare:\r\n```py\r\n>>> np.nan in {np.nan}\r\nTrue\r\n\r\n>>> np.float64(np.nan) in {np.float64(np.nan)}\r\nFalse\r\n\r\n>>> float('nan') in {float('nan')}\r\nFalse\r\n\r\n>>> np.nan is np.nan\r\nTrue\r\n\r\n>>> np.float64(np.nan) is np.float64(np.nan)\r\nFalse\r\n```\r\n\r\nThis bug came up in https://github.com/pandas-dev/pandas/pull/16883#discussion_r126827887. But there are legitimate cases for a (catch-all) nan in index (e.g. https://github.com/pandas-dev/pandas/issues/3729)."
    },
    {
      "id": 315383316,
      "user": "gfyoung",
      "body": "@kernc : Hmmm...I suspect we have the support for this indexing somewhere in the code-base, as this code works below:\r\n\r\n~~~python\r\n>>> df = DataFrame({np.float64(np.nan): [1, 2]})\r\n>>> df[np.nan]\r\n2\r\n>>> type(df.columns[0])\r\n<class 'numpy.float64'>\r\n~~~\r\n\r\nI can't search the code-base from my phone, but I suspect if you walk through the `__getitem__` logic for `DataFrame`, you can find the place where we reconcile the `np.nan`-handling, which we can then apply to initialization in your example."
    },
    {
      "id": 315384829,
      "user": "kernc",
      "body": "Yes, `Index` and subclasses [support nan](https://github.com/pandas-dev/pandas/blob/9c44f9b2cad863bde17c7dd061d5b5b5ccbada21/pandas/core/indexes/base.py#L1828). Initialization from dict doesn't."
    },
    {
      "id": 315386253,
      "user": "gfyoung",
      "body": "Correct, but I'm saying that the logic for `__getitem__` (not the link that you are pointing to, which is for something else) should be located, as that would tell you how the bug can be patched I imagine."
    },
    {
      "id": 315388894,
      "user": "kernc",
      "body": "Not unexpectedly, Indexes use [`isnull()`](https://github.com/pandas-dev/pandas/blob/9c44f9b2cad863bde17c7dd061d5b5b5ccbada21/pandas/core/indexes/base.py#L1810-L1823) or [`isnan()`](https://github.com/pandas-dev/pandas/blob/9c44f9b2cad863bde17c7dd061d5b5b5ccbada21/pandas/core/indexes/base.py#L1810-L1823) checks internally when constructing their indexers."
    },
    {
      "id": 315391551,
      "user": "gfyoung",
      "body": "Okay, might it be possible to use similar logic when constructing from `dict` ?  I'm trying to see (without being able to see the code-base ATM) whether whatever logic being used to handle `np.nan` properly for indexing can be used internally when constructing a `DataFrame` from `dict`."
    },
    {
      "id": 315404093,
      "user": "jreback",
      "body": "in ``DataFrame._init_dict`` this passes thru ``Index``, e.g. \r\n\r\n```\r\nIn [1]: Index([float('nan')])\r\nOut[1]: Float64Index([nan], dtype='float64')\r\n\r\nIn [2]: Index([np.nan])\r\nOut[2]: Float64Index([nan], dtype='float64')\r\n```\r\n\r\nso must be missing this step somewhere.\r\n"
    },
    {
      "id": 315405551,
      "user": "gfyoung",
      "body": "> so must be missing this step somewhere.\r\n\r\n@jreback : confused by what you were just demonstrating."
    },
    {
      "id": 315410548,
      "user": "jreback",
      "body": "> @jreback : confused by what you were just demonstrating.\r\n\r\nThis problem is already solved for several cases (but not the one that illustrated above) and SDF. So the existing solutions should just propagate.\r\n\r\n"
    },
    {
      "id": 315411725,
      "user": "gfyoung",
      "body": "Ah, gotcha, you were referring to what I was saying <a href=\"https://github.com/pandas-dev/pandas/issues/16894#issuecomment-315386253\">above</a>"
    },
    {
      "id": 544338418,
      "user": "mroeschke",
      "body": "Looks to work on master. Could use a test:\r\n\r\n```\r\nIn [51]: >>> df = pd.DataFrame({np.nan: [1, 2], 2: [2, 3]}, columns=[np.nan, 2])\r\n    ...: >>> df   # nan from dict matches nan from Float64Index\r\nOut[51]:\r\n   NaN  2.0\r\n0    1    2\r\n1    2    3\r\n\r\nIn [52]: pd.__version__\r\nOut[52]: '0.26.0.dev0+593.g9d45934af'\r\n```"
    }
  ],
  "text_context": "# DataFrame._init_dict handles columns with nan incorrectly if columns passed separately\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```py\r\n>>> df = pd.DataFrame({np.nan: [1, 2]})\r\n>>> df[np.nan]   # Arguably expectedly, nan matches nan\r\n0    1\r\n1    2\r\nName: nan, dtype: int64\r\n\r\n>>> df = pd.DataFrame({np.nan: [1, 2], 2: [2, 3]}, columns=[np.nan, 2])\r\n>>> df   # nan from dict didn't match nan from ensured Float64Index\r\n  NaN    2.0\r\n0  NaN     2\r\n1  NaN     3\r\n```\r\n#### Problem description\r\nWhen DataFrame is initialized from dict, if columns are passed, nan isn't recognized and retrieved from dict correctly. The problem is in [loops like](https://github.com/pandas-dev/pandas/blob/3e20eab7ad5639810b4824790cd559367b326b0b/pandas/core/frame.py#L427-L428):\r\n```py\r\ncolumns = _ensure_index(columns)  # Float64Index\r\nfor c in columns:  # c = np.float64(np.nan)  (is not np.nan)\r\n    if c in data_dict:  # c is not in dict\r\n        ....\r\n```\r\nIf `columns` aren't passed separately, initialization works as expected.\r\n```py\r\n>>> pd.DataFrame({np.nan: [1, 2], 2: [2, 3]})\r\n   NaN    2.0\r\n0     1     2\r\n1     2     3\r\n```\r\n\r\nConsistentcy would be nice.\r\n\r\n#### Expected Output\r\n```py\r\n>>> df = pd.DataFrame({np.nan: [1, 2], 2: [2, 3]}, columns=[np.nan, 2])\r\n>>> df   # nan from dict matches nan from Float64Index\r\n  NaN    2.0\r\n0  1     2\r\n1  2     3\r\n```\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\npandas 0.21.0.dev+225.gb55b1a2fe\r\n</details>\r\n\n\nYou can actually replicate this using `dict` !\r\n\r\n~~~python\r\n>>> import numpy as np\r\n>>> s = {np.nan : 2}\r\n>>> np.nan in s\r\nTrue\r\n>>>\r\n>>> s = {np.float64(np.nan) : 2}\r\n>>> np.nan in s\r\nFalse\r\n~~~\r\n\r\nHonestly, I blame `numpy` for this :smile:, as this is annoying to patch.  On the one hand, ensuring `Index` makes sense, but then you can't check `np.nan` anymore.\r\n\r\nThe obvious *candidate* workaround is to write `columns=[np.float64(np.nan), 2]`.  Perhaps what we could do is call `ensure_index` (or some form of it) on the columns to perform the same casting?  Not really sure at this point, but let us know if the workaround works for you at least.\n\n`np.float64(np.nan)` wouldn't work, because it's an _instance_ of NaN, and nans don't equate to one another. `dict` probably uses (or falls back to) operator-is equality, which a **singleton** like `np.nan` (`pd.NaT`, ...) responds positive to. Compare:\r\n```py\r\n>>> np.nan in {np.nan}\r\nTrue\r\n\r\n>>> np.float64(np.nan) in {np.float64(np.nan)}\r\nFalse\r\n\r\n>>> float('nan') in {float('nan')}\r\nFalse\r\n\r\n>>> np.nan is np.nan\r\nTrue\r\n\r\n>>> np.float64(np.nan) is np.float64(np.nan)\r\nFalse\r\n```\r\n\r\nThis bug came up in https://github.com/pandas-dev/pandas/pull/16883#discussion_r126827887. But there are legitimate cases for a (catch-all) nan in index (e.g. https://github.com/pandas-dev/pandas/issues/3729).\n\n@kernc : Hmmm...I suspect we have the support for this indexing somewhere in the code-base, as this code works below:\r\n\r\n~~~python\r\n>>> df = DataFrame({np.float64(np.nan): [1, 2]})\r\n>>> df[np.nan]\r\n2\r\n>>> type(df.columns[0])\r\n<class 'numpy.float64'>\r\n~~~\r\n\r\nI can't search the code-base from my phone, but I suspect if you walk through the `__getitem__` logic for `DataFrame`, you can find the place where we reconcile the `np.nan`-handling, which we can then apply to initialization in your example.\n\nYes, `Index` and subclasses [support nan](https://github.com/pandas-dev/pandas/blob/9c44f9b2cad863bde17c7dd061d5b5b5ccbada21/pandas/core/indexes/base.py#L1828). Initialization from dict doesn't.\n\nCorrect, but I'm saying that the logic for `__getitem__` (not the link that you are pointing to, which is for something else) should be located, as that would tell you how the bug can be patched I imagine.\n\nNot unexpectedly, Indexes use [`isnull()`](https://github.com/pandas-dev/pandas/blob/9c44f9b2cad863bde17c7dd061d5b5b5ccbada21/pandas/core/indexes/base.py#L1810-L1823) or [`isnan()`](https://github.com/pandas-dev/pandas/blob/9c44f9b2cad863bde17c7dd061d5b5b5ccbada21/pandas/core/indexes/base.py#L1810-L1823) checks internally when constructing their indexers.\n\nOkay, might it be possible to use similar logic when constructing from `dict` ?  I'm trying to see (without being able to see the code-base ATM) whether whatever logic being used to handle `np.nan` properly for indexing can be used internally when constructing a `DataFrame` from `dict`.\n\nin ``DataFrame._init_dict`` this passes thru ``Index``, e.g. \r\n\r\n```\r\nIn [1]: Index([float('nan')])\r\nOut[1]: Float64Index([nan], dtype='float64')\r\n\r\nIn [2]: Index([np.nan])\r\nOut[2]: Float64Index([nan], dtype='float64')\r\n```\r\n\r\nso must be missing this step somewhere.\r\n\n\n> so must be missing this step somewhere.\r\n\r\n@jreback : confused by what you were just demonstrating.\n\n> @jreback : confused by what you were just demonstrating.\r\n\r\nThis problem is already solved for several cases (but not the one that illustrated above) and SDF. So the existing solutions should just propagate.\r\n\r\n\n\nAh, gotcha, you were referring to what I was saying <a href=\"https://github.com/pandas-dev/pandas/issues/16894#issuecomment-315386253\">above</a>\n\nLooks to work on master. Could use a test:\r\n\r\n```\r\nIn [51]: >>> df = pd.DataFrame({np.nan: [1, 2], 2: [2, 3]}, columns=[np.nan, 2])\r\n    ...: >>> df   # nan from dict matches nan from Float64Index\r\nOut[51]:\r\n   NaN  2.0\r\n0    1    2\r\n1    2    3\r\n\r\nIn [52]: pd.__version__\r\nOut[52]: '0.26.0.dev0+593.g9d45934af'\r\n```",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/16883",
  "code_context": [
    {
      "filename": "asv_bench/benchmarks/sparse.py",
      "content": "from itertools import repeat\n\nfrom .pandas_vb_common import *\nimport scipy.sparse\nfrom pandas import SparseSeries, SparseDataFrame\n\n\nclass sparse_series_to_frame(object):\n    goal_time = 0.2\n\n    def setup(self):\n        self.K = 50\n        self.N = 50000\n        self.rng = np.asarray(date_range('1/1/2000', periods=self.N, freq='T'))\n        self.series = {}\n        for i in range(1, (self.K + 1)):\n            self.data = np.random.randn(self.N)[:(- i)]\n            self.this_rng = self.rng[:(- i)]\n            self.data[100:] = np.nan\n            self.series[i] = SparseSeries(self.data, index=self.this_rng)\n\n    def time_sparse_series_to_frame(self):\n        SparseDataFrame(self.series)\n\n\nclass sparse_frame_constructor(object):\n    goal_time = 0.2\n\n    def time_sparse_frame_constructor(self):\n        SparseDataFrame(columns=np.arange(100), index=np.arange(1000))\n\n    def time_sparse_from_scipy(self):\n        SparseDataFrame(scipy.sparse.rand(1000, 1000, 0.005))\n\n    def time_sparse_from_dict(self):\n        SparseDataFrame(dict(zip(range(1000), repeat([0]))))\n\n\nclass sparse_series_from_coo(object):\n    goal_time = 0.2\n\n    def setup(self):\n        self.A = scipy.sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(100, 100))\n\n    def time_sparse_series_from_coo(self):\n        self.ss = SparseSeries.from_coo(self.A)\n\n\nclass sparse_series_to_coo(object):\n    goal_time = 0.2\n\n    def setup(self):\n        self.s = pd.Series(([np.nan] * 10000))\n        self.s[0] = 3.0\n        self.s[100] = (-1.0)\n        self.s[999] = 12.1\n        self.s.index = pd.MultiIndex.from_product((range(10), range(10), range(10), range(10)))\n        self.ss = self.s.to_sparse()\n\n    def time_sparse_series_to_coo(self):\n        self.ss.to_coo(row_levels=[0, 1], column_levels=[2, 3], sort_labels=True)\n\n\nclass sparse_arithmetic_int(object):\n    goal_time = 0.2\n\n    def setup(self):\n        np.random.seed(1)\n        self.a_10percent = self.make_sparse_array(length=1000000, dense_size=100000, fill_value=np.nan)\n        self.b_10percent = self.make_sparse_array(length=1000000, dense_size=100000, fill_value=np.nan)\n\n        self.a_10percent_zero = self.make_sparse_array(length=1000000, dense_size=100000, fill_value=0)\n        self.b_10percent_zero = self.make_sparse_array(length=1000000, dense_size=100000, fill_value=0)\n\n        self.a_1percent = self.make_sparse_array(length=1000000, dense_size=10000, fill_value=np.nan)\n        self.b_1percent = self.make_sparse_array(length=1000000, dense_size=10000, fill_value=np.nan)\n\n    def make_sparse_array(self, length, dense_size, fill_value):\n        arr = np.array([fill_value] * length, dtype=np.float64)\n        indexer = np.unique(np.random.randint(0, length, dense_size))\n        arr[indexer] = np.random.randint(0, 100, len(indexer))\n        return pd.SparseArray(arr, fill_value=fill_value)\n\n    def time_sparse_make_union(self):\n        self.a_10percent.sp_index.make_union(self.b_10percent.sp_index)\n\n    def time_sparse_intersect(self):\n        self.a_10percent.sp_index.intersect(self.b_10percent.sp_index)\n\n    def time_sparse_addition_10percent(self):\n        self.a_10percent + self.b_10percent\n\n    def time_sparse_addition_10percent_zero(self):\n        self.a_10percent_zero + self.b_10percent_zero\n\n    def time_sparse_addition_1percent(self):\n        self.a_1percent + self.b_1percent\n\n    def time_sparse_division_10percent(self):\n        self.a_10percent / self.b_10percent\n\n    def time_sparse_division_10percent_zero(self):\n        self.a_10percent_zero / self.b_10percent_zero\n\n    def time_sparse_division_1percent(self):\n        self.a_1percent / self.b_1percent\n\n\n\nclass sparse_arithmetic_block(object):\n    goal_time = 0.2\n\n    def setup(self):\n        np.random.seed(1)\n        self.a = self.make_sparse_array(length=1000000, num_blocks=1000,\n                                        block_size=10, fill_value=np.nan)\n        self.b = self.make_sparse_array(length=1000000, num_blocks=1000,\n                                        block_size=10, fill_value=np.nan)\n\n        self.a_zero = self.make_sparse_array(length=1000000, num_blocks=1000,\n                                             block_size=10, fill_value=0)\n        self.b_zero = self.make_sparse_array(length=1000000, num_blocks=1000,\n                                             block_size=10, fill_value=np.nan)\n\n    def make_sparse_array(self, length, num_blocks, block_size, fill_value):\n        a = np.array([fill_value] * length)\n        for block in range(num_blocks):\n            i = np.random.randint(0, length)\n            a[i:i + block_size] = np.random.randint(0, 100, len(a[i:i + block_size]))\n        return pd.SparseArray(a, fill_value=fill_value)\n\n    def time_sparse_make_union(self):\n        self.a.sp_index.make_union(self.b.sp_index)\n\n    def time_sparse_intersect(self):\n        self.a.sp_index.intersect(self.b.sp_index)\n\n    def time_sparse_addition(self):\n        self.a + self.b\n\n    def time_sparse_addition_zero(self):\n        self.a_zero + self.b_zero\n\n    def time_sparse_division(self):\n        self.a / self.b\n\n    def time_sparse_division_zero(self):\n        self.a_zero / self.b_zero\n"
    },
    {
      "filename": "pandas/core/sparse/frame.py",
      "content": "\"\"\"\nData structures for sparse float data. Life is made simpler by dealing only\nwith float64 data\n\"\"\"\nfrom __future__ import division\n# pylint: disable=E1101,E1103,W0231,E0202\n\nfrom numpy import nan\nfrom pandas.compat import lmap\nfrom pandas import compat\nimport numpy as np\n\nfrom pandas.core.dtypes.missing import isnull, notnull\nfrom pandas.core.dtypes.cast import maybe_upcast, find_common_type\nfrom pandas.core.dtypes.common import _ensure_platform_int, is_scipy_sparse\n\nfrom pandas.core.common import _try_sort\nfrom pandas.compat.numpy import function as nv\nfrom pandas.core.index import Index, MultiIndex, _ensure_index\nfrom pandas.core.series import Series\nfrom pandas.core.frame import (DataFrame, extract_index, _prep_ndarray,\n                               _default_index)\nimport pandas.core.algorithms as algos\nfrom pandas.core.internals import (BlockManager,\n                                   create_block_manager_from_arrays)\nimport pandas.core.generic as generic\nfrom pandas.core.sparse.series import SparseSeries, SparseArray\nfrom pandas._libs.sparse import BlockIndex, get_blocks\nfrom pandas.util._decorators import Appender\nimport pandas.core.ops as ops\n\n\n_shared_doc_kwargs = dict(klass='SparseDataFrame')\n\n\nclass SparseDataFrame(DataFrame):\n    \"\"\"\n    DataFrame containing sparse floating point data in the form of SparseSeries\n    objects\n\n    Parameters\n    ----------\n    data : same types as can be passed to DataFrame or scipy.sparse.spmatrix\n    index : array-like, optional\n    column : array-like, optional\n    default_kind : {'block', 'integer'}, default 'block'\n        Default sparse kind for converting Series to SparseSeries. Will not\n        override SparseSeries passed into constructor\n    default_fill_value : float\n        Default fill_value for converting Series to SparseSeries\n        (default: nan). Will not override SparseSeries passed in.\n    \"\"\"\n    _constructor_sliced = SparseSeries\n    _subtyp = 'sparse_frame'\n\n    def __init__(self, data=None, index=None, columns=None, default_kind=None,\n                 default_fill_value=None, dtype=None, copy=False):\n\n        # pick up the defaults from the Sparse structures\n        if isinstance(data, SparseDataFrame):\n            if index is None:\n                index = data.index\n            if columns is None:\n                columns = data.columns\n            if default_fill_value is None:\n                default_fill_value = data.default_fill_value\n            if default_kind is None:\n                default_kind = data.default_kind\n        elif isinstance(data, (SparseSeries, SparseArray)):\n            if index is None:\n                index = data.index\n            if default_fill_value is None:\n                default_fill_value = data.fill_value\n            if columns is None and hasattr(data, 'name'):\n                columns = [data.name]\n            if columns is None:\n                raise Exception(\"cannot pass a series w/o a name or columns\")\n            data = {columns[0]: data}\n\n        if default_fill_value is None:\n            default_fill_value = np.nan\n        if default_kind is None:\n            default_kind = 'block'\n\n        self._default_kind = default_kind\n        self._default_fill_value = default_fill_value\n\n        if is_scipy_sparse(data):\n            mgr = self._init_spmatrix(data, index, columns, dtype=dtype,\n                                      fill_value=default_fill_value)\n        elif isinstance(data, dict):\n            mgr = self._init_dict(data, index, columns, dtype=dtype)\n        elif isinstance(data, (np.ndarray, list)):\n            mgr = self._init_matrix(data, index, columns, dtype=dtype)\n        elif isinstance(data, SparseDataFrame):\n            mgr = self._init_mgr(data._data,\n                                 dict(index=index, columns=columns),\n                                 dtype=dtype, copy=copy)\n        elif isinstance(data, DataFrame):\n            mgr = self._init_dict(data, data.index, data.columns, dtype=dtype)\n        elif isinstance(data, BlockManager):\n            mgr = self._init_mgr(data, axes=dict(index=index, columns=columns),\n                                 dtype=dtype, copy=copy)\n        elif data is None:\n            data = DataFrame()\n\n            if index is None:\n                index = Index([])\n            else:\n                index = _ensure_index(index)\n\n            if columns is None:\n                columns = Index([])\n            else:\n                for c in columns:\n                    data[c] = SparseArray(np.nan, index=index,\n                                          kind=self._default_kind,\n                                          fill_value=self._default_fill_value)\n            mgr = to_manager(data, columns, index)\n            if dtype is not None:\n                mgr = mgr.astype(dtype)\n\n        generic.NDFrame.__init__(self, mgr)\n\n    @property\n    def _constructor(self):\n        return SparseDataFrame\n\n    _constructor_sliced = SparseSeries\n\n    def _init_dict(self, data, index, columns, dtype=None):\n        # pre-filter out columns if we passed it\n        if columns is not None:\n            columns = _ensure_index(columns)\n            data = dict((k, v) for k, v in compat.iteritems(data)\n                        if k in columns)\n        else:\n            columns = Index(_try_sort(list(data.keys())))\n\n        if index is None:\n            index = extract_index(list(data.values()))\n\n        sp_maker = lambda x: SparseArray(x, kind=self._default_kind,\n                                         fill_value=self._default_fill_value,\n                                         copy=True, dtype=dtype)\n        sdict = {}\n        for k, v in compat.iteritems(data):\n            if isinstance(v, Series):\n                # Force alignment, no copy necessary\n                if not v.index.equals(index):\n                    v = v.reindex(index)\n\n                if not isinstance(v, SparseSeries):\n                    v = sp_maker(v.values)\n            elif isinstance(v, SparseArray):\n                v = v.copy()\n            else:\n                if isinstance(v, dict):\n                    v = [v.get(i, nan) for i in index]\n\n                v = sp_maker(v)\n            sdict[k] = v\n\n        # TODO: figure out how to handle this case, all nan's?\n        # add in any other columns we want to have (completeness)\n        nan_arr = sp_maker(np.full(len(index), np.nan))\n        sdict.update((c, nan_arr) for c in columns if c not in sdict)\n\n        return to_manager(sdict, columns, index)\n\n    def _init_matrix(self, data, index, columns, dtype=None):\n        \"\"\" Init self from ndarray or list of lists \"\"\"\n        data = _prep_ndarray(data, copy=False)\n        index, columns = self._prep_index(data, index, columns)\n        data = dict([(idx, data[:, i]) for i, idx in enumerate(columns)])\n        return self._init_dict(data, index, columns, dtype)\n\n    def _init_spmatrix(self, data, index, columns, dtype=None,\n                       fill_value=None):\n        \"\"\" Init self from scipy.sparse matrix \"\"\"\n        index, columns = self._prep_index(data, index, columns)\n        data = data.tocoo()\n        N = len(index)\n\n        # Construct a dict of SparseSeries\n        sdict = {}\n        values = Series(data.data, index=data.row, copy=False)\n        for col, rowvals in values.groupby(data.col):\n            # get_blocks expects int32 row indices in sorted order\n            rowvals = rowvals.sort_index()\n            rows = rowvals.index.values.astype(np.int32)\n            blocs, blens = get_blocks(rows)\n\n            sdict[columns[col]] = SparseSeries(\n                rowvals.values, index=index,\n                fill_value=fill_value,\n                sparse_index=BlockIndex(N, blocs, blens))\n\n        # Add any columns that were empty and thus not grouped on above\n        sdict.update({column: SparseSeries(index=index,\n                                           fill_value=fill_value,\n                                           sparse_index=BlockIndex(N, [], []))\n                      for column in columns\n                      if column not in sdict})\n\n        return self._init_dict(sdict, index, columns, dtype)\n\n    def _prep_index(self, data, index, columns):\n        N, K = data.shape\n        if index is None:\n            index = _default_index(N)\n        if columns is None:\n            columns = _default_index(K)\n\n        if len(columns) != K:\n            raise ValueError('Column length mismatch: %d vs. %d' %\n                             (len(columns), K))\n        if len(index) != N:\n            raise ValueError('Index length mismatch: %d vs. %d' %\n                             (len(index), N))\n        return index, columns\n\n    def to_coo(self):\n        \"\"\"\n        Return the contents of the frame as a sparse SciPy COO matrix.\n\n        .. versionadded:: 0.20.0\n\n        Returns\n        -------\n        coo_matrix : scipy.sparse.spmatrix\n            If the caller is heterogeneous and contains booleans or objects,\n            the result will be of dtype=object. See Notes.\n\n        Notes\n        -----\n        The dtype will be the lowest-common-denominator type (implicit\n        upcasting); that is to say if the dtypes (even of numeric types)\n        are mixed, the one that accommodates all will be chosen.\n\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\n        float32. By numpy.find_common_type convention, mixing int64 and\n        and uint64 will result in a float64 dtype.\n        \"\"\"\n        try:\n            from scipy.sparse import coo_matrix\n        except ImportError:\n            raise ImportError('Scipy is not installed')\n\n        dtype = find_common_type(self.dtypes)\n        cols, rows, datas = [], [], []\n        for col, name in enumerate(self):\n            s = self[name]\n            row = s.sp_index.to_int_index().indices\n            cols.append(np.repeat(col, len(row)))\n            rows.append(row)\n            datas.append(s.sp_values.astype(dtype, copy=False))\n\n        cols = np.concatenate(cols)\n        rows = np.concatenate(rows)\n        datas = np.concatenate(datas)\n        return coo_matrix((datas, (rows, cols)), shape=self.shape)\n\n    def __array_wrap__(self, result):\n        return self._constructor(\n            result, index=self.index, columns=self.columns,\n            default_kind=self._default_kind,\n            default_fill_value=self._default_fill_value).__finalize__(self)\n\n    def __getstate__(self):\n        # pickling\n        return dict(_typ=self._typ, _subtyp=self._subtyp, _data=self._data,\n                    _default_fill_value=self._default_fill_value,\n                    _default_kind=self._default_kind)\n\n    def _unpickle_sparse_frame_compat(self, state):\n        \"\"\" original pickle format \"\"\"\n        series, cols, idx, fv, kind = state\n\n        if not isinstance(cols, Index):  # pragma: no cover\n            from pandas.io.pickle import _unpickle_array\n            columns = _unpickle_array(cols)\n        else:\n            columns = cols\n\n        if not isinstance(idx, Index):  # pragma: no cover\n            from pandas.io.pickle import _unpickle_array\n            index = _unpickle_array(idx)\n        else:\n            index = idx\n\n        series_dict = DataFrame()\n        for col, (sp_index, sp_values) in compat.iteritems(series):\n            series_dict[col] = SparseSeries(sp_values, sparse_index=sp_index,\n                                            fill_value=fv)\n\n        self._data = to_manager(series_dict, columns, index)\n        self._default_fill_value = fv\n        self._default_kind = kind\n\n    def to_dense(self):\n        \"\"\"\n        Convert to dense DataFrame\n\n        Returns\n        -------\n        df : DataFrame\n        \"\"\"\n        data = dict((k, v.to_dense()) for k, v in compat.iteritems(self))\n        return DataFrame(data, index=self.index, columns=self.columns)\n\n    def _apply_columns(self, func):\n        \"\"\" get new SparseDataFrame applying func to each columns \"\"\"\n\n        new_data = {}\n        for col, series in compat.iteritems(self):\n            new_data[col] = func(series)\n\n        return self._constructor(\n            data=new_data, index=self.index, columns=self.columns,\n            default_fill_value=self.default_fill_value).__finalize__(self)\n\n    def astype(self, dtype):\n        return self._apply_columns(lambda x: x.astype(dtype))\n\n    def copy(self, deep=True):\n        \"\"\"\n        Make a copy of this SparseDataFrame\n        \"\"\"\n        result = super(SparseDataFrame, self).copy(deep=deep)\n        result._default_fill_value = self._default_fill_value\n        result._default_kind = self._default_kind\n        return result\n\n    @property\n    def default_fill_value(self):\n        return self._default_fill_value\n\n    @property\n    def default_kind(self):\n        return self._default_kind\n\n    @property\n    def density(self):\n        \"\"\"\n        Ratio of non-sparse points to total (dense) data points\n        represented in the frame\n        \"\"\"\n        tot_nonsparse = sum([ser.sp_index.npoints\n                             for _, ser in compat.iteritems(self)])\n        tot = len(self.index) * len(self.columns)\n        return tot_nonsparse / float(tot)\n\n    def fillna(self, value=None, method=None, axis=0, inplace=False,\n               limit=None, downcast=None):\n        new_self = super(SparseDataFrame,\n                         self).fillna(value=value, method=method, axis=axis,\n                                      inplace=inplace, limit=limit,\n                                      downcast=downcast)\n        if not inplace:\n            self = new_self\n\n        # set the fill value if we are filling as a scalar with nothing special\n        # going on\n        if (value is not None and value == value and method is None and\n                limit is None):\n            self._default_fill_value = value\n\n        if not inplace:\n            return self\n\n    # ----------------------------------------------------------------------\n    # Support different internal representation of SparseDataFrame\n\n    def _sanitize_column(self, key, value, **kwargs):\n        \"\"\"\n        Creates a new SparseArray from the input value.\n\n        Parameters\n        ----------\n        key : object\n        value : scalar, Series, or array-like\n        kwargs : dict\n\n        Returns\n        -------\n        sanitized_column : SparseArray\n\n        \"\"\"\n        sp_maker = lambda x, index=None: SparseArray(\n            x, index=index, fill_value=self._default_fill_value,\n            kind=self._default_kind)\n        if isinstance(value, SparseSeries):\n            clean = value.reindex(self.index).as_sparse_array(\n                fill_value=self._default_fill_value, kind=self._default_kind)\n\n        elif isinstance(value, SparseArray):\n            if len(value) != len(self.index):\n                raise AssertionError('Length of values does not match '\n                                     'length of index')\n            clean = value\n\n        elif hasattr(value, '__iter__'):\n            if isinstance(value, Series):\n                clean = value.reindex(self.index)\n                if not isinstance(value, SparseSeries):\n                    clean = sp_maker(clean)\n            else:\n                if len(value) != len(self.index):\n                    raise AssertionError('Length of values does not match '\n                                         'length of index')\n                clean = sp_maker(value)\n\n        # Scalar\n        else:\n            clean = sp_maker(value, self.index)\n\n        # always return a SparseArray!\n        return clean\n\n    def __getitem__(self, key):\n        \"\"\"\n        Retrieve column or slice from DataFrame\n        \"\"\"\n        if isinstance(key, slice):\n            date_rng = self.index[key]\n            return self.reindex(date_rng)\n        elif isinstance(key, (np.ndarray, list, Series)):\n            return self._getitem_array(key)\n        else:\n            return self._get_item_cache(key)\n\n    @Appender(DataFrame.get_value.__doc__, indents=0)\n    def get_value(self, index, col, takeable=False):\n        if takeable is True:\n            series = self._iget_item_cache(col)\n        else:\n            series = self._get_item_cache(col)\n\n        return series.get_value(index, takeable=takeable)\n\n    def set_value(self, index, col, value, takeable=False):\n        \"\"\"\n        Put single value at passed column and index\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        value : scalar value\n        takeable : interpret the index/col as indexers, default False\n\n        Notes\n        -----\n        This method *always* returns a new object. It is currently not\n        particularly efficient (and potentially very expensive) but is provided\n        for API compatibility with DataFrame\n\n        Returns\n        -------\n        frame : DataFrame\n        \"\"\"\n        dense = self.to_dense().set_value(index, col, value, takeable=takeable)\n        return dense.to_sparse(kind=self._default_kind,\n                               fill_value=self._default_fill_value)\n\n    def _slice(self, slobj, axis=0, kind=None):\n        if axis == 0:\n            new_index = self.index[slobj]\n            new_columns = self.columns\n        else:\n            new_index = self.index\n            new_columns = self.columns[slobj]\n\n        return self.reindex(index=new_index, columns=new_columns)\n\n    def xs(self, key, axis=0, copy=False):\n        \"\"\"\n        Returns a row (cross-section) from the SparseDataFrame as a Series\n        object.\n\n        Parameters\n        ----------\n        key : some index contained in the index\n\n        Returns\n        -------\n        xs : Series\n        \"\"\"\n        if axis == 1:\n            data = self[key]\n            return data\n\n        i = self.index.get_loc(key)\n        data = self.take([i]).get_values()[0]\n        return Series(data, index=self.columns)\n\n    # ----------------------------------------------------------------------\n    # Arithmetic-related methods\n\n    def _combine_frame(self, other, func, fill_value=None, level=None):\n        this, other = self.align(other, join='outer', level=level, copy=False)\n        new_index, new_columns = this.index, this.columns\n\n        if level is not None:\n            raise NotImplementedError(\"'level' argument is not supported\")\n\n        if self.empty and other.empty:\n            return self._constructor(index=new_index).__finalize__(self)\n\n        new_data = {}\n        new_fill_value = None\n        if fill_value is not None:\n            # TODO: be a bit more intelligent here\n            for col in new_columns:\n                if col in this and col in other:\n                    dleft = this[col].to_dense()\n                    dright = other[col].to_dense()\n                    result = dleft._binop(dright, func, fill_value=fill_value)\n                    result = result.to_sparse(fill_value=this[col].fill_value)\n                    new_data[col] = result\n        else:\n\n            for col in new_columns:\n                if col in this and col in other:\n                    new_data[col] = func(this[col], other[col])\n\n        # if the fill values are the same use them? or use a valid one\n        other_fill_value = getattr(other, 'default_fill_value', np.nan)\n        if self.default_fill_value == other_fill_value:\n            new_fill_value = self.default_fill_value\n        elif np.isnan(self.default_fill_value) and not np.isnan(\n                other_fill_value):\n            new_fill_value = other_fill_value\n        elif not np.isnan(self.default_fill_value) and np.isnan(\n                other_fill_value):\n            new_fill_value = self.default_fill_value\n\n        return self._constructor(data=new_data, index=new_index,\n                                 columns=new_columns,\n                                 default_fill_value=new_fill_value\n                                 ).__finalize__(self)\n\n    def _combine_match_index(self, other, func, level=None, fill_value=None):\n        new_data = {}\n\n        if fill_value is not None:\n            raise NotImplementedError(\"'fill_value' argument is not supported\")\n        if level is not None:\n            raise NotImplementedError(\"'level' argument is not supported\")\n\n        new_index = self.index.union(other.index)\n        this = self\n        if self.index is not new_index:\n            this = self.reindex(new_index)\n\n        if other.index is not new_index:\n            other = other.reindex(new_index)\n\n        for col, series in compat.iteritems(this):\n            new_data[col] = func(series.values, other.values)\n\n        # fill_value is a function of our operator\n        if isnull(other.fill_value) or isnull(self.default_fill_value):\n            fill_value = np.nan\n        else:\n            fill_value = func(np.float64(self.default_fill_value),\n                              np.float64(other.fill_value))\n\n        return self._constructor(\n            new_data, index=new_index, columns=self.columns,\n            default_fill_value=fill_value).__finalize__(self)\n\n    def _combine_match_columns(self, other, func, level=None, fill_value=None):\n        # patched version of DataFrame._combine_match_columns to account for\n        # NumPy circumventing __rsub__ with float64 types, e.g.: 3.0 - series,\n        # where 3.0 is numpy.float64 and series is a SparseSeries. Still\n        # possible for this to happen, which is bothersome\n\n        if fill_value is not None:\n            raise NotImplementedError(\"'fill_value' argument is not supported\")\n        if level is not None:\n            raise NotImplementedError(\"'level' argument is not supported\")\n\n        new_data = {}\n\n        union = intersection = self.columns\n\n        if not union.equals(other.index):\n            union = other.index.union(self.columns)\n            intersection = other.index.intersection(self.columns)\n\n        for col in intersection:\n            new_data[col] = func(self[col], float(other[col]))\n\n        return self._constructor(\n            new_data, index=self.index, columns=union,\n            default_fill_value=self.default_fill_value).__finalize__(self)\n\n    def _combine_const(self, other, func, raise_on_error=True):\n        return self._apply_columns(lambda x: func(x, other))\n\n    def _reindex_index(self, index, method, copy, level, fill_value=np.nan,\n                       limit=None, takeable=False):\n        if level is not None:\n            raise TypeError('Reindex by level not supported for sparse')\n\n        if self.index.equals(index):\n            if copy:\n                return self.copy()\n            else:\n                return self\n\n        if len(self.index) == 0:\n            return self._constructor(\n                index=index, columns=self.columns).__finalize__(self)\n\n        indexer = self.index.get_indexer(index, method, limit=limit)\n        indexer = _ensure_platform_int(indexer)\n        mask = indexer == -1\n        need_mask = mask.any()\n\n        new_series = {}\n        for col, series in self.iteritems():\n            if mask.all():\n                continue\n\n            values = series.values\n            # .take returns SparseArray\n            new = values.take(indexer)\n            if need_mask:\n                new = new.values\n                # convert integer to float if necessary. need to do a lot\n                # more than that, handle boolean etc also\n                new, fill_value = maybe_upcast(new, fill_value=fill_value)\n                np.putmask(new, mask, fill_value)\n\n            new_series[col] = new\n\n        return self._constructor(\n            new_series, index=index, columns=self.columns,\n            default_fill_value=self._default_fill_value).__finalize__(self)\n\n    def _reindex_columns(self, columns, method, copy, level, fill_value=None,\n                         limit=None, takeable=False):\n        if level is not None:\n            raise TypeError('Reindex by level not supported for sparse')\n\n        if notnull(fill_value):\n            raise NotImplementedError(\"'fill_value' argument is not supported\")\n\n        if limit:\n            raise NotImplementedError(\"'limit' argument is not supported\")\n\n        if method is not None:\n            raise NotImplementedError(\"'method' argument is not supported\")\n\n        # TODO: fill value handling\n        sdict = dict((k, v) for k, v in compat.iteritems(self) if k in columns)\n        return self._constructor(\n            sdict, index=self.index, columns=columns,\n            default_fill_value=self._default_fill_value).__finalize__(self)\n\n    def _reindex_with_indexers(self, reindexers, method=None, fill_value=None,\n                               limit=None, copy=False, allow_dups=False):\n\n        if method is not None or limit is not None:\n            raise NotImplementedError(\"cannot reindex with a method or limit \"\n                                      \"with sparse\")\n\n        if fill_value is None:\n            fill_value = np.nan\n\n        index, row_indexer = reindexers.get(0, (None, None))\n        columns, col_indexer = reindexers.get(1, (None, None))\n\n        if columns is None:\n            columns = self.columns\n\n        new_arrays = {}\n        for col in columns:\n            if col not in self:\n                continue\n            if row_indexer is not None:\n                new_arrays[col] = algos.take_1d(self[col].get_values(),\n                                                row_indexer,\n                                                fill_value=fill_value)\n            else:\n                new_arrays[col] = self[col]\n\n        return self._constructor(new_arrays, index=index,\n                                 columns=columns).__finalize__(self)\n\n    def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n                     sort=False):\n        if on is not None:\n            raise NotImplementedError(\"'on' keyword parameter is not yet \"\n                                      \"implemented\")\n        return self._join_index(other, how, lsuffix, rsuffix)\n\n    def _join_index(self, other, how, lsuffix, rsuffix):\n        if isinstance(other, Series):\n            if other.name is None:\n                raise ValueError('Other Series must have a name')\n\n            other = SparseDataFrame(\n                {other.name: other},\n                default_fill_value=self._default_fill_value)\n\n        join_index = self.index.join(other.index, how=how)\n\n        this = self.reindex(join_index)\n        other = other.reindex(join_index)\n\n        this, other = this._maybe_rename_join(other, lsuffix, rsuffix)\n\n        from pandas import concat\n        return concat([this, other], axis=1, verify_integrity=True)\n\n    def _maybe_rename_join(self, other, lsuffix, rsuffix):\n        to_rename = self.columns.intersection(other.columns)\n        if len(to_rename) > 0:\n            if not lsuffix and not rsuffix:\n                raise ValueError('columns overlap but no suffix specified: %s'\n                                 % to_rename)\n\n            def lrenamer(x):\n                if x in to_rename:\n                    return '%s%s' % (x, lsuffix)\n                return x\n\n            def rrenamer(x):\n                if x in to_rename:\n                    return '%s%s' % (x, rsuffix)\n                return x\n\n            this = self.rename(columns=lrenamer)\n            other = other.rename(columns=rrenamer)\n        else:\n            this = self\n\n        return this, other\n\n    def transpose(self, *args, **kwargs):\n        \"\"\"\n        Returns a DataFrame with the rows/columns switched.\n        \"\"\"\n        nv.validate_transpose(args, kwargs)\n        return self._constructor(\n            self.values.T, index=self.columns, columns=self.index,\n            default_fill_value=self._default_fill_value,\n            default_kind=self._default_kind).__finalize__(self)\n\n    T = property(transpose)\n\n    @Appender(DataFrame.count.__doc__)\n    def count(self, axis=0, **kwds):\n        if axis is None:\n            axis = self._stat_axis_number\n\n        return self.apply(lambda x: x.count(), axis=axis)\n\n    def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Return SparseDataFrame of cumulative sums over requested axis.\n\n        Parameters\n        ----------\n        axis : {0, 1}\n            0 for row-wise, 1 for column-wise\n\n        Returns\n        -------\n        y : SparseDataFrame\n        \"\"\"\n        nv.validate_cumsum(args, kwargs)\n\n        if axis is None:\n            axis = self._stat_axis_number\n\n        return self.apply(lambda x: x.cumsum(), axis=axis)\n\n    @Appender(generic._shared_docs['isnull'])\n    def isnull(self):\n        return self._apply_columns(lambda x: x.isnull())\n\n    @Appender(generic._shared_docs['isnotnull'])\n    def isnotnull(self):\n        return self._apply_columns(lambda x: x.isnotnull())\n\n    def apply(self, func, axis=0, broadcast=False, reduce=False):\n        \"\"\"\n        Analogous to DataFrame.apply, for SparseDataFrame\n\n        Parameters\n        ----------\n        func : function\n            Function to apply to each column\n        axis : {0, 1, 'index', 'columns'}\n        broadcast : bool, default False\n            For aggregation functions, return object of same size with values\n            propagated\n\n        Returns\n        -------\n        applied : Series or SparseDataFrame\n        \"\"\"\n        if not len(self.columns):\n            return self\n        axis = self._get_axis_number(axis)\n\n        if isinstance(func, np.ufunc):\n            new_series = {}\n            for k, v in compat.iteritems(self):\n                applied = func(v)\n                applied.fill_value = func(v.fill_value)\n                new_series[k] = applied\n            return self._constructor(\n                new_series, index=self.index, columns=self.columns,\n                default_fill_value=self._default_fill_value,\n                default_kind=self._default_kind).__finalize__(self)\n        else:\n            if not broadcast:\n                return self._apply_standard(func, axis, reduce=reduce)\n            else:\n                return self._apply_broadcast(func, axis)\n\n    def applymap(self, func):\n        \"\"\"\n        Apply a function to a DataFrame that is intended to operate\n        elementwise, i.e. like doing map(func, series) for each series in the\n        DataFrame\n\n        Parameters\n        ----------\n        func : function\n            Python function, returns a single value from a single value\n\n        Returns\n        -------\n        applied : DataFrame\n        \"\"\"\n        return self.apply(lambda x: lmap(func, x))\n\n\ndef to_manager(sdf, columns, index):\n    \"\"\" create and return the block manager from a dataframe of series,\n    columns, index\n    \"\"\"\n\n    # from BlockManager perspective\n    axes = [_ensure_index(columns), _ensure_index(index)]\n\n    return create_block_manager_from_arrays(\n        [sdf[c] for c in columns], columns, axes)\n\n\ndef stack_sparse_frame(frame):\n    \"\"\"\n    Only makes sense when fill_value is NaN\n    \"\"\"\n    lengths = [s.sp_index.npoints for _, s in compat.iteritems(frame)]\n    nobs = sum(lengths)\n\n    # this is pretty fast\n    minor_labels = np.repeat(np.arange(len(frame.columns)), lengths)\n\n    inds_to_concat = []\n    vals_to_concat = []\n    # TODO: Figure out whether this can be reached.\n    # I think this currently can't be reached because you can't build a\n    # SparseDataFrame with a non-np.NaN fill value (fails earlier).\n    for _, series in compat.iteritems(frame):\n        if not np.isnan(series.fill_value):\n            raise TypeError('This routine assumes NaN fill value')\n\n        int_index = series.sp_index.to_int_index()\n        inds_to_concat.append(int_index.indices)\n        vals_to_concat.append(series.sp_values)\n\n    major_labels = np.concatenate(inds_to_concat)\n    stacked_values = np.concatenate(vals_to_concat)\n    index = MultiIndex(levels=[frame.index, frame.columns],\n                       labels=[major_labels, minor_labels],\n                       verify_integrity=False)\n\n    lp = DataFrame(stacked_values.reshape((nobs, 1)), index=index,\n                   columns=['foo'])\n    return lp.sort_index(level=0)\n\n\ndef homogenize(series_dict):\n    \"\"\"\n    Conform a set of SparseSeries (with NaN fill_value) to a common SparseIndex\n    corresponding to the locations where they all have data\n\n    Parameters\n    ----------\n    series_dict : dict or DataFrame\n\n    Notes\n    -----\n    Using the dumbest algorithm I could think of. Should put some more thought\n    into this\n\n    Returns\n    -------\n    homogenized : dict of SparseSeries\n    \"\"\"\n    index = None\n\n    need_reindex = False\n\n    for _, series in compat.iteritems(series_dict):\n        if not np.isnan(series.fill_value):\n            raise TypeError('this method is only valid with NaN fill values')\n\n        if index is None:\n            index = series.sp_index\n        elif not series.sp_index.equals(index):\n            need_reindex = True\n            index = index.intersect(series.sp_index)\n\n    if need_reindex:\n        output = {}\n        for name, series in compat.iteritems(series_dict):\n            if not series.sp_index.equals(index):\n                series = series.sparse_reindex(index)\n\n            output[name] = series\n    else:\n        output = series_dict\n\n    return output\n\n\n# use unaccelerated ops for sparse objects\nops.add_flex_arithmetic_methods(SparseDataFrame, use_numexpr=False,\n                                **ops.frame_flex_funcs)\nops.add_special_arithmetic_methods(SparseDataFrame, use_numexpr=False,\n                                   **ops.frame_special_funcs)\n"
    },
    {
      "filename": "pandas/tests/reshape/test_reshape.py",
      "content": "# -*- coding: utf-8 -*-\n# pylint: disable-msg=W0612,E1101\n\nimport pytest\n\nfrom pandas import DataFrame, Series\nimport pandas as pd\n\nfrom numpy import nan\nimport numpy as np\n\nfrom pandas.util.testing import assert_frame_equal\n\nfrom pandas.core.reshape.reshape import (\n    melt, lreshape, get_dummies, wide_to_long)\nimport pandas.util.testing as tm\nfrom pandas.compat import range, u\n\n\nclass TestMelt(object):\n\n    def setup_method(self, method):\n        self.df = tm.makeTimeDataFrame()[:10]\n        self.df['id1'] = (self.df['A'] > 0).astype(np.int64)\n        self.df['id2'] = (self.df['B'] > 0).astype(np.int64)\n\n        self.var_name = 'var'\n        self.value_name = 'val'\n\n        self.df1 = pd.DataFrame([[1.067683, -1.110463, 0.20867\n                                  ], [-1.321405, 0.368915, -1.055342],\n                                 [-0.807333, 0.08298, -0.873361]])\n        self.df1.columns = [list('ABC'), list('abc')]\n        self.df1.columns.names = ['CAP', 'low']\n\n    def test_top_level_method(self):\n        result = melt(self.df)\n        assert result.columns.tolist() == ['variable', 'value']\n\n    def test_method_signatures(self):\n        tm.assert_frame_equal(self.df.melt(),\n                              melt(self.df))\n\n        tm.assert_frame_equal(self.df.melt(id_vars=['id1', 'id2'],\n                                           value_vars=['A', 'B']),\n                              melt(self.df,\n                                   id_vars=['id1', 'id2'],\n                                   value_vars=['A', 'B']))\n\n        tm.assert_frame_equal(self.df.melt(var_name=self.var_name,\n                                           value_name=self.value_name),\n                              melt(self.df,\n                                   var_name=self.var_name,\n                                   value_name=self.value_name))\n\n        tm.assert_frame_equal(self.df1.melt(col_level=0),\n                              melt(self.df1, col_level=0))\n\n    def test_default_col_names(self):\n        result = self.df.melt()\n        assert result.columns.tolist() == ['variable', 'value']\n\n        result1 = self.df.melt(id_vars=['id1'])\n        assert result1.columns.tolist() == ['id1', 'variable', 'value']\n\n        result2 = self.df.melt(id_vars=['id1', 'id2'])\n        assert result2.columns.tolist() == ['id1', 'id2', 'variable', 'value']\n\n    def test_value_vars(self):\n        result3 = self.df.melt(id_vars=['id1', 'id2'], value_vars='A')\n        assert len(result3) == 10\n\n        result4 = self.df.melt(id_vars=['id1', 'id2'], value_vars=['A', 'B'])\n        expected4 = DataFrame({'id1': self.df['id1'].tolist() * 2,\n                               'id2': self.df['id2'].tolist() * 2,\n                               'variable': ['A'] * 10 + ['B'] * 10,\n                               'value': (self.df['A'].tolist() +\n                                         self.df['B'].tolist())},\n                              columns=['id1', 'id2', 'variable', 'value'])\n        tm.assert_frame_equal(result4, expected4)\n\n    def test_value_vars_types(self):\n        # GH 15348\n        expected = DataFrame({'id1': self.df['id1'].tolist() * 2,\n                              'id2': self.df['id2'].tolist() * 2,\n                              'variable': ['A'] * 10 + ['B'] * 10,\n                              'value': (self.df['A'].tolist() +\n                                        self.df['B'].tolist())},\n                             columns=['id1', 'id2', 'variable', 'value'])\n\n        for type_ in (tuple, list, np.array):\n            result = self.df.melt(id_vars=['id1', 'id2'],\n                                  value_vars=type_(('A', 'B')))\n            tm.assert_frame_equal(result, expected)\n\n    def test_vars_work_with_multiindex(self):\n        expected = DataFrame({\n            ('A', 'a'): self.df1[('A', 'a')],\n            'CAP': ['B'] * len(self.df1),\n            'low': ['b'] * len(self.df1),\n            'value': self.df1[('B', 'b')],\n        }, columns=[('A', 'a'), 'CAP', 'low', 'value'])\n\n        result = self.df1.melt(id_vars=[('A', 'a')], value_vars=[('B', 'b')])\n        tm.assert_frame_equal(result, expected)\n\n    def test_tuple_vars_fail_with_multiindex(self):\n        # melt should fail with an informative error message if\n        # the columns have a MultiIndex and a tuple is passed\n        # for id_vars or value_vars.\n        tuple_a = ('A', 'a')\n        list_a = [tuple_a]\n        tuple_b = ('B', 'b')\n        list_b = [tuple_b]\n\n        for id_vars, value_vars in ((tuple_a, list_b), (list_a, tuple_b),\n                                    (tuple_a, tuple_b)):\n            with tm.assert_raises_regex(ValueError, r'MultiIndex'):\n                self.df1.melt(id_vars=id_vars, value_vars=value_vars)\n\n    def test_custom_var_name(self):\n        result5 = self.df.melt(var_name=self.var_name)\n        assert result5.columns.tolist() == ['var', 'value']\n\n        result6 = self.df.melt(id_vars=['id1'], var_name=self.var_name)\n        assert result6.columns.tolist() == ['id1', 'var', 'value']\n\n        result7 = self.df.melt(id_vars=['id1', 'id2'], var_name=self.var_name)\n        assert result7.columns.tolist() == ['id1', 'id2', 'var', 'value']\n\n        result8 = self.df.melt(id_vars=['id1', 'id2'], value_vars='A',\n                               var_name=self.var_name)\n        assert result8.columns.tolist() == ['id1', 'id2', 'var', 'value']\n\n        result9 = self.df.melt(id_vars=['id1', 'id2'], value_vars=['A', 'B'],\n                               var_name=self.var_name)\n        expected9 = DataFrame({'id1': self.df['id1'].tolist() * 2,\n                               'id2': self.df['id2'].tolist() * 2,\n                               self.var_name: ['A'] * 10 + ['B'] * 10,\n                               'value': (self.df['A'].tolist() +\n                                         self.df['B'].tolist())},\n                              columns=['id1', 'id2', self.var_name, 'value'])\n        tm.assert_frame_equal(result9, expected9)\n\n    def test_custom_value_name(self):\n        result10 = self.df.melt(value_name=self.value_name)\n        assert result10.columns.tolist() == ['variable', 'val']\n\n        result11 = self.df.melt(id_vars=['id1'], value_name=self.value_name)\n        assert result11.columns.tolist() == ['id1', 'variable', 'val']\n\n        result12 = self.df.melt(id_vars=['id1', 'id2'],\n                                value_name=self.value_name)\n        assert result12.columns.tolist() == ['id1', 'id2', 'variable', 'val']\n\n        result13 = self.df.melt(id_vars=['id1', 'id2'], value_vars='A',\n                                value_name=self.value_name)\n        assert result13.columns.tolist() == ['id1', 'id2', 'variable', 'val']\n\n        result14 = self.df.melt(id_vars=['id1', 'id2'], value_vars=['A', 'B'],\n                                value_name=self.value_name)\n        expected14 = DataFrame({'id1': self.df['id1'].tolist() * 2,\n                                'id2': self.df['id2'].tolist() * 2,\n                                'variable': ['A'] * 10 + ['B'] * 10,\n                                self.value_name: (self.df['A'].tolist() +\n                                                  self.df['B'].tolist())},\n                               columns=['id1', 'id2', 'variable',\n                                        self.value_name])\n        tm.assert_frame_equal(result14, expected14)\n\n    def test_custom_var_and_value_name(self):\n\n        result15 = self.df.melt(var_name=self.var_name,\n                                value_name=self.value_name)\n        assert result15.columns.tolist() == ['var', 'val']\n\n        result16 = self.df.melt(id_vars=['id1'], var_name=self.var_name,\n                                value_name=self.value_name)\n        assert result16.columns.tolist() == ['id1', 'var', 'val']\n\n        result17 = self.df.melt(id_vars=['id1', 'id2'],\n                                var_name=self.var_name,\n                                value_name=self.value_name)\n        assert result17.columns.tolist() == ['id1', 'id2', 'var', 'val']\n\n        result18 = self.df.melt(id_vars=['id1', 'id2'], value_vars='A',\n                                var_name=self.var_name,\n                                value_name=self.value_name)\n        assert result18.columns.tolist() == ['id1', 'id2', 'var', 'val']\n\n        result19 = self.df.melt(id_vars=['id1', 'id2'], value_vars=['A', 'B'],\n                                var_name=self.var_name,\n                                value_name=self.value_name)\n        expected19 = DataFrame({'id1': self.df['id1'].tolist() * 2,\n                                'id2': self.df['id2'].tolist() * 2,\n                                self.var_name: ['A'] * 10 + ['B'] * 10,\n                                self.value_name: (self.df['A'].tolist() +\n                                                  self.df['B'].tolist())},\n                               columns=['id1', 'id2', self.var_name,\n                                        self.value_name])\n        tm.assert_frame_equal(result19, expected19)\n\n        df20 = self.df.copy()\n        df20.columns.name = 'foo'\n        result20 = df20.melt()\n        assert result20.columns.tolist() == ['foo', 'value']\n\n    def test_col_level(self):\n        res1 = self.df1.melt(col_level=0)\n        res2 = self.df1.melt(col_level='CAP')\n        assert res1.columns.tolist() == ['CAP', 'value']\n        assert res2.columns.tolist() == ['CAP', 'value']\n\n    def test_multiindex(self):\n        res = self.df1.melt()\n        assert res.columns.tolist() == ['CAP', 'low', 'value']\n\n\nclass TestGetDummies(object):\n\n    sparse = False\n\n    def setup_method(self, method):\n        self.df = DataFrame({'A': ['a', 'b', 'a'],\n                             'B': ['b', 'b', 'c'],\n                             'C': [1, 2, 3]})\n\n    def test_basic(self):\n        s_list = list('abc')\n        s_series = Series(s_list)\n        s_series_index = Series(s_list, list('ABC'))\n\n        expected = DataFrame({'a': {0: 1,\n                                    1: 0,\n                                    2: 0},\n                              'b': {0: 0,\n                                    1: 1,\n                                    2: 0},\n                              'c': {0: 0,\n                                    1: 0,\n                                    2: 1}}, dtype=np.uint8)\n        assert_frame_equal(get_dummies(s_list, sparse=self.sparse), expected)\n        assert_frame_equal(get_dummies(s_series, sparse=self.sparse), expected)\n\n        expected.index = list('ABC')\n        assert_frame_equal(\n            get_dummies(s_series_index, sparse=self.sparse), expected)\n\n    def test_basic_types(self):\n        # GH 10531\n        s_list = list('abc')\n        s_series = Series(s_list)\n        s_df = DataFrame({'a': [0, 1, 0, 1, 2],\n                          'b': ['A', 'A', 'B', 'C', 'C'],\n                          'c': [2, 3, 3, 3, 2]})\n\n        expected = DataFrame({'a': [1, 0, 0],\n                              'b': [0, 1, 0],\n                              'c': [0, 0, 1]},\n                             dtype='uint8',\n                             columns=list('abc'))\n        if not self.sparse:\n            compare = tm.assert_frame_equal\n        else:\n            expected = expected.to_sparse(fill_value=0, kind='integer')\n            compare = tm.assert_sp_frame_equal\n\n        result = get_dummies(s_list, sparse=self.sparse)\n        compare(result, expected)\n\n        result = get_dummies(s_series, sparse=self.sparse)\n        compare(result, expected)\n\n        result = get_dummies(s_df, sparse=self.sparse, columns=s_df.columns)\n        tm.assert_series_equal(result.get_dtype_counts(),\n                               Series({'uint8': 8}))\n\n        result = get_dummies(s_df, sparse=self.sparse, columns=['a'])\n        expected = Series({'uint8': 3, 'int64': 1, 'object': 1}).sort_values()\n        tm.assert_series_equal(result.get_dtype_counts().sort_values(),\n                               expected)\n\n    def test_just_na(self):\n        just_na_list = [np.nan]\n        just_na_series = Series(just_na_list)\n        just_na_series_index = Series(just_na_list, index=['A'])\n\n        res_list = get_dummies(just_na_list, sparse=self.sparse)\n        res_series = get_dummies(just_na_series, sparse=self.sparse)\n        res_series_index = get_dummies(just_na_series_index,\n                                       sparse=self.sparse)\n\n        assert res_list.empty\n        assert res_series.empty\n        assert res_series_index.empty\n\n        assert res_list.index.tolist() == [0]\n        assert res_series.index.tolist() == [0]\n        assert res_series_index.index.tolist() == ['A']\n\n    def test_include_na(self):\n        s = ['a', 'b', np.nan]\n        res = get_dummies(s, sparse=self.sparse)\n        exp = DataFrame({'a': {0: 1, 1: 0, 2: 0},\n                         'b': {0: 0, 1: 1, 2: 0}}, dtype=np.uint8)\n        assert_frame_equal(res, exp)\n\n        # Sparse dataframes do not allow nan labelled columns, see #GH8822\n        res_na = get_dummies(s, dummy_na=True, sparse=self.sparse)\n        exp_na = DataFrame({nan: {0: 0, 1: 0, 2: 1},\n                            'a': {0: 1, 1: 0, 2: 0},\n                            'b': {0: 0, 1: 1, 2: 0}},\n                           dtype=np.uint8)\n        exp_na = exp_na.reindex_axis(['a', 'b', nan], 1)\n        # hack (NaN handling in assert_index_equal)\n        exp_na.columns = res_na.columns\n        assert_frame_equal(res_na, exp_na)\n\n        res_just_na = get_dummies([nan], dummy_na=True, sparse=self.sparse)\n        exp_just_na = DataFrame(Series(1, index=[0]), columns=[nan],\n                                dtype=np.uint8)\n        tm.assert_numpy_array_equal(res_just_na.values, exp_just_na.values)\n\n    def test_unicode(self\n                     ):  # See GH 6885 - get_dummies chokes on unicode values\n        import unicodedata\n        e = 'e'\n        eacute = unicodedata.lookup('LATIN SMALL LETTER E WITH ACUTE')\n        s = [e, eacute, eacute]\n        res = get_dummies(s, prefix='letter', sparse=self.sparse)\n        exp = DataFrame({'letter_e': {0: 1,\n                                      1: 0,\n                                      2: 0},\n                         u('letter_%s') % eacute: {0: 0,\n                                                   1: 1,\n                                                   2: 1}},\n                        dtype=np.uint8)\n        assert_frame_equal(res, exp)\n\n    def test_dataframe_dummies_all_obj(self):\n        df = self.df[['A', 'B']]\n        result = get_dummies(df, sparse=self.sparse)\n        expected = DataFrame({'A_a': [1, 0, 1],\n                              'A_b': [0, 1, 0],\n                              'B_b': [1, 1, 0],\n                              'B_c': [0, 0, 1]}, dtype=np.uint8)\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_mix_default(self):\n        df = self.df\n        result = get_dummies(df, sparse=self.sparse)\n        expected = DataFrame({'C': [1, 2, 3],\n                              'A_a': [1, 0, 1],\n                              'A_b': [0, 1, 0],\n                              'B_b': [1, 1, 0],\n                              'B_c': [0, 0, 1]})\n        cols = ['A_a', 'A_b', 'B_b', 'B_c']\n        expected[cols] = expected[cols].astype(np.uint8)\n        expected = expected[['C', 'A_a', 'A_b', 'B_b', 'B_c']]\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_prefix_list(self):\n        prefixes = ['from_A', 'from_B']\n        df = DataFrame({'A': ['a', 'b', 'a'],\n                        'B': ['b', 'b', 'c'],\n                        'C': [1, 2, 3]})\n        result = get_dummies(df, prefix=prefixes, sparse=self.sparse)\n        expected = DataFrame({'C': [1, 2, 3],\n                              'from_A_a': [1, 0, 1],\n                              'from_A_b': [0, 1, 0],\n                              'from_B_b': [1, 1, 0],\n                              'from_B_c': [0, 0, 1]})\n        cols = expected.columns[1:]\n        expected[cols] = expected[cols].astype(np.uint8)\n        expected = expected[['C', 'from_A_a', 'from_A_b', 'from_B_b',\n                             'from_B_c']]\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_prefix_str(self):\n        # not that you should do this...\n        df = self.df\n        result = get_dummies(df, prefix='bad', sparse=self.sparse)\n        expected = DataFrame([[1, 1, 0, 1, 0],\n                              [2, 0, 1, 1, 0],\n                              [3, 1, 0, 0, 1]],\n                             columns=['C', 'bad_a', 'bad_b', 'bad_b', 'bad_c'],\n                             dtype=np.uint8)\n        expected = expected.astype({\"C\": np.int64})\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_subset(self):\n        df = self.df\n        result = get_dummies(df, prefix=['from_A'], columns=['A'],\n                             sparse=self.sparse)\n        expected = DataFrame({'from_A_a': [1, 0, 1],\n                              'from_A_b': [0, 1, 0],\n                              'B': ['b', 'b', 'c'],\n                              'C': [1, 2, 3]})\n        cols = ['from_A_a', 'from_A_b']\n        expected[cols] = expected[cols].astype(np.uint8)\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_prefix_sep(self):\n        df = self.df\n        result = get_dummies(df, prefix_sep='..', sparse=self.sparse)\n        expected = DataFrame({'C': [1, 2, 3],\n                              'A..a': [1, 0, 1],\n                              'A..b': [0, 1, 0],\n                              'B..b': [1, 1, 0],\n                              'B..c': [0, 0, 1]})\n        expected = expected[['C', 'A..a', 'A..b', 'B..b', 'B..c']]\n        cols = expected.columns[1:]\n        expected[cols] = expected[cols].astype(np.uint8)\n        assert_frame_equal(result, expected)\n\n        result = get_dummies(df, prefix_sep=['..', '__'], sparse=self.sparse)\n        expected = expected.rename(columns={'B..b': 'B__b', 'B..c': 'B__c'})\n        assert_frame_equal(result, expected)\n\n        result = get_dummies(df, prefix_sep={'A': '..',\n                                             'B': '__'}, sparse=self.sparse)\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_prefix_bad_length(self):\n        with pytest.raises(ValueError):\n            get_dummies(self.df, prefix=['too few'], sparse=self.sparse)\n\n    def test_dataframe_dummies_prefix_sep_bad_length(self):\n        with pytest.raises(ValueError):\n            get_dummies(self.df, prefix_sep=['bad'], sparse=self.sparse)\n\n    def test_dataframe_dummies_prefix_dict(self):\n        prefixes = {'A': 'from_A', 'B': 'from_B'}\n        df = DataFrame({'A': ['a', 'b', 'a'],\n                        'B': ['b', 'b', 'c'],\n                        'C': [1, 2, 3]})\n        result = get_dummies(df, prefix=prefixes, sparse=self.sparse)\n        expected = DataFrame({'from_A_a': [1, 0, 1],\n                              'from_A_b': [0, 1, 0],\n                              'from_B_b': [1, 1, 0],\n                              'from_B_c': [0, 0, 1],\n                              'C': [1, 2, 3]})\n        cols = ['from_A_a', 'from_A_b', 'from_B_b', 'from_B_c']\n        expected[cols] = expected[cols].astype(np.uint8)\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_with_na(self):\n        df = self.df\n        df.loc[3, :] = [np.nan, np.nan, np.nan]\n        result = get_dummies(df, dummy_na=True, sparse=self.sparse)\n        expected = DataFrame({'C': [1, 2, 3, np.nan],\n                              'A_a': [1, 0, 1, 0],\n                              'A_b': [0, 1, 0, 0],\n                              'A_nan': [0, 0, 0, 1],\n                              'B_b': [1, 1, 0, 0],\n                              'B_c': [0, 0, 1, 0],\n                              'B_nan': [0, 0, 0, 1]})\n        cols = ['A_a', 'A_b', 'A_nan', 'B_b', 'B_c', 'B_nan']\n        expected[cols] = expected[cols].astype(np.uint8)\n        expected = expected[['C', 'A_a', 'A_b', 'A_nan',\n                             'B_b', 'B_c', 'B_nan']]\n        assert_frame_equal(result, expected)\n\n        result = get_dummies(df, dummy_na=False, sparse=self.sparse)\n        expected = expected[['C', 'A_a', 'A_b', 'B_b', 'B_c']]\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_with_categorical(self):\n        df = self.df\n        df['cat'] = pd.Categorical(['x', 'y', 'y'])\n        result = get_dummies(df, sparse=self.sparse)\n        expected = DataFrame({'C': [1, 2, 3],\n                              'A_a': [1, 0, 1],\n                              'A_b': [0, 1, 0],\n                              'B_b': [1, 1, 0],\n                              'B_c': [0, 0, 1],\n                              'cat_x': [1, 0, 0],\n                              'cat_y': [0, 1, 1]})\n        cols = ['A_a', 'A_b', 'B_b', 'B_c', 'cat_x', 'cat_y']\n        expected[cols] = expected[cols].astype(np.uint8)\n        expected = expected[['C', 'A_a', 'A_b', 'B_b', 'B_c',\n                             'cat_x', 'cat_y']]\n        assert_frame_equal(result, expected)\n\n    def test_basic_drop_first(self):\n        # GH12402 Add a new parameter `drop_first` to avoid collinearity\n        # Basic case\n        s_list = list('abc')\n        s_series = Series(s_list)\n        s_series_index = Series(s_list, list('ABC'))\n\n        expected = DataFrame({'b': {0: 0,\n                                    1: 1,\n                                    2: 0},\n                              'c': {0: 0,\n                                    1: 0,\n                                    2: 1}}, dtype=np.uint8)\n\n        result = get_dummies(s_list, sparse=self.sparse, drop_first=True)\n        assert_frame_equal(result, expected)\n\n        result = get_dummies(s_series, sparse=self.sparse, drop_first=True)\n        assert_frame_equal(result, expected)\n\n        expected.index = list('ABC')\n        result = get_dummies(s_series_index, sparse=self.sparse,\n                             drop_first=True)\n        assert_frame_equal(result, expected)\n\n    def test_basic_drop_first_one_level(self):\n        # Test the case that categorical variable only has one level.\n        s_list = list('aaa')\n        s_series = Series(s_list)\n        s_series_index = Series(s_list, list('ABC'))\n\n        expected = DataFrame(index=np.arange(3))\n\n        result = get_dummies(s_list, sparse=self.sparse, drop_first=True)\n        assert_frame_equal(result, expected)\n\n        result = get_dummies(s_series, sparse=self.sparse, drop_first=True)\n        assert_frame_equal(result, expected)\n\n        expected = DataFrame(index=list('ABC'))\n        result = get_dummies(s_series_index, sparse=self.sparse,\n                             drop_first=True)\n        assert_frame_equal(result, expected)\n\n    def test_basic_drop_first_NA(self):\n        # Test NA hadling together with drop_first\n        s_NA = ['a', 'b', np.nan]\n        res = get_dummies(s_NA, sparse=self.sparse, drop_first=True)\n        exp = DataFrame({'b': {0: 0,\n                               1: 1,\n                               2: 0}}, dtype=np.uint8)\n        assert_frame_equal(res, exp)\n\n        res_na = get_dummies(s_NA, dummy_na=True, sparse=self.sparse,\n                             drop_first=True)\n        exp_na = DataFrame({'b': {0: 0,\n                                  1: 1,\n                                  2: 0},\n                            nan: {0: 0,\n                                  1: 0,\n                                  2: 1}}, dtype=np.uint8).reindex_axis(\n                                      ['b', nan], 1)\n        assert_frame_equal(res_na, exp_na)\n\n        res_just_na = get_dummies([nan], dummy_na=True, sparse=self.sparse,\n                                  drop_first=True)\n        exp_just_na = DataFrame(index=np.arange(1))\n        assert_frame_equal(res_just_na, exp_just_na)\n\n    def test_dataframe_dummies_drop_first(self):\n        df = self.df[['A', 'B']]\n        result = get_dummies(df, sparse=self.sparse, drop_first=True)\n        expected = DataFrame({'A_b': [0, 1, 0],\n                              'B_c': [0, 0, 1]}, dtype=np.uint8)\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_drop_first_with_categorical(self):\n        df = self.df\n        df['cat'] = pd.Categorical(['x', 'y', 'y'])\n        result = get_dummies(df, sparse=self.sparse, drop_first=True)\n        expected = DataFrame({'C': [1, 2, 3],\n                              'A_b': [0, 1, 0],\n                              'B_c': [0, 0, 1],\n                              'cat_y': [0, 1, 1]})\n        cols = ['A_b', 'B_c', 'cat_y']\n        expected[cols] = expected[cols].astype(np.uint8)\n        expected = expected[['C', 'A_b', 'B_c', 'cat_y']]\n        assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_drop_first_with_na(self):\n        df = self.df\n        df.loc[3, :] = [np.nan, np.nan, np.nan]\n        result = get_dummies(df, dummy_na=True, sparse=self.sparse,\n                             drop_first=True)\n        expected = DataFrame({'C': [1, 2, 3, np.nan],\n                              'A_b': [0, 1, 0, 0],\n                              'A_nan': [0, 0, 0, 1],\n                              'B_c': [0, 0, 1, 0],\n                              'B_nan': [0, 0, 0, 1]})\n        cols = ['A_b', 'A_nan', 'B_c', 'B_nan']\n        expected[cols] = expected[cols].astype(np.uint8)\n\n        expected = expected[['C', 'A_b', 'A_nan', 'B_c', 'B_nan']]\n        assert_frame_equal(result, expected)\n\n        result = get_dummies(df, dummy_na=False, sparse=self.sparse,\n                             drop_first=True)\n        expected = expected[['C', 'A_b', 'B_c']]\n        assert_frame_equal(result, expected)\n\n    def test_int_int(self):\n        data = Series([1, 2, 1])\n        result = pd.get_dummies(data)\n        expected = DataFrame([[1, 0], [0, 1], [1, 0]], columns=[1, 2],\n                             dtype=np.uint8)\n        tm.assert_frame_equal(result, expected)\n\n        data = Series(pd.Categorical(['a', 'b', 'a']))\n        result = pd.get_dummies(data)\n        expected = DataFrame([[1, 0], [0, 1], [1, 0]],\n                             columns=pd.Categorical(['a', 'b']),\n                             dtype=np.uint8)\n        tm.assert_frame_equal(result, expected)\n\n    def test_int_df(self):\n        data = DataFrame(\n            {'A': [1, 2, 1],\n             'B': pd.Categorical(['a', 'b', 'a']),\n             'C': [1, 2, 1],\n             'D': [1., 2., 1.]\n             }\n        )\n        columns = ['C', 'D', 'A_1', 'A_2', 'B_a', 'B_b']\n        expected = DataFrame([\n            [1, 1., 1, 0, 1, 0],\n            [2, 2., 0, 1, 0, 1],\n            [1, 1., 1, 0, 1, 0]\n        ], columns=columns)\n        expected[columns[2:]] = expected[columns[2:]].astype(np.uint8)\n        result = pd.get_dummies(data, columns=['A', 'B'])\n        tm.assert_frame_equal(result, expected)\n\n    def test_dataframe_dummies_preserve_categorical_dtype(self):\n        # GH13854\n        for ordered in [False, True]:\n            cat = pd.Categorical(list(\"xy\"), categories=list(\"xyz\"),\n                                 ordered=ordered)\n            result = get_dummies(cat)\n\n            data = np.array([[1, 0, 0], [0, 1, 0]], dtype=np.uint8)\n            cols = pd.CategoricalIndex(cat.categories,\n                                       categories=cat.categories,\n                                       ordered=ordered)\n            expected = DataFrame(data, columns=cols)\n\n            tm.assert_frame_equal(result, expected)\n\n\nclass TestGetDummiesSparse(TestGetDummies):\n    sparse = True\n\n    @pytest.mark.xfail(reason='nan in index is problematic (GH 16894)')\n    def test_include_na(self):\n        super(TestGetDummiesSparse, self).test_include_na()\n\n\nclass TestMakeAxisDummies(object):\n\n    def test_preserve_categorical_dtype(self):\n        # GH13854\n        for ordered in [False, True]:\n            cidx = pd.CategoricalIndex(list(\"xyz\"), ordered=ordered)\n            midx = pd.MultiIndex(levels=[['a'], cidx],\n                                 labels=[[0, 0], [0, 1]])\n            df = DataFrame([[10, 11]], index=midx)\n\n            expected = DataFrame([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]],\n                                 index=midx, columns=cidx)\n\n            from pandas.core.reshape.reshape import make_axis_dummies\n            result = make_axis_dummies(df)\n            tm.assert_frame_equal(result, expected)\n\n            result = make_axis_dummies(df, transform=lambda x: x)\n            tm.assert_frame_equal(result, expected)\n\n\nclass TestLreshape(object):\n\n    def test_pairs(self):\n        data = {'birthdt': ['08jan2009', '20dec2008', '30dec2008', '21dec2008',\n                            '11jan2009'],\n                'birthwt': [1766, 3301, 1454, 3139, 4133],\n                'id': [101, 102, 103, 104, 105],\n                'sex': ['Male', 'Female', 'Female', 'Female', 'Female'],\n                'visitdt1': ['11jan2009', '22dec2008', '04jan2009',\n                             '29dec2008', '20jan2009'],\n                'visitdt2':\n                ['21jan2009', nan, '22jan2009', '31dec2008', '03feb2009'],\n                'visitdt3': ['05feb2009', nan, nan, '02jan2009', '15feb2009'],\n                'wt1': [1823, 3338, 1549, 3298, 4306],\n                'wt2': [2011.0, nan, 1892.0, 3338.0, 4575.0],\n                'wt3': [2293.0, nan, nan, 3377.0, 4805.0]}\n\n        df = DataFrame(data)\n\n        spec = {'visitdt': ['visitdt%d' % i for i in range(1, 4)],\n                'wt': ['wt%d' % i for i in range(1, 4)]}\n        result = lreshape(df, spec)\n\n        exp_data = {'birthdt':\n                    ['08jan2009', '20dec2008', '30dec2008', '21dec2008',\n                     '11jan2009', '08jan2009', '30dec2008', '21dec2008',\n                     '11jan2009', '08jan2009', '21dec2008', '11jan2009'],\n                    'birthwt': [1766, 3301, 1454, 3139, 4133, 1766, 1454, 3139,\n                                4133, 1766, 3139, 4133],\n                    'id': [101, 102, 103, 104, 105, 101, 103, 104, 105, 101,\n                           104, 105],\n                    'sex': ['Male', 'Female', 'Female', 'Female', 'Female',\n                            'Male', 'Female', 'Female', 'Female', 'Male',\n                            'Female', 'Female'],\n                    'visitdt': ['11jan2009', '22dec2008', '04jan2009',\n                                '29dec2008', '20jan2009', '21jan2009',\n                                '22jan2009', '31dec2008', '03feb2009',\n                                '05feb2009', '02jan2009', '15feb2009'],\n                    'wt': [1823.0, 3338.0, 1549.0, 3298.0, 4306.0, 2011.0,\n                           1892.0, 3338.0, 4575.0, 2293.0, 3377.0, 4805.0]}\n        exp = DataFrame(exp_data, columns=result.columns)\n        tm.assert_frame_equal(result, exp)\n\n        result = lreshape(df, spec, dropna=False)\n        exp_data = {'birthdt':\n                    ['08jan2009', '20dec2008', '30dec2008', '21dec2008',\n                     '11jan2009', '08jan2009', '20dec2008', '30dec2008',\n                     '21dec2008', '11jan2009', '08jan2009', '20dec2008',\n                     '30dec2008', '21dec2008', '11jan2009'],\n                    'birthwt': [1766, 3301, 1454, 3139, 4133, 1766, 3301, 1454,\n                                3139, 4133, 1766, 3301, 1454, 3139, 4133],\n                    'id': [101, 102, 103, 104, 105, 101, 102, 103, 104, 105,\n                           101, 102, 103, 104, 105],\n                    'sex': ['Male', 'Female', 'Female', 'Female', 'Female',\n                            'Male', 'Female', 'Female', 'Female', 'Female',\n                            'Male', 'Female', 'Female', 'Female', 'Female'],\n                    'visitdt': ['11jan2009', '22dec2008', '04jan2009',\n                                '29dec2008', '20jan2009', '21jan2009', nan,\n                                '22jan2009', '31dec2008', '03feb2009',\n                                '05feb2009', nan, nan, '02jan2009',\n                                '15feb2009'],\n                    'wt': [1823.0, 3338.0, 1549.0, 3298.0, 4306.0, 2011.0, nan,\n                           1892.0, 3338.0, 4575.0, 2293.0, nan, nan, 3377.0,\n                           4805.0]}\n        exp = DataFrame(exp_data, columns=result.columns)\n        tm.assert_frame_equal(result, exp)\n\n        spec = {'visitdt': ['visitdt%d' % i for i in range(1, 3)],\n                'wt': ['wt%d' % i for i in range(1, 4)]}\n        pytest.raises(ValueError, lreshape, df, spec)\n\n\nclass TestWideToLong(object):\n\n    def test_simple(self):\n        np.random.seed(123)\n        x = np.random.randn(3)\n        df = pd.DataFrame({\"A1970\": {0: \"a\",\n                                     1: \"b\",\n                                     2: \"c\"},\n                           \"A1980\": {0: \"d\",\n                                     1: \"e\",\n                                     2: \"f\"},\n                           \"B1970\": {0: 2.5,\n                                     1: 1.2,\n                                     2: .7},\n                           \"B1980\": {0: 3.2,\n                                     1: 1.3,\n                                     2: .1},\n                           \"X\": dict(zip(\n                               range(3), x))})\n        df[\"id\"] = df.index\n        exp_data = {\"X\": x.tolist() + x.tolist(),\n                    \"A\": ['a', 'b', 'c', 'd', 'e', 'f'],\n                    \"B\": [2.5, 1.2, 0.7, 3.2, 1.3, 0.1],\n                    \"year\": ['1970', '1970', '1970', '1980', '1980', '1980'],\n                    \"id\": [0, 1, 2, 0, 1, 2]}\n        exp_frame = DataFrame(exp_data)\n        exp_frame = exp_frame.set_index(['id', 'year'])[[\"X\", \"A\", \"B\"]]\n        long_frame = wide_to_long(df, [\"A\", \"B\"], i=\"id\", j=\"year\")\n        tm.assert_frame_equal(long_frame, exp_frame)\n\n    def test_stubs(self):\n        # GH9204\n        df = pd.DataFrame([[0, 1, 2, 3, 8], [4, 5, 6, 7, 9]])\n        df.columns = ['id', 'inc1', 'inc2', 'edu1', 'edu2']\n        stubs = ['inc', 'edu']\n\n        # TODO: unused?\n        df_long = pd.wide_to_long(df, stubs, i='id', j='age')  # noqa\n\n        assert stubs == ['inc', 'edu']\n\n    def test_separating_character(self):\n        # GH14779\n        np.random.seed(123)\n        x = np.random.randn(3)\n        df = pd.DataFrame({\"A.1970\": {0: \"a\",\n                                      1: \"b\",\n                                      2: \"c\"},\n                           \"A.1980\": {0: \"d\",\n                                      1: \"e\",\n                                      2: \"f\"},\n                           \"B.1970\": {0: 2.5,\n                                      1: 1.2,\n                                      2: .7},\n                           \"B.1980\": {0: 3.2,\n                                      1: 1.3,\n                                      2: .1},\n                           \"X\": dict(zip(\n                               range(3), x))})\n        df[\"id\"] = df.index\n        exp_data = {\"X\": x.tolist() + x.tolist(),\n                    \"A\": ['a', 'b', 'c', 'd', 'e', 'f'],\n                    \"B\": [2.5, 1.2, 0.7, 3.2, 1.3, 0.1],\n                    \"year\": ['1970', '1970', '1970', '1980', '1980', '1980'],\n                    \"id\": [0, 1, 2, 0, 1, 2]}\n        exp_frame = DataFrame(exp_data)\n        exp_frame = exp_frame.set_index(['id', 'year'])[[\"X\", \"A\", \"B\"]]\n        long_frame = wide_to_long(df, [\"A\", \"B\"], i=\"id\", j=\"year\", sep=\".\")\n        tm.assert_frame_equal(long_frame, exp_frame)\n\n    def test_escapable_characters(self):\n        np.random.seed(123)\n        x = np.random.randn(3)\n        df = pd.DataFrame({\"A(quarterly)1970\": {0: \"a\",\n                                                1: \"b\",\n                                                2: \"c\"},\n                           \"A(quarterly)1980\": {0: \"d\",\n                                                1: \"e\",\n                                                2: \"f\"},\n                           \"B(quarterly)1970\": {0: 2.5,\n                                                1: 1.2,\n                                                2: .7},\n                           \"B(quarterly)1980\": {0: 3.2,\n                                                1: 1.3,\n                                                2: .1},\n                           \"X\": dict(zip(\n                               range(3), x))})\n        df[\"id\"] = df.index\n        exp_data = {\"X\": x.tolist() + x.tolist(),\n                    \"A(quarterly)\": ['a', 'b', 'c', 'd', 'e', 'f'],\n                    \"B(quarterly)\": [2.5, 1.2, 0.7, 3.2, 1.3, 0.1],\n                    \"year\": ['1970', '1970', '1970', '1980', '1980', '1980'],\n                    \"id\": [0, 1, 2, 0, 1, 2]}\n        exp_frame = DataFrame(exp_data)\n        exp_frame = exp_frame.set_index(\n            ['id', 'year'])[[\"X\", \"A(quarterly)\", \"B(quarterly)\"]]\n        long_frame = wide_to_long(df, [\"A(quarterly)\", \"B(quarterly)\"],\n                                  i=\"id\", j=\"year\")\n        tm.assert_frame_equal(long_frame, exp_frame)\n\n    def test_unbalanced(self):\n        # test that we can have a varying amount of time variables\n        df = pd.DataFrame({'A2010': [1.0, 2.0],\n                           'A2011': [3.0, 4.0],\n                           'B2010': [5.0, 6.0],\n                           'X': ['X1', 'X2']})\n        df['id'] = df.index\n        exp_data = {'X': ['X1', 'X1', 'X2', 'X2'],\n                    'A': [1.0, 3.0, 2.0, 4.0],\n                    'B': [5.0, np.nan, 6.0, np.nan],\n                    'id': [0, 0, 1, 1],\n                    'year': ['2010', '2011', '2010', '2011']}\n        exp_frame = pd.DataFrame(exp_data)\n        exp_frame = exp_frame.set_index(['id', 'year'])[[\"X\", \"A\", \"B\"]]\n        long_frame = wide_to_long(df, ['A', 'B'], i='id', j='year')\n        tm.assert_frame_equal(long_frame, exp_frame)\n\n    def test_character_overlap(self):\n        # Test we handle overlapping characters in both id_vars and value_vars\n        df = pd.DataFrame({\n            'A11': ['a11', 'a22', 'a33'],\n            'A12': ['a21', 'a22', 'a23'],\n            'B11': ['b11', 'b12', 'b13'],\n            'B12': ['b21', 'b22', 'b23'],\n            'BB11': [1, 2, 3],\n            'BB12': [4, 5, 6],\n            'BBBX': [91, 92, 93],\n            'BBBZ': [91, 92, 93]\n        })\n        df['id'] = df.index\n        exp_frame = pd.DataFrame({\n            'BBBX': [91, 92, 93, 91, 92, 93],\n            'BBBZ': [91, 92, 93, 91, 92, 93],\n            'A': ['a11', 'a22', 'a33', 'a21', 'a22', 'a23'],\n            'B': ['b11', 'b12', 'b13', 'b21', 'b22', 'b23'],\n            'BB': [1, 2, 3, 4, 5, 6],\n            'id': [0, 1, 2, 0, 1, 2],\n            'year': ['11', '11', '11', '12', '12', '12']})\n        exp_frame = exp_frame.set_index(['id', 'year'])[\n            ['BBBX', 'BBBZ', 'A', 'B', 'BB']]\n        long_frame = wide_to_long(df, ['A', 'B', 'BB'], i='id', j='year')\n        tm.assert_frame_equal(long_frame.sort_index(axis=1),\n                              exp_frame.sort_index(axis=1))\n\n    def test_invalid_separator(self):\n        # if an invalid separator is supplied a empty data frame is returned\n        sep = 'nope!'\n        df = pd.DataFrame({'A2010': [1.0, 2.0],\n                           'A2011': [3.0, 4.0],\n                           'B2010': [5.0, 6.0],\n                           'X': ['X1', 'X2']})\n        df['id'] = df.index\n        exp_data = {'X': '',\n                    'A2010': [],\n                    'A2011': [],\n                    'B2010': [],\n                    'id': [],\n                    'year': [],\n                    'A': [],\n                    'B': []}\n        exp_frame = pd.DataFrame(exp_data)\n        exp_frame = exp_frame.set_index(['id', 'year'])[[\n            'X', 'A2010', 'A2011', 'B2010', 'A', 'B']]\n        exp_frame.index.set_levels([[0, 1], []], inplace=True)\n        long_frame = wide_to_long(df, ['A', 'B'], i='id', j='year', sep=sep)\n        tm.assert_frame_equal(long_frame.sort_index(axis=1),\n                              exp_frame.sort_index(axis=1))\n\n    def test_num_string_disambiguation(self):\n        # Test that we can disambiguate number value_vars from\n        # string value_vars\n        df = pd.DataFrame({\n            'A11': ['a11', 'a22', 'a33'],\n            'A12': ['a21', 'a22', 'a23'],\n            'B11': ['b11', 'b12', 'b13'],\n            'B12': ['b21', 'b22', 'b23'],\n            'BB11': [1, 2, 3],\n            'BB12': [4, 5, 6],\n            'Arating': [91, 92, 93],\n            'Arating_old': [91, 92, 93]\n        })\n        df['id'] = df.index\n        exp_frame = pd.DataFrame({\n            'Arating': [91, 92, 93, 91, 92, 93],\n            'Arating_old': [91, 92, 93, 91, 92, 93],\n            'A': ['a11', 'a22', 'a33', 'a21', 'a22', 'a23'],\n            'B': ['b11', 'b12', 'b13', 'b21', 'b22', 'b23'],\n            'BB': [1, 2, 3, 4, 5, 6],\n            'id': [0, 1, 2, 0, 1, 2],\n            'year': ['11', '11', '11', '12', '12', '12']})\n        exp_frame = exp_frame.set_index(['id', 'year'])[\n            ['Arating', 'Arating_old', 'A', 'B', 'BB']]\n        long_frame = wide_to_long(df, ['A', 'B', 'BB'], i='id', j='year')\n        tm.assert_frame_equal(long_frame.sort_index(axis=1),\n                              exp_frame.sort_index(axis=1))\n\n    def test_invalid_suffixtype(self):\n        # If all stubs names end with a string, but a numeric suffix is\n        # assumed,  an empty data frame is returned\n        df = pd.DataFrame({'Aone': [1.0, 2.0],\n                           'Atwo': [3.0, 4.0],\n                           'Bone': [5.0, 6.0],\n                           'X': ['X1', 'X2']})\n        df['id'] = df.index\n        exp_data = {'X': '',\n                    'Aone': [],\n                    'Atwo': [],\n                    'Bone': [],\n                    'id': [],\n                    'year': [],\n                    'A': [],\n                    'B': []}\n        exp_frame = pd.DataFrame(exp_data)\n        exp_frame = exp_frame.set_index(['id', 'year'])[[\n            'X', 'Aone', 'Atwo', 'Bone', 'A', 'B']]\n        exp_frame.index.set_levels([[0, 1], []], inplace=True)\n        long_frame = wide_to_long(df, ['A', 'B'], i='id', j='year')\n        tm.assert_frame_equal(long_frame.sort_index(axis=1),\n                              exp_frame.sort_index(axis=1))\n\n    def test_multiple_id_columns(self):\n        # Taken from http://www.ats.ucla.edu/stat/stata/modules/reshapel.htm\n        df = pd.DataFrame({\n            'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n            'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n            'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n            'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n        })\n        exp_frame = pd.DataFrame({\n            'ht': [2.8, 3.4, 2.9, 3.8, 2.2, 2.9, 2.0, 3.2, 1.8,\n                   2.8, 1.9, 2.4, 2.2, 3.3, 2.3, 3.4, 2.1, 2.9],\n            'famid': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3],\n            'birth': [1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3],\n            'age': ['1', '2', '1', '2', '1', '2', '1', '2', '1',\n                    '2', '1', '2', '1', '2', '1', '2', '1', '2']\n        })\n        exp_frame = exp_frame.set_index(['famid', 'birth', 'age'])[['ht']]\n        long_frame = wide_to_long(df, 'ht', i=['famid', 'birth'], j='age')\n        tm.assert_frame_equal(long_frame, exp_frame)\n\n    def test_non_unique_idvars(self):\n        # GH16382\n        # Raise an error message if non unique id vars (i) are passed\n        df = pd.DataFrame({\n            'A_A1': [1, 2, 3, 4, 5],\n            'B_B1': [1, 2, 3, 4, 5],\n            'x': [1, 1, 1, 1, 1]\n        })\n        with pytest.raises(ValueError):\n            wide_to_long(df, ['A_A', 'B_B'], i='x', j='colname')\n"
    },
    {
      "filename": "pandas/tests/sparse/test_frame.py",
      "content": "# pylint: disable-msg=E1101,W0612\n\nimport operator\n\nimport pytest\nfrom warnings import catch_warnings\nfrom numpy import nan\nimport numpy as np\nimport pandas as pd\n\nfrom pandas import Series, DataFrame, bdate_range, Panel\nfrom pandas.core.dtypes.common import (\n    is_bool_dtype,\n    is_float_dtype,\n    is_object_dtype,\n    is_float)\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.tseries.offsets import BDay\nfrom pandas.util import testing as tm\nfrom pandas.compat import lrange\nfrom pandas import compat\nfrom pandas.core.sparse import frame as spf\n\nfrom pandas._libs.sparse import BlockIndex, IntIndex\nfrom pandas.core.sparse.api import SparseSeries, SparseDataFrame, SparseArray\nfrom pandas.tests.frame.test_api import SharedWithSparse\n\n\nclass TestSparseDataFrame(SharedWithSparse):\n    klass = SparseDataFrame\n\n    def setup_method(self, method):\n        self.data = {'A': [nan, nan, nan, 0, 1, 2, 3, 4, 5, 6],\n                     'B': [0, 1, 2, nan, nan, nan, 3, 4, 5, 6],\n                     'C': np.arange(10, dtype=np.float64),\n                     'D': [0, 1, 2, 3, 4, 5, nan, nan, nan, nan]}\n\n        self.dates = bdate_range('1/1/2011', periods=10)\n\n        self.orig = pd.DataFrame(self.data, index=self.dates)\n        self.iorig = pd.DataFrame(self.data, index=self.dates)\n\n        self.frame = SparseDataFrame(self.data, index=self.dates)\n        self.iframe = SparseDataFrame(self.data, index=self.dates,\n                                      default_kind='integer')\n\n        values = self.frame.values.copy()\n        values[np.isnan(values)] = 0\n\n        self.zorig = pd.DataFrame(values, columns=['A', 'B', 'C', 'D'],\n                                  index=self.dates)\n        self.zframe = SparseDataFrame(values, columns=['A', 'B', 'C', 'D'],\n                                      default_fill_value=0, index=self.dates)\n\n        values = self.frame.values.copy()\n        values[np.isnan(values)] = 2\n\n        self.fill_orig = pd.DataFrame(values, columns=['A', 'B', 'C', 'D'],\n                                      index=self.dates)\n        self.fill_frame = SparseDataFrame(values, columns=['A', 'B', 'C', 'D'],\n                                          default_fill_value=2,\n                                          index=self.dates)\n\n        self.empty = SparseDataFrame()\n\n    def test_fill_value_when_combine_const(self):\n        # GH12723\n        dat = np.array([0, 1, np.nan, 3, 4, 5], dtype='float')\n        df = SparseDataFrame({'foo': dat}, index=range(6))\n\n        exp = df.fillna(0).add(2)\n        res = df.add(2, fill_value=0)\n        tm.assert_sp_frame_equal(res, exp)\n\n    def test_as_matrix(self):\n        empty = self.empty.as_matrix()\n        assert empty.shape == (0, 0)\n\n        no_cols = SparseDataFrame(index=np.arange(10))\n        mat = no_cols.as_matrix()\n        assert mat.shape == (10, 0)\n\n        no_index = SparseDataFrame(columns=np.arange(10))\n        mat = no_index.as_matrix()\n        assert mat.shape == (0, 10)\n\n    def test_copy(self):\n        cp = self.frame.copy()\n        assert isinstance(cp, SparseDataFrame)\n        tm.assert_sp_frame_equal(cp, self.frame)\n\n        # as of v0.15.0\n        # this is now identical (but not is_a )\n        assert cp.index.identical(self.frame.index)\n\n    def test_constructor(self):\n        for col, series in compat.iteritems(self.frame):\n            assert isinstance(series, SparseSeries)\n\n        assert isinstance(self.iframe['A'].sp_index, IntIndex)\n\n        # constructed zframe from matrix above\n        assert self.zframe['A'].fill_value == 0\n        tm.assert_numpy_array_equal(pd.SparseArray([1., 2., 3., 4., 5., 6.]),\n                                    self.zframe['A'].values)\n        tm.assert_numpy_array_equal(np.array([0., 0., 0., 0., 1., 2.,\n                                              3., 4., 5., 6.]),\n                                    self.zframe['A'].to_dense().values)\n\n        # construct no data\n        sdf = SparseDataFrame(columns=np.arange(10), index=np.arange(10))\n        for col, series in compat.iteritems(sdf):\n            assert isinstance(series, SparseSeries)\n\n        # construct from nested dict\n        data = {}\n        for c, s in compat.iteritems(self.frame):\n            data[c] = s.to_dict()\n\n        sdf = SparseDataFrame(data)\n        tm.assert_sp_frame_equal(sdf, self.frame)\n\n        # TODO: test data is copied from inputs\n\n        # init dict with different index\n        idx = self.frame.index[:5]\n        cons = SparseDataFrame(\n            self.frame, index=idx, columns=self.frame.columns,\n            default_fill_value=self.frame.default_fill_value,\n            default_kind=self.frame.default_kind, copy=True)\n        reindexed = self.frame.reindex(idx)\n\n        tm.assert_sp_frame_equal(cons, reindexed, exact_indices=False)\n\n        # assert level parameter breaks reindex\n        with pytest.raises(TypeError):\n            self.frame.reindex(idx, level=0)\n\n        repr(self.frame)\n\n    def test_constructor_ndarray(self):\n        # no index or columns\n        sp = SparseDataFrame(self.frame.values)\n\n        # 1d\n        sp = SparseDataFrame(self.data['A'], index=self.dates, columns=['A'])\n        tm.assert_sp_frame_equal(sp, self.frame.reindex(columns=['A']))\n\n        # raise on level argument\n        pytest.raises(TypeError, self.frame.reindex, columns=['A'],\n                      level=1)\n\n        # wrong length index / columns\n        with tm.assert_raises_regex(ValueError, \"^Index length\"):\n            SparseDataFrame(self.frame.values, index=self.frame.index[:-1])\n\n        with tm.assert_raises_regex(ValueError, \"^Column length\"):\n            SparseDataFrame(self.frame.values, columns=self.frame.columns[:-1])\n\n    # GH 9272\n    def test_constructor_empty(self):\n        sp = SparseDataFrame()\n        assert len(sp.index) == 0\n        assert len(sp.columns) == 0\n\n    def test_constructor_dataframe(self):\n        dense = self.frame.to_dense()\n        sp = SparseDataFrame(dense)\n        tm.assert_sp_frame_equal(sp, self.frame)\n\n    def test_constructor_convert_index_once(self):\n        arr = np.array([1.5, 2.5, 3.5])\n        sdf = SparseDataFrame(columns=lrange(4), index=arr)\n        assert sdf[0].index is sdf[1].index\n\n    def test_constructor_from_series(self):\n\n        # GH 2873\n        x = Series(np.random.randn(10000), name='a')\n        x = x.to_sparse(fill_value=0)\n        assert isinstance(x, SparseSeries)\n        df = SparseDataFrame(x)\n        assert isinstance(df, SparseDataFrame)\n\n        x = Series(np.random.randn(10000), name='a')\n        y = Series(np.random.randn(10000), name='b')\n        x2 = x.astype(float)\n        x2.loc[:9998] = np.NaN\n        # TODO: x_sparse is unused...fix\n        x_sparse = x2.to_sparse(fill_value=np.NaN)  # noqa\n\n        # Currently fails too with weird ufunc error\n        # df1 = SparseDataFrame([x_sparse, y])\n\n        y.loc[:9998] = 0\n        # TODO: y_sparse is unsused...fix\n        y_sparse = y.to_sparse(fill_value=0)  # noqa\n        # without sparse value raises error\n        # df2 = SparseDataFrame([x2_sparse, y])\n\n    def test_constructor_preserve_attr(self):\n        # GH 13866\n        arr = pd.SparseArray([1, 0, 3, 0], dtype=np.int64, fill_value=0)\n        assert arr.dtype == np.int64\n        assert arr.fill_value == 0\n\n        df = pd.SparseDataFrame({'x': arr})\n        assert df['x'].dtype == np.int64\n        assert df['x'].fill_value == 0\n\n        s = pd.SparseSeries(arr, name='x')\n        assert s.dtype == np.int64\n        assert s.fill_value == 0\n\n        df = pd.SparseDataFrame(s)\n        assert df['x'].dtype == np.int64\n        assert df['x'].fill_value == 0\n\n        df = pd.SparseDataFrame({'x': s})\n        assert df['x'].dtype == np.int64\n        assert df['x'].fill_value == 0\n\n    def test_constructor_nan_dataframe(self):\n        # GH 10079\n        trains = np.arange(100)\n        tresholds = [10, 20, 30, 40, 50, 60]\n        tuples = [(i, j) for i in trains for j in tresholds]\n        index = pd.MultiIndex.from_tuples(tuples,\n                                          names=['trains', 'tresholds'])\n        matrix = np.empty((len(index), len(trains)))\n        matrix.fill(np.nan)\n        df = pd.DataFrame(matrix, index=index, columns=trains, dtype=float)\n        result = df.to_sparse()\n        expected = pd.SparseDataFrame(matrix, index=index, columns=trains,\n                                      dtype=float)\n        tm.assert_sp_frame_equal(result, expected)\n\n    def test_type_coercion_at_construction(self):\n        # GH 15682\n        result = pd.SparseDataFrame(\n            {'a': [1, 0, 0], 'b': [0, 1, 0], 'c': [0, 0, 1]}, dtype='uint8',\n            default_fill_value=0)\n        expected = pd.SparseDataFrame(\n            {'a': pd.SparseSeries([1, 0, 0], dtype='uint8'),\n             'b': pd.SparseSeries([0, 1, 0], dtype='uint8'),\n             'c': pd.SparseSeries([0, 0, 1], dtype='uint8')},\n            default_fill_value=0)\n        tm.assert_sp_frame_equal(result, expected)\n\n    def test_dtypes(self):\n        df = DataFrame(np.random.randn(10000, 4))\n        df.loc[:9998] = np.nan\n        sdf = df.to_sparse()\n\n        result = sdf.get_dtype_counts()\n        expected = Series({'float64': 4})\n        tm.assert_series_equal(result, expected)\n\n    def test_shape(self):\n        # see gh-10452\n        assert self.frame.shape == (10, 4)\n        assert self.iframe.shape == (10, 4)\n        assert self.zframe.shape == (10, 4)\n        assert self.fill_frame.shape == (10, 4)\n\n    def test_str(self):\n        df = DataFrame(np.random.randn(10000, 4))\n        df.loc[:9998] = np.nan\n\n        sdf = df.to_sparse()\n        str(sdf)\n\n    def test_array_interface(self):\n        res = np.sqrt(self.frame)\n        dres = np.sqrt(self.frame.to_dense())\n        tm.assert_frame_equal(res.to_dense(), dres)\n\n    def test_pickle(self):\n\n        def _test_roundtrip(frame, orig):\n            result = tm.round_trip_pickle(frame)\n            tm.assert_sp_frame_equal(frame, result)\n            tm.assert_frame_equal(result.to_dense(), orig, check_dtype=False)\n\n        _test_roundtrip(SparseDataFrame(), DataFrame())\n        self._check_all(_test_roundtrip)\n\n    def test_dense_to_sparse(self):\n        df = DataFrame({'A': [nan, nan, nan, 1, 2],\n                        'B': [1, 2, nan, nan, nan]})\n        sdf = df.to_sparse()\n        assert isinstance(sdf, SparseDataFrame)\n        assert np.isnan(sdf.default_fill_value)\n        assert isinstance(sdf['A'].sp_index, BlockIndex)\n        tm.assert_frame_equal(sdf.to_dense(), df)\n\n        sdf = df.to_sparse(kind='integer')\n        assert isinstance(sdf['A'].sp_index, IntIndex)\n\n        df = DataFrame({'A': [0, 0, 0, 1, 2],\n                        'B': [1, 2, 0, 0, 0]}, dtype=float)\n        sdf = df.to_sparse(fill_value=0)\n        assert sdf.default_fill_value == 0\n        tm.assert_frame_equal(sdf.to_dense(), df)\n\n    def test_density(self):\n        df = SparseSeries([nan, nan, nan, 0, 1, 2, 3, 4, 5, 6])\n        assert df.density == 0.7\n\n        df = SparseDataFrame({'A': [nan, nan, nan, 0, 1, 2, 3, 4, 5, 6],\n                              'B': [0, 1, 2, nan, nan, nan, 3, 4, 5, 6],\n                              'C': np.arange(10),\n                              'D': [0, 1, 2, 3, 4, 5, nan, nan, nan, nan]})\n\n        assert df.density == 0.75\n\n    def test_sparse_to_dense(self):\n        pass\n\n    def test_sparse_series_ops(self):\n        self._check_frame_ops(self.frame)\n\n    def test_sparse_series_ops_i(self):\n        self._check_frame_ops(self.iframe)\n\n    def test_sparse_series_ops_z(self):\n        self._check_frame_ops(self.zframe)\n\n    def test_sparse_series_ops_fill(self):\n        self._check_frame_ops(self.fill_frame)\n\n    def _check_frame_ops(self, frame):\n\n        def _compare_to_dense(a, b, da, db, op):\n            sparse_result = op(a, b)\n            dense_result = op(da, db)\n\n            fill = sparse_result.default_fill_value\n            dense_result = dense_result.to_sparse(fill_value=fill)\n            tm.assert_sp_frame_equal(sparse_result, dense_result,\n                                     exact_indices=False)\n\n            if isinstance(a, DataFrame) and isinstance(db, DataFrame):\n                mixed_result = op(a, db)\n                assert isinstance(mixed_result, SparseDataFrame)\n                tm.assert_sp_frame_equal(mixed_result, sparse_result,\n                                         exact_indices=False)\n\n        opnames = ['add', 'sub', 'mul', 'truediv', 'floordiv']\n        ops = [getattr(operator, name) for name in opnames]\n\n        fidx = frame.index\n\n        # time series operations\n\n        series = [frame['A'], frame['B'], frame['C'], frame['D'],\n                  frame['A'].reindex(fidx[:7]), frame['A'].reindex(fidx[::2]),\n                  SparseSeries(\n                      [], index=[])]\n\n        for op in opnames:\n            _compare_to_dense(frame, frame[::2], frame.to_dense(),\n                              frame[::2].to_dense(), getattr(operator, op))\n\n            # 2304, no auto-broadcasting\n            for i, s in enumerate(series):\n                f = lambda a, b: getattr(a, op)(b, axis='index')\n                _compare_to_dense(frame, s, frame.to_dense(), s.to_dense(), f)\n\n                # rops are not implemented\n                # _compare_to_dense(s, frame, s.to_dense(),\n                #                   frame.to_dense(), f)\n\n                # cross-sectional operations\n        series = [frame.xs(fidx[0]), frame.xs(fidx[3]), frame.xs(fidx[5]),\n                  frame.xs(fidx[7]), frame.xs(fidx[5])[:2]]\n\n        for op in ops:\n            for s in series:\n                _compare_to_dense(frame, s, frame.to_dense(), s, op)\n                _compare_to_dense(s, frame, s, frame.to_dense(), op)\n\n        # it works!\n        result = self.frame + self.frame.loc[:, ['A', 'B']]  # noqa\n\n    def test_op_corners(self):\n        empty = self.empty + self.empty\n        assert empty.empty\n\n        foo = self.frame + self.empty\n        assert isinstance(foo.index, DatetimeIndex)\n        tm.assert_frame_equal(foo, self.frame * np.nan)\n\n        foo = self.empty + self.frame\n        tm.assert_frame_equal(foo, self.frame * np.nan)\n\n    def test_scalar_ops(self):\n        pass\n\n    def test_getitem(self):\n        # 1585 select multiple columns\n        sdf = SparseDataFrame(index=[0, 1, 2], columns=['a', 'b', 'c'])\n\n        result = sdf[['a', 'b']]\n        exp = sdf.reindex(columns=['a', 'b'])\n        tm.assert_sp_frame_equal(result, exp)\n\n        pytest.raises(Exception, sdf.__getitem__, ['a', 'd'])\n\n    def test_iloc(self):\n\n        # 2227\n        result = self.frame.iloc[:, 0]\n        assert isinstance(result, SparseSeries)\n        tm.assert_sp_series_equal(result, self.frame['A'])\n\n        # preserve sparse index type. #2251\n        data = {'A': [0, 1]}\n        iframe = SparseDataFrame(data, default_kind='integer')\n        tm.assert_class_equal(iframe['A'].sp_index,\n                              iframe.iloc[:, 0].sp_index)\n\n    def test_set_value(self):\n\n        # ok, as the index gets converted to object\n        frame = self.frame.copy()\n        res = frame.set_value('foobar', 'B', 1.5)\n        assert res.index.dtype == 'object'\n\n        res = self.frame\n        res.index = res.index.astype(object)\n\n        res = self.frame.set_value('foobar', 'B', 1.5)\n        assert res is not self.frame\n        assert res.index[-1] == 'foobar'\n        assert res.get_value('foobar', 'B') == 1.5\n\n        res2 = res.set_value('foobar', 'qux', 1.5)\n        assert res2 is not res\n        tm.assert_index_equal(res2.columns,\n                              pd.Index(list(self.frame.columns) + ['qux']))\n        assert res2.get_value('foobar', 'qux') == 1.5\n\n    def test_fancy_index_misc(self):\n        # axis = 0\n        sliced = self.frame.iloc[-2:, :]\n        expected = self.frame.reindex(index=self.frame.index[-2:])\n        tm.assert_sp_frame_equal(sliced, expected)\n\n        # axis = 1\n        sliced = self.frame.iloc[:, -2:]\n        expected = self.frame.reindex(columns=self.frame.columns[-2:])\n        tm.assert_sp_frame_equal(sliced, expected)\n\n    def test_getitem_overload(self):\n        # slicing\n        sl = self.frame[:20]\n        tm.assert_sp_frame_equal(sl, self.frame.reindex(self.frame.index[:20]))\n\n        # boolean indexing\n        d = self.frame.index[5]\n        indexer = self.frame.index > d\n\n        subindex = self.frame.index[indexer]\n        subframe = self.frame[indexer]\n\n        tm.assert_index_equal(subindex, subframe.index)\n        pytest.raises(Exception, self.frame.__getitem__, indexer[:-1])\n\n    def test_setitem(self):\n\n        def _check_frame(frame, orig):\n            N = len(frame)\n\n            # insert SparseSeries\n            frame['E'] = frame['A']\n            assert isinstance(frame['E'], SparseSeries)\n            tm.assert_sp_series_equal(frame['E'], frame['A'],\n                                      check_names=False)\n\n            # insert SparseSeries differently-indexed\n            to_insert = frame['A'][::2]\n            frame['E'] = to_insert\n            expected = to_insert.to_dense().reindex(frame.index)\n            result = frame['E'].to_dense()\n            tm.assert_series_equal(result, expected, check_names=False)\n            assert result.name == 'E'\n\n            # insert Series\n            frame['F'] = frame['A'].to_dense()\n            assert isinstance(frame['F'], SparseSeries)\n            tm.assert_sp_series_equal(frame['F'], frame['A'],\n                                      check_names=False)\n\n            # insert Series differently-indexed\n            to_insert = frame['A'].to_dense()[::2]\n            frame['G'] = to_insert\n            expected = to_insert.reindex(frame.index)\n            expected.name = 'G'\n            tm.assert_series_equal(frame['G'].to_dense(), expected)\n\n            # insert ndarray\n            frame['H'] = np.random.randn(N)\n            assert isinstance(frame['H'], SparseSeries)\n\n            to_sparsify = np.random.randn(N)\n            to_sparsify[N // 2:] = frame.default_fill_value\n            frame['I'] = to_sparsify\n            assert len(frame['I'].sp_values) == N // 2\n\n            # insert ndarray wrong size\n            pytest.raises(Exception, frame.__setitem__, 'foo',\n                          np.random.randn(N - 1))\n\n            # scalar value\n            frame['J'] = 5\n            assert len(frame['J'].sp_values) == N\n            assert (frame['J'].sp_values == 5).all()\n\n            frame['K'] = frame.default_fill_value\n            assert len(frame['K'].sp_values) == 0\n\n        self._check_all(_check_frame)\n\n    def test_setitem_corner(self):\n        self.frame['a'] = self.frame['B']\n        tm.assert_sp_series_equal(self.frame['a'], self.frame['B'],\n                                  check_names=False)\n\n    def test_setitem_array(self):\n        arr = self.frame['B']\n\n        self.frame['E'] = arr\n        tm.assert_sp_series_equal(self.frame['E'], self.frame['B'],\n                                  check_names=False)\n\n        self.frame['F'] = arr[:-1]\n        index = self.frame.index[:-1]\n        tm.assert_sp_series_equal(self.frame['E'].reindex(index),\n                                  self.frame['F'].reindex(index),\n                                  check_names=False)\n\n    def test_delitem(self):\n        A = self.frame['A']\n        C = self.frame['C']\n\n        del self.frame['B']\n        assert 'B' not in self.frame\n        tm.assert_sp_series_equal(self.frame['A'], A)\n        tm.assert_sp_series_equal(self.frame['C'], C)\n\n        del self.frame['D']\n        assert 'D' not in self.frame\n\n        del self.frame['A']\n        assert 'A' not in self.frame\n\n    def test_set_columns(self):\n        self.frame.columns = self.frame.columns\n        pytest.raises(Exception, setattr, self.frame, 'columns',\n                      self.frame.columns[:-1])\n\n    def test_set_index(self):\n        self.frame.index = self.frame.index\n        pytest.raises(Exception, setattr, self.frame, 'index',\n                      self.frame.index[:-1])\n\n    def test_append(self):\n        a = self.frame[:5]\n        b = self.frame[5:]\n\n        appended = a.append(b)\n        tm.assert_sp_frame_equal(appended, self.frame, exact_indices=False)\n\n        a = self.frame.iloc[:5, :3]\n        b = self.frame.iloc[5:]\n        appended = a.append(b)\n        tm.assert_sp_frame_equal(appended.iloc[:, :3], self.frame.iloc[:, :3],\n                                 exact_indices=False)\n\n    def test_apply(self):\n        applied = self.frame.apply(np.sqrt)\n        assert isinstance(applied, SparseDataFrame)\n        tm.assert_almost_equal(applied.values, np.sqrt(self.frame.values))\n\n        applied = self.fill_frame.apply(np.sqrt)\n        assert applied['A'].fill_value == np.sqrt(2)\n\n        # agg / broadcast\n        broadcasted = self.frame.apply(np.sum, broadcast=True)\n        assert isinstance(broadcasted, SparseDataFrame)\n\n        exp = self.frame.to_dense().apply(np.sum, broadcast=True)\n        tm.assert_frame_equal(broadcasted.to_dense(), exp)\n\n        assert self.empty.apply(np.sqrt) is self.empty\n\n        from pandas.core import nanops\n        applied = self.frame.apply(np.sum)\n        tm.assert_series_equal(applied,\n                               self.frame.to_dense().apply(nanops.nansum))\n\n    def test_apply_nonuq(self):\n        orig = DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                         index=['a', 'a', 'c'])\n        sparse = orig.to_sparse()\n        res = sparse.apply(lambda s: s[0], axis=1)\n        exp = orig.apply(lambda s: s[0], axis=1)\n        # dtype must be kept\n        assert res.dtype == np.int64\n        # ToDo: apply must return subclassed dtype\n        assert isinstance(res, pd.Series)\n        tm.assert_series_equal(res.to_dense(), exp)\n\n        # df.T breaks\n        sparse = orig.T.to_sparse()\n        res = sparse.apply(lambda s: s[0], axis=0)  # noqa\n        exp = orig.T.apply(lambda s: s[0], axis=0)\n        # TODO: no non-unique columns supported in sparse yet\n        # tm.assert_series_equal(res.to_dense(), exp)\n\n    def test_applymap(self):\n        # just test that it works\n        result = self.frame.applymap(lambda x: x * 2)\n        assert isinstance(result, SparseDataFrame)\n\n    def test_astype(self):\n        sparse = pd.SparseDataFrame({'A': SparseArray([1, 2, 3, 4],\n                                                      dtype=np.int64),\n                                     'B': SparseArray([4, 5, 6, 7],\n                                                      dtype=np.int64)})\n        assert sparse['A'].dtype == np.int64\n        assert sparse['B'].dtype == np.int64\n\n        res = sparse.astype(np.float64)\n        exp = pd.SparseDataFrame({'A': SparseArray([1., 2., 3., 4.],\n                                                   fill_value=0.),\n                                  'B': SparseArray([4., 5., 6., 7.],\n                                                   fill_value=0.)},\n                                 default_fill_value=np.nan)\n        tm.assert_sp_frame_equal(res, exp)\n        assert res['A'].dtype == np.float64\n        assert res['B'].dtype == np.float64\n\n        sparse = pd.SparseDataFrame({'A': SparseArray([0, 2, 0, 4],\n                                                      dtype=np.int64),\n                                     'B': SparseArray([0, 5, 0, 7],\n                                                      dtype=np.int64)},\n                                    default_fill_value=0)\n        assert sparse['A'].dtype == np.int64\n        assert sparse['B'].dtype == np.int64\n\n        res = sparse.astype(np.float64)\n        exp = pd.SparseDataFrame({'A': SparseArray([0., 2., 0., 4.],\n                                                   fill_value=0.),\n                                  'B': SparseArray([0., 5., 0., 7.],\n                                                   fill_value=0.)},\n                                 default_fill_value=0.)\n        tm.assert_sp_frame_equal(res, exp)\n        assert res['A'].dtype == np.float64\n        assert res['B'].dtype == np.float64\n\n    def test_astype_bool(self):\n        sparse = pd.SparseDataFrame({'A': SparseArray([0, 2, 0, 4],\n                                                      fill_value=0,\n                                                      dtype=np.int64),\n                                     'B': SparseArray([0, 5, 0, 7],\n                                                      fill_value=0,\n                                                      dtype=np.int64)},\n                                    default_fill_value=0)\n        assert sparse['A'].dtype == np.int64\n        assert sparse['B'].dtype == np.int64\n\n        res = sparse.astype(bool)\n        exp = pd.SparseDataFrame({'A': SparseArray([False, True, False, True],\n                                                   dtype=np.bool,\n                                                   fill_value=False),\n                                  'B': SparseArray([False, True, False, True],\n                                                   dtype=np.bool,\n                                                   fill_value=False)},\n                                 default_fill_value=False)\n        tm.assert_sp_frame_equal(res, exp)\n        assert res['A'].dtype == np.bool\n        assert res['B'].dtype == np.bool\n\n    def test_fillna(self):\n        df = self.zframe.reindex(lrange(5))\n        dense = self.zorig.reindex(lrange(5))\n\n        result = df.fillna(0)\n        expected = dense.fillna(0)\n        tm.assert_sp_frame_equal(result, expected.to_sparse(fill_value=0),\n                                 exact_indices=False)\n        tm.assert_frame_equal(result.to_dense(), expected)\n\n        result = df.copy()\n        result.fillna(0, inplace=True)\n        expected = dense.fillna(0)\n\n        tm.assert_sp_frame_equal(result, expected.to_sparse(fill_value=0),\n                                 exact_indices=False)\n        tm.assert_frame_equal(result.to_dense(), expected)\n\n        result = df.copy()\n        result = df['A']\n        result.fillna(0, inplace=True)\n\n        expected = dense['A'].fillna(0)\n        # this changes internal SparseArray repr\n        # tm.assert_sp_series_equal(result, expected.to_sparse(fill_value=0))\n        tm.assert_series_equal(result.to_dense(), expected)\n\n    def test_fillna_fill_value(self):\n        df = pd.DataFrame({'A': [1, 0, 0], 'B': [np.nan, np.nan, 4]})\n\n        sparse = pd.SparseDataFrame(df)\n        tm.assert_frame_equal(sparse.fillna(-1).to_dense(),\n                              df.fillna(-1), check_dtype=False)\n\n        sparse = pd.SparseDataFrame(df, default_fill_value=0)\n        tm.assert_frame_equal(sparse.fillna(-1).to_dense(),\n                              df.fillna(-1), check_dtype=False)\n\n    def test_sparse_frame_pad_backfill_limit(self):\n        index = np.arange(10)\n        df = DataFrame(np.random.randn(10, 4), index=index)\n        sdf = df.to_sparse()\n\n        result = sdf[:2].reindex(index, method='pad', limit=5)\n\n        expected = sdf[:2].reindex(index).fillna(method='pad')\n        expected = expected.to_dense()\n        expected.values[-3:] = np.nan\n        expected = expected.to_sparse()\n        tm.assert_frame_equal(result, expected)\n\n        result = sdf[-2:].reindex(index, method='backfill', limit=5)\n\n        expected = sdf[-2:].reindex(index).fillna(method='backfill')\n        expected = expected.to_dense()\n        expected.values[:3] = np.nan\n        expected = expected.to_sparse()\n        tm.assert_frame_equal(result, expected)\n\n    def test_sparse_frame_fillna_limit(self):\n        index = np.arange(10)\n        df = DataFrame(np.random.randn(10, 4), index=index)\n        sdf = df.to_sparse()\n\n        result = sdf[:2].reindex(index)\n        result = result.fillna(method='pad', limit=5)\n\n        expected = sdf[:2].reindex(index).fillna(method='pad')\n        expected = expected.to_dense()\n        expected.values[-3:] = np.nan\n        expected = expected.to_sparse()\n        tm.assert_frame_equal(result, expected)\n\n        result = sdf[-2:].reindex(index)\n        result = result.fillna(method='backfill', limit=5)\n\n        expected = sdf[-2:].reindex(index).fillna(method='backfill')\n        expected = expected.to_dense()\n        expected.values[:3] = np.nan\n        expected = expected.to_sparse()\n        tm.assert_frame_equal(result, expected)\n\n    def test_rename(self):\n        result = self.frame.rename(index=str)\n        expected = SparseDataFrame(self.data, index=self.dates.strftime(\n            \"%Y-%m-%d %H:%M:%S\"))\n        tm.assert_sp_frame_equal(result, expected)\n\n        result = self.frame.rename(columns=lambda x: '%s%d' % (x, len(x)))\n        data = {'A1': [nan, nan, nan, 0, 1, 2, 3, 4, 5, 6],\n                'B1': [0, 1, 2, nan, nan, nan, 3, 4, 5, 6],\n                'C1': np.arange(10, dtype=np.float64),\n                'D1': [0, 1, 2, 3, 4, 5, nan, nan, nan, nan]}\n        expected = SparseDataFrame(data, index=self.dates)\n        tm.assert_sp_frame_equal(result, expected)\n\n    def test_corr(self):\n        res = self.frame.corr()\n        tm.assert_frame_equal(res, self.frame.to_dense().corr())\n\n    def test_describe(self):\n        self.frame['foo'] = np.nan\n        self.frame.get_dtype_counts()\n        str(self.frame)\n        desc = self.frame.describe()  # noqa\n\n    def test_join(self):\n        left = self.frame.loc[:, ['A', 'B']]\n        right = self.frame.loc[:, ['C', 'D']]\n        joined = left.join(right)\n        tm.assert_sp_frame_equal(joined, self.frame, exact_indices=False)\n\n        right = self.frame.loc[:, ['B', 'D']]\n        pytest.raises(Exception, left.join, right)\n\n        with tm.assert_raises_regex(ValueError,\n                                    'Other Series must have a name'):\n            self.frame.join(Series(\n                np.random.randn(len(self.frame)), index=self.frame.index))\n\n    def test_reindex(self):\n\n        def _check_frame(frame):\n            index = frame.index\n            sidx = index[::2]\n            sidx2 = index[:5]  # noqa\n\n            sparse_result = frame.reindex(sidx)\n            dense_result = frame.to_dense().reindex(sidx)\n            tm.assert_frame_equal(sparse_result.to_dense(), dense_result)\n\n            tm.assert_frame_equal(frame.reindex(list(sidx)).to_dense(),\n                                  dense_result)\n\n            sparse_result2 = sparse_result.reindex(index)\n            dense_result2 = dense_result.reindex(index)\n            tm.assert_frame_equal(sparse_result2.to_dense(), dense_result2)\n\n            # propagate CORRECT fill value\n            tm.assert_almost_equal(sparse_result.default_fill_value,\n                                   frame.default_fill_value)\n            tm.assert_almost_equal(sparse_result['A'].fill_value,\n                                   frame['A'].fill_value)\n\n            # length zero\n            length_zero = frame.reindex([])\n            assert len(length_zero) == 0\n            assert len(length_zero.columns) == len(frame.columns)\n            assert len(length_zero['A']) == 0\n\n            # frame being reindexed has length zero\n            length_n = length_zero.reindex(index)\n            assert len(length_n) == len(frame)\n            assert len(length_n.columns) == len(frame.columns)\n            assert len(length_n['A']) == len(frame)\n\n            # reindex columns\n            reindexed = frame.reindex(columns=['A', 'B', 'Z'])\n            assert len(reindexed.columns) == 3\n            tm.assert_almost_equal(reindexed['Z'].fill_value,\n                                   frame.default_fill_value)\n            assert np.isnan(reindexed['Z'].sp_values).all()\n\n        _check_frame(self.frame)\n        _check_frame(self.iframe)\n        _check_frame(self.zframe)\n        _check_frame(self.fill_frame)\n\n        # with copy=False\n        reindexed = self.frame.reindex(self.frame.index, copy=False)\n        reindexed['F'] = reindexed['A']\n        assert 'F' in self.frame\n\n        reindexed = self.frame.reindex(self.frame.index)\n        reindexed['G'] = reindexed['A']\n        assert 'G' not in self.frame\n\n    def test_reindex_fill_value(self):\n        rng = bdate_range('20110110', periods=20)\n\n        result = self.zframe.reindex(rng, fill_value=0)\n        exp = self.zorig.reindex(rng, fill_value=0)\n        exp = exp.to_sparse(self.zframe.default_fill_value)\n        tm.assert_sp_frame_equal(result, exp)\n\n    def test_reindex_method(self):\n\n        sparse = SparseDataFrame(data=[[11., 12., 14.],\n                                       [21., 22., 24.],\n                                       [41., 42., 44.]],\n                                 index=[1, 2, 4],\n                                 columns=[1, 2, 4],\n                                 dtype=float)\n\n        # Over indices\n\n        # default method\n        result = sparse.reindex(index=range(6))\n        expected = SparseDataFrame(data=[[nan, nan, nan],\n                                         [11., 12., 14.],\n                                         [21., 22., 24.],\n                                         [nan, nan, nan],\n                                         [41., 42., 44.],\n                                         [nan, nan, nan]],\n                                   index=range(6),\n                                   columns=[1, 2, 4],\n                                   dtype=float)\n        tm.assert_sp_frame_equal(result, expected)\n\n        # method='bfill'\n        result = sparse.reindex(index=range(6), method='bfill')\n        expected = SparseDataFrame(data=[[11., 12., 14.],\n                                         [11., 12., 14.],\n                                         [21., 22., 24.],\n                                         [41., 42., 44.],\n                                         [41., 42., 44.],\n                                         [nan, nan, nan]],\n                                   index=range(6),\n                                   columns=[1, 2, 4],\n                                   dtype=float)\n        tm.assert_sp_frame_equal(result, expected)\n\n        # method='ffill'\n        result = sparse.reindex(index=range(6), method='ffill')\n        expected = SparseDataFrame(data=[[nan, nan, nan],\n                                         [11., 12., 14.],\n                                         [21., 22., 24.],\n                                         [21., 22., 24.],\n                                         [41., 42., 44.],\n                                         [41., 42., 44.]],\n                                   index=range(6),\n                                   columns=[1, 2, 4],\n                                   dtype=float)\n        tm.assert_sp_frame_equal(result, expected)\n\n        # Over columns\n\n        # default method\n        result = sparse.reindex(columns=range(6))\n        expected = SparseDataFrame(data=[[nan, 11., 12., nan, 14., nan],\n                                         [nan, 21., 22., nan, 24., nan],\n                                         [nan, 41., 42., nan, 44., nan]],\n                                   index=[1, 2, 4],\n                                   columns=range(6),\n                                   dtype=float)\n        tm.assert_sp_frame_equal(result, expected)\n\n        # method='bfill'\n        with pytest.raises(NotImplementedError):\n            sparse.reindex(columns=range(6), method='bfill')\n\n        # method='ffill'\n        with pytest.raises(NotImplementedError):\n            sparse.reindex(columns=range(6), method='ffill')\n\n    def test_take(self):\n        result = self.frame.take([1, 0, 2], axis=1)\n        expected = self.frame.reindex(columns=['B', 'A', 'C'])\n        tm.assert_sp_frame_equal(result, expected)\n\n    def test_to_dense(self):\n        def _check(frame, orig):\n            dense_dm = frame.to_dense()\n            tm.assert_frame_equal(frame, dense_dm)\n            tm.assert_frame_equal(dense_dm, orig, check_dtype=False)\n\n        self._check_all(_check)\n\n    def test_stack_sparse_frame(self):\n        with catch_warnings(record=True):\n\n            def _check(frame):\n                dense_frame = frame.to_dense()  # noqa\n\n                wp = Panel.from_dict({'foo': frame})\n                from_dense_lp = wp.to_frame()\n\n                from_sparse_lp = spf.stack_sparse_frame(frame)\n\n                tm.assert_numpy_array_equal(from_dense_lp.values,\n                                            from_sparse_lp.values)\n\n            _check(self.frame)\n            _check(self.iframe)\n\n            # for now\n            pytest.raises(Exception, _check, self.zframe)\n            pytest.raises(Exception, _check, self.fill_frame)\n\n    def test_transpose(self):\n\n        def _check(frame, orig):\n            transposed = frame.T\n            untransposed = transposed.T\n            tm.assert_sp_frame_equal(frame, untransposed)\n\n            tm.assert_frame_equal(frame.T.to_dense(), orig.T)\n            tm.assert_frame_equal(frame.T.T.to_dense(), orig.T.T)\n            tm.assert_sp_frame_equal(frame, frame.T.T, exact_indices=False)\n\n        self._check_all(_check)\n\n    def test_shift(self):\n\n        def _check(frame, orig):\n            shifted = frame.shift(0)\n            exp = orig.shift(0)\n            tm.assert_frame_equal(shifted.to_dense(), exp)\n\n            shifted = frame.shift(1)\n            exp = orig.shift(1)\n            tm.assert_frame_equal(shifted, exp)\n\n            shifted = frame.shift(-2)\n            exp = orig.shift(-2)\n            tm.assert_frame_equal(shifted, exp)\n\n            shifted = frame.shift(2, freq='B')\n            exp = orig.shift(2, freq='B')\n            exp = exp.to_sparse(frame.default_fill_value)\n            tm.assert_frame_equal(shifted, exp)\n\n            shifted = frame.shift(2, freq=BDay())\n            exp = orig.shift(2, freq=BDay())\n            exp = exp.to_sparse(frame.default_fill_value)\n            tm.assert_frame_equal(shifted, exp)\n\n        self._check_all(_check)\n\n    def test_count(self):\n        dense_result = self.frame.to_dense().count()\n\n        result = self.frame.count()\n        tm.assert_series_equal(result, dense_result)\n\n        result = self.frame.count(axis=None)\n        tm.assert_series_equal(result, dense_result)\n\n        result = self.frame.count(axis=0)\n        tm.assert_series_equal(result, dense_result)\n\n        result = self.frame.count(axis=1)\n        dense_result = self.frame.to_dense().count(axis=1)\n\n        # win32 don't check dtype\n        tm.assert_series_equal(result, dense_result, check_dtype=False)\n\n    def _check_all(self, check_func):\n        check_func(self.frame, self.orig)\n        check_func(self.iframe, self.iorig)\n        check_func(self.zframe, self.zorig)\n        check_func(self.fill_frame, self.fill_orig)\n\n    def test_numpy_transpose(self):\n        sdf = SparseDataFrame([1, 2, 3], index=[1, 2, 3], columns=['a'])\n        result = np.transpose(np.transpose(sdf))\n        tm.assert_sp_frame_equal(result, sdf)\n\n        msg = \"the 'axes' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.transpose, sdf, axes=1)\n\n    def test_combine_first(self):\n        df = self.frame\n\n        result = df[::2].combine_first(df)\n        result2 = df[::2].combine_first(df.to_dense())\n\n        expected = df[::2].to_dense().combine_first(df.to_dense())\n        expected = expected.to_sparse(fill_value=df.default_fill_value)\n\n        tm.assert_sp_frame_equal(result, result2)\n        tm.assert_sp_frame_equal(result, expected)\n\n    def test_combine_add(self):\n        df = self.frame.to_dense()\n        df2 = df.copy()\n        df2['C'][:3] = np.nan\n        df['A'][:3] = 5.7\n\n        result = df.to_sparse().add(df2.to_sparse(), fill_value=0)\n        expected = df.add(df2, fill_value=0).to_sparse()\n        tm.assert_sp_frame_equal(result, expected)\n\n    def test_isin(self):\n        sparse_df = DataFrame({'flag': [1., 0., 1.]}).to_sparse(fill_value=0.)\n        xp = sparse_df[sparse_df.flag == 1.]\n        rs = sparse_df[sparse_df.flag.isin([1.])]\n        tm.assert_frame_equal(xp, rs)\n\n    def test_sparse_pow_issue(self):\n        # 2220\n        df = SparseDataFrame({'A': [1.1, 3.3], 'B': [2.5, -3.9]})\n\n        # note : no error without nan\n        df = SparseDataFrame({'A': [nan, 0, 1]})\n\n        # note that 2 ** df works fine, also df ** 1\n        result = 1 ** df\n\n        r1 = result.take([0], 1)['A']\n        r2 = result['A']\n\n        assert len(r2.sp_values) == len(r1.sp_values)\n\n    def test_as_blocks(self):\n        df = SparseDataFrame({'A': [1.1, 3.3], 'B': [nan, -3.9]},\n                             dtype='float64')\n\n        df_blocks = df.blocks\n        assert list(df_blocks.keys()) == ['float64']\n        tm.assert_frame_equal(df_blocks['float64'], df)\n\n    @pytest.mark.xfail(reason='nan column names in _init_dict problematic '\n                              '(GH 16894)')\n    def test_nan_columnname(self):\n        # GH 8822\n        nan_colname = DataFrame(Series(1.0, index=[0]), columns=[nan])\n        nan_colname_sparse = nan_colname.to_sparse()\n        assert np.isnan(nan_colname_sparse.columns[0])\n\n    def test_isnull(self):\n        # GH 8276\n        df = pd.SparseDataFrame({'A': [np.nan, np.nan, 1, 2, np.nan],\n                                 'B': [0, np.nan, np.nan, 2, np.nan]})\n\n        res = df.isnull()\n        exp = pd.SparseDataFrame({'A': [True, True, False, False, True],\n                                  'B': [False, True, True, False, True]},\n                                 default_fill_value=True)\n        exp._default_fill_value = np.nan\n        tm.assert_sp_frame_equal(res, exp)\n\n        # if fill_value is not nan, True can be included in sp_values\n        df = pd.SparseDataFrame({'A': [0, 0, 1, 2, np.nan],\n                                 'B': [0, np.nan, 0, 2, np.nan]},\n                                default_fill_value=0.)\n        res = df.isnull()\n        assert isinstance(res, pd.SparseDataFrame)\n        exp = pd.DataFrame({'A': [False, False, False, False, True],\n                            'B': [False, True, False, False, True]})\n        tm.assert_frame_equal(res.to_dense(), exp)\n\n    def test_isnotnull(self):\n        # GH 8276\n        df = pd.SparseDataFrame({'A': [np.nan, np.nan, 1, 2, np.nan],\n                                 'B': [0, np.nan, np.nan, 2, np.nan]})\n\n        res = df.isnotnull()\n        exp = pd.SparseDataFrame({'A': [False, False, True, True, False],\n                                  'B': [True, False, False, True, False]},\n                                 default_fill_value=False)\n        exp._default_fill_value = np.nan\n        tm.assert_sp_frame_equal(res, exp)\n\n        # if fill_value is not nan, True can be included in sp_values\n        df = pd.SparseDataFrame({'A': [0, 0, 1, 2, np.nan],\n                                 'B': [0, np.nan, 0, 2, np.nan]},\n                                default_fill_value=0.)\n        res = df.isnotnull()\n        assert isinstance(res, pd.SparseDataFrame)\n        exp = pd.DataFrame({'A': [True, True, True, True, False],\n                            'B': [True, False, True, True, False]})\n        tm.assert_frame_equal(res.to_dense(), exp)\n\n\n@pytest.mark.parametrize('index', [None, list('abc')])  # noqa: F811\n@pytest.mark.parametrize('columns', [None, list('def')])\n@pytest.mark.parametrize('fill_value', [None, 0, np.nan])\n@pytest.mark.parametrize('dtype', [bool, int, float, np.uint16])\ndef test_from_to_scipy(spmatrix, index, columns, fill_value, dtype):\n    # GH 4343\n    tm.skip_if_no_package('scipy')\n\n    # Make one ndarray and from it one sparse matrix, both to be used for\n    # constructing frames and comparing results\n    arr = np.eye(3, dtype=dtype)\n    # GH 16179\n    arr[0, 1] = dtype(2)\n    try:\n        spm = spmatrix(arr)\n        assert spm.dtype == arr.dtype\n    except (TypeError, AssertionError):\n        # If conversion to sparse fails for this spmatrix type and arr.dtype,\n        # then the combination is not currently supported in NumPy, so we\n        # can just skip testing it thoroughly\n        return\n\n    sdf = pd.SparseDataFrame(spm, index=index, columns=columns,\n                             default_fill_value=fill_value)\n\n    # Expected result construction is kind of tricky for all\n    # dtype-fill_value combinations; easiest to cast to something generic\n    # and except later on\n    rarr = arr.astype(object)\n    rarr[arr == 0] = np.nan\n    expected = pd.SparseDataFrame(rarr, index=index, columns=columns).fillna(\n        fill_value if fill_value is not None else np.nan)\n\n    # Assert frame is as expected\n    sdf_obj = sdf.astype(object)\n    tm.assert_sp_frame_equal(sdf_obj, expected)\n    tm.assert_frame_equal(sdf_obj.to_dense(), expected.to_dense())\n\n    # Assert spmatrices equal\n    assert dict(sdf.to_coo().todok()) == dict(spm.todok())\n\n    # Ensure dtype is preserved if possible\n    was_upcast = ((fill_value is None or is_float(fill_value)) and\n                  not is_object_dtype(dtype) and\n                  not is_float_dtype(dtype))\n    res_dtype = (bool if is_bool_dtype(dtype) else\n                 float if was_upcast else\n                 dtype)\n    tm.assert_contains_all(sdf.dtypes, {np.dtype(res_dtype)})\n    assert sdf.to_coo().dtype == res_dtype\n\n    # However, adding a str column results in an upcast to object\n    sdf['strings'] = np.arange(len(sdf)).astype(str)\n    assert sdf.to_coo().dtype == np.object_\n\n\n@pytest.mark.parametrize('fill_value', [None, 0, np.nan])  # noqa: F811\ndef test_from_to_scipy_object(spmatrix, fill_value):\n    # GH 4343\n    dtype = object\n    columns = list('cd')\n    index = list('ab')\n    tm.skip_if_no_package('scipy', max_version='0.19.0')\n\n    # Make one ndarray and from it one sparse matrix, both to be used for\n    # constructing frames and comparing results\n    arr = np.eye(2, dtype=dtype)\n    try:\n        spm = spmatrix(arr)\n        assert spm.dtype == arr.dtype\n    except (TypeError, AssertionError):\n        # If conversion to sparse fails for this spmatrix type and arr.dtype,\n        # then the combination is not currently supported in NumPy, so we\n        # can just skip testing it thoroughly\n        return\n\n    sdf = pd.SparseDataFrame(spm, index=index, columns=columns,\n                             default_fill_value=fill_value)\n\n    # Expected result construction is kind of tricky for all\n    # dtype-fill_value combinations; easiest to cast to something generic\n    # and except later on\n    rarr = arr.astype(object)\n    rarr[arr == 0] = np.nan\n    expected = pd.SparseDataFrame(rarr, index=index, columns=columns).fillna(\n        fill_value if fill_value is not None else np.nan)\n\n    # Assert frame is as expected\n    sdf_obj = sdf.astype(object)\n    tm.assert_sp_frame_equal(sdf_obj, expected)\n    tm.assert_frame_equal(sdf_obj.to_dense(), expected.to_dense())\n\n    # Assert spmatrices equal\n    assert dict(sdf.to_coo().todok()) == dict(spm.todok())\n\n    # Ensure dtype is preserved if possible\n    res_dtype = object\n    tm.assert_contains_all(sdf.dtypes, {np.dtype(res_dtype)})\n    assert sdf.to_coo().dtype == res_dtype\n\n\ndef test_from_scipy_correct_ordering(spmatrix):\n    # GH 16179\n    tm.skip_if_no_package('scipy')\n\n    arr = np.arange(1, 5).reshape(2, 2)\n    try:\n        spm = spmatrix(arr)\n        assert spm.dtype == arr.dtype\n    except (TypeError, AssertionError):\n        # If conversion to sparse fails for this spmatrix type and arr.dtype,\n        # then the combination is not currently supported in NumPy, so we\n        # can just skip testing it thoroughly\n        return\n\n    sdf = pd.SparseDataFrame(spm)\n    expected = pd.SparseDataFrame(arr)\n    tm.assert_sp_frame_equal(sdf, expected)\n    tm.assert_frame_equal(sdf.to_dense(), expected.to_dense())\n\n\nclass TestSparseDataFrameArithmetic(object):\n\n    def test_numeric_op_scalar(self):\n        df = pd.DataFrame({'A': [nan, nan, 0, 1, ],\n                           'B': [0, 1, 2, nan],\n                           'C': [1., 2., 3., 4.],\n                           'D': [nan, nan, nan, nan]})\n        sparse = df.to_sparse()\n\n        tm.assert_sp_frame_equal(sparse + 1, (df + 1).to_sparse())\n\n    def test_comparison_op_scalar(self):\n        # GH 13001\n        df = pd.DataFrame({'A': [nan, nan, 0, 1, ],\n                           'B': [0, 1, 2, nan],\n                           'C': [1., 2., 3., 4.],\n                           'D': [nan, nan, nan, nan]})\n        sparse = df.to_sparse()\n\n        # comparison changes internal repr, compare with dense\n        res = sparse > 1\n        assert isinstance(res, pd.SparseDataFrame)\n        tm.assert_frame_equal(res.to_dense(), df > 1)\n\n        res = sparse != 0\n        assert isinstance(res, pd.SparseDataFrame)\n        tm.assert_frame_equal(res.to_dense(), df != 0)\n\n\nclass TestSparseDataFrameAnalytics(object):\n    def setup_method(self, method):\n        self.data = {'A': [nan, nan, nan, 0, 1, 2, 3, 4, 5, 6],\n                     'B': [0, 1, 2, nan, nan, nan, 3, 4, 5, 6],\n                     'C': np.arange(10, dtype=float),\n                     'D': [0, 1, 2, 3, 4, 5, nan, nan, nan, nan]}\n\n        self.dates = bdate_range('1/1/2011', periods=10)\n\n        self.frame = SparseDataFrame(self.data, index=self.dates)\n\n    def test_cumsum(self):\n        expected = SparseDataFrame(self.frame.to_dense().cumsum())\n\n        result = self.frame.cumsum()\n        tm.assert_sp_frame_equal(result, expected)\n\n        result = self.frame.cumsum(axis=None)\n        tm.assert_sp_frame_equal(result, expected)\n\n        result = self.frame.cumsum(axis=0)\n        tm.assert_sp_frame_equal(result, expected)\n\n    def test_numpy_cumsum(self):\n        result = np.cumsum(self.frame)\n        expected = SparseDataFrame(self.frame.to_dense().cumsum())\n        tm.assert_sp_frame_equal(result, expected)\n\n        msg = \"the 'dtype' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.cumsum,\n                               self.frame, dtype=np.int64)\n\n        msg = \"the 'out' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.cumsum,\n                               self.frame, out=result)\n\n    def test_numpy_func_call(self):\n        # no exception should be raised even though\n        # numpy passes in 'axis=None' or `axis=-1'\n        funcs = ['sum', 'cumsum', 'var',\n                 'mean', 'prod', 'cumprod',\n                 'std', 'min', 'max']\n        for func in funcs:\n            getattr(np, func)(self.frame)\n"
    }
  ],
  "questions": [
    "You can actually replicate this using `dict` !\r\n\r\n~~~python\r\n>>> import numpy as np\r\n>>> s = {np.nan : 2}\r\n>>> np.nan in s\r\nTrue\r\n>>>\r\n>>> s = {np.float64(np.nan) : 2}\r\n>>> np.nan in s\r\nFalse\r\n~~~\r\n\r\nHonestly, I blame `numpy` for this :smile:, as this is annoying to patch.  On the one hand, ensuring `Index` makes sense, but then you can't check `np.nan` anymore.\r\n\r\nThe obvious *candidate* workaround is to write `columns=[np.float64(np.nan), 2]`.  Perhaps what we could do is call `ensure_index` (or some form of it) on the columns to perform the same casting?  Not really sure at this point, but let us know if the workaround works for you at least.",
    "Okay, might it be possible to use similar logic when constructing from `dict` ?  I'm trying to see (without being able to see the code-base ATM) whether whatever logic being used to handle `np.nan` properly for indexing can be used internally when constructing a `DataFrame` from `dict`."
  ],
  "golden_answers": [
    "`np.float64(np.nan)` wouldn't work, because it's an _instance_ of NaN, and nans don't equate to one another. `dict` probably uses (or falls back to) operator-is equality, which a **singleton** like `np.nan` (`pd.NaT`, ...) responds positive to. Compare:\r\n```py\r\n>>> np.nan in {np.nan}\r\nTrue\r\n\r\n>>> np.float64(np.nan) in {np.float64(np.nan)}\r\nFalse\r\n\r\n>>> float('nan') in {float('nan')}\r\nFalse\r\n\r\n>>> np.nan is np.nan\r\nTrue\r\n\r\n>>> np.float64(np.nan) is np.float64(np.nan)\r\nFalse\r\n```\r\n\r\nThis bug came up in https://github.com/pandas-dev/pandas/pull/16883#discussion_r126827887. But there are legitimate cases for a (catch-all) nan in index (e.g. https://github.com/pandas-dev/pandas/issues/3729).",
    "in ``DataFrame._init_dict`` this passes thru ``Index``, e.g. \r\n\r\n```\r\nIn [1]: Index([float('nan')])\r\nOut[1]: Float64Index([nan], dtype='float64')\r\n\r\nIn [2]: Index([np.nan])\r\nOut[2]: Float64Index([nan], dtype='float64')\r\n```\r\n\r\nso must be missing this step somewhere."
  ],
  "questions_generated": [
    "What is the primary issue with initializing a DataFrame using a dictionary with NaN keys when columns are passed separately?",
    "Why does np.float64(np.nan) not equate to np.nan, and how does this affect dictionary key retrieval?",
    "How does the proposed workaround of using columns=[np.float64(np.nan), 2] attempt to solve the NaN matching problem?",
    "What role does the _ensure_index function play in the DataFrame initialization process, and why does it contribute to the NaN issue?",
    "In what way does the behavior of NaN indexing in Pandas differ from that of standard Python dictionaries, and how is this evident in the issue discussed?"
  ],
  "golden_answers_generated": [
    "The primary issue is that when a DataFrame is initialized with a dictionary that contains NaN as keys and columns are specified separately, the NaN keys from the dictionary do not match the NaN in the ensured Float64Index. This discrepancy occurs because np.float64(np.nan) is not equivalent to np.nan, leading to the failure of key retrieval from the dictionary.",
    "np.float64(np.nan) does not equate to np.nan because they are different instances. NaNs in NumPy are not equal to each other, which affects dictionary key retrieval because a NaN key created with np.float64(np.nan) will not match a NaN key created with np.nan. This behavior causes issues when trying to retrieve data from a dictionary using NaN as a key.",
    "The workaround of using columns=[np.float64(np.nan), 2] attempts to solve the issue by ensuring that the NaN in the columns list matches the NaN keys in the dictionary. By using np.float64(np.nan) for both, it attempts to align the types, allowing for successful key matching and retrieval from the dictionary.",
    "The _ensure_index function is used to convert the columns list into an Index, specifically a Float64Index when NaN is involved. This conversion is meant to standardize the column labels but contributes to the NaN issue because it converts np.nan to np.float64(np.nan), which does not match np.nan keys in the dictionary, leading to retrieval failures.",
    "In Pandas, NaN indexing can be more flexible due to internal handling that allows for NaN to be used as a valid index label, whereas in standard Python dictionaries, NaN instances (like np.float64(np.nan)) do not equate to each other and do not match np.nan keys. This difference is evident in the issue discussed, where NaN keys in a dictionary do not match when passed separately as column labels, leading to unexpected DataFrame outputs."
  ]
}