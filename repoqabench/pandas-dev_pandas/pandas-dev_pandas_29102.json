{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "29102",
  "issue_description": "# Misleading error messages when opening inexistent json file\n\nWhen opening a json file that doesn't exist with `read_json`, we get a `ValueError` with error messages like:\r\n\r\n```python\r\n>>> import pandas\r\n>>> pandas.read_json('no_file.json')\r\nValueError: Unexpected character found when decoding 'null'\r\n\r\n>>> pandas.read_json('a_dir/no_file.json')\r\nValueError: Expected object or value\r\n```\r\n\r\nIt'd probably be better to raise an exception like:\r\n```\r\nFileNotFoundError: No such file: 'no_file.json'\r\n```",
  "issue_comments": [
    {
      "id": 544183247,
      "user": "mohitanand001",
      "body": "Hi, @datapythonista can I take this up?"
    },
    {
      "id": 544183491,
      "user": "datapythonista",
      "body": "sure, thanks"
    },
    {
      "id": 548129180,
      "user": "deepandas11",
      "body": "Hi! Would like to work on this if it is still open."
    },
    {
      "id": 597978788,
      "user": "sathyz",
      "body": "take"
    },
    {
      "id": 604192237,
      "user": "kevin-meyers",
      "body": "Hey @sathyz if you don't finish it by the end of the month I think I'll give it a shot."
    },
    {
      "id": 612710715,
      "user": "devjeetr",
      "body": "take"
    },
    {
      "id": 633152741,
      "user": "vampypandya",
      "body": "@devjeetr  Is this issue still open? Can I give a shot?"
    },
    {
      "id": 991338711,
      "user": "cheungje",
      "body": "Hi, I would like to to take this issue if it's still not resolved."
    },
    {
      "id": 991414207,
      "user": "cheungje",
      "body": "take"
    },
    {
      "id": 991598155,
      "user": "mohitanand001",
      "body": "Anyone willing to take this up might want to go through this thread once. \r\nhttps://github.com/pandas-dev/pandas/pull/29104\r\nI had taken this 2 years back, but it was closed after some discussion. "
    },
    {
      "id": 1092862479,
      "user": "janosh",
      "body": "Would a PR for this still be welcomed? Reading #29104 makes it sounds like the conclusion there was no good solution exists but tbh I don't understand why not."
    },
    {
      "id": 1092873209,
      "user": "MarcoGorelli",
      "body": "If I've understood the discussion correctly, then I think that at least for extensioned json files, it should be possible to assume user intent and raise an informative error message if the file doesn't exist"
    },
    {
      "id": 1092876336,
      "user": "janosh",
      "body": "@MarcoGorelli So if I submit a PR that raises `FNFError`\r\n\r\n```py\r\nif (\r\n    isinstance(filepath_or_buffer, str)\r\n    and filepath_or_buffer.lower().endswith(('.json', '.json.gz', '.json.bz2'))\r\n    and not isfile(filepath_or_buffer)\r\n)\r\n\r\n```\r\n\r\nthat would be acceptable? Seems like a definite improvement over current behavior to me."
    },
    {
      "id": 1092879005,
      "user": "MarcoGorelli",
      "body": "TBH that would look good to me, I often run into this error when trying to read json files - cc @gfyoung @WillAyd any further thoughts on this?"
    }
  ],
  "text_context": "# Misleading error messages when opening inexistent json file\n\nWhen opening a json file that doesn't exist with `read_json`, we get a `ValueError` with error messages like:\r\n\r\n```python\r\n>>> import pandas\r\n>>> pandas.read_json('no_file.json')\r\nValueError: Unexpected character found when decoding 'null'\r\n\r\n>>> pandas.read_json('a_dir/no_file.json')\r\nValueError: Expected object or value\r\n```\r\n\r\nIt'd probably be better to raise an exception like:\r\n```\r\nFileNotFoundError: No such file: 'no_file.json'\r\n```\n\nHi, @datapythonista can I take this up?\n\nsure, thanks\n\nHi! Would like to work on this if it is still open.\n\ntake\n\nHey @sathyz if you don't finish it by the end of the month I think I'll give it a shot.\n\ntake\n\n@devjeetr  Is this issue still open? Can I give a shot?\n\nHi, I would like to to take this issue if it's still not resolved.\n\ntake\n\nAnyone willing to take this up might want to go through this thread once. \r\nhttps://github.com/pandas-dev/pandas/pull/29104\r\nI had taken this 2 years back, but it was closed after some discussion. \n\nWould a PR for this still be welcomed? Reading #29104 makes it sounds like the conclusion there was no good solution exists but tbh I don't understand why not.\n\nIf I've understood the discussion correctly, then I think that at least for extensioned json files, it should be possible to assume user intent and raise an informative error message if the file doesn't exist\n\n@MarcoGorelli So if I submit a PR that raises `FNFError`\r\n\r\n```py\r\nif (\r\n    isinstance(filepath_or_buffer, str)\r\n    and filepath_or_buffer.lower().endswith(('.json', '.json.gz', '.json.bz2'))\r\n    and not isfile(filepath_or_buffer)\r\n)\r\n\r\n```\r\n\r\nthat would be acceptable? Seems like a definite improvement over current behavior to me.\n\nTBH that would look good to me, I often run into this error when trying to read json files - cc @gfyoung @WillAyd any further thoughts on this?",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/29104",
  "code_context": [
    {
      "filename": "pandas/io/json/_json.py",
      "content": "from collections import OrderedDict\nfrom errno import ENOENT\nfrom io import StringIO\nfrom itertools import islice\nimport os\nfrom typing import Any, Callable, Dict, List, Optional, Type, Union\n\nimport numpy as np\n\nimport pandas._libs.json as json\nfrom pandas._libs.tslibs import iNaT\nfrom pandas.errors import AbstractMethodError\n\nfrom pandas.core.dtypes.common import ensure_str, is_period_dtype\n\nfrom pandas import DataFrame, MultiIndex, Series, compat, isna, to_datetime\nfrom pandas._typing import Scalar\nfrom pandas.core.reshape.concat import concat\n\nfrom pandas.io.common import (\n    BaseIterator,\n    _get_handle,\n    _infer_compression,\n    _stringify_path,\n    get_filepath_or_buffer,\n)\nfrom pandas.io.formats.printing import pprint_thing\nfrom pandas.io.parsers import _validate_integer\n\nfrom ._normalize import convert_to_line_delimits\nfrom ._table_schema import build_table_schema, parse_table_schema\n\nloads = json.loads\ndumps = json.dumps\n\nTABLE_SCHEMA_VERSION = \"0.20.0\"\n\nSerializable = Union[Scalar, List, Dict]\n\n\n# interface to/from\ndef to_json(\n    path_or_buf,\n    obj,\n    orient: Optional[str] = None,\n    date_format: str = \"epoch\",\n    double_precision: int = 10,\n    force_ascii: bool = True,\n    date_unit: str = \"ms\",\n    default_handler: Optional[Callable[[Any], Serializable]] = None,\n    lines: bool = False,\n    compression: Optional[str] = \"infer\",\n    index: bool = True,\n    indent: int = 0,\n):\n\n    if not index and orient not in [\"split\", \"table\"]:\n        raise ValueError(\n            \"'index=False' is only valid when 'orient' is \" \"'split' or 'table'\"\n        )\n\n    path_or_buf = _stringify_path(path_or_buf)\n    if lines and orient != \"records\":\n        raise ValueError(\"'lines' keyword only valid when 'orient' is records\")\n\n    if orient == \"table\" and isinstance(obj, Series):\n        obj = obj.to_frame(name=obj.name or \"values\")\n    if orient == \"table\" and isinstance(obj, DataFrame):\n        writer = JSONTableWriter  # type: Type[\"Writer\"]\n    elif isinstance(obj, Series):\n        writer = SeriesWriter\n    elif isinstance(obj, DataFrame):\n        writer = FrameWriter\n    else:\n        raise NotImplementedError(\"'obj' should be a Series or a DataFrame\")\n\n    s = writer(\n        obj,\n        orient=orient,\n        date_format=date_format,\n        double_precision=double_precision,\n        ensure_ascii=force_ascii,\n        date_unit=date_unit,\n        default_handler=default_handler,\n        index=index,\n        indent=indent,\n    ).write()\n\n    if lines:\n        s = convert_to_line_delimits(s)\n\n    if isinstance(path_or_buf, str):\n        fh, handles = _get_handle(path_or_buf, \"w\", compression=compression)\n        try:\n            fh.write(s)\n        finally:\n            fh.close()\n    elif path_or_buf is None:\n        return s\n    else:\n        path_or_buf.write(s)\n\n\nclass Writer:\n    def __init__(\n        self,\n        obj,\n        orient: Optional[str],\n        date_format: str,\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        index: bool,\n        default_handler: Optional[Callable[[Any], Serializable]] = None,\n        indent: int = 0,\n    ):\n        self.obj = obj\n\n        if orient is None:\n            orient = self._default_orient  # type: ignore\n\n        self.orient = orient\n        self.date_format = date_format\n        self.double_precision = double_precision\n        self.ensure_ascii = ensure_ascii\n        self.date_unit = date_unit\n        self.default_handler = default_handler\n        self.index = index\n        self.indent = indent\n\n        self.is_copy = None\n        self._format_axes()\n\n    def _format_axes(self):\n        raise AbstractMethodError(self)\n\n    def write(self):\n        return self._write(\n            self.obj,\n            self.orient,\n            self.double_precision,\n            self.ensure_ascii,\n            self.date_unit,\n            self.date_format == \"iso\",\n            self.default_handler,\n            self.indent,\n        )\n\n    def _write(\n        self,\n        obj,\n        orient: Optional[str],\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        iso_dates: bool,\n        default_handler: Optional[Callable[[Any], Serializable]],\n        indent: int,\n    ):\n        return dumps(\n            obj,\n            orient=orient,\n            double_precision=double_precision,\n            ensure_ascii=ensure_ascii,\n            date_unit=date_unit,\n            iso_dates=iso_dates,\n            default_handler=default_handler,\n            indent=indent,\n        )\n\n\nclass SeriesWriter(Writer):\n    _default_orient = \"index\"\n\n    def _format_axes(self):\n        if not self.obj.index.is_unique and self.orient == \"index\":\n            raise ValueError(\n                \"Series index must be unique for orient=\"\n                \"'{orient}'\".format(orient=self.orient)\n            )\n\n    def _write(\n        self,\n        obj,\n        orient: Optional[str],\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        iso_dates: bool,\n        default_handler: Optional[Callable[[Any], Serializable]],\n        indent: int,\n    ):\n        if not self.index and orient == \"split\":\n            obj = {\"name\": obj.name, \"data\": obj.values}\n        return super()._write(\n            obj,\n            orient,\n            double_precision,\n            ensure_ascii,\n            date_unit,\n            iso_dates,\n            default_handler,\n            indent,\n        )\n\n\nclass FrameWriter(Writer):\n    _default_orient = \"columns\"\n\n    def _format_axes(self):\n        \"\"\"\n        Try to format axes if they are datelike.\n        \"\"\"\n        if not self.obj.index.is_unique and self.orient in (\"index\", \"columns\"):\n            raise ValueError(\n                \"DataFrame index must be unique for orient=\"\n                \"'{orient}'.\".format(orient=self.orient)\n            )\n        if not self.obj.columns.is_unique and self.orient in (\n            \"index\",\n            \"columns\",\n            \"records\",\n        ):\n            raise ValueError(\n                \"DataFrame columns must be unique for orient=\"\n                \"'{orient}'.\".format(orient=self.orient)\n            )\n\n    def _write(\n        self,\n        obj,\n        orient: Optional[str],\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        iso_dates: bool,\n        default_handler: Optional[Callable[[Any], Serializable]],\n        indent: int,\n    ):\n        if not self.index and orient == \"split\":\n            obj = obj.to_dict(orient=\"split\")\n            del obj[\"index\"]\n        return super()._write(\n            obj,\n            orient,\n            double_precision,\n            ensure_ascii,\n            date_unit,\n            iso_dates,\n            default_handler,\n            indent,\n        )\n\n\nclass JSONTableWriter(FrameWriter):\n    _default_orient = \"records\"\n\n    def __init__(\n        self,\n        obj,\n        orient: Optional[str],\n        date_format: str,\n        double_precision: int,\n        ensure_ascii: bool,\n        date_unit: str,\n        index: bool,\n        default_handler: Optional[Callable[[Any], Serializable]] = None,\n        indent: int = 0,\n    ):\n        \"\"\"\n        Adds a `schema` attribute with the Table Schema, resets\n        the index (can't do in caller, because the schema inference needs\n        to know what the index is, forces orient to records, and forces\n        date_format to 'iso'.\n        \"\"\"\n\n        super().__init__(\n            obj,\n            orient,\n            date_format,\n            double_precision,\n            ensure_ascii,\n            date_unit,\n            index,\n            default_handler=default_handler,\n            indent=indent,\n        )\n\n        if date_format != \"iso\":\n            msg = (\n                \"Trying to write with `orient='table'` and \"\n                \"`date_format='{fmt}'`. Table Schema requires dates \"\n                \"to be formatted with `date_format='iso'`\".format(fmt=date_format)\n            )\n            raise ValueError(msg)\n\n        self.schema = build_table_schema(obj, index=self.index)\n\n        # NotImplemented on a column MultiIndex\n        if obj.ndim == 2 and isinstance(obj.columns, MultiIndex):\n            raise NotImplementedError(\"orient='table' is not supported for MultiIndex\")\n\n        # TODO: Do this timedelta properly in objToJSON.c See GH #15137\n        if (\n            (obj.ndim == 1)\n            and (obj.name in set(obj.index.names))\n            or len(obj.columns & obj.index.names)\n        ):\n            msg = \"Overlapping names between the index and columns\"\n            raise ValueError(msg)\n\n        obj = obj.copy()\n        timedeltas = obj.select_dtypes(include=[\"timedelta\"]).columns\n        if len(timedeltas):\n            obj[timedeltas] = obj[timedeltas].applymap(lambda x: x.isoformat())\n        # Convert PeriodIndex to datetimes before serialzing\n        if is_period_dtype(obj.index):\n            obj.index = obj.index.to_timestamp()\n\n        # exclude index from obj if index=False\n        if not self.index:\n            self.obj = obj.reset_index(drop=True)\n        else:\n            self.obj = obj.reset_index(drop=False)\n        self.date_format = \"iso\"\n        self.orient = \"records\"\n        self.index = index\n\n    def _write(\n        self,\n        obj,\n        orient,\n        double_precision,\n        ensure_ascii,\n        date_unit,\n        iso_dates,\n        default_handler,\n        indent,\n    ):\n        table_obj = OrderedDict(((\"schema\", self.schema), (\"data\", obj)))\n        serialized = super()._write(\n            table_obj,\n            orient,\n            double_precision,\n            ensure_ascii,\n            date_unit,\n            iso_dates,\n            default_handler,\n            indent,\n        )\n\n        return serialized\n\n\ndef read_json(\n    path_or_buf=None,\n    orient=None,\n    typ=\"frame\",\n    dtype=None,\n    convert_axes=None,\n    convert_dates=True,\n    keep_default_dates=True,\n    numpy=False,\n    precise_float=False,\n    date_unit=None,\n    encoding=None,\n    lines=False,\n    chunksize=None,\n    compression=\"infer\",\n):\n    \"\"\"\n    Convert a JSON string to pandas object.\n\n    Parameters\n    ----------\n    path_or_buf : a valid JSON str, path object or file-like object\n        Any valid string path is acceptable. The string could be a URL. Valid\n        URL schemes include http, ftp, s3, and file. For file URLs, a host is\n        expected. A local file could be:\n        ``file://localhost/path/to/table.json``.\n\n        If you want to pass in a path object, pandas accepts any\n        ``os.PathLike``.\n\n        By file-like object, we refer to objects with a ``read()`` method,\n        such as a file handler (e.g. via builtin ``open`` function)\n        or ``StringIO``.\n    orient : string,\n        Indication of expected JSON string format.\n        Compatible JSON strings can be produced by ``to_json()`` with a\n        corresponding orient value.\n        The set of possible orients is:\n\n        - ``'split'`` : dict like\n          ``{index -> [index], columns -> [columns], data -> [values]}``\n        - ``'records'`` : list like\n          ``[{column -> value}, ... , {column -> value}]``\n        - ``'index'`` : dict like ``{index -> {column -> value}}``\n        - ``'columns'`` : dict like ``{column -> {index -> value}}``\n        - ``'values'`` : just the values array\n\n        The allowed and default values depend on the value\n        of the `typ` parameter.\n\n        * when ``typ == 'series'``,\n\n          - allowed orients are ``{'split','records','index'}``\n          - default is ``'index'``\n          - The Series index must be unique for orient ``'index'``.\n\n        * when ``typ == 'frame'``,\n\n          - allowed orients are ``{'split','records','index',\n            'columns','values', 'table'}``\n          - default is ``'columns'``\n          - The DataFrame index must be unique for orients ``'index'`` and\n            ``'columns'``.\n          - The DataFrame columns must be unique for orients ``'index'``,\n            ``'columns'``, and ``'records'``.\n\n        .. versionadded:: 0.23.0\n           'table' as an allowed value for the ``orient`` argument\n\n    typ : {'frame', 'series'}, default 'frame'\n        The type of object to recover.\n\n    dtype : bool or dict, default None\n        If True, infer dtypes; if a dict of column to dtype, then use those;\n        if False, then don't infer dtypes at all, applies only to the data.\n\n        For all ``orient`` values except ``'table'``, default is True.\n\n        .. versionchanged:: 0.25.0\n\n           Not applicable for ``orient='table'``.\n\n    convert_axes : bool, default None\n        Try to convert the axes to the proper dtypes.\n\n        For all ``orient`` values except ``'table'``, default is True.\n\n        .. versionchanged:: 0.25.0\n\n           Not applicable for ``orient='table'``.\n\n    convert_dates : bool or list of str, default True\n        List of columns to parse for dates. If True, then try to parse\n        datelike columns. A column label is datelike if\n\n        * it ends with ``'_at'``,\n\n        * it ends with ``'_time'``,\n\n        * it begins with ``'timestamp'``,\n\n        * it is ``'modified'``, or\n\n        * it is ``'date'``.\n\n    keep_default_dates : bool, default True\n        If parsing dates, then parse the default datelike columns.\n\n    numpy : bool, default False\n        Direct decoding to numpy arrays. Supports numeric data only, but\n        non-numeric column and index labels are supported. Note also that the\n        JSON ordering MUST be the same for each term if numpy=True.\n\n    precise_float : bool, default False\n        Set to enable usage of higher precision (strtod) function when\n        decoding string to double values. Default (False) is to use fast but\n        less precise builtin functionality.\n\n    date_unit : str, default None\n        The timestamp unit to detect if converting dates. The default behaviour\n        is to try and detect the correct precision, but if this is not desired\n        then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n        milliseconds, microseconds or nanoseconds respectively.\n\n    encoding : str, default is 'utf-8'\n        The encoding to use to decode py3 bytes.\n\n    lines : bool, default False\n        Read the file as a json object per line.\n\n    chunksize : int, optional\n        Return JsonReader object for iteration.\n        See the `line-delimited json docs\n        <http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_\n        for more information on ``chunksize``.\n        This can only be passed if `lines=True`.\n        If this is None, the file will be read into memory all at once.\n\n        .. versionadded:: 0.21.0\n\n    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        For on-the-fly decompression of on-disk data. If 'infer', then use\n        gzip, bz2, zip or xz if path_or_buf is a string ending in\n        '.gz', '.bz2', '.zip', or 'xz', respectively, and no decompression\n        otherwise. If using 'zip', the ZIP file must contain only one data\n        file to be read in. Set to None for no decompression.\n\n        .. versionadded:: 0.21.0\n\n    Returns\n    -------\n    Series or DataFrame\n        The type returned depends on the value of `typ`.\n\n    See Also\n    --------\n    DataFrame.to_json : Convert a DataFrame to a JSON string.\n    Series.to_json : Convert a Series to a JSON string.\n\n    Notes\n    -----\n    Specific to ``orient='table'``, if a :class:`DataFrame` with a literal\n    :class:`Index` name of `index` gets written with :func:`to_json`, the\n    subsequent read operation will incorrectly set the :class:`Index` name to\n    ``None``. This is because `index` is also used by :func:`DataFrame.to_json`\n    to denote a missing :class:`Index` name, and the subsequent\n    :func:`read_json` operation cannot distinguish between the two. The same\n    limitation is encountered with a :class:`MultiIndex` and any names\n    beginning with ``'level_'``.\n\n    Examples\n    --------\n\n    >>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n    ...                   index=['row 1', 'row 2'],\n    ...                   columns=['col 1', 'col 2'])\n\n    Encoding/decoding a Dataframe using ``'split'`` formatted JSON:\n\n    >>> df.to_json(orient='split')\n    '{\"columns\":[\"col 1\",\"col 2\"],\n      \"index\":[\"row 1\",\"row 2\"],\n      \"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}'\n    >>> pd.read_json(_, orient='split')\n          col 1 col 2\n    row 1     a     b\n    row 2     c     d\n\n    Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n\n    >>> df.to_json(orient='index')\n    '{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}'\n    >>> pd.read_json(_, orient='index')\n          col 1 col 2\n    row 1     a     b\n    row 2     c     d\n\n    Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n    Note that index labels are not preserved with this encoding.\n\n    >>> df.to_json(orient='records')\n    '[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]'\n    >>> pd.read_json(_, orient='records')\n      col 1 col 2\n    0     a     b\n    1     c     d\n\n    Encoding with Table Schema\n\n    >>> df.to_json(orient='table')\n    '{\"schema\": {\"fields\": [{\"name\": \"index\", \"type\": \"string\"},\n                            {\"name\": \"col 1\", \"type\": \"string\"},\n                            {\"name\": \"col 2\", \"type\": \"string\"}],\n                    \"primaryKey\": \"index\",\n                    \"pandas_version\": \"0.20.0\"},\n        \"data\": [{\"index\": \"row 1\", \"col 1\": \"a\", \"col 2\": \"b\"},\n                {\"index\": \"row 2\", \"col 1\": \"c\", \"col 2\": \"d\"}]}'\n    \"\"\"\n\n    if orient == \"table\" and dtype:\n        raise ValueError(\"cannot pass both dtype and orient='table'\")\n    if orient == \"table\" and convert_axes:\n        raise ValueError(\"cannot pass both convert_axes and orient='table'\")\n\n    if dtype is None and orient != \"table\":\n        dtype = True\n    if convert_axes is None and orient != \"table\":\n        convert_axes = True\n\n    compression = _infer_compression(path_or_buf, compression)\n    filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\n        path_or_buf, encoding=encoding, compression=compression\n    )\n\n    json_reader = JsonReader(\n        filepath_or_buffer,\n        orient=orient,\n        typ=typ,\n        dtype=dtype,\n        convert_axes=convert_axes,\n        convert_dates=convert_dates,\n        keep_default_dates=keep_default_dates,\n        numpy=numpy,\n        precise_float=precise_float,\n        date_unit=date_unit,\n        encoding=encoding,\n        lines=lines,\n        chunksize=chunksize,\n        compression=compression,\n    )\n\n    if chunksize:\n        return json_reader\n\n    result = json_reader.read()\n    if should_close:\n        filepath_or_buffer.close()\n\n    return result\n\n\nclass JsonReader(BaseIterator):\n    \"\"\"\n    JsonReader provides an interface for reading in a JSON file.\n\n    If initialized with ``lines=True`` and ``chunksize``, can be iterated over\n    ``chunksize`` lines at a time. Otherwise, calling ``read`` reads in the\n    whole document.\n    \"\"\"\n\n    def __init__(\n        self,\n        filepath_or_buffer,\n        orient,\n        typ,\n        dtype,\n        convert_axes,\n        convert_dates,\n        keep_default_dates,\n        numpy,\n        precise_float,\n        date_unit,\n        encoding,\n        lines,\n        chunksize,\n        compression,\n    ):\n\n        self.path_or_buf = filepath_or_buffer\n        self.orient = orient\n        self.typ = typ\n        self.dtype = dtype\n        self.convert_axes = convert_axes\n        self.convert_dates = convert_dates\n        self.keep_default_dates = keep_default_dates\n        self.numpy = numpy\n        self.precise_float = precise_float\n        self.date_unit = date_unit\n        self.encoding = encoding\n        self.compression = compression\n        self.lines = lines\n        self.chunksize = chunksize\n        self.nrows_seen = 0\n        self.should_close = False\n\n        if self.chunksize is not None:\n            self.chunksize = _validate_integer(\"chunksize\", self.chunksize, 1)\n            if not self.lines:\n                raise ValueError(\"chunksize can only be passed if lines=True\")\n\n        data = self._get_data_from_filepath(filepath_or_buffer)\n        self.data = self._preprocess_data(data)\n\n    def _preprocess_data(self, data):\n        \"\"\"\n        At this point, the data either has a `read` attribute (e.g. a file\n        object or a StringIO) or is a string that is a JSON document.\n\n        If self.chunksize, we prepare the data for the `__next__` method.\n        Otherwise, we read it into memory for the `read` method.\n        \"\"\"\n        if hasattr(data, \"read\") and not self.chunksize:\n            data = data.read()\n        if not hasattr(data, \"read\") and self.chunksize:\n            data = StringIO(data)\n\n        return data\n\n    def _get_data_from_filepath(self, filepath_or_buffer):\n        \"\"\"\n        The function read_json accepts three input types:\n            1. filepath (string-like)\n            2. file-like object (e.g. open file object, StringIO)\n            3. JSON string\n\n        This method turns (1) into (2) to simplify the rest of the processing.\n        It returns input types (2) and (3) unchanged.\n        \"\"\"\n        data = filepath_or_buffer\n\n        exists = False\n        if isinstance(data, str):\n            try:\n                exists = os.path.exists(filepath_or_buffer)\n            # gh-5874: if the filepath is too long will raise here\n            except (TypeError, ValueError):\n                pass\n\n        if exists or self.compression is not None:\n            data, _ = _get_handle(\n                filepath_or_buffer,\n                \"r\",\n                encoding=self.encoding,\n                compression=self.compression,\n            )\n            self.should_close = True\n            self.open_stream = data\n        else:\n            raise FileNotFoundError(\n                ENOENT,\n                \"File {filepath_or_buffer} does not exist\".format(\n                    filepath_or_buffer=filepath_or_buffer\n                ),\n                filepath_or_buffer,\n            )\n\n        return data\n\n    def _combine_lines(self, lines):\n        \"\"\"\n        Combines a list of JSON objects into one JSON object.\n        \"\"\"\n        lines = filter(None, map(lambda x: x.strip(), lines))\n        return \"[\" + \",\".join(lines) + \"]\"\n\n    def read(self):\n        \"\"\"\n        Read the whole JSON input into a pandas object.\n        \"\"\"\n        if self.lines and self.chunksize:\n            obj = concat(self)\n        elif self.lines:\n            data = ensure_str(self.data)\n            obj = self._get_object_parser(self._combine_lines(data.split(\"\\n\")))\n        else:\n            obj = self._get_object_parser(self.data)\n        self.close()\n        return obj\n\n    def _get_object_parser(self, json):\n        \"\"\"\n        Parses a json document into a pandas object.\n        \"\"\"\n        typ = self.typ\n        dtype = self.dtype\n        kwargs = {\n            \"orient\": self.orient,\n            \"dtype\": self.dtype,\n            \"convert_axes\": self.convert_axes,\n            \"convert_dates\": self.convert_dates,\n            \"keep_default_dates\": self.keep_default_dates,\n            \"numpy\": self.numpy,\n            \"precise_float\": self.precise_float,\n            \"date_unit\": self.date_unit,\n        }\n        obj = None\n        if typ == \"frame\":\n            obj = FrameParser(json, **kwargs).parse()\n\n        if typ == \"series\" or obj is None:\n            if not isinstance(dtype, bool):\n                kwargs[\"dtype\"] = dtype\n            obj = SeriesParser(json, **kwargs).parse()\n\n        return obj\n\n    def close(self):\n        \"\"\"\n        If we opened a stream earlier, in _get_data_from_filepath, we should\n        close it.\n\n        If an open stream or file was passed, we leave it open.\n        \"\"\"\n        if self.should_close:\n            try:\n                self.open_stream.close()\n            except (IOError, AttributeError):\n                pass\n\n    def __next__(self):\n        lines = list(islice(self.data, self.chunksize))\n        if lines:\n            lines_json = self._combine_lines(lines)\n            obj = self._get_object_parser(lines_json)\n\n            # Make sure that the returned objects have the right index.\n            obj.index = range(self.nrows_seen, self.nrows_seen + len(obj))\n            self.nrows_seen += len(obj)\n\n            return obj\n\n        self.close()\n        raise StopIteration\n\n\nclass Parser:\n\n    _STAMP_UNITS = (\"s\", \"ms\", \"us\", \"ns\")\n    _MIN_STAMPS = {\n        \"s\": 31536000,\n        \"ms\": 31536000000,\n        \"us\": 31536000000000,\n        \"ns\": 31536000000000000,\n    }\n\n    def __init__(\n        self,\n        json,\n        orient,\n        dtype=None,\n        convert_axes=True,\n        convert_dates=True,\n        keep_default_dates=False,\n        numpy=False,\n        precise_float=False,\n        date_unit=None,\n    ):\n        self.json = json\n\n        if orient is None:\n            orient = self._default_orient\n        self.orient = orient\n\n        self.dtype = dtype\n\n        if orient == \"split\":\n            numpy = False\n\n        if date_unit is not None:\n            date_unit = date_unit.lower()\n            if date_unit not in self._STAMP_UNITS:\n                raise ValueError(\n                    \"date_unit must be one of {units}\".format(units=self._STAMP_UNITS)\n                )\n            self.min_stamp = self._MIN_STAMPS[date_unit]\n        else:\n            self.min_stamp = self._MIN_STAMPS[\"s\"]\n\n        self.numpy = numpy\n        self.precise_float = precise_float\n        self.convert_axes = convert_axes\n        self.convert_dates = convert_dates\n        self.date_unit = date_unit\n        self.keep_default_dates = keep_default_dates\n        self.obj = None\n\n    def check_keys_split(self, decoded):\n        \"\"\"\n        Checks that dict has only the appropriate keys for orient='split'.\n        \"\"\"\n        bad_keys = set(decoded.keys()).difference(set(self._split_keys))\n        if bad_keys:\n            bad_keys = \", \".join(bad_keys)\n            raise ValueError(\n                \"JSON data had unexpected key(s): {bad_keys}\".format(\n                    bad_keys=pprint_thing(bad_keys)\n                )\n            )\n\n    def parse(self):\n\n        # try numpy\n        numpy = self.numpy\n        if numpy:\n            self._parse_numpy()\n\n        else:\n            self._parse_no_numpy()\n\n        if self.obj is None:\n            return None\n        if self.convert_axes:\n            self._convert_axes()\n        self._try_convert_types()\n        return self.obj\n\n    def _convert_axes(self):\n        \"\"\"\n        Try to convert axes.\n        \"\"\"\n        for axis in self.obj._AXIS_NUMBERS.keys():\n            new_axis, result = self._try_convert_data(\n                axis, self.obj._get_axis(axis), use_dtypes=False, convert_dates=True\n            )\n            if result:\n                setattr(self.obj, axis, new_axis)\n\n    def _try_convert_types(self):\n        raise AbstractMethodError(self)\n\n    def _try_convert_data(self, name, data, use_dtypes=True, convert_dates=True):\n        \"\"\"\n        Try to parse a ndarray like into a column by inferring dtype.\n        \"\"\"\n\n        # don't try to coerce, unless a force conversion\n        if use_dtypes:\n            if not self.dtype:\n                return data, False\n            elif self.dtype is True:\n                pass\n            else:\n                # dtype to force\n                dtype = (\n                    self.dtype.get(name) if isinstance(self.dtype, dict) else self.dtype\n                )\n                if dtype is not None:\n                    try:\n                        dtype = np.dtype(dtype)\n                        return data.astype(dtype), True\n                    except (TypeError, ValueError):\n                        return data, False\n\n        if convert_dates:\n            new_data, result = self._try_convert_to_date(data)\n            if result:\n                return new_data, True\n\n        result = False\n\n        if data.dtype == \"object\":\n\n            # try float\n            try:\n                data = data.astype(\"float64\")\n                result = True\n            except (TypeError, ValueError):\n                pass\n\n        if data.dtype.kind == \"f\":\n\n            if data.dtype != \"float64\":\n\n                # coerce floats to 64\n                try:\n                    data = data.astype(\"float64\")\n                    result = True\n                except (TypeError, ValueError):\n                    pass\n\n        # don't coerce 0-len data\n        if len(data) and (data.dtype == \"float\" or data.dtype == \"object\"):\n\n            # coerce ints if we can\n            try:\n                new_data = data.astype(\"int64\")\n                if (new_data == data).all():\n                    data = new_data\n                    result = True\n            except (TypeError, ValueError):\n                pass\n\n        # coerce ints to 64\n        if data.dtype == \"int\":\n\n            # coerce floats to 64\n            try:\n                data = data.astype(\"int64\")\n                result = True\n            except (TypeError, ValueError):\n                pass\n\n        return data, result\n\n    def _try_convert_to_date(self, data):\n        \"\"\"\n        Try to parse a ndarray like into a date column.\n\n        Try to coerce object in epoch/iso formats and integer/float in epoch\n        formats. Return a boolean if parsing was successful.\n        \"\"\"\n\n        # no conversion on empty\n        if not len(data):\n            return data, False\n\n        new_data = data\n        if new_data.dtype == \"object\":\n            try:\n                new_data = data.astype(\"int64\")\n            except (TypeError, ValueError, OverflowError):\n                pass\n\n        # ignore numbers that are out of range\n        if issubclass(new_data.dtype.type, np.number):\n            in_range = (\n                isna(new_data.values)\n                | (new_data > self.min_stamp)\n                | (new_data.values == iNaT)\n            )\n            if not in_range.all():\n                return data, False\n\n        date_units = (self.date_unit,) if self.date_unit else self._STAMP_UNITS\n        for date_unit in date_units:\n            try:\n                new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError):\n                continue\n            return new_data, True\n        return data, False\n\n    def _try_convert_dates(self):\n        raise AbstractMethodError(self)\n\n\nclass SeriesParser(Parser):\n    _default_orient = \"index\"\n    _split_keys = (\"name\", \"index\", \"data\")\n\n    def _parse_no_numpy(self):\n\n        json = self.json\n        orient = self.orient\n        if orient == \"split\":\n            decoded = {\n                str(k): v\n                for k, v in loads(json, precise_float=self.precise_float).items()\n            }\n            self.check_keys_split(decoded)\n            self.obj = Series(dtype=None, **decoded)\n        else:\n            self.obj = Series(loads(json, precise_float=self.precise_float), dtype=None)\n\n    def _parse_numpy(self):\n\n        json = self.json\n        orient = self.orient\n        if orient == \"split\":\n            decoded = loads(\n                json, dtype=None, numpy=True, precise_float=self.precise_float\n            )\n            decoded = {str(k): v for k, v in decoded.items()}\n            self.check_keys_split(decoded)\n            self.obj = Series(**decoded)\n        elif orient == \"columns\" or orient == \"index\":\n            self.obj = Series(\n                *loads(\n                    json,\n                    dtype=None,\n                    numpy=True,\n                    labelled=True,\n                    precise_float=self.precise_float,\n                )\n            )\n        else:\n            self.obj = Series(\n                loads(json, dtype=None, numpy=True, precise_float=self.precise_float)\n            )\n\n    def _try_convert_types(self):\n        if self.obj is None:\n            return\n        obj, result = self._try_convert_data(\n            \"data\", self.obj, convert_dates=self.convert_dates\n        )\n        if result:\n            self.obj = obj\n\n\nclass FrameParser(Parser):\n    _default_orient = \"columns\"\n    _split_keys = (\"columns\", \"index\", \"data\")\n\n    def _parse_numpy(self):\n\n        json = self.json\n        orient = self.orient\n\n        if orient == \"columns\":\n            args = loads(\n                json,\n                dtype=None,\n                numpy=True,\n                labelled=True,\n                precise_float=self.precise_float,\n            )\n            if len(args):\n                args = (args[0].T, args[2], args[1])\n            self.obj = DataFrame(*args)\n        elif orient == \"split\":\n            decoded = loads(\n                json, dtype=None, numpy=True, precise_float=self.precise_float\n            )\n            decoded = {str(k): v for k, v in decoded.items()}\n            self.check_keys_split(decoded)\n            self.obj = DataFrame(**decoded)\n        elif orient == \"values\":\n            self.obj = DataFrame(\n                loads(json, dtype=None, numpy=True, precise_float=self.precise_float)\n            )\n        else:\n            self.obj = DataFrame(\n                *loads(\n                    json,\n                    dtype=None,\n                    numpy=True,\n                    labelled=True,\n                    precise_float=self.precise_float,\n                )\n            )\n\n    def _parse_no_numpy(self):\n\n        json = self.json\n        orient = self.orient\n\n        if orient == \"columns\":\n            self.obj = DataFrame(\n                loads(json, precise_float=self.precise_float), dtype=None\n            )\n        elif orient == \"split\":\n            decoded = {\n                str(k): v\n                for k, v in loads(json, precise_float=self.precise_float).items()\n            }\n            self.check_keys_split(decoded)\n            self.obj = DataFrame(dtype=None, **decoded)\n        elif orient == \"index\":\n            self.obj = DataFrame.from_dict(\n                loads(json, precise_float=self.precise_float),\n                dtype=None,\n                orient=\"index\",\n            )\n            if compat.PY35:\n                self.obj = self.obj.sort_index(axis=\"columns\").sort_index(axis=\"index\")\n        elif orient == \"table\":\n            self.obj = parse_table_schema(json, precise_float=self.precise_float)\n        else:\n            self.obj = DataFrame(\n                loads(json, precise_float=self.precise_float), dtype=None\n            )\n\n    def _process_converter(self, f, filt=None):\n        \"\"\"\n        Take a conversion function and possibly recreate the frame.\n        \"\"\"\n\n        if filt is None:\n            filt = lambda col, c: True\n\n        needs_new_obj = False\n        new_obj = dict()\n        for i, (col, c) in enumerate(self.obj.items()):\n            if filt(col, c):\n                new_data, result = f(col, c)\n                if result:\n                    c = new_data\n                    needs_new_obj = True\n            new_obj[i] = c\n\n        if needs_new_obj:\n\n            # possibly handle dup columns\n            new_obj = DataFrame(new_obj, index=self.obj.index)\n            new_obj.columns = self.obj.columns\n            self.obj = new_obj\n\n    def _try_convert_types(self):\n        if self.obj is None:\n            return\n        if self.convert_dates:\n            self._try_convert_dates()\n\n        self._process_converter(\n            lambda col, c: self._try_convert_data(col, c, convert_dates=False)\n        )\n\n    def _try_convert_dates(self):\n        if self.obj is None:\n            return\n\n        # our columns to parse\n        convert_dates = self.convert_dates\n        if convert_dates is True:\n            convert_dates = []\n        convert_dates = set(convert_dates)\n\n        def is_ok(col):\n            \"\"\"\n            Return if this col is ok to try for a date parse.\n            \"\"\"\n            if not isinstance(col, str):\n                return False\n\n            col_lower = col.lower()\n            if (\n                col_lower.endswith(\"_at\")\n                or col_lower.endswith(\"_time\")\n                or col_lower == \"modified\"\n                or col_lower == \"date\"\n                or col_lower == \"datetime\"\n                or col_lower.startswith(\"timestamp\")\n            ):\n                return True\n            return False\n\n        self._process_converter(\n            lambda col, c: self._try_convert_to_date(c),\n            lambda col, c: (\n                (self.keep_default_dates and is_ok(col)) or col in convert_dates\n            ),\n        )\n"
    },
    {
      "filename": "pandas/tests/io/test_common.py",
      "content": "\"\"\"\nTests for the pandas.io.common functionalities\n\"\"\"\nfrom io import StringIO\nimport mmap\nimport os\n\nimport pytest\n\nfrom pandas.compat import is_platform_windows\nimport pandas.util._test_decorators as td\n\nimport pandas as pd\nimport pandas.util.testing as tm\n\nimport pandas.io.common as icom\n\n\nclass CustomFSPath:\n    \"\"\"For testing fspath on unknown objects\"\"\"\n\n    def __init__(self, path):\n        self.path = path\n\n    def __fspath__(self):\n        return self.path\n\n\n# Functions that consume a string path and return a string or path-like object\npath_types = [str, CustomFSPath]\n\ntry:\n    from pathlib import Path\n\n    path_types.append(Path)\nexcept ImportError:\n    pass\n\ntry:\n    from py.path import local as LocalPath\n\n    path_types.append(LocalPath)\nexcept ImportError:\n    pass\n\nHERE = os.path.abspath(os.path.dirname(__file__))\n\n\n# https://github.com/cython/cython/issues/1720\n@pytest.mark.filterwarnings(\"ignore:can't resolve package:ImportWarning\")\n@pytest.mark.filterwarnings(\"ignore:.*msgpack:FutureWarning\")\nclass TestCommonIOCapabilities:\n    data1 = \"\"\"index,A,B,C,D\nfoo,2,3,4,5\nbar,7,8,9,10\nbaz,12,13,14,15\nqux,12,13,14,15\nfoo2,12,13,14,15\nbar2,12,13,14,15\n\"\"\"\n\n    def test_expand_user(self):\n        filename = \"~/sometest\"\n        expanded_name = icom._expand_user(filename)\n\n        assert expanded_name != filename\n        assert os.path.isabs(expanded_name)\n        assert os.path.expanduser(filename) == expanded_name\n\n    def test_expand_user_normal_path(self):\n        filename = \"/somefolder/sometest\"\n        expanded_name = icom._expand_user(filename)\n\n        assert expanded_name == filename\n        assert os.path.expanduser(filename) == expanded_name\n\n    @td.skip_if_no(\"pathlib\")\n    def test_stringify_path_pathlib(self):\n        rel_path = icom._stringify_path(Path(\".\"))\n        assert rel_path == \".\"\n        redundant_path = icom._stringify_path(Path(\"foo//bar\"))\n        assert redundant_path == os.path.join(\"foo\", \"bar\")\n\n    @td.skip_if_no(\"py.path\")\n    def test_stringify_path_localpath(self):\n        path = os.path.join(\"foo\", \"bar\")\n        abs_path = os.path.abspath(path)\n        lpath = LocalPath(path)\n        assert icom._stringify_path(lpath) == abs_path\n\n    def test_stringify_path_fspath(self):\n        p = CustomFSPath(\"foo/bar.csv\")\n        result = icom._stringify_path(p)\n        assert result == \"foo/bar.csv\"\n\n    @pytest.mark.parametrize(\n        \"extension,expected\",\n        [(\"\", None), (\".gz\", \"gzip\"), (\".bz2\", \"bz2\"), (\".zip\", \"zip\"), (\".xz\", \"xz\")],\n    )\n    @pytest.mark.parametrize(\"path_type\", path_types)\n    def test_infer_compression_from_path(self, extension, expected, path_type):\n        path = path_type(\"foo/bar.csv\" + extension)\n        compression = icom._infer_compression(path, compression=\"infer\")\n        assert compression == expected\n\n    def test_get_filepath_or_buffer_with_path(self):\n        filename = \"~/sometest\"\n        filepath_or_buffer, _, _, should_close = icom.get_filepath_or_buffer(filename)\n        assert filepath_or_buffer != filename\n        assert os.path.isabs(filepath_or_buffer)\n        assert os.path.expanduser(filename) == filepath_or_buffer\n        assert not should_close\n\n    def test_get_filepath_or_buffer_with_buffer(self):\n        input_buffer = StringIO()\n        filepath_or_buffer, _, _, should_close = icom.get_filepath_or_buffer(\n            input_buffer\n        )\n        assert filepath_or_buffer == input_buffer\n        assert not should_close\n\n    def test_iterator(self):\n        reader = pd.read_csv(StringIO(self.data1), chunksize=1)\n        result = pd.concat(reader, ignore_index=True)\n        expected = pd.read_csv(StringIO(self.data1))\n        tm.assert_frame_equal(result, expected)\n\n        # GH12153\n        it = pd.read_csv(StringIO(self.data1), chunksize=1)\n        first = next(it)\n        tm.assert_frame_equal(first, expected.iloc[[0]])\n        tm.assert_frame_equal(pd.concat(it), expected.iloc[1:])\n\n    @pytest.mark.parametrize(\n        \"reader, module, error_class, fn_ext\",\n        [\n            (pd.read_csv, \"os\", FileNotFoundError, \"csv\"),\n            (pd.read_fwf, \"os\", FileNotFoundError, \"txt\"),\n            (pd.read_excel, \"xlrd\", FileNotFoundError, \"xlsx\"),\n            (pd.read_feather, \"feather\", Exception, \"feather\"),\n            (pd.read_hdf, \"tables\", FileNotFoundError, \"h5\"),\n            (pd.read_stata, \"os\", FileNotFoundError, \"dta\"),\n            (pd.read_sas, \"os\", FileNotFoundError, \"sas7bdat\"),\n            (pd.read_json, \"os\", FileNotFoundError, \"json\"),\n            (pd.read_msgpack, \"os\", FileNotFoundError, \"mp\"),\n            (pd.read_pickle, \"os\", FileNotFoundError, \"pickle\"),\n        ],\n    )\n    def test_read_non_existant(self, reader, module, error_class, fn_ext):\n        pytest.importorskip(module)\n\n        path = os.path.join(HERE, \"data\", \"does_not_exist.\" + fn_ext)\n        msg1 = r\"File (b')?.+does_not_exist\\.{}'? does not exist\".format(fn_ext)\n        msg2 = (\n            r\"\\[Errno 2\\] No such file or directory: '.+does_not_exist\" r\"\\.{}'\"\n        ).format(fn_ext)\n        msg3 = \"Expected object or value\"\n        msg4 = \"path_or_buf needs to be a string file path or file-like\"\n        msg5 = (\n            r\"\\[Errno 2\\] File .+does_not_exist\\.{} does not exist:\"\n            r\" '.+does_not_exist\\.{}'\"\n        ).format(fn_ext, fn_ext)\n        with pytest.raises(\n            error_class, match=r\"({}|{}|{}|{}|{})\".format(msg1, msg2, msg3, msg4, msg5)\n        ):\n            reader(path)\n\n    @pytest.mark.parametrize(\n        \"reader, module, error_class, fn_ext\",\n        [\n            (pd.read_csv, \"os\", FileNotFoundError, \"csv\"),\n            (pd.read_table, \"os\", FileNotFoundError, \"csv\"),\n            (pd.read_fwf, \"os\", FileNotFoundError, \"txt\"),\n            (pd.read_excel, \"xlrd\", FileNotFoundError, \"xlsx\"),\n            (pd.read_feather, \"feather\", Exception, \"feather\"),\n            (pd.read_hdf, \"tables\", FileNotFoundError, \"h5\"),\n            (pd.read_stata, \"os\", FileNotFoundError, \"dta\"),\n            (pd.read_sas, \"os\", FileNotFoundError, \"sas7bdat\"),\n            (pd.read_json, \"os\", FileNotFoundError, \"json\"),\n            (pd.read_msgpack, \"os\", FileNotFoundError, \"mp\"),\n            (pd.read_pickle, \"os\", FileNotFoundError, \"pickle\"),\n        ],\n    )\n    def test_read_expands_user_home_dir(\n        self, reader, module, error_class, fn_ext, monkeypatch\n    ):\n        pytest.importorskip(module)\n\n        path = os.path.join(\"~\", \"does_not_exist.\" + fn_ext)\n        monkeypatch.setattr(icom, \"_expand_user\", lambda x: os.path.join(\"foo\", x))\n\n        msg1 = r\"File (b')?.+does_not_exist\\.{}'? does not exist\".format(fn_ext)\n        msg2 = (\n            r\"\\[Errno 2\\] No such file or directory:\" r\" '.+does_not_exist\\.{}'\"\n        ).format(fn_ext)\n        msg3 = \"Unexpected character found when decoding 'false'\"\n        msg4 = \"path_or_buf needs to be a string file path or file-like\"\n        msg5 = (\n            r\"\\[Errno 2\\] File .+does_not_exist\\.{} does not exist:\"\n            r\" '.+does_not_exist\\.{}'\"\n        ).format(fn_ext, fn_ext)\n\n        with pytest.raises(\n            error_class, match=r\"({}|{}|{}|{}|{})\".format(msg1, msg2, msg3, msg4, msg5)\n        ):\n            reader(path)\n\n    @pytest.mark.parametrize(\n        \"reader, module, path\",\n        [\n            (pd.read_csv, \"os\", (\"io\", \"data\", \"iris.csv\")),\n            (pd.read_table, \"os\", (\"io\", \"data\", \"iris.csv\")),\n            (pd.read_fwf, \"os\", (\"io\", \"data\", \"fixed_width_format.txt\")),\n            (pd.read_excel, \"xlrd\", (\"io\", \"data\", \"test1.xlsx\")),\n            (pd.read_feather, \"feather\", (\"io\", \"data\", \"feather-0_3_1.feather\")),\n            (\n                pd.read_hdf,\n                \"tables\",\n                (\"io\", \"data\", \"legacy_hdf\", \"datetimetz_object.h5\"),\n            ),\n            (pd.read_stata, \"os\", (\"io\", \"data\", \"stata10_115.dta\")),\n            (pd.read_sas, \"os\", (\"io\", \"sas\", \"data\", \"test1.sas7bdat\")),\n            (pd.read_json, \"os\", (\"io\", \"json\", \"data\", \"tsframe_v012.json\")),\n            (pd.read_msgpack, \"os\", (\"io\", \"msgpack\", \"data\", \"frame.mp\")),\n            (pd.read_pickle, \"os\", (\"io\", \"data\", \"categorical.0.25.0.pickle\")),\n        ],\n    )\n    def test_read_fspath_all(self, reader, module, path, datapath):\n        pytest.importorskip(module)\n        path = datapath(*path)\n\n        mypath = CustomFSPath(path)\n        result = reader(mypath)\n        expected = reader(path)\n\n        if path.endswith(\".pickle\"):\n            # categorical\n            tm.assert_categorical_equal(result, expected)\n        else:\n            tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"writer_name, writer_kwargs, module\",\n        [\n            (\"to_csv\", {}, \"os\"),\n            (\"to_excel\", {\"engine\": \"xlwt\"}, \"xlwt\"),\n            (\"to_feather\", {}, \"feather\"),\n            (\"to_html\", {}, \"os\"),\n            (\"to_json\", {}, \"os\"),\n            (\"to_latex\", {}, \"os\"),\n            (\"to_msgpack\", {}, \"os\"),\n            (\"to_pickle\", {}, \"os\"),\n            (\"to_stata\", {\"time_stamp\": pd.to_datetime(\"2019-01-01 00:00\")}, \"os\"),\n        ],\n    )\n    def test_write_fspath_all(self, writer_name, writer_kwargs, module):\n        p1 = tm.ensure_clean(\"string\")\n        p2 = tm.ensure_clean(\"fspath\")\n        df = pd.DataFrame({\"A\": [1, 2]})\n\n        with p1 as string, p2 as fspath:\n            pytest.importorskip(module)\n            mypath = CustomFSPath(fspath)\n            writer = getattr(df, writer_name)\n\n            writer(string, **writer_kwargs)\n            with open(string, \"rb\") as f:\n                expected = f.read()\n\n            writer(mypath, **writer_kwargs)\n            with open(fspath, \"rb\") as f:\n                result = f.read()\n\n            assert result == expected\n\n    def test_write_fspath_hdf5(self):\n        # Same test as write_fspath_all, except HDF5 files aren't\n        # necessarily byte-for-byte identical for a given dataframe, so we'll\n        # have to read and compare equality\n        pytest.importorskip(\"tables\")\n\n        df = pd.DataFrame({\"A\": [1, 2]})\n        p1 = tm.ensure_clean(\"string\")\n        p2 = tm.ensure_clean(\"fspath\")\n\n        with p1 as string, p2 as fspath:\n            mypath = CustomFSPath(fspath)\n            df.to_hdf(mypath, key=\"bar\")\n            df.to_hdf(string, key=\"bar\")\n\n            result = pd.read_hdf(fspath, key=\"bar\")\n            expected = pd.read_hdf(string, key=\"bar\")\n\n        tm.assert_frame_equal(result, expected)\n\n\n@pytest.fixture\ndef mmap_file(datapath):\n    return datapath(\"io\", \"data\", \"test_mmap.csv\")\n\n\nclass TestMMapWrapper:\n    def test_constructor_bad_file(self, mmap_file):\n        non_file = StringIO(\"I am not a file\")\n        non_file.fileno = lambda: -1\n\n        # the error raised is different on Windows\n        if is_platform_windows():\n            msg = \"The parameter is incorrect\"\n            err = OSError\n        else:\n            msg = \"[Errno 22]\"\n            err = mmap.error\n\n        with pytest.raises(err, match=msg):\n            icom.MMapWrapper(non_file)\n\n        target = open(mmap_file, \"r\")\n        target.close()\n\n        msg = \"I/O operation on closed file\"\n        with pytest.raises(ValueError, match=msg):\n            icom.MMapWrapper(target)\n\n    def test_get_attr(self, mmap_file):\n        with open(mmap_file, \"r\") as target:\n            wrapper = icom.MMapWrapper(target)\n\n        attrs = dir(wrapper.mmap)\n        attrs = [attr for attr in attrs if not attr.startswith(\"__\")]\n        attrs.append(\"__next__\")\n\n        for attr in attrs:\n            assert hasattr(wrapper, attr)\n\n        assert not hasattr(wrapper, \"foo\")\n\n    def test_next(self, mmap_file):\n        with open(mmap_file, \"r\") as target:\n            wrapper = icom.MMapWrapper(target)\n            lines = target.readlines()\n\n        for line in lines:\n            next_line = next(wrapper)\n            assert next_line.strip() == line.strip()\n\n        with pytest.raises(StopIteration, match=r\"^$\"):\n            next(wrapper)\n\n    def test_unknown_engine(self):\n        with tm.ensure_clean() as path:\n            df = tm.makeDataFrame()\n            df.to_csv(path)\n            with pytest.raises(ValueError, match=\"Unknown engine\"):\n                pd.read_csv(path, engine=\"pyt\")\n"
    }
  ],
  "questions": [],
  "golden_answers": [],
  "questions_generated": [
    "Why does the `read_json` function currently raise a `ValueError` instead of a `FileNotFoundError` when attempting to read a non-existent JSON file?",
    "In which file and line should the check for file existence be added to raise a `FileNotFoundError` for non-existent JSON files?",
    "What was a proposed solution in the discussion to handle the misleading error message when opening a non-existent JSON file?",
    "What is a potential benefit of implementing a `FileNotFoundError` for non-existent JSON files in the `read_json` function?",
    "What considerations should be taken into account when modifying the `read_json` function to check for file existence?"
  ],
  "golden_answers_generated": [
    "The `read_json` function raises a `ValueError` because it attempts to read and parse the file content assuming the file exists. When the file is absent, the function tries to interpret an empty or non-existent stream, leading to unexpected behavior during JSON decoding. This results in misleading error messages like 'Unexpected character found when decoding 'null'' instead of a more appropriate `FileNotFoundError`.",
    "The check for file existence should be added in the `pandas/io/json/_json.py` file, specifically in the logic that handles file path input in the `read_json` function. This is where the function first interacts with the file system to open the JSON file, making it the ideal place to implement a conditional check using `os.path.isfile()` to raise a `FileNotFoundError` if the file does not exist.",
    "A proposed solution was to check if the file path provided is a string that ends with typical JSON file extensions (e.g., '.json', '.json.gz', '.json.bz2') and verify if the file exists using `os.path.isfile()`. If the file does not exist, a `FileNotFoundError` should be raised with a clear message indicating the absence of the file, such as 'No such file: 'filename.json'.",
    "Implementing a `FileNotFoundError` would provide a more intuitive and informative error message for users, helping them quickly identify that the issue is related to the file's existence rather than a JSON parsing error. This clarity can improve user experience by reducing confusion and aiding in faster debugging.",
    "When modifying the `read_json` function, it's important to ensure that the file existence check is efficient and does not disrupt the function's existing logic. The check should be positioned such that it only runs when a file path string is provided, not when a file-like object is passed. Additionally, it should handle various file extensions and consider cross-platform compatibility issues with file path handling."
  ]
}