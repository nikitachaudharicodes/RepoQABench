{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "16829",
  "issue_description": "# DOC: Series.idxmax actually returns a label, not an index\n\nIn https://github.com/pandas-dev/pandas/blob/master/pandas/core/series.py, the DocStrings for `idxmax` and `idxmin` are misleading (in my opinion).  They say they return the index of the max/min, but they actually return the label (if I've got my Pandas lingo right).\r\n\r\nSame with `DataFrame.idxmax` and `DataFrame.idxmin`.  \r\n\r\nAlso, is there a reason `Series` provides `argmax` as a synonym for `idxmax`, and `DataFrame` does not?\r\n\r\nOne more thought: I find it surprising that `idxmax` and `argmax` do the same thing.  I expected `idxmax` to return an index and `argmax` to return a label.  I suppose it's too late to make that change.\r\n\r\n",
  "issue_comments": [
    {
      "id": 313117624,
      "user": "TomAugspurger",
      "body": "Agreed that changing to label would be clearer.\r\n\r\nFor the `idxmax` vs. `argmax`, I think that was accidentally broken once `Series` no longer inherited from `ndarray`, see https://github.com/pandas-dev/pandas/issues/6214.\r\nI think we could issue a `FutureWarning` in 0.21.0, saying to use `.idxmax` instead of `.argmax`, and then followup to change `.argmax` to always be positional."
    },
    {
      "id": 313117815,
      "user": "TomAugspurger",
      "body": "Let's leave this issue for the documentation, and I'll open a new one for the argmax.\r\n\r\nedit: https://github.com/pandas-dev/pandas/issues/16830"
    },
    {
      "id": 313147839,
      "user": "AllenDowney",
      "body": "Thanks, Tom.  Please let me know if there's anything I can do to help.\n\nOn Wed, Jul 5, 2017 at 10:22 AM, Tom Augspurger <notifications@github.com>\nwrote:\n\n> Let's leave this issue for the documentation, and I'll open a new one for\n> the argmax.\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/16829#issuecomment-313117815>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABy37fO8EHOFEnYcCGnnBqgMcxUi63jLks5sK5wJgaJpZM4OOYys>\n> .\n>\n"
    },
    {
      "id": 324457500,
      "user": "iuliakhomenko",
      "body": "Hi! I will correct `idxmax` and `idxmin` documentation for both `Series` and `DataFrame` "
    },
    {
      "id": 324458312,
      "user": "gfyoung",
      "body": "@iuliakhomenko : Go for it!  Thanks for volunteering!"
    },
    {
      "id": 324458679,
      "user": "TomAugspurger",
      "body": "Hi @iuliakhomenko. I think the ongoing work in https://github.com/pandas-dev/pandas/pull/16955 will make the changes here unnecessary, so this may not be a good issue to work on.\r\n\r\nWe have plenty of other issues to take on if you're interested in contributing though :) Let us know if you need help finding one."
    },
    {
      "id": 324459567,
      "user": "iuliakhomenko",
      "body": "@TomAugspurger ok, I will look into other issues. Thanks :)"
    },
    {
      "id": 324460231,
      "user": "gfyoung",
      "body": "> I think the ongoing work in #16955 will make the changes here unnecessary\r\n\r\n@TomAugspurger : Except that the work isn't going ATM ;)\r\n\r\nI don't see why we shouldn't address this issue unless we *definitely* plan on pushing #16955 through, which judging from the lack of tagging, isn't going to happen for now."
    },
    {
      "id": 324478315,
      "user": "jorisvandenbossche",
      "body": "@gfyoung I think that PR should certainly go in, and is in fact almost ready. In the submitter doesn't respond the coming week(s), we can easily fix up and merge."
    },
    {
      "id": 324479491,
      "user": "gfyoung",
      "body": "@jorisvandenbossche : Sounds good.  I didn't know where that PR stood currently in the pipeline.  Thanks for clarifying!"
    },
    {
      "id": 324479684,
      "user": "jorisvandenbossche",
      "body": "Just my personal opinion :-) But I have tagged it now as such to make it clear"
    },
    {
      "id": 370449932,
      "user": "jorisvandenbossche",
      "body": "This is fixed in the meantime by https://github.com/pandas-dev/pandas/pull/16955"
    }
  ],
  "text_context": "# DOC: Series.idxmax actually returns a label, not an index\n\nIn https://github.com/pandas-dev/pandas/blob/master/pandas/core/series.py, the DocStrings for `idxmax` and `idxmin` are misleading (in my opinion).  They say they return the index of the max/min, but they actually return the label (if I've got my Pandas lingo right).\r\n\r\nSame with `DataFrame.idxmax` and `DataFrame.idxmin`.  \r\n\r\nAlso, is there a reason `Series` provides `argmax` as a synonym for `idxmax`, and `DataFrame` does not?\r\n\r\nOne more thought: I find it surprising that `idxmax` and `argmax` do the same thing.  I expected `idxmax` to return an index and `argmax` to return a label.  I suppose it's too late to make that change.\r\n\r\n\n\nAgreed that changing to label would be clearer.\r\n\r\nFor the `idxmax` vs. `argmax`, I think that was accidentally broken once `Series` no longer inherited from `ndarray`, see https://github.com/pandas-dev/pandas/issues/6214.\r\nI think we could issue a `FutureWarning` in 0.21.0, saying to use `.idxmax` instead of `.argmax`, and then followup to change `.argmax` to always be positional.\n\nLet's leave this issue for the documentation, and I'll open a new one for the argmax.\r\n\r\nedit: https://github.com/pandas-dev/pandas/issues/16830\n\nThanks, Tom.  Please let me know if there's anything I can do to help.\n\nOn Wed, Jul 5, 2017 at 10:22 AM, Tom Augspurger <notifications@github.com>\nwrote:\n\n> Let's leave this issue for the documentation, and I'll open a new one for\n> the argmax.\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/16829#issuecomment-313117815>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABy37fO8EHOFEnYcCGnnBqgMcxUi63jLks5sK5wJgaJpZM4OOYys>\n> .\n>\n\n\nHi! I will correct `idxmax` and `idxmin` documentation for both `Series` and `DataFrame` \n\n@iuliakhomenko : Go for it!  Thanks for volunteering!\n\nHi @iuliakhomenko. I think the ongoing work in https://github.com/pandas-dev/pandas/pull/16955 will make the changes here unnecessary, so this may not be a good issue to work on.\r\n\r\nWe have plenty of other issues to take on if you're interested in contributing though :) Let us know if you need help finding one.\n\n@TomAugspurger ok, I will look into other issues. Thanks :)\n\n> I think the ongoing work in #16955 will make the changes here unnecessary\r\n\r\n@TomAugspurger : Except that the work isn't going ATM ;)\r\n\r\nI don't see why we shouldn't address this issue unless we *definitely* plan on pushing #16955 through, which judging from the lack of tagging, isn't going to happen for now.\n\n@gfyoung I think that PR should certainly go in, and is in fact almost ready. In the submitter doesn't respond the coming week(s), we can easily fix up and merge.\n\n@jorisvandenbossche : Sounds good.  I didn't know where that PR stood currently in the pipeline.  Thanks for clarifying!\n\nJust my personal opinion :-) But I have tagged it now as such to make it clear\n\nThis is fixed in the meantime by https://github.com/pandas-dev/pandas/pull/16955",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/16955",
  "code_context": [
    {
      "filename": "pandas/core/series.py",
      "content": "\"\"\"\nData structure for 1-dimensional cross-sectional and time series data\n\"\"\"\nfrom __future__ import division\n\n# pylint: disable=E1101,E1103\n# pylint: disable=W0703,W0622,W0613,W0201\n\nimport types\nimport warnings\nfrom textwrap import dedent\n\nimport numpy as np\nimport numpy.ma as ma\n\nfrom pandas.core.dtypes.common import (\n    is_categorical_dtype,\n    is_bool,\n    is_integer, is_integer_dtype,\n    is_float_dtype,\n    is_extension_type, is_datetimetz,\n    is_datetime64tz_dtype,\n    is_timedelta64_dtype,\n    is_list_like,\n    is_hashable,\n    is_iterator,\n    is_dict_like,\n    is_scalar,\n    _is_unorderable_exception,\n    _ensure_platform_int,\n    pandas_dtype)\nfrom pandas.core.dtypes.generic import ABCSparseArray, ABCDataFrame\nfrom pandas.core.dtypes.cast import (\n    maybe_upcast, infer_dtype_from_scalar,\n    maybe_convert_platform,\n    maybe_cast_to_datetime, maybe_castable)\nfrom pandas.core.dtypes.missing import isna, notna, remove_na_arraylike\n\nfrom pandas.core.common import (is_bool_indexer,\n                                _default_index,\n                                _asarray_tuplesafe,\n                                _values_from_object,\n                                _try_sort,\n                                _maybe_match_name,\n                                SettingWithCopyError,\n                                _maybe_box_datetimelike,\n                                _dict_compat,\n                                standardize_mapping)\nfrom pandas.core.index import (Index, MultiIndex, InvalidIndexError,\n                               Float64Index, _ensure_index)\nfrom pandas.core.indexing import check_bool_indexer, maybe_convert_indices\nfrom pandas.core import generic, base\nfrom pandas.core.internals import SingleBlockManager\nfrom pandas.core.categorical import Categorical, CategoricalAccessor\nimport pandas.core.strings as strings\nfrom pandas.core.indexes.accessors import CombinedDatetimelikeProperties\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.timedeltas import TimedeltaIndex\nfrom pandas.core.indexes.period import PeriodIndex\nfrom pandas import compat\nfrom pandas.io.formats.terminal import get_terminal_size\nfrom pandas.compat import zip, u, OrderedDict, StringIO\nfrom pandas.compat.numpy import function as nv\n\nfrom pandas.core import accessor\nimport pandas.core.ops as ops\nimport pandas.core.algorithms as algorithms\n\nimport pandas.core.common as com\nimport pandas.core.nanops as nanops\nimport pandas.io.formats.format as fmt\nfrom pandas.util._decorators import (\n    Appender, deprecate, deprecate_kwarg, Substitution)\nfrom pandas.util._validators import validate_bool_kwarg\n\nfrom pandas._libs import index as libindex, tslib as libts, lib, iNaT\nfrom pandas.core.config import get_option\n\nimport pandas.plotting._core as gfx\n\n__all__ = ['Series']\n\n_shared_doc_kwargs = dict(\n    axes='index', klass='Series', axes_single_arg=\"{0, 'index'}\",\n    inplace=\"\"\"inplace : boolean, default False\n        If True, performs operation inplace and returns None.\"\"\",\n    unique='np.ndarray', duplicated='Series',\n    optional_by='',\n    versionadded_to_excel='\\n    .. versionadded:: 0.20.0\\n')\n\n\n# see gh-16971\ndef remove_na(arr):\n    \"\"\"\n    DEPRECATED : this function will be removed in a future version.\n    \"\"\"\n\n    warnings.warn(\"remove_na is deprecated and is a private \"\n                  \"function. Do not use.\", FutureWarning, stacklevel=2)\n    return remove_na_arraylike(arr)\n\n\ndef _coerce_method(converter):\n    \"\"\" install the scalar coercion methods \"\"\"\n\n    def wrapper(self):\n        if len(self) == 1:\n            return converter(self.iloc[0])\n        raise TypeError(\"cannot convert the series to \"\n                        \"{0}\".format(str(converter)))\n\n    return wrapper\n\n# ----------------------------------------------------------------------\n# Series class\n\n\nclass Series(base.IndexOpsMixin, generic.NDFrame):\n    \"\"\"\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, dict, or scalar value\n        Contains data stored in Series\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex(len(data)) if not provided. If both a dict and index\n        sequence are used, the index will override the keys found in the\n        dict.\n    dtype : numpy.dtype or None\n        If None, dtype will be inferred\n    copy : boolean, default False\n        Copy input data\n    \"\"\"\n    _metadata = ['name']\n    _accessors = frozenset(['dt', 'cat', 'str'])\n    _allow_index_ops = True\n\n    def __init__(self, data=None, index=None, dtype=None, name=None,\n                 copy=False, fastpath=False):\n\n        # we are called internally, so short-circuit\n        if fastpath:\n\n            # data is an ndarray, index is defined\n            if not isinstance(data, SingleBlockManager):\n                data = SingleBlockManager(data, index, fastpath=True)\n            if copy:\n                data = data.copy()\n            if index is None:\n                index = data.index\n\n        else:\n\n            if index is not None:\n                index = _ensure_index(index)\n\n            if data is None:\n                data = {}\n            if dtype is not None:\n                dtype = self._validate_dtype(dtype)\n\n            if isinstance(data, MultiIndex):\n                raise NotImplementedError(\"initializing a Series from a \"\n                                          \"MultiIndex is not supported\")\n            elif isinstance(data, Index):\n                # need to copy to avoid aliasing issues\n                if name is None:\n                    name = data.name\n\n                data = data._to_embed(keep_tz=True)\n                copy = True\n            elif isinstance(data, np.ndarray):\n                pass\n            elif isinstance(data, Series):\n                if name is None:\n                    name = data.name\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n                data = data._data\n            elif isinstance(data, dict):\n                if index is None:\n                    if isinstance(data, OrderedDict):\n                        index = Index(data)\n                    else:\n                        index = Index(_try_sort(data))\n                try:\n                    if isinstance(index, DatetimeIndex):\n                        if len(data):\n                            # coerce back to datetime objects for lookup\n                            data = _dict_compat(data)\n                            data = lib.fast_multiget(data,\n                                                     index.asobject.values,\n                                                     default=np.nan)\n                        else:\n                            data = np.nan\n                    # GH #12169\n                    elif isinstance(index, (PeriodIndex, TimedeltaIndex)):\n                        data = ([data.get(i, np.nan) for i in index]\n                                if data else np.nan)\n                    else:\n                        data = lib.fast_multiget(data, index.values,\n                                                 default=np.nan)\n                except TypeError:\n                    data = ([data.get(i, np.nan) for i in index]\n                            if data else np.nan)\n\n            elif isinstance(data, SingleBlockManager):\n                if index is None:\n                    index = data.index\n                else:\n                    data = data.reindex(index, copy=copy)\n            elif isinstance(data, Categorical):\n                # GH12574: Allow dtype=category only, otherwise error\n                if ((dtype is not None) and\n                        not is_categorical_dtype(dtype)):\n                    raise ValueError(\"cannot specify a dtype with a \"\n                                     \"Categorical unless \"\n                                     \"dtype='category'\")\n            elif (isinstance(data, types.GeneratorType) or\n                  (compat.PY3 and isinstance(data, map))):\n                data = list(data)\n            elif isinstance(data, (set, frozenset)):\n                raise TypeError(\"{0!r} type is unordered\"\n                                \"\".format(data.__class__.__name__))\n            else:\n\n                # handle sparse passed here (and force conversion)\n                if isinstance(data, ABCSparseArray):\n                    data = data.to_dense()\n\n            if index is None:\n                if not is_list_like(data):\n                    data = [data]\n                index = _default_index(len(data))\n\n            # create/copy the manager\n            if isinstance(data, SingleBlockManager):\n                if dtype is not None:\n                    data = data.astype(dtype=dtype, raise_on_error=False,\n                                       copy=copy)\n                elif copy:\n                    data = data.copy()\n            else:\n                data = _sanitize_array(data, index, dtype, copy,\n                                       raise_cast_failure=True)\n\n                data = SingleBlockManager(data, index, fastpath=True)\n\n        generic.NDFrame.__init__(self, data, fastpath=True)\n\n        self.name = name\n        self._set_axis(0, index, fastpath=True)\n\n    @classmethod\n    def from_array(cls, arr, index=None, name=None, dtype=None, copy=False,\n                   fastpath=False):\n        # return a sparse series here\n        if isinstance(arr, ABCSparseArray):\n            from pandas.core.sparse.series import SparseSeries\n            cls = SparseSeries\n\n        return cls(arr, index=index, name=name, dtype=dtype, copy=copy,\n                   fastpath=fastpath)\n\n    @property\n    def _constructor(self):\n        return Series\n\n    @property\n    def _constructor_expanddim(self):\n        from pandas.core.frame import DataFrame\n        return DataFrame\n\n    # types\n    @property\n    def _can_hold_na(self):\n        return self._data._can_hold_na\n\n    _index = None\n\n    def _set_axis(self, axis, labels, fastpath=False):\n        \"\"\" override generic, we want to set the _typ here \"\"\"\n\n        if not fastpath:\n            labels = _ensure_index(labels)\n\n        is_all_dates = labels.is_all_dates\n        if is_all_dates:\n            if not isinstance(labels,\n                              (DatetimeIndex, PeriodIndex, TimedeltaIndex)):\n                try:\n                    labels = DatetimeIndex(labels)\n                    # need to set here becuase we changed the index\n                    if fastpath:\n                        self._data.set_axis(axis, labels)\n                except (libts.OutOfBoundsDatetime, ValueError):\n                    # labels may exceeds datetime bounds,\n                    # or not be a DatetimeIndex\n                    pass\n\n        self._set_subtyp(is_all_dates)\n\n        object.__setattr__(self, '_index', labels)\n        if not fastpath:\n            self._data.set_axis(axis, labels)\n\n    def _set_subtyp(self, is_all_dates):\n        if is_all_dates:\n            object.__setattr__(self, '_subtyp', 'time_series')\n        else:\n            object.__setattr__(self, '_subtyp', 'series')\n\n    def _update_inplace(self, result, **kwargs):\n        # we want to call the generic version and not the IndexOpsMixin\n        return generic.NDFrame._update_inplace(self, result, **kwargs)\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        if value is not None and not is_hashable(value):\n            raise TypeError('Series.name must be a hashable type')\n        object.__setattr__(self, '_name', value)\n\n    # ndarray compatibility\n    @property\n    def dtype(self):\n        \"\"\" return the dtype object of the underlying data \"\"\"\n        return self._data.dtype\n\n    @property\n    def dtypes(self):\n        \"\"\" return the dtype object of the underlying data \"\"\"\n        return self._data.dtype\n\n    @property\n    def ftype(self):\n        \"\"\" return if the data is sparse|dense \"\"\"\n        return self._data.ftype\n\n    @property\n    def ftypes(self):\n        \"\"\" return if the data is sparse|dense \"\"\"\n        return self._data.ftype\n\n    @property\n    def values(self):\n        \"\"\"\n        Return Series as ndarray or ndarray-like\n        depending on the dtype\n\n        Returns\n        -------\n        arr : numpy.ndarray or ndarray-like\n\n        Examples\n        --------\n        >>> pd.Series([1, 2, 3]).values\n        array([1, 2, 3])\n\n        >>> pd.Series(list('aabc')).values\n        array(['a', 'a', 'b', 'c'], dtype=object)\n\n        >>> pd.Series(list('aabc')).astype('category').values\n        [a, a, b, c]\n        Categories (3, object): [a, b, c]\n\n        Timezone aware datetime data is converted to UTC:\n\n        >>> pd.Series(pd.date_range('20130101', periods=3,\n        ...                         tz='US/Eastern')).values\n        array(['2013-01-01T05:00:00.000000000',\n               '2013-01-02T05:00:00.000000000',\n               '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\n\n        \"\"\"\n        return self._data.external_values()\n\n    @property\n    def _values(self):\n        \"\"\" return the internal repr of this data \"\"\"\n        return self._data.internal_values()\n\n    def _formatting_values(self):\n        \"\"\"Return the values that can be formatted (used by SeriesFormatter\n        and DataFrameFormatter)\n        \"\"\"\n        return self._data.formatting_values()\n\n    def get_values(self):\n        \"\"\" same as values (but handles sparseness conversions); is a view \"\"\"\n        return self._data.get_values()\n\n    @property\n    def asobject(self):\n        \"\"\"\n        return object Series which contains boxed values\n\n        *this is an internal non-public method*\n        \"\"\"\n        return self._data.asobject\n\n    # ops\n    def ravel(self, order='C'):\n        \"\"\"\n        Return the flattened underlying data as an ndarray\n\n        See also\n        --------\n        numpy.ndarray.ravel\n        \"\"\"\n        return self._values.ravel(order=order)\n\n    def compress(self, condition, *args, **kwargs):\n        \"\"\"\n        Return selected slices of an array along given axis as a Series\n\n        See also\n        --------\n        numpy.ndarray.compress\n        \"\"\"\n        nv.validate_compress(args, kwargs)\n        return self[condition]\n\n    def nonzero(self):\n        \"\"\"\n        Return the indices of the elements that are non-zero\n\n        This method is equivalent to calling `numpy.nonzero` on the\n        series data. For compatability with NumPy, the return value is\n        the same (a tuple with an array of indices for each dimension),\n        but it will always be a one-item tuple because series only have\n        one dimension.\n\n        Examples\n        --------\n        >>> s = pd.Series([0, 3, 0, 4])\n        >>> s.nonzero()\n        (array([1, 3]),)\n        >>> s.iloc[s.nonzero()[0]]\n        1    3\n        3    4\n        dtype: int64\n\n        See Also\n        --------\n        numpy.nonzero\n        \"\"\"\n        return self._values.nonzero()\n\n    def put(self, *args, **kwargs):\n        \"\"\"\n        Applies the `put` method to its `values` attribute\n        if it has one.\n\n        See also\n        --------\n        numpy.ndarray.put\n        \"\"\"\n        self._values.put(*args, **kwargs)\n\n    def __len__(self):\n        \"\"\"\n        return the length of the Series\n        \"\"\"\n        return len(self._data)\n\n    def view(self, dtype=None):\n        return self._constructor(self._values.view(dtype),\n                                 index=self.index).__finalize__(self)\n\n    def __array__(self, result=None):\n        \"\"\"\n        the array interface, return my values\n        \"\"\"\n        return self.get_values()\n\n    def __array_wrap__(self, result, context=None):\n        \"\"\"\n        Gets called after a ufunc\n        \"\"\"\n        return self._constructor(result, index=self.index,\n                                 copy=False).__finalize__(self)\n\n    def __array_prepare__(self, result, context=None):\n        \"\"\"\n        Gets called prior to a ufunc\n        \"\"\"\n\n        # nice error message for non-ufunc types\n        if context is not None and not isinstance(self._values, np.ndarray):\n            obj = context[1][0]\n            raise TypeError(\"{obj} with dtype {dtype} cannot perform \"\n                            \"the numpy op {op}\".format(\n                                obj=type(obj).__name__,\n                                dtype=getattr(obj, 'dtype', None),\n                                op=context[0].__name__))\n        return result\n\n    # complex\n    @property\n    def real(self):\n        return self.values.real\n\n    @real.setter\n    def real(self, v):\n        self.values.real = v\n\n    @property\n    def imag(self):\n        return self.values.imag\n\n    @imag.setter\n    def imag(self, v):\n        self.values.imag = v\n\n    # coercion\n    __float__ = _coerce_method(float)\n    __long__ = _coerce_method(int)\n    __int__ = _coerce_method(int)\n\n    def _unpickle_series_compat(self, state):\n        if isinstance(state, dict):\n            self._data = state['_data']\n            self.name = state['name']\n            self.index = self._data.index\n\n        elif isinstance(state, tuple):\n\n            # < 0.12 series pickle\n\n            nd_state, own_state = state\n\n            # recreate the ndarray\n            data = np.empty(nd_state[1], dtype=nd_state[2])\n            np.ndarray.__setstate__(data, nd_state)\n\n            # backwards compat\n            index, name = own_state[0], None\n            if len(own_state) > 1:\n                name = own_state[1]\n\n            # recreate\n            self._data = SingleBlockManager(data, index, fastpath=True)\n            self._index = index\n            self.name = name\n\n        else:\n            raise Exception(\"cannot unpickle legacy formats -> [%s]\" % state)\n\n    # indexers\n    @property\n    def axes(self):\n        \"\"\"Return a list of the row axis labels\"\"\"\n        return [self.index]\n\n    def _ixs(self, i, axis=0):\n        \"\"\"\n        Return the i-th value or values in the Series by location\n\n        Parameters\n        ----------\n        i : int, slice, or sequence of integers\n\n        Returns\n        -------\n        value : scalar (int) or Series (slice, sequence)\n        \"\"\"\n        try:\n\n            # dispatch to the values if we need\n            values = self._values\n            if isinstance(values, np.ndarray):\n                return libindex.get_value_at(values, i)\n            else:\n                return values[i]\n        except IndexError:\n            raise\n        except:\n            if isinstance(i, slice):\n                indexer = self.index._convert_slice_indexer(i, kind='iloc')\n                return self._get_values(indexer)\n            else:\n                label = self.index[i]\n                if isinstance(label, Index):\n                    return self.take(i, axis=axis, convert=True)\n                else:\n                    return libindex.get_value_at(self, i)\n\n    @property\n    def _is_mixed_type(self):\n        return False\n\n    def _slice(self, slobj, axis=0, kind=None):\n        slobj = self.index._convert_slice_indexer(slobj,\n                                                  kind=kind or 'getitem')\n        return self._get_values(slobj)\n\n    def __getitem__(self, key):\n        key = com._apply_if_callable(key, self)\n        try:\n            result = self.index.get_value(self, key)\n\n            if not is_scalar(result):\n                if is_list_like(result) and not isinstance(result, Series):\n\n                    # we need to box if we have a non-unique index here\n                    # otherwise have inline ndarray/lists\n                    if not self.index.is_unique:\n                        result = self._constructor(\n                            result, index=[key] * len(result),\n                            dtype=self.dtype).__finalize__(self)\n\n            return result\n        except InvalidIndexError:\n            pass\n        except (KeyError, ValueError):\n            if isinstance(key, tuple) and isinstance(self.index, MultiIndex):\n                # kludge\n                pass\n            elif key is Ellipsis:\n                return self\n            elif is_bool_indexer(key):\n                pass\n            else:\n\n                # we can try to coerce the indexer (or this will raise)\n                new_key = self.index._convert_scalar_indexer(key,\n                                                             kind='getitem')\n                if type(new_key) != type(key):\n                    return self.__getitem__(new_key)\n                raise\n\n        except Exception:\n            raise\n\n        if is_iterator(key):\n            key = list(key)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(self.index, key)\n\n        return self._get_with(key)\n\n    def _get_with(self, key):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            indexer = self.index._convert_slice_indexer(key, kind='getitem')\n            return self._get_values(indexer)\n        elif isinstance(key, ABCDataFrame):\n            raise TypeError('Indexing a Series with DataFrame is not '\n                            'supported, use the appropriate DataFrame column')\n        else:\n            if isinstance(key, tuple):\n                try:\n                    return self._get_values_tuple(key)\n                except:\n                    if len(key) == 1:\n                        key = key[0]\n                        if isinstance(key, slice):\n                            return self._get_values(key)\n                    raise\n\n            # pragma: no cover\n            if not isinstance(key, (list, np.ndarray, Series, Index)):\n                key = list(key)\n\n            if isinstance(key, Index):\n                key_type = key.inferred_type\n            else:\n                key_type = lib.infer_dtype(key)\n\n            if key_type == 'integer':\n                if self.index.is_integer() or self.index.is_floating():\n                    return self.reindex(key)\n                else:\n                    return self._get_values(key)\n            elif key_type == 'boolean':\n                return self._get_values(key)\n            else:\n                try:\n                    # handle the dup indexing case (GH 4246)\n                    if isinstance(key, (list, tuple)):\n                        return self.loc[key]\n\n                    return self.reindex(key)\n                except Exception:\n                    # [slice(0, 5, None)] will break if you convert to ndarray,\n                    # e.g. as requested by np.median\n                    # hack\n                    if isinstance(key[0], slice):\n                        return self._get_values(key)\n                    raise\n\n    def _get_values_tuple(self, key):\n        # mpl hackaround\n        if any(k is None for k in key):\n            return self._get_values(key)\n\n        if not isinstance(self.index, MultiIndex):\n            raise ValueError('Can only tuple-index with a MultiIndex')\n\n        # If key is contained, would have returned by now\n        indexer, new_index = self.index.get_loc_level(key)\n        return self._constructor(self._values[indexer],\n                                 index=new_index).__finalize__(self)\n\n    def _get_values(self, indexer):\n        try:\n            return self._constructor(self._data.get_slice(indexer),\n                                     fastpath=True).__finalize__(self)\n        except Exception:\n            return self._values[indexer]\n\n    def __setitem__(self, key, value):\n        key = com._apply_if_callable(key, self)\n\n        def setitem(key, value):\n            try:\n                self._set_with_engine(key, value)\n                return\n            except (SettingWithCopyError):\n                raise\n            except (KeyError, ValueError):\n                values = self._values\n                if (is_integer(key) and\n                        not self.index.inferred_type == 'integer'):\n\n                    values[key] = value\n                    return\n                elif key is Ellipsis:\n                    self[:] = value\n                    return\n                elif com.is_bool_indexer(key):\n                    pass\n                elif is_timedelta64_dtype(self.dtype):\n                    # reassign a null value to iNaT\n                    if isna(value):\n                        value = iNaT\n\n                        try:\n                            self.index._engine.set_value(self._values, key,\n                                                         value)\n                            return\n                        except TypeError:\n                            pass\n\n                self.loc[key] = value\n                return\n\n            except TypeError as e:\n                if (isinstance(key, tuple) and\n                        not isinstance(self.index, MultiIndex)):\n                    raise ValueError(\"Can only tuple-index with a MultiIndex\")\n\n                # python 3 type errors should be raised\n                if _is_unorderable_exception(e):\n                    raise IndexError(key)\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self.index, key)\n                try:\n                    self._where(~key, value, inplace=True)\n                    return\n                except InvalidIndexError:\n                    pass\n\n            self._set_with(key, value)\n\n        # do the setitem\n        cacher_needs_updating = self._check_is_chained_assignment_possible()\n        setitem(key, value)\n        if cacher_needs_updating:\n            self._maybe_update_cacher()\n\n    def _set_with_engine(self, key, value):\n        values = self._values\n        try:\n            self.index._engine.set_value(values, key, value)\n            return\n        except KeyError:\n            values[self.index.get_loc(key)] = value\n            return\n\n    def _set_with(self, key, value):\n        # other: fancy integer or otherwise\n        if isinstance(key, slice):\n            indexer = self.index._convert_slice_indexer(key, kind='getitem')\n            return self._set_values(indexer, value)\n        else:\n            if isinstance(key, tuple):\n                try:\n                    self._set_values(key, value)\n                except Exception:\n                    pass\n\n            if not isinstance(key, (list, Series, np.ndarray, Series)):\n                try:\n                    key = list(key)\n                except:\n                    key = [key]\n\n            if isinstance(key, Index):\n                key_type = key.inferred_type\n            else:\n                key_type = lib.infer_dtype(key)\n\n            if key_type == 'integer':\n                if self.index.inferred_type == 'integer':\n                    self._set_labels(key, value)\n                else:\n                    return self._set_values(key, value)\n            elif key_type == 'boolean':\n                self._set_values(key.astype(np.bool_), value)\n            else:\n                self._set_labels(key, value)\n\n    def _set_labels(self, key, value):\n        if isinstance(key, Index):\n            key = key.values\n        else:\n            key = _asarray_tuplesafe(key)\n        indexer = self.index.get_indexer(key)\n        mask = indexer == -1\n        if mask.any():\n            raise ValueError('%s not contained in the index' % str(key[mask]))\n        self._set_values(indexer, value)\n\n    def _set_values(self, key, value):\n        if isinstance(key, Series):\n            key = key._values\n        self._data = self._data.setitem(indexer=key, value=value)\n        self._maybe_update_cacher()\n\n    @deprecate_kwarg(old_arg_name='reps', new_arg_name='repeats')\n    def repeat(self, repeats, *args, **kwargs):\n        \"\"\"\n        Repeat elements of an Series. Refer to `numpy.ndarray.repeat`\n        for more information about the `repeats` argument.\n\n        See also\n        --------\n        numpy.ndarray.repeat\n        \"\"\"\n        nv.validate_repeat(args, kwargs)\n        new_index = self.index.repeat(repeats)\n        new_values = self._values.repeat(repeats)\n        return self._constructor(new_values,\n                                 index=new_index).__finalize__(self)\n\n    def reshape(self, *args, **kwargs):\n        \"\"\"\n        .. deprecated:: 0.19.0\n           Calling this method will raise an error. Please call\n           ``.values.reshape(...)`` instead.\n\n        return an ndarray with the values shape\n        if the specified shape matches exactly the current shape, then\n        return self (for compat)\n\n        See also\n        --------\n        numpy.ndarray.reshape\n        \"\"\"\n        warnings.warn(\"reshape is deprecated and will raise \"\n                      \"in a subsequent release. Please use \"\n                      \".values.reshape(...) instead\", FutureWarning,\n                      stacklevel=2)\n\n        if len(args) == 1 and hasattr(args[0], '__iter__'):\n            shape = args[0]\n        else:\n            shape = args\n\n        if tuple(shape) == self.shape:\n            # XXX ignoring the \"order\" keyword.\n            nv.validate_reshape(tuple(), kwargs)\n            return self\n\n        return self._values.reshape(shape, **kwargs)\n\n    def get_value(self, label, takeable=False):\n        \"\"\"\n        Quickly retrieve single value at passed index label\n\n        Parameters\n        ----------\n        index : label\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        value : scalar value\n        \"\"\"\n        if takeable is True:\n            return _maybe_box_datetimelike(self._values[label])\n        return self.index.get_value(self._values, label)\n\n    def set_value(self, label, value, takeable=False):\n        \"\"\"\n        Quickly set single value at passed label. If label is not contained, a\n        new object is created with the label placed at the end of the result\n        index\n\n        Parameters\n        ----------\n        label : object\n            Partial indexing with MultiIndex not allowed\n        value : object\n            Scalar value\n        takeable : interpret the index as indexers, default False\n\n        Returns\n        -------\n        series : Series\n            If label is contained, will be reference to calling Series,\n            otherwise a new object\n        \"\"\"\n        try:\n            if takeable:\n                self._values[label] = value\n            else:\n                self.index._engine.set_value(self._values, label, value)\n            return self\n        except KeyError:\n\n            # set using a non-recursive method\n            self.loc[label] = value\n            return self\n\n    def reset_index(self, level=None, drop=False, name=None, inplace=False):\n        \"\"\"\n        Analogous to the :meth:`pandas.DataFrame.reset_index` function, see\n        docstring there.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default None\n            Only remove the given levels from the index. Removes all levels by\n            default\n        drop : boolean, default False\n            Do not try to insert index into dataframe columns\n        name : object, default None\n            The name of the column corresponding to the Series values\n        inplace : boolean, default False\n            Modify the Series in place (do not create a new object)\n\n        Returns\n        ----------\n        resetted : DataFrame, or Series if drop == True\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4], index=pd.Index(['a', 'b', 'c', 'd'],\n        ...                                            name = 'idx'))\n        >>> s.reset_index()\n           index  0\n        0      0  1\n        1      1  2\n        2      2  3\n        3      3  4\n\n        >>> arrays = [np.array(['bar', 'bar', 'baz', 'baz', 'foo',\n        ...                     'foo', 'qux', 'qux']),\n        ...           np.array(['one', 'two', 'one', 'two', 'one', 'two',\n        ...                     'one', 'two'])]\n        >>> s2 = pd.Series(\n        ...     np.random.randn(8),\n        ...     index=pd.MultiIndex.from_arrays(arrays,\n        ...                                     names=['a', 'b']))\n        >>> s2.reset_index(level='a')\n               a         0\n        b\n        one  bar -0.286320\n        two  bar -0.587934\n        one  baz  0.710491\n        two  baz -1.429006\n        one  foo  0.790700\n        two  foo  0.824863\n        one  qux -0.718963\n        two  qux -0.055028\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if drop:\n            new_index = _default_index(len(self))\n            if level is not None and isinstance(self.index, MultiIndex):\n                if not isinstance(level, (tuple, list)):\n                    level = [level]\n                level = [self.index._get_level_number(lev) for lev in level]\n                if len(level) < len(self.index.levels):\n                    new_index = self.index.droplevel(level)\n\n            if inplace:\n                self.index = new_index\n                # set name if it was passed, otherwise, keep the previous name\n                self.name = name or self.name\n            else:\n                return self._constructor(self._values.copy(),\n                                         index=new_index).__finalize__(self)\n        elif inplace:\n            raise TypeError('Cannot reset_index inplace on a Series '\n                            'to create a DataFrame')\n        else:\n            df = self.to_frame(name)\n            return df.reset_index(level=level, drop=drop)\n\n    def __unicode__(self):\n        \"\"\"\n        Return a string representation for a particular DataFrame\n\n        Invoked by unicode(df) in py2 only. Yields a Unicode String in both\n        py2/py3.\n        \"\"\"\n        buf = StringIO(u(\"\"))\n        width, height = get_terminal_size()\n        max_rows = (height if get_option(\"display.max_rows\") == 0 else\n                    get_option(\"display.max_rows\"))\n        show_dimensions = get_option(\"display.show_dimensions\")\n\n        self.to_string(buf=buf, name=self.name, dtype=self.dtype,\n                       max_rows=max_rows, length=show_dimensions)\n        result = buf.getvalue()\n\n        return result\n\n    def to_string(self, buf=None, na_rep='NaN', float_format=None, header=True,\n                  index=True, length=False, dtype=False, name=False,\n                  max_rows=None):\n        \"\"\"\n        Render a string representation of the Series\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            buffer to write to\n        na_rep : string, optional\n            string representation of NAN to use, default 'NaN'\n        float_format : one-parameter function, optional\n            formatter function to apply to columns' elements if they are floats\n            default None\n        header: boolean, default True\n            Add the Series header (index name)\n        index : bool, optional\n            Add index (row) labels, default True\n        length : boolean, default False\n            Add the Series length\n        dtype : boolean, default False\n            Add the Series dtype\n        name : boolean, default False\n            Add the Series name if not None\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n\n        Returns\n        -------\n        formatted : string (if not buffer passed)\n        \"\"\"\n\n        formatter = fmt.SeriesFormatter(self, name=name, length=length,\n                                        header=header, index=index,\n                                        dtype=dtype, na_rep=na_rep,\n                                        float_format=float_format,\n                                        max_rows=max_rows)\n        result = formatter.to_string()\n\n        # catch contract violations\n        if not isinstance(result, compat.text_type):\n            raise AssertionError(\"result must be of type unicode, type\"\n                                 \" of result is {0!r}\"\n                                 \"\".format(result.__class__.__name__))\n\n        if buf is None:\n            return result\n        else:\n            try:\n                buf.write(result)\n            except AttributeError:\n                with open(buf, 'w') as f:\n                    f.write(result)\n\n    def iteritems(self):\n        \"\"\"\n        Lazily iterate over (index, value) tuples\n        \"\"\"\n        return zip(iter(self.index), iter(self))\n\n    items = iteritems\n\n    # ----------------------------------------------------------------------\n    # Misc public methods\n\n    def keys(self):\n        \"\"\"Alias for index\"\"\"\n        return self.index\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n            .. versionadded:: 0.21.0\n\n        Returns\n        -------\n        value_dict : collections.Mapping\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4])\n        >>> s.to_dict()\n        {0: 1, 1: 2, 2: 3, 3: 4}\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)\n        defaultdict(<type 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})\n        \"\"\"\n        # GH16122\n        into_c = standardize_mapping(into)\n        return into_c(compat.iteritems(self))\n\n    def to_frame(self, name=None):\n        \"\"\"\n        Convert Series to DataFrame\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        data_frame : DataFrame\n        \"\"\"\n        if name is None:\n            df = self._constructor_expanddim(self)\n        else:\n            df = self._constructor_expanddim({name: self})\n\n        return df\n\n    def to_sparse(self, kind='block', fill_value=None):\n        \"\"\"\n        Convert Series to SparseSeries\n\n        Parameters\n        ----------\n        kind : {'block', 'integer'}\n        fill_value : float, defaults to NaN (missing)\n\n        Returns\n        -------\n        sp : SparseSeries\n        \"\"\"\n        from pandas.core.sparse.series import SparseSeries\n        return SparseSeries(self, kind=kind,\n                            fill_value=fill_value).__finalize__(self)\n\n    def _set_name(self, name, inplace=False):\n        \"\"\"\n        Set the Series name.\n\n        Parameters\n        ----------\n        name : str\n        inplace : bool\n            whether to modify `self` directly or return a copy\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        ser = self if inplace else self.copy()\n        ser.name = name\n        return ser\n\n    # ----------------------------------------------------------------------\n    # Statistics, overridden ndarray methods\n\n    # TODO: integrate bottleneck\n\n    def count(self, level=None):\n        \"\"\"\n        Return number of non-NA/null observations in the Series\n\n        Parameters\n        ----------\n        level : int or level name, default None\n            If the axis is a MultiIndex (hierarchical), count along a\n            particular level, collapsing into a smaller Series\n\n        Returns\n        -------\n        nobs : int or Series (if level specified)\n        \"\"\"\n        from pandas.core.index import _get_na_value\n\n        if level is None:\n            return notna(_values_from_object(self)).sum()\n\n        if isinstance(level, compat.string_types):\n            level = self.index._get_level_number(level)\n\n        lev = self.index.levels[level]\n        lab = np.array(self.index.labels[level], subok=False, copy=True)\n\n        mask = lab == -1\n        if mask.any():\n            lab[mask] = cnt = len(lev)\n            lev = lev.insert(cnt, _get_na_value(lev.dtype.type))\n\n        obs = lab[notna(self.values)]\n        out = np.bincount(obs, minlength=len(lev) or None)\n        return self._constructor(out, index=lev,\n                                 dtype='int64').__finalize__(self)\n\n    def mode(self):\n        \"\"\"Return the mode(s) of the dataset.\n\n        Always returns Series even if only one value is returned.\n\n        Returns\n        -------\n        modes : Series (sorted)\n        \"\"\"\n        # TODO: Add option for bins like value_counts()\n        return algorithms.mode(self)\n\n    @Appender(base._shared_docs['unique'] % _shared_doc_kwargs)\n    def unique(self):\n        result = super(Series, self).unique()\n\n        if is_datetime64tz_dtype(self.dtype):\n            # we are special casing datetime64tz_dtype\n            # to return an object array of tz-aware Timestamps\n\n            # TODO: it must return DatetimeArray with tz in pandas 2.0\n            result = result.asobject.values\n\n        return result\n\n    @Appender(base._shared_docs['drop_duplicates'] % _shared_doc_kwargs)\n    def drop_duplicates(self, keep='first', inplace=False):\n        return super(Series, self).drop_duplicates(keep=keep, inplace=inplace)\n\n    @Appender(base._shared_docs['duplicated'] % _shared_doc_kwargs)\n    def duplicated(self, keep='first'):\n        return super(Series, self).duplicated(keep=keep)\n\n    def idxmin(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Index *label* of the first occurrence of minimum of values.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        idxmin : Index of minimum of values\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        See Also\n        --------\n        DataFrame.idxmin\n        numpy.ndarray.argmin\n        \"\"\"\n        skipna = nv.validate_argmin_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmin(_values_from_object(self), skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    def idxmax(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Index *label* of the first occurrence of maximum of values.\n\n        Parameters\n        ----------\n        skipna : boolean, default True\n            Exclude NA/null values\n\n        Returns\n        -------\n        idxmax : Index of maximum of values\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmax``. This method\n        returns the label of the maximum, while ``ndarray.argmax`` returns\n        the position. To get the position, use ``series.values.argmax()``.\n\n        See Also\n        --------\n        DataFrame.idxmax\n        numpy.ndarray.argmax\n        \"\"\"\n        skipna = nv.validate_argmax_with_skipna(skipna, args, kwargs)\n        i = nanops.nanargmax(_values_from_object(self), skipna=skipna)\n        if i == -1:\n            return np.nan\n        return self.index[i]\n\n    # ndarray compat\n    argmin = deprecate('argmin', idxmin,\n                       msg=\"'argmin' is deprecated. Use 'idxmin' instead. \"\n                           \"The behavior of 'argmin' will be corrected to \"\n                           \"return the positional minimum in the future. \"\n                           \"Use 'series.values.argmin' to get the position of \"\n                           \"the minimum now.\")\n    argmax = deprecate('argmax', idxmax,\n                       msg=\"'argmax' is deprecated. Use 'idxmax' instead. \"\n                           \"The behavior of 'argmax' will be corrected to \"\n                           \"return the positional maximum in the future. \"\n                           \"Use 'series.values.argmax' to get the position of \"\n                           \"the maximum now.\")\n\n    def round(self, decimals=0, *args, **kwargs):\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int\n            Number of decimal places to round to (default: 0).\n            If decimals is negative, it specifies the number of\n            positions to the left of the decimal point.\n\n        Returns\n        -------\n        Series object\n\n        See Also\n        --------\n        numpy.around\n        DataFrame.round\n\n        \"\"\"\n        nv.validate_round(args, kwargs)\n        result = _values_from_object(self).round(decimals)\n        result = self._constructor(result, index=self.index).__finalize__(self)\n\n        return result\n\n    def quantile(self, q=0.5, interpolation='linear'):\n        \"\"\"\n        Return value at the given quantile, a la numpy.percentile.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            0 <= q <= 1, the quantile(s) to compute\n        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n            .. versionadded:: 0.18.0\n\n            This optional parameter specifies the interpolation method to use,\n            when the desired quantile lies between two data points `i` and `j`:\n\n                * linear: `i + (j - i) * fraction`, where `fraction` is the\n                  fractional part of the index surrounded by `i` and `j`.\n                * lower: `i`.\n                * higher: `j`.\n                * nearest: `i` or `j` whichever is nearest.\n                * midpoint: (`i` + `j`) / 2.\n\n        Returns\n        -------\n        quantile : float or Series\n            if ``q`` is an array, a Series will be returned where the\n            index is ``q`` and the values are the quantiles.\n\n        Examples\n        --------\n        >>> s = Series([1, 2, 3, 4])\n        >>> s.quantile(.5)\n        2.5\n        >>> s.quantile([.25, .5, .75])\n        0.25    1.75\n        0.50    2.50\n        0.75    3.25\n        dtype: float64\n\n        \"\"\"\n\n        self._check_percentile(q)\n\n        result = self._data.quantile(qs=q, interpolation=interpolation)\n\n        if is_list_like(q):\n            return self._constructor(result,\n                                     index=Float64Index(q),\n                                     name=self.name)\n        else:\n            # scalar\n            return result\n\n    def corr(self, other, method='pearson', min_periods=None):\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values\n\n        Parameters\n        ----------\n        other : Series\n        method : {'pearson', 'kendall', 'spearman'}\n            * pearson : standard correlation coefficient\n            * kendall : Kendall Tau correlation coefficient\n            * spearman : Spearman rank correlation\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result\n\n\n        Returns\n        -------\n        correlation : float\n        \"\"\"\n        this, other = self.align(other, join='inner', copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancorr(this.values, other.values, method=method,\n                              min_periods=min_periods)\n\n    def cov(self, other, min_periods=None):\n        \"\"\"\n        Compute covariance with Series, excluding missing values\n\n        Parameters\n        ----------\n        other : Series\n        min_periods : int, optional\n            Minimum number of observations needed to have a valid result\n\n        Returns\n        -------\n        covariance : float\n\n        Normalized by N-1 (unbiased estimator).\n        \"\"\"\n        this, other = self.align(other, join='inner', copy=False)\n        if len(this) == 0:\n            return np.nan\n        return nanops.nancov(this.values, other.values,\n                             min_periods=min_periods)\n\n    def diff(self, periods=1):\n        \"\"\"\n        1st discrete difference of object\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for forming difference\n\n        Returns\n        -------\n        diffed : Series\n        \"\"\"\n        result = algorithms.diff(_values_from_object(self), periods)\n        return self._constructor(result, index=self.index).__finalize__(self)\n\n    def autocorr(self, lag=1):\n        \"\"\"\n        Lag-N autocorrelation\n\n        Parameters\n        ----------\n        lag : int, default 1\n            Number of lags to apply before performing autocorrelation.\n\n        Returns\n        -------\n        autocorr : float\n        \"\"\"\n        return self.corr(self.shift(lag))\n\n    def dot(self, other):\n        \"\"\"\n        Matrix multiplication with DataFrame or inner-product with Series\n        objects\n\n        Parameters\n        ----------\n        other : Series or DataFrame\n\n        Returns\n        -------\n        dot_product : scalar or Series\n        \"\"\"\n        from pandas.core.frame import DataFrame\n        if isinstance(other, (Series, DataFrame)):\n            common = self.index.union(other.index)\n            if (len(common) > len(self.index) or\n                    len(common) > len(other.index)):\n                raise ValueError('matrices are not aligned')\n\n            left = self.reindex(index=common, copy=False)\n            right = other.reindex(index=common, copy=False)\n            lvals = left.values\n            rvals = right.values\n        else:\n            left = self\n            lvals = self.values\n            rvals = np.asarray(other)\n            if lvals.shape[0] != rvals.shape[0]:\n                raise Exception('Dot product shape mismatch, %s vs %s' %\n                                (lvals.shape, rvals.shape))\n\n        if isinstance(other, DataFrame):\n            return self._constructor(np.dot(lvals, rvals),\n                                     index=other.columns).__finalize__(self)\n        elif isinstance(other, Series):\n            return np.dot(lvals, rvals)\n        elif isinstance(rvals, np.ndarray):\n            return np.dot(lvals, rvals)\n        else:  # pragma: no cover\n            raise TypeError('unsupported type: %s' % type(other))\n\n    @Substitution(klass='Series')\n    @Appender(base._shared_docs['searchsorted'])\n    @deprecate_kwarg(old_arg_name='v', new_arg_name='value')\n    def searchsorted(self, value, side='left', sorter=None):\n        if sorter is not None:\n            sorter = _ensure_platform_int(sorter)\n        return self._values.searchsorted(Series(value)._values,\n                                         side=side, sorter=sorter)\n\n    # -------------------------------------------------------------------\n    # Combination\n\n    def append(self, to_append, ignore_index=False, verify_integrity=False):\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n        ignore_index : boolean, default False\n            If True, do not use the index labels.\n\n            .. versionadded: 0.19.0\n\n        verify_integrity : boolean, default False\n            If True, raise Exception on creating index with duplicates\n\n        Notes\n        -----\n        Iteratively appending to a Series can be more computationally intensive\n        than a single concatenate. A better solution is to append values to a\n        list and then concatenate the list with the original Series all at\n        once.\n\n        See also\n        --------\n        pandas.concat : General function to concatenate DataFrame, Series\n            or Panel objects\n\n        Returns\n        -------\n        appended : Series\n\n        Examples\n        --------\n        >>> s1 = pd.Series([1, 2, 3])\n        >>> s2 = pd.Series([4, 5, 6])\n        >>> s3 = pd.Series([4, 5, 6], index=[3,4,5])\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `ignore_index` set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        dtype: int64\n\n        With `verify_integrity` set to True:\n\n        >>> s1.append(s2, verify_integrity=True)\n        Traceback (most recent call last):\n        ...\n        ValueError: Indexes have overlapping values: [0, 1, 2]\n\n\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        if isinstance(to_append, (list, tuple)):\n            to_concat = [self] + to_append\n        else:\n            to_concat = [self, to_append]\n        return concat(to_concat, ignore_index=ignore_index,\n                      verify_integrity=verify_integrity)\n\n    def _binop(self, other, func, level=None, fill_value=None):\n        \"\"\"\n        Perform generic binary operation with optional fill value\n\n        Parameters\n        ----------\n        other : Series\n        func : binary operator\n        fill_value : float or object\n            Value to substitute for NA/null values. If both Series are NA in a\n            location, the result will be NA regardless of the passed fill value\n        level : int or level name, default None\n            Broadcast across a level, matching Index values on the\n            passed MultiIndex level\n\n        Returns\n        -------\n        combined : Series\n        \"\"\"\n        if not isinstance(other, Series):\n            raise AssertionError('Other operand must be Series')\n\n        new_index = self.index\n        this = self\n\n        if not self.index.equals(other.index):\n            this, other = self.align(other, level=level, join='outer',\n                                     copy=False)\n            new_index = this.index\n\n        this_vals = this.values\n        other_vals = other.values\n\n        if fill_value is not None:\n            this_mask = isna(this_vals)\n            other_mask = isna(other_vals)\n            this_vals = this_vals.copy()\n            other_vals = other_vals.copy()\n\n            # one but not both\n            mask = this_mask ^ other_mask\n            this_vals[this_mask & mask] = fill_value\n            other_vals[other_mask & mask] = fill_value\n\n        with np.errstate(all='ignore'):\n            result = func(this_vals, other_vals)\n        name = _maybe_match_name(self, other)\n        result = self._constructor(result, index=new_index, name=name)\n        result = result.__finalize__(self)\n        if name is None:\n            # When name is None, __finalize__ overwrites current name\n            result.name = None\n        return result\n\n    def combine(self, other, func, fill_value=np.nan):\n        \"\"\"\n        Perform elementwise binary operation on two Series using given function\n        with optional fill value when an index is missing from one Series or\n        the other\n\n        Parameters\n        ----------\n        other : Series or scalar value\n        func : function\n        fill_value : scalar value\n\n        Returns\n        -------\n        result : Series\n        \"\"\"\n        if isinstance(other, Series):\n            new_index = self.index.union(other.index)\n            new_name = _maybe_match_name(self, other)\n            new_values = np.empty(len(new_index), dtype=self.dtype)\n            for i, idx in enumerate(new_index):\n                lv = self.get(idx, fill_value)\n                rv = other.get(idx, fill_value)\n                with np.errstate(all='ignore'):\n                    new_values[i] = func(lv, rv)\n        else:\n            new_index = self.index\n            with np.errstate(all='ignore'):\n                new_values = func(self._values, other)\n            new_name = self.name\n        return self._constructor(new_values, index=new_index, name=new_name)\n\n    def combine_first(self, other):\n        \"\"\"\n        Combine Series values, choosing the calling Series's values\n        first. Result index will be the union of the two indexes\n\n        Parameters\n        ----------\n        other : Series\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        new_index = self.index.union(other.index)\n        this = self.reindex(new_index, copy=False)\n        other = other.reindex(new_index, copy=False)\n        # TODO: do we need name?\n        name = _maybe_match_name(self, other)  # noqa\n        rs_vals = com._where_compat(isna(this), other._values, this._values)\n        return self._constructor(rs_vals, index=new_index).__finalize__(self)\n\n    def update(self, other):\n        \"\"\"\n        Modify Series in place using non-NA values from passed\n        Series. Aligns on index\n\n        Parameters\n        ----------\n        other : Series\n        \"\"\"\n        other = other.reindex_like(self)\n        mask = notna(other)\n\n        self._data = self._data.putmask(mask=mask, new=other, inplace=True)\n        self._maybe_update_cacher()\n\n    # ----------------------------------------------------------------------\n    # Reindexing, sorting\n\n    @Appender(generic._shared_docs['sort_values'] % _shared_doc_kwargs)\n    def sort_values(self, axis=0, ascending=True, inplace=False,\n                    kind='quicksort', na_position='last'):\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        axis = self._get_axis_number(axis)\n\n        # GH 5856/5853\n        if inplace and self._is_cached:\n            raise ValueError(\"This Series is a view of some other array, to \"\n                             \"sort in-place you must create a copy\")\n\n        def _try_kind_sort(arr):\n            # easier to ask forgiveness than permission\n            try:\n                # if kind==mergesort, it can fail for object dtype\n                return arr.argsort(kind=kind)\n            except TypeError:\n                # stable sort not available for object dtype\n                # uses the argsort default quicksort\n                return arr.argsort(kind='quicksort')\n\n        arr = self._values\n        sortedIdx = np.empty(len(self), dtype=np.int32)\n\n        bad = isna(arr)\n\n        good = ~bad\n        idx = _default_index(len(self))\n\n        argsorted = _try_kind_sort(arr[good])\n\n        if is_list_like(ascending):\n            if len(ascending) != 1:\n                raise ValueError('Length of ascending (%d) must be 1 '\n                                 'for Series' % (len(ascending)))\n            ascending = ascending[0]\n\n        if not is_bool(ascending):\n            raise ValueError('ascending must be boolean')\n\n        if not ascending:\n            argsorted = argsorted[::-1]\n\n        if na_position == 'last':\n            n = good.sum()\n            sortedIdx[:n] = idx[good][argsorted]\n            sortedIdx[n:] = idx[bad]\n        elif na_position == 'first':\n            n = bad.sum()\n            sortedIdx[n:] = idx[good][argsorted]\n            sortedIdx[:n] = idx[bad]\n        else:\n            raise ValueError('invalid na_position: {!r}'.format(na_position))\n\n        result = self._constructor(arr[sortedIdx], index=self.index[sortedIdx])\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self)\n\n    @Appender(generic._shared_docs['sort_index'] % _shared_doc_kwargs)\n    def sort_index(self, axis=0, level=None, ascending=True, inplace=False,\n                   kind='quicksort', na_position='last', sort_remaining=True):\n\n        # TODO: this can be combined with DataFrame.sort_index impl as\n        # almost identical\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        axis = self._get_axis_number(axis)\n        index = self.index\n\n        if level:\n            new_index, indexer = index.sortlevel(level, ascending=ascending,\n                                                 sort_remaining=sort_remaining)\n        elif isinstance(index, MultiIndex):\n            from pandas.core.sorting import lexsort_indexer\n            labels = index._sort_levels_monotonic()\n            indexer = lexsort_indexer(labels._get_labels_for_sorting(),\n                                      orders=ascending,\n                                      na_position=na_position)\n        else:\n            from pandas.core.sorting import nargsort\n\n            # Check monotonic-ness before sort an index\n            # GH11080\n            if ((ascending and index.is_monotonic_increasing) or\n                    (not ascending and index.is_monotonic_decreasing)):\n                if inplace:\n                    return\n                else:\n                    return self.copy()\n\n            indexer = nargsort(index, kind=kind, ascending=ascending,\n                               na_position=na_position)\n\n        indexer = _ensure_platform_int(indexer)\n        new_index = index.take(indexer)\n        new_index = new_index._sort_levels_monotonic()\n\n        new_values = self._values.take(indexer)\n        result = self._constructor(new_values, index=new_index)\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result.__finalize__(self)\n\n    def argsort(self, axis=0, kind='quicksort', order=None):\n        \"\"\"\n        Overrides ndarray.argsort. Argsorts the value, omitting NA/null values,\n        and places the result in the same locations as the non-NA values\n\n        Parameters\n        ----------\n        axis : int (can only be zero)\n        kind : {'mergesort', 'quicksort', 'heapsort'}, default 'quicksort'\n            Choice of sorting algorithm. See np.sort for more\n            information. 'mergesort' is the only stable algorithm\n        order : ignored\n\n        Returns\n        -------\n        argsorted : Series, with -1 indicated where nan values are present\n\n        See also\n        --------\n        numpy.ndarray.argsort\n        \"\"\"\n        values = self._values\n        mask = isna(values)\n\n        if mask.any():\n            result = Series(-1, index=self.index, name=self.name,\n                            dtype='int64')\n            notmask = ~mask\n            result[notmask] = np.argsort(values[notmask], kind=kind)\n            return self._constructor(result,\n                                     index=self.index).__finalize__(self)\n        else:\n            return self._constructor(\n                np.argsort(values, kind=kind), index=self.index,\n                dtype='int64').__finalize__(self)\n\n    def nlargest(self, n=5, keep='first'):\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int\n            Return this many descending sorted values\n        keep : {'first', 'last', False}, default 'first'\n            Where there are duplicate values:\n            - ``first`` : take the first occurrence.\n            - ``last`` : take the last occurrence.\n\n        Returns\n        -------\n        top_n : Series\n            The n largest values in the Series, in sorted order\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        See Also\n        --------\n        Series.nsmallest\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> import numpy as np\n        >>> s = pd.Series(np.random.randn(10**6))\n        >>> s.nlargest(10)  # only sorts up to the N requested\n        219921    4.644710\n        82124     4.608745\n        421689    4.564644\n        425277    4.447014\n        718691    4.414137\n        43154     4.403520\n        283187    4.313922\n        595519    4.273635\n        503969    4.250236\n        121637    4.240952\n        dtype: float64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nlargest()\n\n    def nsmallest(self, n=5, keep='first'):\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int\n            Return this many ascending sorted values\n        keep : {'first', 'last', False}, default 'first'\n            Where there are duplicate values:\n            - ``first`` : take the first occurrence.\n            - ``last`` : take the last occurrence.\n\n        Returns\n        -------\n        bottom_n : Series\n            The n smallest values in the Series, in sorted order\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n\n        See Also\n        --------\n        Series.nlargest\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> import numpy as np\n        >>> s = pd.Series(np.random.randn(10**6))\n        >>> s.nsmallest(10)  # only sorts up to the N requested\n        288532   -4.954580\n        732345   -4.835960\n        64803    -4.812550\n        446457   -4.609998\n        501225   -4.483945\n        669476   -4.472935\n        973615   -4.401699\n        621279   -4.355126\n        773916   -4.347355\n        359919   -4.331927\n        dtype: float64\n        \"\"\"\n        return algorithms.SelectNSeries(self, n=n, keep=keep).nsmallest()\n\n    def sortlevel(self, level=0, ascending=True, sort_remaining=True):\n        \"\"\"\n        DEPRECATED: use :meth:`Series.sort_index`\n\n        Sort Series with MultiIndex by chosen level. Data will be\n        lexicographically sorted by the chosen level followed by the other\n        levels (in order)\n\n        Parameters\n        ----------\n        level : int or level name, default None\n        ascending : bool, default True\n\n        Returns\n        -------\n        sorted : Series\n\n        See Also\n        --------\n        Series.sort_index(level=...)\n\n        \"\"\"\n        warnings.warn(\"sortlevel is deprecated, use sort_index(level=...)\",\n                      FutureWarning, stacklevel=2)\n        return self.sort_index(level=level, ascending=ascending,\n                               sort_remaining=sort_remaining)\n\n    def swaplevel(self, i=-2, j=-1, copy=True):\n        \"\"\"\n        Swap levels i and j in a MultiIndex\n\n        Parameters\n        ----------\n        i, j : int, string (can be mixed)\n            Level of index to be swapped. Can pass level name as string.\n\n        Returns\n        -------\n        swapped : Series\n\n        .. versionchanged:: 0.18.1\n\n           The indexes ``i`` and ``j`` are now optional, and default to\n           the two innermost levels of the index.\n\n        \"\"\"\n        new_index = self.index.swaplevel(i, j)\n        return self._constructor(self._values, index=new_index,\n                                 copy=copy).__finalize__(self)\n\n    def reorder_levels(self, order):\n        \"\"\"\n        Rearrange index levels using input order. May not drop or duplicate\n        levels\n\n        Parameters\n        ----------\n        order : list of int representing new level order.\n               (reference level by number or key)\n        axis : where to reorder levels\n\n        Returns\n        -------\n        type of caller (new object)\n        \"\"\"\n        if not isinstance(self.index, MultiIndex):  # pragma: no cover\n            raise Exception('Can only reorder levels on a hierarchical axis.')\n\n        result = self.copy()\n        result.index = result.index.reorder_levels(order)\n        return result\n\n    def unstack(self, level=-1, fill_value=None):\n        \"\"\"\n        Unstack, a.k.a. pivot, Series with MultiIndex to produce DataFrame.\n        The level involved will automatically get sorted.\n\n        Parameters\n        ----------\n        level : int, string, or list of these, default last level\n            Level(s) to unstack, can pass level name\n        fill_value : replace NaN with this value if the unstack produces\n            missing values\n\n            .. versionadded: 0.18.0\n\n        Examples\n        --------\n        >>> s = pd.Series([1, 2, 3, 4],\n        ...     index=pd.MultiIndex.from_product([['one', 'two'], ['a', 'b']]))\n        >>> s\n        one  a    1\n             b    2\n        two  a    3\n             b    4\n        dtype: int64\n\n        >>> s.unstack(level=-1)\n             a  b\n        one  1  2\n        two  3  4\n\n        >>> s.unstack(level=0)\n           one  two\n        a    1    3\n        b    2    4\n\n        Returns\n        -------\n        unstacked : DataFrame\n        \"\"\"\n        from pandas.core.reshape.reshape import unstack\n        return unstack(self, level, fill_value)\n\n    # ----------------------------------------------------------------------\n    # function application\n\n    def map(self, arg, na_action=None):\n        \"\"\"\n        Map values of Series using input correspondence (which can be\n        a dict, Series, or function)\n\n        Parameters\n        ----------\n        arg : function, dict, or Series\n        na_action : {None, 'ignore'}\n            If 'ignore', propagate NA values, without passing them to the\n            mapping function\n\n        Returns\n        -------\n        y : Series\n            same index as caller\n\n        Examples\n        --------\n\n        Map inputs to outputs (both of type `Series`)\n\n        >>> x = pd.Series([1,2,3], index=['one', 'two', 'three'])\n        >>> x\n        one      1\n        two      2\n        three    3\n        dtype: int64\n\n        >>> y = pd.Series(['foo', 'bar', 'baz'], index=[1,2,3])\n        >>> y\n        1    foo\n        2    bar\n        3    baz\n\n        >>> x.map(y)\n        one   foo\n        two   bar\n        three baz\n\n        If `arg` is a dictionary, return a new Series with values converted\n        according to the dictionary's mapping:\n\n        >>> z = {1: 'A', 2: 'B', 3: 'C'}\n\n        >>> x.map(z)\n        one   A\n        two   B\n        three C\n\n        Use na_action to control whether NA values are affected by the mapping\n        function.\n\n        >>> s = pd.Series([1, 2, 3, np.nan])\n\n        >>> s2 = s.map('this is a string {}'.format, na_action=None)\n        0    this is a string 1.0\n        1    this is a string 2.0\n        2    this is a string 3.0\n        3    this is a string nan\n        dtype: object\n\n        >>> s3 = s.map('this is a string {}'.format, na_action='ignore')\n        0    this is a string 1.0\n        1    this is a string 2.0\n        2    this is a string 3.0\n        3                     NaN\n        dtype: object\n\n        See Also\n        --------\n        Series.apply: For applying more complex functions on a Series\n        DataFrame.apply: Apply a function row-/column-wise\n        DataFrame.applymap: Apply a function elementwise on a whole DataFrame\n\n        Notes\n        -----\n        When `arg` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``NaN``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``NaN``:\n\n        >>> from collections import Counter\n        >>> counter = Counter()\n        >>> counter['bar'] += 1\n        >>> y.map(counter)\n        1    0\n        2    1\n        3    0\n        dtype: int64\n        \"\"\"\n\n        if is_extension_type(self.dtype):\n            values = self._values\n            if na_action is not None:\n                raise NotImplementedError\n            map_f = lambda values, f: values.map(f)\n        else:\n            values = self.asobject\n\n            if na_action == 'ignore':\n                def map_f(values, f):\n                    return lib.map_infer_mask(values, f,\n                                              isna(values).view(np.uint8))\n            else:\n                map_f = lib.map_infer\n\n        if isinstance(arg, dict):\n            if hasattr(arg, '__missing__'):\n                # If a dictionary subclass defines a default value method,\n                # convert arg to a lookup function (GH #15999).\n                dict_with_default = arg\n                arg = lambda x: dict_with_default[x]\n            else:\n                # Dictionary does not have a default. Thus it's safe to\n                # convert to an indexed series for efficiency.\n                arg = self._constructor(arg, index=arg.keys())\n\n        if isinstance(arg, Series):\n            # arg is a Series\n            indexer = arg.index.get_indexer(values)\n            new_values = algorithms.take_1d(arg._values, indexer)\n        else:\n            # arg is a function\n            new_values = map_f(values, arg)\n\n        return self._constructor(new_values,\n                                 index=self.index).__finalize__(self)\n\n    def _gotitem(self, key, ndim, subset=None):\n        \"\"\"\n        sub-classes to define\n        return a sliced object\n\n        Parameters\n        ----------\n        key : string / list of selections\n        ndim : 1,2\n            requested ndim of result\n        subset : object, default None\n            subset to act on\n        \"\"\"\n        return self\n\n    _agg_doc = dedent(\"\"\"\n    Examples\n    --------\n\n    >>> s = Series(np.random.randn(10))\n\n    >>> s.agg('min')\n    -1.3018049988556679\n\n    >>> s.agg(['min', 'max'])\n    min   -1.301805\n    max    1.127688\n    dtype: float64\n\n    See also\n    --------\n    pandas.Series.apply\n    pandas.Series.transform\n\n    \"\"\")\n\n    @Appender(_agg_doc)\n    @Appender(generic._shared_docs['aggregate'] % dict(\n        versionadded='.. versionadded:: 0.20.0',\n        **_shared_doc_kwargs))\n    def aggregate(self, func, axis=0, *args, **kwargs):\n        axis = self._get_axis_number(axis)\n        result, how = self._aggregate(func, *args, **kwargs)\n        if result is None:\n\n            # we can be called from an inner function which\n            # passes this meta-data\n            kwargs.pop('_axis', None)\n            kwargs.pop('_level', None)\n\n            # try a regular apply, this evaluates lambdas\n            # row-by-row; however if the lambda is expected a Series\n            # expression, e.g.: lambda x: x-x.quantile(0.25)\n            # this will fail, so we can try a vectorized evaluation\n\n            # we cannot FIRST try the vectorized evaluation, becuase\n            # then .agg and .apply would have different semantics if the\n            # operation is actually defined on the Series, e.g. str\n            try:\n                result = self.apply(func, *args, **kwargs)\n            except (ValueError, AttributeError, TypeError):\n                result = func(self, *args, **kwargs)\n\n        return result\n\n    agg = aggregate\n\n    def apply(self, func, convert_dtype=True, args=(), **kwds):\n        \"\"\"\n        Invoke function on values of Series. Can be ufunc (a NumPy function\n        that applies to the entire Series) or a Python function that only works\n        on single values\n\n        Parameters\n        ----------\n        func : function\n        convert_dtype : boolean, default True\n            Try to find better dtype for elementwise function results. If\n            False, leave as dtype=object\n        args : tuple\n            Positional arguments to pass to function in addition to the value\n        Additional keyword arguments will be passed as keywords to the function\n\n        Returns\n        -------\n        y : Series or DataFrame if func returns a Series\n\n        See also\n        --------\n        Series.map: For element-wise operations\n        Series.agg: only perform aggregating type operations\n        Series.transform: only perform transformating type operations\n\n        Examples\n        --------\n\n        Create a series with typical summer temperatures for each city.\n\n        >>> import pandas as pd\n        >>> import numpy as np\n        >>> series = pd.Series([20, 21, 12], index=['London',\n        ... 'New York','Helsinki'])\n        >>> series\n        London      20\n        New York    21\n        Helsinki    12\n        dtype: int64\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x):\n        ...     return x**2\n        >>> series.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Square the values by passing an anonymous function as an\n        argument to ``apply()``.\n\n        >>> series.apply(lambda x: x**2)\n        London      400\n        New York    441\n        Helsinki    144\n        dtype: int64\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword.\n\n        >>> def subtract_custom_value(x, custom_value):\n        ...     return x-custom_value\n\n        >>> series.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        dtype: int64\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``.\n\n        >>> def add_custom_values(x, **kwargs):\n        ...     for month in kwargs:\n        ...         x+=kwargs[month]\n        ...         return x\n\n        >>> series.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        dtype: int64\n\n        Use a function from the Numpy library.\n\n        >>> series.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        dtype: float64\n\n\n        \"\"\"\n        if len(self) == 0:\n            return self._constructor(dtype=self.dtype,\n                                     index=self.index).__finalize__(self)\n\n        # dispatch to agg\n        if isinstance(func, (list, dict)):\n            return self.aggregate(func, *args, **kwds)\n\n        # if we are a string, try to dispatch\n        if isinstance(func, compat.string_types):\n            return self._try_aggregate_string_function(func, *args, **kwds)\n\n        # handle ufuncs and lambdas\n        if kwds or args and not isinstance(func, np.ufunc):\n            f = lambda x: func(x, *args, **kwds)\n        else:\n            f = func\n\n        with np.errstate(all='ignore'):\n            if isinstance(f, np.ufunc):\n                return f(self)\n\n            # row-wise access\n            if is_extension_type(self.dtype):\n                mapped = self._values.map(f)\n            else:\n                values = self.asobject\n                mapped = lib.map_infer(values, f, convert=convert_dtype)\n\n        if len(mapped) and isinstance(mapped[0], Series):\n            from pandas.core.frame import DataFrame\n            return DataFrame(mapped.tolist(), index=self.index)\n        else:\n            return self._constructor(mapped,\n                                     index=self.index).__finalize__(self)\n\n    def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\n                filter_type=None, **kwds):\n        \"\"\"\n        perform a reduction operation\n\n        if we have an ndarray as a value, then simply perform the operation,\n        otherwise delegate to the object\n\n        \"\"\"\n        delegate = self._values\n        if isinstance(delegate, np.ndarray):\n            # Validate that 'axis' is consistent with Series's single axis.\n            self._get_axis_number(axis)\n            if numeric_only:\n                raise NotImplementedError('Series.{0} does not implement '\n                                          'numeric_only.'.format(name))\n            with np.errstate(all='ignore'):\n                return op(delegate, skipna=skipna, **kwds)\n\n        return delegate._reduce(op=op, name=name, axis=axis, skipna=skipna,\n                                numeric_only=numeric_only,\n                                filter_type=filter_type, **kwds)\n\n    def _reindex_indexer(self, new_index, indexer, copy):\n        if indexer is None:\n            if copy:\n                return self.copy()\n            return self\n\n        # be subclass-friendly\n        new_values = algorithms.take_1d(self.get_values(), indexer)\n        return self._constructor(new_values, index=new_index)\n\n    def _needs_reindex_multi(self, axes, method, level):\n        \"\"\" check if we do need a multi reindex; this is for compat with\n        higher dims\n        \"\"\"\n        return False\n\n    @Appender(generic._shared_docs['align'] % _shared_doc_kwargs)\n    def align(self, other, join='outer', axis=None, level=None, copy=True,\n              fill_value=None, method=None, limit=None, fill_axis=0,\n              broadcast_axis=None):\n        return super(Series, self).align(other, join=join, axis=axis,\n                                         level=level, copy=copy,\n                                         fill_value=fill_value, method=method,\n                                         limit=limit, fill_axis=fill_axis,\n                                         broadcast_axis=broadcast_axis)\n\n    @Appender(generic._shared_docs['rename'] % _shared_doc_kwargs)\n    def rename(self, index=None, **kwargs):\n        kwargs['inplace'] = validate_bool_kwarg(kwargs.get('inplace', False),\n                                                'inplace')\n\n        non_mapping = is_scalar(index) or (is_list_like(index) and\n                                           not is_dict_like(index))\n        if non_mapping:\n            return self._set_name(index, inplace=kwargs.get('inplace'))\n        return super(Series, self).rename(index=index, **kwargs)\n\n    @Appender(generic._shared_docs['reindex'] % _shared_doc_kwargs)\n    def reindex(self, index=None, **kwargs):\n        return super(Series, self).reindex(index=index, **kwargs)\n\n    @Appender(generic._shared_docs['fillna'] % _shared_doc_kwargs)\n    def fillna(self, value=None, method=None, axis=None, inplace=False,\n               limit=None, downcast=None, **kwargs):\n        return super(Series, self).fillna(value=value, method=method,\n                                          axis=axis, inplace=inplace,\n                                          limit=limit, downcast=downcast,\n                                          **kwargs)\n\n    @Appender(generic._shared_docs['shift'] % _shared_doc_kwargs)\n    def shift(self, periods=1, freq=None, axis=0):\n        return super(Series, self).shift(periods=periods, freq=freq, axis=axis)\n\n    def reindex_axis(self, labels, axis=0, **kwargs):\n        \"\"\" for compatibility with higher dims \"\"\"\n        if axis != 0:\n            raise ValueError(\"cannot reindex series on non-zero axis!\")\n        return self.reindex(index=labels, **kwargs)\n\n    def memory_usage(self, index=True, deep=False):\n        \"\"\"Memory usage of the Series\n\n        Parameters\n        ----------\n        index : bool\n            Specifies whether to include memory usage of Series index\n        deep : bool\n            Introspect the data deeply, interrogate\n            `object` dtypes for system-level memory consumption\n\n        Returns\n        -------\n        scalar bytes of memory consumed\n\n        Notes\n        -----\n        Memory usage does not include memory consumed by elements that\n        are not components of the array if deep=False\n\n        See Also\n        --------\n        numpy.ndarray.nbytes\n        \"\"\"\n        v = super(Series, self).memory_usage(deep=deep)\n        if index:\n            v += self.index.memory_usage(deep=deep)\n        return v\n\n    def take(self, indices, axis=0, convert=True, is_copy=False, **kwargs):\n        \"\"\"\n        return Series corresponding to requested indices\n\n        Parameters\n        ----------\n        indices : list / array of ints\n        convert : translate negative to positive indices (default)\n\n        Returns\n        -------\n        taken : Series\n\n        See also\n        --------\n        numpy.ndarray.take\n        \"\"\"\n        if kwargs:\n            nv.validate_take(tuple(), kwargs)\n\n        # check/convert indicies here\n        if convert:\n            indices = maybe_convert_indices(indices, len(self._get_axis(axis)))\n\n        indices = _ensure_platform_int(indices)\n        new_index = self.index.take(indices)\n        new_values = self._values.take(indices)\n        return (self._constructor(new_values, index=new_index, fastpath=True)\n                    .__finalize__(self))\n\n    def isin(self, values):\n        \"\"\"\n        Return a boolean :class:`~pandas.Series` showing whether each element\n        in the :class:`~pandas.Series` is exactly contained in the passed\n        sequence of ``values``.\n\n        Parameters\n        ----------\n        values : set or list-like\n            The sequence of values to test. Passing in a single string will\n            raise a ``TypeError``. Instead, turn a single string into a\n            ``list`` of one element.\n\n            .. versionadded:: 0.18.1\n\n            Support for values as a set\n\n        Returns\n        -------\n        isin : Series (bool dtype)\n\n        Raises\n        ------\n        TypeError\n          * If ``values`` is a string\n\n        See Also\n        --------\n        pandas.DataFrame.isin\n\n        Examples\n        --------\n\n        >>> s = pd.Series(list('abc'))\n        >>> s.isin(['a', 'c', 'e'])\n        0     True\n        1    False\n        2     True\n        dtype: bool\n\n        Passing a single string as ``s.isin('a')`` will raise an error. Use\n        a list of one element instead:\n\n        >>> s.isin(['a'])\n        0     True\n        1    False\n        2    False\n        dtype: bool\n\n        \"\"\"\n        result = algorithms.isin(_values_from_object(self), values)\n        return self._constructor(result, index=self.index).__finalize__(self)\n\n    def between(self, left, right, inclusive=True):\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right. NA values\n        will be treated as False\n\n        Parameters\n        ----------\n        left : scalar\n            Left boundary\n        right : scalar\n            Right boundary\n\n        Returns\n        -------\n        is_between : Series\n        \"\"\"\n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return lmask & rmask\n\n    @classmethod\n    def from_csv(cls, path, sep=',', parse_dates=True, header=None,\n                 index_col=0, encoding=None, infer_datetime_format=False):\n        \"\"\"\n        Read CSV file (DISCOURAGED, please use :func:`pandas.read_csv`\n        instead).\n\n        It is preferable to use the more powerful :func:`pandas.read_csv`\n        for most general purposes, but ``from_csv`` makes for an easy\n        roundtrip to and from a file (the exact counterpart of\n        ``to_csv``), especially with a time Series.\n\n        This method only differs from :func:`pandas.read_csv` in some defaults:\n\n        - `index_col` is ``0`` instead of ``None`` (take first column as index\n          by default)\n        - `header` is ``None`` instead of ``0`` (the first row is not used as\n          the column names)\n        - `parse_dates` is ``True`` instead of ``False`` (try parsing the index\n          as datetime by default)\n\n        With :func:`pandas.read_csv`, the option ``squeeze=True`` can be used\n        to return a Series like ``from_csv``.\n\n        Parameters\n        ----------\n        path : string file path or file handle / StringIO\n        sep : string, default ','\n            Field delimiter\n        parse_dates : boolean, default True\n            Parse dates. Different default from read_table\n        header : int, default None\n            Row to use as header (skip prior rows)\n        index_col : int or sequence, default 0\n            Column to use for index. If a sequence is given, a MultiIndex\n            is used. Different default from read_table\n        encoding : string, optional\n            a string representing the encoding to use if the contents are\n            non-ascii, for python versions prior to 3\n        infer_datetime_format: boolean, default False\n            If True and `parse_dates` is True for a column, try to infer the\n            datetime format based on the first datetime string. If the format\n            can be inferred, there often will be a large parsing speed-up.\n\n        See also\n        --------\n        pandas.read_csv\n\n        Returns\n        -------\n        y : Series\n        \"\"\"\n        from pandas.core.frame import DataFrame\n        df = DataFrame.from_csv(path, header=header, index_col=index_col,\n                                sep=sep, parse_dates=parse_dates,\n                                encoding=encoding,\n                                infer_datetime_format=infer_datetime_format)\n        result = df.iloc[:, 0]\n        if header is None:\n            result.index.name = result.name = None\n\n        return result\n\n    def to_csv(self, path=None, index=True, sep=\",\", na_rep='',\n               float_format=None, header=False, index_label=None,\n               mode='w', encoding=None, date_format=None, decimal='.'):\n        \"\"\"\n        Write Series to a comma-separated values (csv) file\n\n        Parameters\n        ----------\n        path : string or file handle, default None\n            File path or object, if None is provided the result is returned as\n            a string.\n        na_rep : string, default ''\n            Missing data representation\n        float_format : string, default None\n            Format string for floating point numbers\n        header : boolean, default False\n            Write out series name\n        index : boolean, default True\n            Write row names (index)\n        index_label : string or sequence, default None\n            Column label for index column(s) if desired. If None is given, and\n            `header` and `index` are True, then the index names are used. A\n            sequence should be given if the DataFrame uses MultiIndex.\n        mode : Python write mode, default 'w'\n        sep : character, default \",\"\n            Field delimiter for the output file.\n        encoding : string, optional\n            a string representing the encoding to use if the contents are\n            non-ascii, for python versions prior to 3\n        date_format: string, default None\n            Format string for datetime objects.\n        decimal: string, default '.'\n            Character recognized as decimal separator. E.g. use ',' for\n            European data\n        \"\"\"\n        from pandas.core.frame import DataFrame\n        df = DataFrame(self)\n        # result is only a string if no path provided, otherwise None\n        result = df.to_csv(path, index=index, sep=sep, na_rep=na_rep,\n                           float_format=float_format, header=header,\n                           index_label=index_label, mode=mode,\n                           encoding=encoding, date_format=date_format,\n                           decimal=decimal)\n        if path is None:\n            return result\n\n    @Appender(generic._shared_docs['to_excel'] % _shared_doc_kwargs)\n    def to_excel(self, excel_writer, sheet_name='Sheet1', na_rep='',\n                 float_format=None, columns=None, header=True, index=True,\n                 index_label=None, startrow=0, startcol=0, engine=None,\n                 merge_cells=True, encoding=None, inf_rep='inf', verbose=True):\n        df = self.to_frame()\n        df.to_excel(excel_writer=excel_writer, sheet_name=sheet_name,\n                    na_rep=na_rep, float_format=float_format, columns=columns,\n                    header=header, index=index, index_label=index_label,\n                    startrow=startrow, startcol=startcol, engine=engine,\n                    merge_cells=merge_cells, encoding=encoding,\n                    inf_rep=inf_rep, verbose=verbose)\n\n    @Appender(generic._shared_docs['isna'] % _shared_doc_kwargs)\n    def isna(self):\n        return super(Series, self).isna()\n\n    @Appender(generic._shared_docs['isna'] % _shared_doc_kwargs)\n    def isnull(self):\n        return super(Series, self).isnull()\n\n    @Appender(generic._shared_docs['notna'] % _shared_doc_kwargs)\n    def notna(self):\n        return super(Series, self).notna()\n\n    @Appender(generic._shared_docs['notna'] % _shared_doc_kwargs)\n    def notnull(self):\n        return super(Series, self).notnull()\n\n    def dropna(self, axis=0, inplace=False, **kwargs):\n        \"\"\"\n        Return Series without null values\n\n        Returns\n        -------\n        valid : Series\n        inplace : boolean, default False\n            Do operation in place.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        kwargs.pop('how', None)\n        if kwargs:\n            raise TypeError('dropna() got an unexpected keyword '\n                            'argument \"{0}\"'.format(list(kwargs.keys())[0]))\n\n        axis = self._get_axis_number(axis or 0)\n\n        if self._can_hold_na:\n            result = remove_na_arraylike(self)\n            if inplace:\n                self._update_inplace(result)\n            else:\n                return result\n        else:\n            if inplace:\n                # do nothing\n                pass\n            else:\n                return self.copy()\n\n    valid = lambda self, inplace=False, **kwargs: self.dropna(inplace=inplace,\n                                                              **kwargs)\n\n    @Appender(generic._shared_docs['valid_index'] % {\n        'position': 'first', 'klass': 'Series'})\n    def first_valid_index(self):\n        if len(self) == 0:\n            return None\n\n        mask = isna(self._values)\n        i = mask.argmin()\n        if mask[i]:\n            return None\n        else:\n            return self.index[i]\n\n    @Appender(generic._shared_docs['valid_index'] % {\n        'position': 'last', 'klass': 'Series'})\n    def last_valid_index(self):\n        if len(self) == 0:\n            return None\n\n        mask = isna(self._values[::-1])\n        i = mask.argmin()\n        if mask[i]:\n            return None\n        else:\n            return self.index[len(self) - i - 1]\n\n    # ----------------------------------------------------------------------\n    # Time series-oriented methods\n\n    def to_timestamp(self, freq=None, how='start', copy=True):\n        \"\"\"\n        Cast to datetimeindex of timestamps, at *beginning* of period\n\n        Parameters\n        ----------\n        freq : string, default frequency of PeriodIndex\n            Desired frequency\n        how : {'s', 'e', 'start', 'end'}\n            Convention for converting period to timestamp; start of period\n            vs. end\n\n        Returns\n        -------\n        ts : Series with DatetimeIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        new_index = self.index.to_timestamp(freq=freq, how=how)\n        return self._constructor(new_values,\n                                 index=new_index).__finalize__(self)\n\n    def to_period(self, freq=None, copy=True):\n        \"\"\"\n        Convert Series from DatetimeIndex to PeriodIndex with desired\n        frequency (inferred from index if not passed)\n\n        Parameters\n        ----------\n        freq : string, default\n\n        Returns\n        -------\n        ts : Series with PeriodIndex\n        \"\"\"\n        new_values = self._values\n        if copy:\n            new_values = new_values.copy()\n\n        new_index = self.index.to_period(freq=freq)\n        return self._constructor(new_values,\n                                 index=new_index).__finalize__(self)\n\n    # -------------------------------------------------------------------------\n    # Datetimelike delegation methods\n    dt = accessor.AccessorProperty(CombinedDatetimelikeProperties)\n\n    # -------------------------------------------------------------------------\n    # Categorical methods\n    cat = accessor.AccessorProperty(CategoricalAccessor)\n\n    # String Methods\n    str = accessor.AccessorProperty(strings.StringMethods)\n\n    # ----------------------------------------------------------------------\n    # Add plotting methods to Series\n    plot = accessor.AccessorProperty(gfx.SeriesPlotMethods,\n                                     gfx.SeriesPlotMethods)\n    hist = gfx.hist_series\n\n\nSeries._setup_axes(['index'], info_axis=0, stat_axis=0, aliases={'rows': 0})\nSeries._add_numeric_operations()\nSeries._add_series_only_operations()\nSeries._add_series_or_dataframe_operations()\n\n# Add arithmetic!\nops.add_flex_arithmetic_methods(Series, **ops.series_flex_funcs)\nops.add_special_arithmetic_methods(Series, **ops.series_special_funcs)\n\n\n# -----------------------------------------------------------------------------\n# Supplementary functions\n\n\ndef _sanitize_index(data, index, copy=False):\n    \"\"\" sanitize an index type to return an ndarray of the underlying, pass\n    thru a non-Index\n    \"\"\"\n\n    if index is None:\n        return data\n\n    if len(data) != len(index):\n        raise ValueError('Length of values does not match length of ' 'index')\n\n    if isinstance(data, PeriodIndex):\n        data = data.asobject\n    elif isinstance(data, DatetimeIndex):\n        data = data._to_embed(keep_tz=True)\n    elif isinstance(data, np.ndarray):\n\n        # coerce datetimelike types\n        if data.dtype.kind in ['M', 'm']:\n            data = _sanitize_array(data, index, copy=copy)\n\n    return data\n\n\ndef _sanitize_array(data, index, dtype=None, copy=False,\n                    raise_cast_failure=False):\n    \"\"\" sanitize input data to an ndarray, copy if specified, coerce to the\n    dtype if specified\n    \"\"\"\n\n    if dtype is not None:\n        dtype = pandas_dtype(dtype)\n\n    if isinstance(data, ma.MaskedArray):\n        mask = ma.getmaskarray(data)\n        if mask.any():\n            data, fill_value = maybe_upcast(data, copy=True)\n            data[mask] = fill_value\n        else:\n            data = data.copy()\n\n    def _try_cast(arr, take_fast_path):\n\n        # perf shortcut as this is the most common case\n        if take_fast_path:\n            if maybe_castable(arr) and not copy and dtype is None:\n                return arr\n\n        try:\n            subarr = maybe_cast_to_datetime(arr, dtype)\n            if not is_extension_type(subarr):\n                subarr = np.array(subarr, dtype=dtype, copy=copy)\n        except (ValueError, TypeError):\n            if is_categorical_dtype(dtype):\n                subarr = Categorical(arr, dtype.categories,\n                                     ordered=dtype.ordered)\n            elif dtype is not None and raise_cast_failure:\n                raise\n            else:\n                subarr = np.array(arr, dtype=object, copy=copy)\n        return subarr\n\n    # GH #846\n    if isinstance(data, (np.ndarray, Index, Series)):\n\n        if dtype is not None:\n            subarr = np.array(data, copy=False)\n\n            # possibility of nan -> garbage\n            if is_float_dtype(data.dtype) and is_integer_dtype(dtype):\n                if not isna(data).any():\n                    subarr = _try_cast(data, True)\n                elif copy:\n                    subarr = data.copy()\n            else:\n                subarr = _try_cast(data, True)\n        elif isinstance(data, Index):\n            # don't coerce Index types\n            # e.g. indexes can have different conversions (so don't fast path\n            # them)\n            # GH 6140\n            subarr = _sanitize_index(data, index, copy=True)\n        else:\n            subarr = _try_cast(data, True)\n\n        if copy:\n            subarr = data.copy()\n\n    elif isinstance(data, Categorical):\n        subarr = data\n\n        if copy:\n            subarr = data.copy()\n        return subarr\n\n    elif isinstance(data, (list, tuple)) and len(data) > 0:\n        if dtype is not None:\n            try:\n                subarr = _try_cast(data, False)\n            except Exception:\n                if raise_cast_failure:  # pragma: no cover\n                    raise\n                subarr = np.array(data, dtype=object, copy=copy)\n                subarr = lib.maybe_convert_objects(subarr)\n\n        else:\n            subarr = maybe_convert_platform(data)\n\n        subarr = maybe_cast_to_datetime(subarr, dtype)\n\n    else:\n        subarr = _try_cast(data, False)\n\n    def create_from_value(value, index, dtype):\n        # return a new empty value suitable for the dtype\n\n        if is_datetimetz(dtype):\n            subarr = DatetimeIndex([value] * len(index), dtype=dtype)\n        elif is_categorical_dtype(dtype):\n            subarr = Categorical([value] * len(index))\n        else:\n            if not isinstance(dtype, (np.dtype, type(np.dtype))):\n                dtype = dtype.dtype\n            subarr = np.empty(len(index), dtype=dtype)\n            subarr.fill(value)\n\n        return subarr\n\n    # scalar like, GH\n    if getattr(subarr, 'ndim', 0) == 0:\n        if isinstance(data, list):  # pragma: no cover\n            subarr = np.array(data, dtype=object)\n        elif index is not None:\n            value = data\n\n            # figure out the dtype from the value (upcast if necessary)\n            if dtype is None:\n                dtype, value = infer_dtype_from_scalar(value)\n            else:\n                # need to possibly convert the value here\n                value = maybe_cast_to_datetime(value, dtype)\n\n            subarr = create_from_value(value, index, dtype)\n\n        else:\n            return subarr.item()\n\n    # the result that we want\n    elif subarr.ndim == 1:\n        if index is not None:\n\n            # a 1-element ndarray\n            if len(subarr) != len(index) and len(subarr) == 1:\n                subarr = create_from_value(subarr[0], index,\n                                           subarr.dtype)\n\n    elif subarr.ndim > 1:\n        if isinstance(data, np.ndarray):\n            raise Exception('Data must be 1-dimensional')\n        else:\n            subarr = _asarray_tuplesafe(data, dtype=dtype)\n\n    # This is to prevent mixed-type Series getting all casted to\n    # NumPy string type, e.g. NaN --> '-1#IND'.\n    if issubclass(subarr.dtype.type, compat.string_types):\n        subarr = np.array(data, dtype=object, copy=copy)\n\n    return subarr\n"
    },
    {
      "filename": "pandas/io/formats/format.py",
      "content": "# -*- coding: utf-8 -*-\n\"\"\"\nInternal module for formatting output data in csv, html,\nand latex files. This module also applies to display formatting.\n\"\"\"\n\nfrom __future__ import print_function\nfrom distutils.version import LooseVersion\n# pylint: disable=W0141\n\nfrom textwrap import dedent\n\nfrom pandas.core.dtypes.missing import isna, notna\nfrom pandas.core.dtypes.common import (\n    is_categorical_dtype,\n    is_float_dtype,\n    is_period_arraylike,\n    is_integer_dtype,\n    is_interval_dtype,\n    is_datetimetz,\n    is_integer,\n    is_float,\n    is_numeric_dtype,\n    is_datetime64_dtype,\n    is_timedelta64_dtype,\n    is_list_like)\nfrom pandas.core.dtypes.generic import ABCSparseArray\nfrom pandas.core.base import PandasObject\nfrom pandas.core.index import Index, MultiIndex, _ensure_index\nfrom pandas import compat\nfrom pandas.compat import (StringIO, lzip, range, map, zip, u,\n                           OrderedDict, unichr)\nfrom pandas.io.formats.terminal import get_terminal_size\nfrom pandas.core.config import get_option, set_option\nfrom pandas.io.common import (_get_handle, UnicodeWriter, _expand_user,\n                              _stringify_path)\nfrom pandas.io.formats.printing import adjoin, justify, pprint_thing\nfrom pandas.io.formats.common import get_level_lengths\nimport pandas.core.common as com\nimport pandas._libs.lib as lib\nfrom pandas._libs.tslib import (iNaT, Timestamp, Timedelta,\n                                format_array_from_datetime)\nfrom pandas.core.indexes.datetimes import DatetimeIndex\nfrom pandas.core.indexes.period import PeriodIndex\nimport pandas as pd\nimport numpy as np\n\nimport itertools\nimport csv\nfrom functools import partial\n\ncommon_docstring = \"\"\"\n    Parameters\n    ----------\n    buf : StringIO-like, optional\n        buffer to write to\n    columns : sequence, optional\n        the subset of columns to write; default None writes all columns\n    col_space : int, optional\n        the minimum width of each column\n    header : bool, optional\n        %(header)s\n    index : bool, optional\n        whether to print index (row) labels, default True\n    na_rep : string, optional\n        string representation of NAN to use, default 'NaN'\n    formatters : list or dict of one-parameter functions, optional\n        formatter functions to apply to columns' elements by position or name,\n        default None. The result of each function must be a unicode string.\n        List must be of length equal to the number of columns.\n    float_format : one-parameter function, optional\n        formatter function to apply to columns' elements if they are floats,\n        default None. The result of this function must be a unicode string.\n    sparsify : bool, optional\n        Set to False for a DataFrame with a hierarchical index to print every\n        multiindex key at each row, default True\n    index_names : bool, optional\n        Prints the names of the indexes, default True\n    line_width : int, optional\n        Width to wrap a line in characters, default no wrap\"\"\"\n\njustify_docstring = \"\"\"\n    justify : {'left', 'right'}, default None\n        Left or right-justify the column labels. If None uses the option from\n        the print configuration (controlled by set_option), 'right' out\n        of the box.\"\"\"\n\nreturn_docstring = \"\"\"\n\n    Returns\n    -------\n    formatted : string (or unicode, depending on data and options)\"\"\"\n\ndocstring_to_string = common_docstring + justify_docstring + return_docstring\n\n\nclass CategoricalFormatter(object):\n\n    def __init__(self, categorical, buf=None, length=True, na_rep='NaN',\n                 footer=True):\n        self.categorical = categorical\n        self.buf = buf if buf is not None else StringIO(u(\"\"))\n        self.na_rep = na_rep\n        self.length = length\n        self.footer = footer\n\n    def _get_footer(self):\n        footer = ''\n\n        if self.length:\n            if footer:\n                footer += ', '\n            footer += \"Length: {length}\".format(length=len(self.categorical))\n\n        level_info = self.categorical._repr_categories_info()\n\n        # Levels are added in a newline\n        if footer:\n            footer += '\\n'\n        footer += level_info\n\n        return compat.text_type(footer)\n\n    def _get_formatted_values(self):\n        return format_array(self.categorical.get_values(), None,\n                            float_format=None, na_rep=self.na_rep)\n\n    def to_string(self):\n        categorical = self.categorical\n\n        if len(categorical) == 0:\n            if self.footer:\n                return self._get_footer()\n            else:\n                return u('')\n\n        fmt_values = self._get_formatted_values()\n\n        result = [u('{i}').format(i=i) for i in fmt_values]\n        result = [i.strip() for i in result]\n        result = u(', ').join(result)\n        result = [u('[') + result + u(']')]\n        if self.footer:\n            footer = self._get_footer()\n            if footer:\n                result.append(footer)\n\n        return compat.text_type(u('\\n').join(result))\n\n\nclass SeriesFormatter(object):\n\n    def __init__(self, series, buf=None, length=True, header=True, index=True,\n                 na_rep='NaN', name=False, float_format=None, dtype=True,\n                 max_rows=None):\n        self.series = series\n        self.buf = buf if buf is not None else StringIO()\n        self.name = name\n        self.na_rep = na_rep\n        self.header = header\n        self.length = length\n        self.index = index\n        self.max_rows = max_rows\n\n        if float_format is None:\n            float_format = get_option(\"display.float_format\")\n        self.float_format = float_format\n        self.dtype = dtype\n        self.adj = _get_adjustment()\n\n        self._chk_truncate()\n\n    def _chk_truncate(self):\n        from pandas.core.reshape.concat import concat\n        max_rows = self.max_rows\n        truncate_v = max_rows and (len(self.series) > max_rows)\n        series = self.series\n        if truncate_v:\n            if max_rows == 1:\n                row_num = max_rows\n                series = series.iloc[:max_rows]\n            else:\n                row_num = max_rows // 2\n                series = concat((series.iloc[:row_num],\n                                 series.iloc[-row_num:]))\n            self.tr_row_num = row_num\n        self.tr_series = series\n        self.truncate_v = truncate_v\n\n    def _get_footer(self):\n        name = self.series.name\n        footer = u('')\n\n        if getattr(self.series.index, 'freq', None) is not None:\n            footer += 'Freq: {freq}'.format(freq=self.series.index.freqstr)\n\n        if self.name is not False and name is not None:\n            if footer:\n                footer += ', '\n\n            series_name = pprint_thing(name,\n                                       escape_chars=('\\t', '\\r', '\\n'))\n            footer += ((u\"Name: {sname}\".format(sname=series_name))\n                       if name is not None else \"\")\n\n        if (self.length is True or\n                (self.length == 'truncate' and self.truncate_v)):\n            if footer:\n                footer += ', '\n            footer += 'Length: {length}'.format(length=len(self.series))\n\n        if self.dtype is not False and self.dtype is not None:\n            name = getattr(self.tr_series.dtype, 'name', None)\n            if name:\n                if footer:\n                    footer += ', '\n                footer += u'dtype: {typ}'.format(typ=pprint_thing(name))\n\n        # level infos are added to the end and in a new line, like it is done\n        # for Categoricals\n        if is_categorical_dtype(self.tr_series.dtype):\n            level_info = self.tr_series._values._repr_categories_info()\n            if footer:\n                footer += \"\\n\"\n            footer += level_info\n\n        return compat.text_type(footer)\n\n    def _get_formatted_index(self):\n        index = self.tr_series.index\n        is_multi = isinstance(index, MultiIndex)\n\n        if is_multi:\n            have_header = any(name for name in index.names)\n            fmt_index = index.format(names=True)\n        else:\n            have_header = index.name is not None\n            fmt_index = index.format(name=True)\n        return fmt_index, have_header\n\n    def _get_formatted_values(self):\n        values_to_format = self.tr_series._formatting_values()\n        return format_array(values_to_format, None,\n                            float_format=self.float_format, na_rep=self.na_rep)\n\n    def to_string(self):\n        series = self.tr_series\n        footer = self._get_footer()\n\n        if len(series) == 0:\n            return 'Series([], ' + footer + ')'\n\n        fmt_index, have_header = self._get_formatted_index()\n        fmt_values = self._get_formatted_values()\n\n        if self.truncate_v:\n            n_header_rows = 0\n            row_num = self.tr_row_num\n            width = self.adj.len(fmt_values[row_num - 1])\n            if width > 3:\n                dot_str = '...'\n            else:\n                dot_str = '..'\n            # Series uses mode=center because it has single value columns\n            # DataFrame uses mode=left\n            dot_str = self.adj.justify([dot_str], width, mode='center')[0]\n            fmt_values.insert(row_num + n_header_rows, dot_str)\n            fmt_index.insert(row_num + 1, '')\n\n        if self.index:\n            result = self.adj.adjoin(3, *[fmt_index[1:], fmt_values])\n        else:\n            result = self.adj.adjoin(3, fmt_values).replace('\\n ',\n                                                            '\\n').strip()\n\n        if self.header and have_header:\n            result = fmt_index[0] + '\\n' + result\n\n        if footer:\n            result += '\\n' + footer\n\n        return compat.text_type(u('').join(result))\n\n\nclass TextAdjustment(object):\n\n    def __init__(self):\n        self.encoding = get_option(\"display.encoding\")\n\n    def len(self, text):\n        return compat.strlen(text, encoding=self.encoding)\n\n    def justify(self, texts, max_len, mode='right'):\n        return justify(texts, max_len, mode=mode)\n\n    def adjoin(self, space, *lists, **kwargs):\n        return adjoin(space, *lists, strlen=self.len,\n                      justfunc=self.justify, **kwargs)\n\n\nclass EastAsianTextAdjustment(TextAdjustment):\n\n    def __init__(self):\n        super(EastAsianTextAdjustment, self).__init__()\n        if get_option(\"display.unicode.ambiguous_as_wide\"):\n            self.ambiguous_width = 2\n        else:\n            self.ambiguous_width = 1\n\n    def len(self, text):\n        return compat.east_asian_len(text, encoding=self.encoding,\n                                     ambiguous_width=self.ambiguous_width)\n\n    def justify(self, texts, max_len, mode='right'):\n        # re-calculate padding space per str considering East Asian Width\n        def _get_pad(t):\n            return max_len - self.len(t) + len(t)\n\n        if mode == 'left':\n            return [x.ljust(_get_pad(x)) for x in texts]\n        elif mode == 'center':\n            return [x.center(_get_pad(x)) for x in texts]\n        else:\n            return [x.rjust(_get_pad(x)) for x in texts]\n\n\ndef _get_adjustment():\n    use_east_asian_width = get_option(\"display.unicode.east_asian_width\")\n    if use_east_asian_width:\n        return EastAsianTextAdjustment()\n    else:\n        return TextAdjustment()\n\n\nclass TableFormatter(object):\n    is_truncated = False\n    show_dimensions = None\n\n    @property\n    def should_show_dimensions(self):\n        return (self.show_dimensions is True or\n                (self.show_dimensions == 'truncate' and self.is_truncated))\n\n    def _get_formatter(self, i):\n        if isinstance(self.formatters, (list, tuple)):\n            if is_integer(i):\n                return self.formatters[i]\n            else:\n                return None\n        else:\n            if is_integer(i) and i not in self.columns:\n                i = self.columns[i]\n            return self.formatters.get(i, None)\n\n\nclass DataFrameFormatter(TableFormatter):\n    \"\"\"\n    Render a DataFrame\n\n    self.to_string() : console-friendly tabular output\n    self.to_html()   : html table\n    self.to_latex()   : LaTeX tabular environment table\n\n    \"\"\"\n\n    __doc__ = __doc__ if __doc__ else ''\n    __doc__ += common_docstring + justify_docstring + return_docstring\n\n    def __init__(self, frame, buf=None, columns=None, col_space=None,\n                 header=True, index=True, na_rep='NaN', formatters=None,\n                 justify=None, float_format=None, sparsify=None,\n                 index_names=True, line_width=None, max_rows=None,\n                 max_cols=None, show_dimensions=False, decimal='.', **kwds):\n        self.frame = frame\n        if buf is not None:\n            self.buf = _expand_user(_stringify_path(buf))\n        else:\n            self.buf = StringIO()\n        self.show_index_names = index_names\n\n        if sparsify is None:\n            sparsify = get_option(\"display.multi_sparse\")\n\n        self.sparsify = sparsify\n\n        self.float_format = float_format\n        self.formatters = formatters if formatters is not None else {}\n        self.na_rep = na_rep\n        self.decimal = decimal\n        self.col_space = col_space\n        self.header = header\n        self.index = index\n        self.line_width = line_width\n        self.max_rows = max_rows\n        self.max_cols = max_cols\n        self.max_rows_displayed = min(max_rows or len(self.frame),\n                                      len(self.frame))\n        self.show_dimensions = show_dimensions\n\n        if justify is None:\n            self.justify = get_option(\"display.colheader_justify\")\n        else:\n            self.justify = justify\n\n        self.kwds = kwds\n\n        if columns is not None:\n            self.columns = _ensure_index(columns)\n            self.frame = self.frame[self.columns]\n        else:\n            self.columns = frame.columns\n\n        self._chk_truncate()\n        self.adj = _get_adjustment()\n\n    def _chk_truncate(self):\n        \"\"\"\n        Checks whether the frame should be truncated. If so, slices\n        the frame up.\n        \"\"\"\n        from pandas.core.reshape.concat import concat\n\n        # Column of which first element is used to determine width of a dot col\n        self.tr_size_col = -1\n\n        # Cut the data to the information actually printed\n        max_cols = self.max_cols\n        max_rows = self.max_rows\n\n        if max_cols == 0 or max_rows == 0:  # assume we are in the terminal\n                                            # (why else = 0)\n            (w, h) = get_terminal_size()\n            self.w = w\n            self.h = h\n            if self.max_rows == 0:\n                dot_row = 1\n                prompt_row = 1\n                if self.show_dimensions:\n                    show_dimension_rows = 3\n                n_add_rows = (self.header + dot_row + show_dimension_rows +\n                              prompt_row)\n                # rows available to fill with actual data\n                max_rows_adj = self.h - n_add_rows\n                self.max_rows_adj = max_rows_adj\n\n            # Format only rows and columns that could potentially fit the\n            # screen\n            if max_cols == 0 and len(self.frame.columns) > w:\n                max_cols = w\n            if max_rows == 0 and len(self.frame) > h:\n                max_rows = h\n\n        if not hasattr(self, 'max_rows_adj'):\n            self.max_rows_adj = max_rows\n        if not hasattr(self, 'max_cols_adj'):\n            self.max_cols_adj = max_cols\n\n        max_cols_adj = self.max_cols_adj\n        max_rows_adj = self.max_rows_adj\n\n        truncate_h = max_cols_adj and (len(self.columns) > max_cols_adj)\n        truncate_v = max_rows_adj and (len(self.frame) > max_rows_adj)\n\n        frame = self.frame\n        if truncate_h:\n            if max_cols_adj == 0:\n                col_num = len(frame.columns)\n            elif max_cols_adj == 1:\n                frame = frame.iloc[:, :max_cols]\n                col_num = max_cols\n            else:\n                col_num = (max_cols_adj // 2)\n                frame = concat((frame.iloc[:, :col_num],\n                                frame.iloc[:, -col_num:]), axis=1)\n            self.tr_col_num = col_num\n        if truncate_v:\n            if max_rows_adj == 0:\n                row_num = len(frame)\n            if max_rows_adj == 1:\n                row_num = max_rows\n                frame = frame.iloc[:max_rows, :]\n            else:\n                row_num = max_rows_adj // 2\n                frame = concat((frame.iloc[:row_num, :],\n                                frame.iloc[-row_num:, :]))\n            self.tr_row_num = row_num\n\n        self.tr_frame = frame\n        self.truncate_h = truncate_h\n        self.truncate_v = truncate_v\n        self.is_truncated = self.truncate_h or self.truncate_v\n\n    def _to_str_columns(self):\n        \"\"\"\n        Render a DataFrame to a list of columns (as lists of strings).\n        \"\"\"\n        frame = self.tr_frame\n\n        # may include levels names also\n\n        str_index = self._get_formatted_index(frame)\n\n        if not is_list_like(self.header) and not self.header:\n            stringified = []\n            for i, c in enumerate(frame):\n                fmt_values = self._format_col(i)\n                fmt_values = _make_fixed_width(fmt_values, self.justify,\n                                               minimum=(self.col_space or 0),\n                                               adj=self.adj)\n                stringified.append(fmt_values)\n        else:\n            if is_list_like(self.header):\n                if len(self.header) != len(self.columns):\n                    raise ValueError(('Writing {ncols} cols but got {nalias} '\n                                      'aliases'\n                                      .format(ncols=len(self.columns),\n                                              nalias=len(self.header))))\n                str_columns = [[label] for label in self.header]\n            else:\n                str_columns = self._get_formatted_column_labels(frame)\n\n            stringified = []\n            for i, c in enumerate(frame):\n                cheader = str_columns[i]\n                header_colwidth = max(self.col_space or 0,\n                                      *(self.adj.len(x) for x in cheader))\n                fmt_values = self._format_col(i)\n                fmt_values = _make_fixed_width(fmt_values, self.justify,\n                                               minimum=header_colwidth,\n                                               adj=self.adj)\n\n                max_len = max(np.max([self.adj.len(x) for x in fmt_values]),\n                              header_colwidth)\n                cheader = self.adj.justify(cheader, max_len, mode=self.justify)\n                stringified.append(cheader + fmt_values)\n\n        strcols = stringified\n        if self.index:\n            strcols.insert(0, str_index)\n\n        # Add ... to signal truncated\n        truncate_h = self.truncate_h\n        truncate_v = self.truncate_v\n\n        if truncate_h:\n            col_num = self.tr_col_num\n            # infer from column header\n            col_width = self.adj.len(strcols[self.tr_size_col][0])\n            strcols.insert(self.tr_col_num + 1, ['...'.center(col_width)] *\n                           (len(str_index)))\n        if truncate_v:\n            n_header_rows = len(str_index) - len(frame)\n            row_num = self.tr_row_num\n            for ix, col in enumerate(strcols):\n                # infer from above row\n                cwidth = self.adj.len(strcols[ix][row_num])\n                is_dot_col = False\n                if truncate_h:\n                    is_dot_col = ix == col_num + 1\n                if cwidth > 3 or is_dot_col:\n                    my_str = '...'\n                else:\n                    my_str = '..'\n\n                if ix == 0:\n                    dot_mode = 'left'\n                elif is_dot_col:\n                    cwidth = self.adj.len(strcols[self.tr_size_col][0])\n                    dot_mode = 'center'\n                else:\n                    dot_mode = 'right'\n                dot_str = self.adj.justify([my_str], cwidth, mode=dot_mode)[0]\n                strcols[ix].insert(row_num + n_header_rows, dot_str)\n        return strcols\n\n    def to_string(self):\n        \"\"\"\n        Render a DataFrame to a console-friendly tabular output.\n        \"\"\"\n        from pandas import Series\n\n        frame = self.frame\n\n        if len(frame.columns) == 0 or len(frame.index) == 0:\n            info_line = (u('Empty {name}\\nColumns: {col}\\nIndex: {idx}')\n                         .format(name=type(self.frame).__name__,\n                         col=pprint_thing(frame.columns),\n                         idx=pprint_thing(frame.index)))\n            text = info_line\n        else:\n\n            strcols = self._to_str_columns()\n            if self.line_width is None:  # no need to wrap around just print\n                # the whole frame\n                text = self.adj.adjoin(1, *strcols)\n            elif (not isinstance(self.max_cols, int) or\n                    self.max_cols > 0):  # need to wrap around\n                text = self._join_multiline(*strcols)\n            else:  # max_cols == 0. Try to fit frame to terminal\n                text = self.adj.adjoin(1, *strcols).split('\\n')\n                max_len = Series(text).str.len().max()\n                headers = [ele[0] for ele in strcols]\n                # Size of last col determines dot col size. See\n                # `self._to_str_columns\n                size_tr_col = len(headers[self.tr_size_col])\n                max_len += size_tr_col  # Need to make space for largest row\n                # plus truncate dot col\n                dif = max_len - self.w\n                adj_dif = dif\n                col_lens = Series([Series(ele).apply(len).max()\n                                   for ele in strcols])\n                n_cols = len(col_lens)\n                counter = 0\n                while adj_dif > 0 and n_cols > 1:\n                    counter += 1\n                    mid = int(round(n_cols / 2.))\n                    mid_ix = col_lens.index[mid]\n                    col_len = col_lens[mid_ix]\n                    adj_dif -= (col_len + 1)  # adjoin adds one\n                    col_lens = col_lens.drop(mid_ix)\n                    n_cols = len(col_lens)\n                max_cols_adj = n_cols - self.index  # subtract index column\n                self.max_cols_adj = max_cols_adj\n\n                # Call again _chk_truncate to cut frame appropriately\n                # and then generate string representation\n                self._chk_truncate()\n                strcols = self._to_str_columns()\n                text = self.adj.adjoin(1, *strcols)\n        if not self.index:\n            text = text.replace('\\n ', '\\n').strip()\n        self.buf.writelines(text)\n\n        if self.should_show_dimensions:\n            self.buf.write(\"\\n\\n[{nrows} rows x {ncols} columns]\"\n                           .format(nrows=len(frame), ncols=len(frame.columns)))\n\n    def _join_multiline(self, *strcols):\n        lwidth = self.line_width\n        adjoin_width = 1\n        strcols = list(strcols)\n        if self.index:\n            idx = strcols.pop(0)\n            lwidth -= np.array([self.adj.len(x)\n                                for x in idx]).max() + adjoin_width\n\n        col_widths = [np.array([self.adj.len(x) for x in col]).max() if\n                      len(col) > 0 else 0 for col in strcols]\n        col_bins = _binify(col_widths, lwidth)\n        nbins = len(col_bins)\n\n        if self.truncate_v:\n            nrows = self.max_rows_adj + 1\n        else:\n            nrows = len(self.frame)\n\n        str_lst = []\n        st = 0\n        for i, ed in enumerate(col_bins):\n            row = strcols[st:ed]\n            if self.index:\n                row.insert(0, idx)\n            if nbins > 1:\n                if ed <= len(strcols) and i < nbins - 1:\n                    row.append([' \\\\'] + ['  '] * (nrows - 1))\n                else:\n                    row.append([' '] * nrows)\n            str_lst.append(self.adj.adjoin(adjoin_width, *row))\n            st = ed\n        return '\\n\\n'.join(str_lst)\n\n    def to_latex(self, column_format=None, longtable=False, encoding=None,\n                 multicolumn=False, multicolumn_format=None, multirow=False):\n        \"\"\"\n        Render a DataFrame to a LaTeX tabular/longtable environment output.\n        \"\"\"\n\n        latex_renderer = LatexFormatter(self, column_format=column_format,\n                                        longtable=longtable,\n                                        multicolumn=multicolumn,\n                                        multicolumn_format=multicolumn_format,\n                                        multirow=multirow)\n\n        if encoding is None:\n            encoding = 'ascii' if compat.PY2 else 'utf-8'\n\n        if hasattr(self.buf, 'write'):\n            latex_renderer.write_result(self.buf)\n        elif isinstance(self.buf, compat.string_types):\n            import codecs\n            with codecs.open(self.buf, 'w', encoding=encoding) as f:\n                latex_renderer.write_result(f)\n        else:\n            raise TypeError('buf is not a file name and it has no write '\n                            'method')\n\n    def _format_col(self, i):\n        frame = self.tr_frame\n        formatter = self._get_formatter(i)\n        values_to_format = frame.iloc[:, i]._formatting_values()\n        return format_array(values_to_format, formatter,\n                            float_format=self.float_format, na_rep=self.na_rep,\n                            space=self.col_space, decimal=self.decimal)\n\n    def to_html(self, classes=None, notebook=False, border=None):\n        \"\"\"\n        Render a DataFrame to a html table.\n\n        Parameters\n        ----------\n        classes : str or list-like\n            classes to include in the `class` attribute of the opening\n            ``<table>`` tag, in addition to the default \"dataframe\".\n        notebook : {True, False}, optional, default False\n            Whether the generated HTML is for IPython Notebook.\n        border : int\n            A ``border=border`` attribute is included in the opening\n            ``<table>`` tag. Default ``pd.options.html.border``.\n\n            .. versionadded:: 0.19.0\n         \"\"\"\n        html_renderer = HTMLFormatter(self, classes=classes,\n                                      max_rows=self.max_rows,\n                                      max_cols=self.max_cols,\n                                      notebook=notebook,\n                                      border=border)\n        if hasattr(self.buf, 'write'):\n            html_renderer.write_result(self.buf)\n        elif isinstance(self.buf, compat.string_types):\n            with open(self.buf, 'w') as f:\n                html_renderer.write_result(f)\n        else:\n            raise TypeError('buf is not a file name and it has no write '\n                            ' method')\n\n    def _get_formatted_column_labels(self, frame):\n        from pandas.core.index import _sparsify\n\n        columns = frame.columns\n\n        if isinstance(columns, MultiIndex):\n            fmt_columns = columns.format(sparsify=False, adjoin=False)\n            fmt_columns = lzip(*fmt_columns)\n            dtypes = self.frame.dtypes._values\n\n            # if we have a Float level, they don't use leading space at all\n            restrict_formatting = any([l.is_floating for l in columns.levels])\n            need_leadsp = dict(zip(fmt_columns, map(is_numeric_dtype, dtypes)))\n\n            def space_format(x, y):\n                if (y not in self.formatters and\n                        need_leadsp[x] and not restrict_formatting):\n                    return ' ' + y\n                return y\n\n            str_columns = list(zip(*[[space_format(x, y) for y in x]\n                                     for x in fmt_columns]))\n            if self.sparsify:\n                str_columns = _sparsify(str_columns)\n\n            str_columns = [list(x) for x in zip(*str_columns)]\n        else:\n            fmt_columns = columns.format()\n            dtypes = self.frame.dtypes\n            need_leadsp = dict(zip(fmt_columns, map(is_numeric_dtype, dtypes)))\n            str_columns = [[' ' + x if not self._get_formatter(i) and\n                            need_leadsp[x] else x]\n                           for i, (col, x) in enumerate(zip(columns,\n                                                            fmt_columns))]\n\n        if self.show_index_names and self.has_index_names:\n            for x in str_columns:\n                x.append('')\n\n        # self.str_columns = str_columns\n        return str_columns\n\n    @property\n    def has_index_names(self):\n        return _has_names(self.frame.index)\n\n    @property\n    def has_column_names(self):\n        return _has_names(self.frame.columns)\n\n    def _get_formatted_index(self, frame):\n        # Note: this is only used by to_string() and to_latex(), not by\n        # to_html().\n        index = frame.index\n        columns = frame.columns\n\n        show_index_names = self.show_index_names and self.has_index_names\n        show_col_names = (self.show_index_names and self.has_column_names)\n\n        fmt = self._get_formatter('__index__')\n\n        if isinstance(index, MultiIndex):\n            fmt_index = index.format(sparsify=self.sparsify, adjoin=False,\n                                     names=show_index_names, formatter=fmt)\n        else:\n            fmt_index = [index.format(name=show_index_names, formatter=fmt)]\n        fmt_index = [tuple(_make_fixed_width(list(x), justify='left',\n                                             minimum=(self.col_space or 0),\n                                             adj=self.adj)) for x in fmt_index]\n\n        adjoined = self.adj.adjoin(1, *fmt_index).split('\\n')\n\n        # empty space for columns\n        if show_col_names:\n            col_header = ['{x}'.format(x=x)\n                          for x in self._get_column_name_list()]\n        else:\n            col_header = [''] * columns.nlevels\n\n        if self.header:\n            return col_header + adjoined\n        else:\n            return adjoined\n\n    def _get_column_name_list(self):\n        names = []\n        columns = self.frame.columns\n        if isinstance(columns, MultiIndex):\n            names.extend('' if name is None else name\n                         for name in columns.names)\n        else:\n            names.append('' if columns.name is None else columns.name)\n        return names\n\n\nclass LatexFormatter(TableFormatter):\n    \"\"\" Used to render a DataFrame to a LaTeX tabular/longtable environment\n    output.\n\n    Parameters\n    ----------\n    formatter : `DataFrameFormatter`\n    column_format : str, default None\n        The columns format as specified in `LaTeX table format\n        <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g 'rcl' for 3 columns\n    longtable : boolean, default False\n        Use a longtable environment instead of tabular.\n\n    See also\n    --------\n    HTMLFormatter\n    \"\"\"\n\n    def __init__(self, formatter, column_format=None, longtable=False,\n                 multicolumn=False, multicolumn_format=None, multirow=False):\n        self.fmt = formatter\n        self.frame = self.fmt.frame\n        self.bold_rows = self.fmt.kwds.get('bold_rows', False)\n        self.column_format = column_format\n        self.longtable = longtable\n        self.multicolumn = multicolumn\n        self.multicolumn_format = multicolumn_format\n        self.multirow = multirow\n\n    def write_result(self, buf):\n        \"\"\"\n        Render a DataFrame to a LaTeX tabular/longtable environment output.\n        \"\"\"\n\n        # string representation of the columns\n        if len(self.frame.columns) == 0 or len(self.frame.index) == 0:\n            info_line = (u('Empty {name}\\nColumns: {col}\\nIndex: {idx}')\n                         .format(name=type(self.frame).__name__,\n                                 col=self.frame.columns,\n                                 idx=self.frame.index))\n            strcols = [[info_line]]\n        else:\n            strcols = self.fmt._to_str_columns()\n\n        def get_col_type(dtype):\n            if issubclass(dtype.type, np.number):\n                return 'r'\n            else:\n                return 'l'\n\n        # reestablish the MultiIndex that has been joined by _to_str_column\n        if self.fmt.index and isinstance(self.frame.index, MultiIndex):\n            clevels = self.frame.columns.nlevels\n            strcols.pop(0)\n            name = any(self.frame.index.names)\n            cname = any(self.frame.columns.names)\n            lastcol = self.frame.index.nlevels - 1\n            for i, lev in enumerate(self.frame.index.levels):\n                lev2 = lev.format()\n                blank = ' ' * len(lev2[0])\n                # display column names in last index-column\n                if cname and i == lastcol:\n                    lev3 = [x if x else '{}' for x in self.frame.columns.names]\n                else:\n                    lev3 = [blank] * clevels\n                if name:\n                    lev3.append(lev.name)\n                for level_idx, group in itertools.groupby(\n                        self.frame.index.labels[i]):\n                    count = len(list(group))\n                    lev3.extend([lev2[level_idx]] + [blank] * (count - 1))\n                strcols.insert(i, lev3)\n\n        column_format = self.column_format\n        if column_format is None:\n            dtypes = self.frame.dtypes._values\n            column_format = ''.join(map(get_col_type, dtypes))\n            if self.fmt.index:\n                index_format = 'l' * self.frame.index.nlevels\n                column_format = index_format + column_format\n        elif not isinstance(column_format,\n                            compat.string_types):  # pragma: no cover\n            raise AssertionError('column_format must be str or unicode, '\n                                 'not {typ}'.format(typ=type(column_format)))\n\n        if not self.longtable:\n            buf.write('\\\\begin{{tabular}}{{{fmt}}}\\n'\n                      .format(fmt=column_format))\n            buf.write('\\\\toprule\\n')\n        else:\n            buf.write('\\\\begin{{longtable}}{{{fmt}}}\\n'\n                      .format(fmt=column_format))\n            buf.write('\\\\toprule\\n')\n\n        ilevels = self.frame.index.nlevels\n        clevels = self.frame.columns.nlevels\n        nlevels = clevels\n        if any(self.frame.index.names):\n            nlevels += 1\n        strrows = list(zip(*strcols))\n        self.clinebuf = []\n\n        for i, row in enumerate(strrows):\n            if i == nlevels and self.fmt.header:\n                buf.write('\\\\midrule\\n')  # End of header\n                if self.longtable:\n                    buf.write('\\\\endhead\\n')\n                    buf.write('\\\\midrule\\n')\n                    buf.write('\\\\multicolumn{3}{r}{{Continued on next '\n                              'page}} \\\\\\\\\\n')\n                    buf.write('\\\\midrule\\n')\n                    buf.write('\\\\endfoot\\n\\n')\n                    buf.write('\\\\bottomrule\\n')\n                    buf.write('\\\\endlastfoot\\n')\n            if self.fmt.kwds.get('escape', True):\n                # escape backslashes first\n                crow = [(x.replace('\\\\', '\\\\textbackslash').replace('_', '\\\\_')\n                         .replace('%', '\\\\%').replace('$', '\\\\$')\n                         .replace('#', '\\\\#').replace('{', '\\\\{')\n                         .replace('}', '\\\\}').replace('~', '\\\\textasciitilde')\n                         .replace('^', '\\\\textasciicircum').replace('&', '\\\\&')\n                         if x else '{}') for x in row]\n            else:\n                crow = [x if x else '{}' for x in row]\n            if self.bold_rows and self.fmt.index:\n                # bold row labels\n                crow = ['\\\\textbf{{{x}}}'.format(x=x)\n                        if j < ilevels and x.strip() not in ['', '{}'] else x\n                        for j, x in enumerate(crow)]\n            if i < clevels and self.fmt.header and self.multicolumn:\n                # sum up columns to multicolumns\n                crow = self._format_multicolumn(crow, ilevels)\n            if (i >= nlevels and self.fmt.index and self.multirow and\n                    ilevels > 1):\n                # sum up rows to multirows\n                crow = self._format_multirow(crow, ilevels, i, strrows)\n            buf.write(' & '.join(crow))\n            buf.write(' \\\\\\\\\\n')\n            if self.multirow and i < len(strrows) - 1:\n                self._print_cline(buf, i, len(strcols))\n\n        if not self.longtable:\n            buf.write('\\\\bottomrule\\n')\n            buf.write('\\\\end{tabular}\\n')\n        else:\n            buf.write('\\\\end{longtable}\\n')\n\n    def _format_multicolumn(self, row, ilevels):\n        \"\"\"\n        Combine columns belonging to a group to a single multicolumn entry\n        according to self.multicolumn_format\n\n        e.g.:\n        a &  &  & b & c &\n        will become\n        \\multicolumn{3}{l}{a} & b & \\multicolumn{2}{l}{c}\n        \"\"\"\n        row2 = list(row[:ilevels])\n        ncol = 1\n        coltext = ''\n\n        def append_col():\n            # write multicolumn if needed\n            if ncol > 1:\n                row2.append('\\\\multicolumn{{{ncol:d}}}{{{fmt:s}}}{{{txt:s}}}'\n                            .format(ncol=ncol, fmt=self.multicolumn_format,\n                                    txt=coltext.strip()))\n            # don't modify where not needed\n            else:\n                row2.append(coltext)\n        for c in row[ilevels:]:\n            # if next col has text, write the previous\n            if c.strip():\n                if coltext:\n                    append_col()\n                coltext = c\n                ncol = 1\n            # if not, add it to the previous multicolumn\n            else:\n                ncol += 1\n        # write last column name\n        if coltext:\n            append_col()\n        return row2\n\n    def _format_multirow(self, row, ilevels, i, rows):\n        \"\"\"\n        Check following rows, whether row should be a multirow\n\n        e.g.:     becomes:\n        a & 0 &   \\multirow{2}{*}{a} & 0 &\n          & 1 &     & 1 &\n        b & 0 &   \\cline{1-2}\n                  b & 0 &\n        \"\"\"\n        for j in range(ilevels):\n            if row[j].strip():\n                nrow = 1\n                for r in rows[i + 1:]:\n                    if not r[j].strip():\n                        nrow += 1\n                    else:\n                        break\n                if nrow > 1:\n                    # overwrite non-multirow entry\n                    row[j] = '\\\\multirow{{{nrow:d}}}{{*}}{{{row:s}}}'.format(\n                        nrow=nrow, row=row[j].strip())\n                    # save when to end the current block with \\cline\n                    self.clinebuf.append([i + nrow - 1, j + 1])\n        return row\n\n    def _print_cline(self, buf, i, icol):\n        \"\"\"\n        Print clines after multirow-blocks are finished\n        \"\"\"\n        for cl in self.clinebuf:\n            if cl[0] == i:\n                buf.write('\\cline{{{cl:d}-{icol:d}}}\\n'\n                          .format(cl=cl[1], icol=icol))\n        # remove entries that have been written to buffer\n        self.clinebuf = [x for x in self.clinebuf if x[0] != i]\n\n\nclass HTMLFormatter(TableFormatter):\n\n    indent_delta = 2\n\n    def __init__(self, formatter, classes=None, max_rows=None, max_cols=None,\n                 notebook=False, border=None):\n        self.fmt = formatter\n        self.classes = classes\n\n        self.frame = self.fmt.frame\n        self.columns = self.fmt.tr_frame.columns\n        self.elements = []\n        self.bold_rows = self.fmt.kwds.get('bold_rows', False)\n        self.escape = self.fmt.kwds.get('escape', True)\n\n        self.max_rows = max_rows or len(self.fmt.frame)\n        self.max_cols = max_cols or len(self.fmt.columns)\n        self.show_dimensions = self.fmt.show_dimensions\n        self.is_truncated = (self.max_rows < len(self.fmt.frame) or\n                             self.max_cols < len(self.fmt.columns))\n        self.notebook = notebook\n        if border is None:\n            border = get_option('display.html.border')\n        self.border = border\n\n    def write(self, s, indent=0):\n        rs = pprint_thing(s)\n        self.elements.append(' ' * indent + rs)\n\n    def write_th(self, s, indent=0, tags=None):\n        if self.fmt.col_space is not None and self.fmt.col_space > 0:\n            tags = (tags or \"\")\n            tags += ('style=\"min-width: {colspace};\"'\n                     .format(colspace=self.fmt.col_space))\n\n        return self._write_cell(s, kind='th', indent=indent, tags=tags)\n\n    def write_td(self, s, indent=0, tags=None):\n        return self._write_cell(s, kind='td', indent=indent, tags=tags)\n\n    def _write_cell(self, s, kind='td', indent=0, tags=None):\n        if tags is not None:\n            start_tag = '<{kind} {tags}>'.format(kind=kind, tags=tags)\n        else:\n            start_tag = '<{kind}>'.format(kind=kind)\n\n        if self.escape:\n            # escape & first to prevent double escaping of &\n            esc = OrderedDict([('&', r'&amp;'), ('<', r'&lt;'),\n                               ('>', r'&gt;')])\n        else:\n            esc = {}\n        rs = pprint_thing(s, escape_chars=esc).strip()\n        self.write(u'{start}{rs}</{kind}>'\n                   .format(start=start_tag, rs=rs, kind=kind), indent)\n\n    def write_tr(self, line, indent=0, indent_delta=4, header=False,\n                 align=None, tags=None, nindex_levels=0):\n        if tags is None:\n            tags = {}\n\n        if align is None:\n            self.write('<tr>', indent)\n        else:\n            self.write('<tr style=\"text-align: {align};\">'\n                       .format(align=align), indent)\n        indent += indent_delta\n\n        for i, s in enumerate(line):\n            val_tag = tags.get(i, None)\n            if header or (self.bold_rows and i < nindex_levels):\n                self.write_th(s, indent, tags=val_tag)\n            else:\n                self.write_td(s, indent, tags=val_tag)\n\n        indent -= indent_delta\n        self.write('</tr>', indent)\n\n    def write_style(self):\n        # We use the \"scoped\" attribute here so that the desired\n        # style properties for the data frame are not then applied\n        # throughout the entire notebook.\n        template_first = \"\"\"\\\n            <style scoped>\"\"\"\n        template_last = \"\"\"\\\n            </style>\"\"\"\n        template_select = \"\"\"\\\n                .dataframe %s {\n                    %s: %s;\n                }\"\"\"\n        element_props = [('tbody tr th:only-of-type',\n                          'vertical-align',\n                          'middle'),\n                         ('tbody tr th',\n                          'vertical-align',\n                          'top')]\n        if isinstance(self.columns, MultiIndex):\n            element_props.append(('thead tr th',\n                                  'text-align',\n                                  'left'))\n            if all((self.fmt.has_index_names,\n                    self.fmt.index,\n                    self.fmt.show_index_names)):\n                element_props.append(('thead tr:last-of-type th',\n                                      'text-align',\n                                      'right'))\n        else:\n            element_props.append(('thead th',\n                                  'text-align',\n                                  'right'))\n        template_mid = '\\n\\n'.join(map(lambda t: template_select % t,\n                                       element_props))\n        template = dedent('\\n'.join((template_first,\n                                     template_mid,\n                                     template_last)))\n        if self.notebook:\n            self.write(template)\n\n    def write_result(self, buf):\n        indent = 0\n        frame = self.frame\n\n        _classes = ['dataframe']  # Default class.\n        if self.classes is not None:\n            if isinstance(self.classes, str):\n                self.classes = self.classes.split()\n            if not isinstance(self.classes, (list, tuple)):\n                raise AssertionError('classes must be list or tuple, not {typ}'\n                                     .format(typ=type(self.classes)))\n            _classes.extend(self.classes)\n\n        if self.notebook:\n            div_style = ''\n            try:\n                import IPython\n                if IPython.__version__ < LooseVersion('3.0.0'):\n                    div_style = ' style=\"max-width:1500px;overflow:auto;\"'\n            except (ImportError, AttributeError):\n                pass\n\n            self.write('<div{style}>'.format(style=div_style))\n\n        self.write_style()\n        self.write('<table border=\"{border}\" class=\"{cls}\">'\n                   .format(border=self.border, cls=' '.join(_classes)), indent)\n\n        indent += self.indent_delta\n        indent = self._write_header(indent)\n        indent = self._write_body(indent)\n\n        self.write('</table>', indent)\n        if self.should_show_dimensions:\n            by = chr(215) if compat.PY3 else unichr(215)  # ×\n            self.write(u('<p>{rows} rows {by} {cols} columns</p>')\n                       .format(rows=len(frame),\n                               by=by,\n                               cols=len(frame.columns)))\n\n        if self.notebook:\n            self.write('</div>')\n\n        _put_lines(buf, self.elements)\n\n    def _write_header(self, indent):\n        truncate_h = self.fmt.truncate_h\n        row_levels = self.frame.index.nlevels\n        if not self.fmt.header:\n            # write nothing\n            return indent\n\n        def _column_header():\n            if self.fmt.index:\n                row = [''] * (self.frame.index.nlevels - 1)\n            else:\n                row = []\n\n            if isinstance(self.columns, MultiIndex):\n                if self.fmt.has_column_names and self.fmt.index:\n                    row.append(single_column_table(self.columns.names))\n                else:\n                    row.append('')\n                style = \"text-align: {just};\".format(just=self.fmt.justify)\n                row.extend([single_column_table(c, self.fmt.justify, style)\n                            for c in self.columns])\n            else:\n                if self.fmt.index:\n                    row.append(self.columns.name or '')\n                row.extend(self.columns)\n            return row\n\n        self.write('<thead>', indent)\n        row = []\n\n        indent += self.indent_delta\n\n        if isinstance(self.columns, MultiIndex):\n            template = 'colspan=\"{span:d}\" halign=\"left\"'\n\n            if self.fmt.sparsify:\n                # GH3547\n                sentinel = com.sentinel_factory()\n            else:\n                sentinel = None\n            levels = self.columns.format(sparsify=sentinel, adjoin=False,\n                                         names=False)\n            level_lengths = get_level_lengths(levels, sentinel)\n            inner_lvl = len(level_lengths) - 1\n            for lnum, (records, values) in enumerate(zip(level_lengths,\n                                                         levels)):\n                if truncate_h:\n                    # modify the header lines\n                    ins_col = self.fmt.tr_col_num\n                    if self.fmt.sparsify:\n                        recs_new = {}\n                        # Increment tags after ... col.\n                        for tag, span in list(records.items()):\n                            if tag >= ins_col:\n                                recs_new[tag + 1] = span\n                            elif tag + span > ins_col:\n                                recs_new[tag] = span + 1\n                                if lnum == inner_lvl:\n                                    values = (values[:ins_col] + (u('...'),) +\n                                              values[ins_col:])\n                                else:\n                                    # sparse col headers do not receive a ...\n                                    values = (values[:ins_col] +\n                                              (values[ins_col - 1], ) +\n                                              values[ins_col:])\n                            else:\n                                recs_new[tag] = span\n                            # if ins_col lies between tags, all col headers\n                            # get ...\n                            if tag + span == ins_col:\n                                recs_new[ins_col] = 1\n                                values = (values[:ins_col] + (u('...'),) +\n                                          values[ins_col:])\n                        records = recs_new\n                        inner_lvl = len(level_lengths) - 1\n                        if lnum == inner_lvl:\n                            records[ins_col] = 1\n                    else:\n                        recs_new = {}\n                        for tag, span in list(records.items()):\n                            if tag >= ins_col:\n                                recs_new[tag + 1] = span\n                            else:\n                                recs_new[tag] = span\n                        recs_new[ins_col] = 1\n                        records = recs_new\n                        values = (values[:ins_col] + [u('...')] +\n                                  values[ins_col:])\n\n                name = self.columns.names[lnum]\n                row = [''] * (row_levels - 1) + ['' if name is None else\n                                                 pprint_thing(name)]\n\n                if row == [\"\"] and self.fmt.index is False:\n                    row = []\n\n                tags = {}\n                j = len(row)\n                for i, v in enumerate(values):\n                    if i in records:\n                        if records[i] > 1:\n                            tags[j] = template.format(span=records[i])\n                    else:\n                        continue\n                    j += 1\n                    row.append(v)\n                self.write_tr(row, indent, self.indent_delta, tags=tags,\n                              header=True)\n        else:\n            col_row = _column_header()\n            align = self.fmt.justify\n\n            if truncate_h:\n                ins_col = row_levels + self.fmt.tr_col_num\n                col_row.insert(ins_col, '...')\n\n            self.write_tr(col_row, indent, self.indent_delta, header=True,\n                          align=align)\n\n        if all((self.fmt.has_index_names,\n                self.fmt.index,\n                self.fmt.show_index_names)):\n            row = ([x if x is not None else ''\n                    for x in self.frame.index.names] +\n                   [''] * min(len(self.columns), self.max_cols))\n            if truncate_h:\n                ins_col = row_levels + self.fmt.tr_col_num\n                row.insert(ins_col, '')\n            self.write_tr(row, indent, self.indent_delta, header=True)\n\n        indent -= self.indent_delta\n        self.write('</thead>', indent)\n\n        return indent\n\n    def _write_body(self, indent):\n        self.write('<tbody>', indent)\n        indent += self.indent_delta\n\n        fmt_values = {}\n        for i in range(min(len(self.columns), self.max_cols)):\n            fmt_values[i] = self.fmt._format_col(i)\n\n        # write values\n        if self.fmt.index:\n            if isinstance(self.frame.index, MultiIndex):\n                self._write_hierarchical_rows(fmt_values, indent)\n            else:\n                self._write_regular_rows(fmt_values, indent)\n        else:\n            for i in range(min(len(self.frame), self.max_rows)):\n                row = [fmt_values[j][i] for j in range(len(self.columns))]\n                self.write_tr(row, indent, self.indent_delta, tags=None)\n\n        indent -= self.indent_delta\n        self.write('</tbody>', indent)\n        indent -= self.indent_delta\n\n        return indent\n\n    def _write_regular_rows(self, fmt_values, indent):\n        truncate_h = self.fmt.truncate_h\n        truncate_v = self.fmt.truncate_v\n\n        ncols = len(self.fmt.tr_frame.columns)\n        nrows = len(self.fmt.tr_frame)\n        fmt = self.fmt._get_formatter('__index__')\n        if fmt is not None:\n            index_values = self.fmt.tr_frame.index.map(fmt)\n        else:\n            index_values = self.fmt.tr_frame.index.format()\n\n        row = []\n        for i in range(nrows):\n\n            if truncate_v and i == (self.fmt.tr_row_num):\n                str_sep_row = ['...' for ele in row]\n                self.write_tr(str_sep_row, indent, self.indent_delta,\n                              tags=None, nindex_levels=1)\n\n            row = []\n            row.append(index_values[i])\n            row.extend(fmt_values[j][i] for j in range(ncols))\n\n            if truncate_h:\n                dot_col_ix = self.fmt.tr_col_num + 1\n                row.insert(dot_col_ix, '...')\n            self.write_tr(row, indent, self.indent_delta, tags=None,\n                          nindex_levels=1)\n\n    def _write_hierarchical_rows(self, fmt_values, indent):\n        template = 'rowspan=\"{span}\" valign=\"top\"'\n\n        truncate_h = self.fmt.truncate_h\n        truncate_v = self.fmt.truncate_v\n        frame = self.fmt.tr_frame\n        ncols = len(frame.columns)\n        nrows = len(frame)\n        row_levels = self.frame.index.nlevels\n\n        idx_values = frame.index.format(sparsify=False, adjoin=False,\n                                        names=False)\n        idx_values = lzip(*idx_values)\n\n        if self.fmt.sparsify:\n            # GH3547\n            sentinel = com.sentinel_factory()\n            levels = frame.index.format(sparsify=sentinel, adjoin=False,\n                                        names=False)\n\n            level_lengths = get_level_lengths(levels, sentinel)\n            inner_lvl = len(level_lengths) - 1\n            if truncate_v:\n                # Insert ... row and adjust idx_values and\n                # level_lengths to take this into account.\n                ins_row = self.fmt.tr_row_num\n                inserted = False\n                for lnum, records in enumerate(level_lengths):\n                    rec_new = {}\n                    for tag, span in list(records.items()):\n                        if tag >= ins_row:\n                            rec_new[tag + 1] = span\n                        elif tag + span > ins_row:\n                            rec_new[tag] = span + 1\n\n                            # GH 14882 - Make sure insertion done once\n                            if not inserted:\n                                dot_row = list(idx_values[ins_row - 1])\n                                dot_row[-1] = u('...')\n                                idx_values.insert(ins_row, tuple(dot_row))\n                                inserted = True\n                            else:\n                                dot_row = list(idx_values[ins_row])\n                                dot_row[inner_lvl - lnum] = u('...')\n                                idx_values[ins_row] = tuple(dot_row)\n                        else:\n                            rec_new[tag] = span\n                        # If ins_row lies between tags, all cols idx cols\n                        # receive ...\n                        if tag + span == ins_row:\n                            rec_new[ins_row] = 1\n                            if lnum == 0:\n                                idx_values.insert(ins_row, tuple(\n                                    [u('...')] * len(level_lengths)))\n\n                            # GH 14882 - Place ... in correct level\n                            elif inserted:\n                                dot_row = list(idx_values[ins_row])\n                                dot_row[inner_lvl - lnum] = u('...')\n                                idx_values[ins_row] = tuple(dot_row)\n                    level_lengths[lnum] = rec_new\n\n                level_lengths[inner_lvl][ins_row] = 1\n                for ix_col in range(len(fmt_values)):\n                    fmt_values[ix_col].insert(ins_row, '...')\n                nrows += 1\n\n            for i in range(nrows):\n                row = []\n                tags = {}\n\n                sparse_offset = 0\n                j = 0\n                for records, v in zip(level_lengths, idx_values[i]):\n                    if i in records:\n                        if records[i] > 1:\n                            tags[j] = template.format(span=records[i])\n                    else:\n                        sparse_offset += 1\n                        continue\n\n                    j += 1\n                    row.append(v)\n\n                row.extend(fmt_values[j][i] for j in range(ncols))\n                if truncate_h:\n                    row.insert(row_levels - sparse_offset +\n                               self.fmt.tr_col_num, '...')\n                self.write_tr(row, indent, self.indent_delta, tags=tags,\n                              nindex_levels=len(levels) - sparse_offset)\n        else:\n            for i in range(len(frame)):\n                idx_values = list(zip(*frame.index.format(\n                    sparsify=False, adjoin=False, names=False)))\n                row = []\n                row.extend(idx_values[i])\n                row.extend(fmt_values[j][i] for j in range(ncols))\n                if truncate_h:\n                    row.insert(row_levels + self.fmt.tr_col_num, '...')\n                self.write_tr(row, indent, self.indent_delta, tags=None,\n                              nindex_levels=frame.index.nlevels)\n\n\nclass CSVFormatter(object):\n\n    def __init__(self, obj, path_or_buf=None, sep=\",\", na_rep='',\n                 float_format=None, cols=None, header=True, index=True,\n                 index_label=None, mode='w', nanRep=None, encoding=None,\n                 compression=None, quoting=None, line_terminator='\\n',\n                 chunksize=None, tupleize_cols=False, quotechar='\"',\n                 date_format=None, doublequote=True, escapechar=None,\n                 decimal='.'):\n\n        self.obj = obj\n\n        if path_or_buf is None:\n            path_or_buf = StringIO()\n\n        self.path_or_buf = _expand_user(_stringify_path(path_or_buf))\n        self.sep = sep\n        self.na_rep = na_rep\n        self.float_format = float_format\n        self.decimal = decimal\n\n        self.header = header\n        self.index = index\n        self.index_label = index_label\n        self.mode = mode\n        self.encoding = encoding\n        self.compression = compression\n\n        if quoting is None:\n            quoting = csv.QUOTE_MINIMAL\n        self.quoting = quoting\n\n        if quoting == csv.QUOTE_NONE:\n            # prevents crash in _csv\n            quotechar = None\n        self.quotechar = quotechar\n\n        self.doublequote = doublequote\n        self.escapechar = escapechar\n\n        self.line_terminator = line_terminator\n\n        self.date_format = date_format\n\n        self.tupleize_cols = tupleize_cols\n        self.has_mi_columns = (isinstance(obj.columns, MultiIndex) and\n                               not self.tupleize_cols)\n\n        # validate mi options\n        if self.has_mi_columns:\n            if cols is not None:\n                raise TypeError(\"cannot specify cols with a MultiIndex on the \"\n                                \"columns\")\n\n        if cols is not None:\n            if isinstance(cols, Index):\n                cols = cols.to_native_types(na_rep=na_rep,\n                                            float_format=float_format,\n                                            date_format=date_format,\n                                            quoting=self.quoting)\n            else:\n                cols = list(cols)\n            self.obj = self.obj.loc[:, cols]\n\n        # update columns to include possible multiplicity of dupes\n        # and make sure sure cols is just a list of labels\n        cols = self.obj.columns\n        if isinstance(cols, Index):\n            cols = cols.to_native_types(na_rep=na_rep,\n                                        float_format=float_format,\n                                        date_format=date_format,\n                                        quoting=self.quoting)\n        else:\n            cols = list(cols)\n\n        # save it\n        self.cols = cols\n\n        # preallocate data 2d list\n        self.blocks = self.obj._data.blocks\n        ncols = sum(b.shape[0] for b in self.blocks)\n        self.data = [None] * ncols\n\n        if chunksize is None:\n            chunksize = (100000 // (len(self.cols) or 1)) or 1\n        self.chunksize = int(chunksize)\n\n        self.data_index = obj.index\n        if (isinstance(self.data_index, (DatetimeIndex, PeriodIndex)) and\n                date_format is not None):\n            self.data_index = Index([x.strftime(date_format) if notna(x) else\n                                     '' for x in self.data_index])\n\n        self.nlevels = getattr(self.data_index, 'nlevels', 1)\n        if not index:\n            self.nlevels = 0\n\n    def save(self):\n        # create the writer & save\n        if hasattr(self.path_or_buf, 'write'):\n            f = self.path_or_buf\n            close = False\n        else:\n            f, handles = _get_handle(self.path_or_buf, self.mode,\n                                     encoding=self.encoding,\n                                     compression=self.compression)\n            close = True\n\n        try:\n            writer_kwargs = dict(lineterminator=self.line_terminator,\n                                 delimiter=self.sep, quoting=self.quoting,\n                                 doublequote=self.doublequote,\n                                 escapechar=self.escapechar,\n                                 quotechar=self.quotechar)\n            if self.encoding is not None:\n                writer_kwargs['encoding'] = self.encoding\n                self.writer = UnicodeWriter(f, **writer_kwargs)\n            else:\n                self.writer = csv.writer(f, **writer_kwargs)\n\n            self._save()\n\n        finally:\n            if close:\n                f.close()\n\n    def _save_header(self):\n\n        writer = self.writer\n        obj = self.obj\n        index_label = self.index_label\n        cols = self.cols\n        has_mi_columns = self.has_mi_columns\n        header = self.header\n        encoded_labels = []\n\n        has_aliases = isinstance(header, (tuple, list, np.ndarray, Index))\n        if not (has_aliases or self.header):\n            return\n        if has_aliases:\n            if len(header) != len(cols):\n                raise ValueError(('Writing {ncols} cols but got {nalias} '\n                                 'aliases'.format(ncols=len(cols),\n                                                  nalias=len(header))))\n            else:\n                write_cols = header\n        else:\n            write_cols = cols\n\n        if self.index:\n            # should write something for index label\n            if index_label is not False:\n                if index_label is None:\n                    if isinstance(obj.index, MultiIndex):\n                        index_label = []\n                        for i, name in enumerate(obj.index.names):\n                            if name is None:\n                                name = ''\n                            index_label.append(name)\n                    else:\n                        index_label = obj.index.name\n                        if index_label is None:\n                            index_label = ['']\n                        else:\n                            index_label = [index_label]\n                elif not isinstance(index_label,\n                                    (list, tuple, np.ndarray, Index)):\n                    # given a string for a DF with Index\n                    index_label = [index_label]\n\n                encoded_labels = list(index_label)\n            else:\n                encoded_labels = []\n\n        if not has_mi_columns:\n            encoded_labels += list(write_cols)\n            writer.writerow(encoded_labels)\n        else:\n            # write out the mi\n            columns = obj.columns\n\n            # write out the names for each level, then ALL of the values for\n            # each level\n            for i in range(columns.nlevels):\n\n                # we need at least 1 index column to write our col names\n                col_line = []\n                if self.index:\n\n                    # name is the first column\n                    col_line.append(columns.names[i])\n\n                    if isinstance(index_label, list) and len(index_label) > 1:\n                        col_line.extend([''] * (len(index_label) - 1))\n\n                col_line.extend(columns._get_level_values(i))\n\n                writer.writerow(col_line)\n\n            # Write out the index line if it's not empty.\n            # Otherwise, we will print out an extraneous\n            # blank line between the mi and the data rows.\n            if encoded_labels and set(encoded_labels) != set(['']):\n                encoded_labels.extend([''] * len(columns))\n                writer.writerow(encoded_labels)\n\n    def _save(self):\n\n        self._save_header()\n\n        nrows = len(self.data_index)\n\n        # write in chunksize bites\n        chunksize = self.chunksize\n        chunks = int(nrows / chunksize) + 1\n\n        for i in range(chunks):\n            start_i = i * chunksize\n            end_i = min((i + 1) * chunksize, nrows)\n            if start_i >= end_i:\n                break\n\n            self._save_chunk(start_i, end_i)\n\n    def _save_chunk(self, start_i, end_i):\n\n        data_index = self.data_index\n\n        # create the data for a chunk\n        slicer = slice(start_i, end_i)\n        for i in range(len(self.blocks)):\n            b = self.blocks[i]\n            d = b.to_native_types(slicer=slicer, na_rep=self.na_rep,\n                                  float_format=self.float_format,\n                                  decimal=self.decimal,\n                                  date_format=self.date_format,\n                                  quoting=self.quoting)\n\n            for col_loc, col in zip(b.mgr_locs, d):\n                # self.data is a preallocated list\n                self.data[col_loc] = col\n\n        ix = data_index.to_native_types(slicer=slicer, na_rep=self.na_rep,\n                                        float_format=self.float_format,\n                                        decimal=self.decimal,\n                                        date_format=self.date_format,\n                                        quoting=self.quoting)\n\n        lib.write_csv_rows(self.data, ix, self.nlevels, self.cols, self.writer)\n\n\n# ----------------------------------------------------------------------\n# Array formatters\n\n\ndef format_array(values, formatter, float_format=None, na_rep='NaN',\n                 digits=None, space=None, justify='right', decimal='.'):\n\n    if is_categorical_dtype(values):\n        fmt_klass = CategoricalArrayFormatter\n    elif is_interval_dtype(values):\n        fmt_klass = IntervalArrayFormatter\n    elif is_float_dtype(values.dtype):\n        fmt_klass = FloatArrayFormatter\n    elif is_period_arraylike(values):\n        fmt_klass = PeriodArrayFormatter\n    elif is_integer_dtype(values.dtype):\n        fmt_klass = IntArrayFormatter\n    elif is_datetimetz(values):\n        fmt_klass = Datetime64TZFormatter\n    elif is_datetime64_dtype(values.dtype):\n        fmt_klass = Datetime64Formatter\n    elif is_timedelta64_dtype(values.dtype):\n        fmt_klass = Timedelta64Formatter\n    else:\n        fmt_klass = GenericArrayFormatter\n\n    if space is None:\n        space = get_option(\"display.column_space\")\n\n    if float_format is None:\n        float_format = get_option(\"display.float_format\")\n\n    if digits is None:\n        digits = get_option(\"display.precision\")\n\n    fmt_obj = fmt_klass(values, digits=digits, na_rep=na_rep,\n                        float_format=float_format, formatter=formatter,\n                        space=space, justify=justify, decimal=decimal)\n\n    return fmt_obj.get_result()\n\n\nclass GenericArrayFormatter(object):\n\n    def __init__(self, values, digits=7, formatter=None, na_rep='NaN',\n                 space=12, float_format=None, justify='right', decimal='.',\n                 quoting=None, fixed_width=True):\n        self.values = values\n        self.digits = digits\n        self.na_rep = na_rep\n        self.space = space\n        self.formatter = formatter\n        self.float_format = float_format\n        self.justify = justify\n        self.decimal = decimal\n        self.quoting = quoting\n        self.fixed_width = fixed_width\n\n    def get_result(self):\n        fmt_values = self._format_strings()\n        return _make_fixed_width(fmt_values, self.justify)\n\n    def _format_strings(self):\n        if self.float_format is None:\n            float_format = get_option(\"display.float_format\")\n            if float_format is None:\n                fmt_str = ('{{x: .{prec:d}g}}'\n                           .format(prec=get_option(\"display.precision\")))\n                float_format = lambda x: fmt_str.format(x=x)\n        else:\n            float_format = self.float_format\n\n        formatter = (\n            self.formatter if self.formatter is not None else\n            (lambda x: pprint_thing(x, escape_chars=('\\t', '\\r', '\\n'))))\n\n        def _format(x):\n            if self.na_rep is not None and lib.checknull(x):\n                if x is None:\n                    return 'None'\n                elif x is pd.NaT:\n                    return 'NaT'\n                return self.na_rep\n            elif isinstance(x, PandasObject):\n                return u'{x}'.format(x=x)\n            else:\n                # object dtype\n                return u'{x}'.format(x=formatter(x))\n\n        vals = self.values\n        if isinstance(vals, Index):\n            vals = vals._values\n        elif isinstance(vals, ABCSparseArray):\n            vals = vals.values\n\n        is_float_type = lib.map_infer(vals, is_float) & notna(vals)\n        leading_space = is_float_type.any()\n\n        fmt_values = []\n        for i, v in enumerate(vals):\n            if not is_float_type[i] and leading_space:\n                fmt_values.append(u' {v}'.format(v=_format(v)))\n            elif is_float_type[i]:\n                fmt_values.append(float_format(v))\n            else:\n                fmt_values.append(u' {v}'.format(v=_format(v)))\n\n        return fmt_values\n\n\nclass FloatArrayFormatter(GenericArrayFormatter):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        GenericArrayFormatter.__init__(self, *args, **kwargs)\n\n        # float_format is expected to be a string\n        # formatter should be used to pass a function\n        if self.float_format is not None and self.formatter is None:\n            if callable(self.float_format):\n                self.formatter = self.float_format\n                self.float_format = None\n\n    def _value_formatter(self, float_format=None, threshold=None):\n        \"\"\"Returns a function to be applied on each value to format it\n        \"\"\"\n\n        # the float_format parameter supersedes self.float_format\n        if float_format is None:\n            float_format = self.float_format\n\n        # we are going to compose different functions, to first convert to\n        # a string, then replace the decimal symbol, and finally chop according\n        # to the threshold\n\n        # when there is no float_format, we use str instead of '%g'\n        # because str(0.0) = '0.0' while '%g' % 0.0 = '0'\n        if float_format:\n            def base_formatter(v):\n                return float_format(value=v) if notna(v) else self.na_rep\n        else:\n            def base_formatter(v):\n                return str(v) if notna(v) else self.na_rep\n\n        if self.decimal != '.':\n            def decimal_formatter(v):\n                return base_formatter(v).replace('.', self.decimal, 1)\n        else:\n            decimal_formatter = base_formatter\n\n        if threshold is None:\n            return decimal_formatter\n\n        def formatter(value):\n            if notna(value):\n                if abs(value) > threshold:\n                    return decimal_formatter(value)\n                else:\n                    return decimal_formatter(0.0)\n            else:\n                return self.na_rep\n\n        return formatter\n\n    def get_result_as_array(self):\n        \"\"\"\n        Returns the float values converted into strings using\n        the parameters given at initalisation, as a numpy array\n        \"\"\"\n\n        if self.formatter is not None:\n            return np.array([self.formatter(x) for x in self.values])\n\n        if self.fixed_width:\n            threshold = get_option(\"display.chop_threshold\")\n        else:\n            threshold = None\n\n        # if we have a fixed_width, we'll need to try different float_format\n        def format_values_with(float_format):\n            formatter = self._value_formatter(float_format, threshold)\n\n            # separate the wheat from the chaff\n            values = self.values\n            mask = isna(values)\n            if hasattr(values, 'to_dense'):  # sparse numpy ndarray\n                values = values.to_dense()\n            values = np.array(values, dtype='object')\n            values[mask] = self.na_rep\n            imask = (~mask).ravel()\n            values.flat[imask] = np.array([formatter(val)\n                                           for val in values.ravel()[imask]])\n\n            if self.fixed_width:\n                return _trim_zeros(values, self.na_rep)\n\n            return values\n\n        # There is a special default string when we are fixed-width\n        # The default is otherwise to use str instead of a formatting string\n        if self.float_format is None:\n            if self.fixed_width:\n                float_format = partial('{value: .{digits:d}f}'.format,\n                                       digits=self.digits)\n            else:\n                float_format = self.float_format\n        else:\n            float_format = lambda value: self.float_format % value\n\n        formatted_values = format_values_with(float_format)\n\n        if not self.fixed_width:\n            return formatted_values\n\n        # we need do convert to engineering format if some values are too small\n        # and would appear as 0, or if some values are too big and take too\n        # much space\n\n        if len(formatted_values) > 0:\n            maxlen = max(len(x) for x in formatted_values)\n            too_long = maxlen > self.digits + 6\n        else:\n            too_long = False\n\n        with np.errstate(invalid='ignore'):\n            abs_vals = np.abs(self.values)\n            # this is pretty arbitrary for now\n            # large values: more that 8 characters including decimal symbol\n            # and first digit, hence > 1e6\n            has_large_values = (abs_vals > 1e6).any()\n            has_small_values = ((abs_vals < 10**(-self.digits)) &\n                                (abs_vals > 0)).any()\n\n        if has_small_values or (too_long and has_large_values):\n            float_format = partial('{value: .{digits:d}e}'.format,\n                                   digits=self.digits)\n            formatted_values = format_values_with(float_format)\n\n        return formatted_values\n\n    def _format_strings(self):\n        # shortcut\n        if self.formatter is not None:\n            return [self.formatter(x) for x in self.values]\n\n        return list(self.get_result_as_array())\n\n\nclass IntArrayFormatter(GenericArrayFormatter):\n\n    def _format_strings(self):\n        formatter = self.formatter or (lambda x: '{x: d}'.format(x=x))\n        fmt_values = [formatter(x) for x in self.values]\n        return fmt_values\n\n\nclass Datetime64Formatter(GenericArrayFormatter):\n\n    def __init__(self, values, nat_rep='NaT', date_format=None, **kwargs):\n        super(Datetime64Formatter, self).__init__(values, **kwargs)\n        self.nat_rep = nat_rep\n        self.date_format = date_format\n\n    def _format_strings(self):\n        \"\"\" we by definition have DO NOT have a TZ \"\"\"\n\n        values = self.values\n\n        if not isinstance(values, DatetimeIndex):\n            values = DatetimeIndex(values)\n\n        if self.formatter is not None and callable(self.formatter):\n            return [self.formatter(x) for x in values]\n\n        fmt_values = format_array_from_datetime(\n            values.asi8.ravel(),\n            format=_get_format_datetime64_from_values(values,\n                                                      self.date_format),\n            na_rep=self.nat_rep).reshape(values.shape)\n        return fmt_values.tolist()\n\n\nclass IntervalArrayFormatter(GenericArrayFormatter):\n\n    def __init__(self, values, *args, **kwargs):\n        GenericArrayFormatter.__init__(self, values, *args, **kwargs)\n\n    def _format_strings(self):\n        formatter = self.formatter or str\n        fmt_values = np.array([formatter(x) for x in self.values])\n        return fmt_values\n\n\nclass PeriodArrayFormatter(IntArrayFormatter):\n\n    def _format_strings(self):\n        from pandas.core.indexes.period import IncompatibleFrequency\n        try:\n            values = PeriodIndex(self.values).to_native_types()\n        except IncompatibleFrequency:\n            # periods may contains different freq\n            values = Index(self.values, dtype='object').to_native_types()\n\n        formatter = self.formatter or (lambda x: '{x}'.format(x=x))\n        fmt_values = [formatter(x) for x in values]\n        return fmt_values\n\n\nclass CategoricalArrayFormatter(GenericArrayFormatter):\n\n    def __init__(self, values, *args, **kwargs):\n        GenericArrayFormatter.__init__(self, values, *args, **kwargs)\n\n    def _format_strings(self):\n        fmt_values = format_array(self.values.get_values(), self.formatter,\n                                  float_format=self.float_format,\n                                  na_rep=self.na_rep, digits=self.digits,\n                                  space=self.space, justify=self.justify)\n        return fmt_values\n\n\ndef format_percentiles(percentiles):\n    \"\"\"\n    Outputs rounded and formatted percentiles.\n\n    Parameters\n    ----------\n    percentiles : list-like, containing floats from interval [0,1]\n\n    Returns\n    -------\n    formatted : list of strings\n\n    Notes\n    -----\n    Rounding precision is chosen so that: (1) if any two elements of\n    ``percentiles`` differ, they remain different after rounding\n    (2) no entry is *rounded* to 0% or 100%.\n    Any non-integer is always rounded to at least 1 decimal place.\n\n    Examples\n    --------\n    Keeps all entries different after rounding:\n\n    >>> format_percentiles([0.01999, 0.02001, 0.5, 0.666666, 0.9999])\n    ['1.999%', '2.001%', '50%', '66.667%', '99.99%']\n\n    No element is rounded to 0% or 100% (unless already equal to it).\n    Duplicates are allowed:\n\n    >>> format_percentiles([0, 0.5, 0.02001, 0.5, 0.666666, 0.9999])\n    ['0%', '50%', '2.0%', '50%', '66.67%', '99.99%']\n    \"\"\"\n\n    percentiles = np.asarray(percentiles)\n\n    # It checks for np.NaN as well\n    with np.errstate(invalid='ignore'):\n        if not is_numeric_dtype(percentiles) or not np.all(percentiles >= 0) \\\n                or not np.all(percentiles <= 1):\n            raise ValueError(\"percentiles should all be in the interval [0,1]\")\n\n    percentiles = 100 * percentiles\n    int_idx = (percentiles.astype(int) == percentiles)\n\n    if np.all(int_idx):\n        out = percentiles.astype(int).astype(str)\n        return [i + '%' for i in out]\n\n    unique_pcts = np.unique(percentiles)\n    to_begin = unique_pcts[0] if unique_pcts[0] > 0 else None\n    to_end = 100 - unique_pcts[-1] if unique_pcts[-1] < 100 else None\n\n    # Least precision that keeps percentiles unique after rounding\n    prec = -np.floor(np.log10(np.min(\n        np.ediff1d(unique_pcts, to_begin=to_begin, to_end=to_end)\n    ))).astype(int)\n    prec = max(1, prec)\n    out = np.empty_like(percentiles, dtype=object)\n    out[int_idx] = percentiles[int_idx].astype(int).astype(str)\n    out[~int_idx] = percentiles[~int_idx].round(prec).astype(str)\n    return [i + '%' for i in out]\n\n\ndef _is_dates_only(values):\n    # return a boolean if we are only dates (and don't have a timezone)\n    values = DatetimeIndex(values)\n    if values.tz is not None:\n        return False\n\n    values_int = values.asi8\n    consider_values = values_int != iNaT\n    one_day_nanos = (86400 * 1e9)\n    even_days = np.logical_and(consider_values,\n                               values_int % one_day_nanos != 0).sum() == 0\n    if even_days:\n        return True\n    return False\n\n\ndef _format_datetime64(x, tz=None, nat_rep='NaT'):\n    if x is None or lib.checknull(x):\n        return nat_rep\n\n    if tz is not None or not isinstance(x, Timestamp):\n        x = Timestamp(x, tz=tz)\n\n    return str(x)\n\n\ndef _format_datetime64_dateonly(x, nat_rep='NaT', date_format=None):\n    if x is None or lib.checknull(x):\n        return nat_rep\n\n    if not isinstance(x, Timestamp):\n        x = Timestamp(x)\n\n    if date_format:\n        return x.strftime(date_format)\n    else:\n        return x._date_repr\n\n\ndef _get_format_datetime64(is_dates_only, nat_rep='NaT', date_format=None):\n\n    if is_dates_only:\n        return lambda x, tz=None: _format_datetime64_dateonly(\n            x, nat_rep=nat_rep, date_format=date_format)\n    else:\n        return lambda x, tz=None: _format_datetime64(x, tz=tz, nat_rep=nat_rep)\n\n\ndef _get_format_datetime64_from_values(values, date_format):\n    \"\"\" given values and a date_format, return a string format \"\"\"\n    is_dates_only = _is_dates_only(values)\n    if is_dates_only:\n        return date_format or \"%Y-%m-%d\"\n    return date_format\n\n\nclass Datetime64TZFormatter(Datetime64Formatter):\n\n    def _format_strings(self):\n        \"\"\" we by definition have a TZ \"\"\"\n\n        values = self.values.asobject\n        is_dates_only = _is_dates_only(values)\n        formatter = (self.formatter or\n                     _get_format_datetime64(is_dates_only,\n                                            date_format=self.date_format))\n        fmt_values = [formatter(x) for x in values]\n\n        return fmt_values\n\n\nclass Timedelta64Formatter(GenericArrayFormatter):\n\n    def __init__(self, values, nat_rep='NaT', box=False, **kwargs):\n        super(Timedelta64Formatter, self).__init__(values, **kwargs)\n        self.nat_rep = nat_rep\n        self.box = box\n\n    def _format_strings(self):\n        formatter = (self.formatter or\n                     _get_format_timedelta64(self.values, nat_rep=self.nat_rep,\n                                             box=self.box))\n        fmt_values = np.array([formatter(x) for x in self.values])\n        return fmt_values\n\n\ndef _get_format_timedelta64(values, nat_rep='NaT', box=False):\n    \"\"\"\n    Return a formatter function for a range of timedeltas.\n    These will all have the same format argument\n\n    If box, then show the return in quotes\n    \"\"\"\n\n    values_int = values.astype(np.int64)\n\n    consider_values = values_int != iNaT\n\n    one_day_nanos = (86400 * 1e9)\n    even_days = np.logical_and(consider_values,\n                               values_int % one_day_nanos != 0).sum() == 0\n    all_sub_day = np.logical_and(\n        consider_values, np.abs(values_int) >= one_day_nanos).sum() == 0\n\n    if even_days:\n        format = 'even_day'\n    elif all_sub_day:\n        format = 'sub_day'\n    else:\n        format = 'long'\n\n    def _formatter(x):\n        if x is None or lib.checknull(x):\n            return nat_rep\n\n        if not isinstance(x, Timedelta):\n            x = Timedelta(x)\n        result = x._repr_base(format=format)\n        if box:\n            result = \"'{res}'\".format(res=result)\n        return result\n\n    return _formatter\n\n\ndef _make_fixed_width(strings, justify='right', minimum=None, adj=None):\n\n    if len(strings) == 0 or justify == 'all':\n        return strings\n\n    if adj is None:\n        adj = _get_adjustment()\n\n    max_len = np.max([adj.len(x) for x in strings])\n\n    if minimum is not None:\n        max_len = max(minimum, max_len)\n\n    conf_max = get_option(\"display.max_colwidth\")\n    if conf_max is not None and max_len > conf_max:\n        max_len = conf_max\n\n    def just(x):\n        if conf_max is not None:\n            if (conf_max > 3) & (adj.len(x) > max_len):\n                x = x[:max_len - 3] + '...'\n        return x\n\n    strings = [just(x) for x in strings]\n    result = adj.justify(strings, max_len, mode=justify)\n    return result\n\n\ndef _trim_zeros(str_floats, na_rep='NaN'):\n    \"\"\"\n    Trims zeros, leaving just one before the decimal points if need be.\n    \"\"\"\n    trimmed = str_floats\n\n    def _cond(values):\n        non_na = [x for x in values if x != na_rep]\n        return (len(non_na) > 0 and all([x.endswith('0') for x in non_na]) and\n                not (any([('e' in x) or ('E' in x) for x in non_na])))\n\n    while _cond(trimmed):\n        trimmed = [x[:-1] if x != na_rep else x for x in trimmed]\n\n    # leave one 0 after the decimal points if need be.\n    return [x + \"0\" if x.endswith('.') and x != na_rep else x for x in trimmed]\n\n\ndef single_column_table(column, align=None, style=None):\n    table = '<table'\n    if align is not None:\n        table += (' align=\"{align}\"'.format(align=align))\n    if style is not None:\n        table += (' style=\"{style}\"'.format(style=style))\n    table += '><tbody>'\n    for i in column:\n        table += ('<tr><td>{i!s}</td></tr>'.format(i=i))\n    table += '</tbody></table>'\n    return table\n\n\ndef single_row_table(row):  # pragma: no cover\n    table = '<table><tbody><tr>'\n    for i in row:\n        table += ('<td>{i!s}</td>'.format(i=i))\n    table += '</tr></tbody></table>'\n    return table\n\n\ndef _has_names(index):\n    if isinstance(index, MultiIndex):\n        return any([x is not None for x in index.names])\n    else:\n        return index.name is not None\n\n\nclass EngFormatter(object):\n    \"\"\"\n    Formats float values according to engineering format.\n\n    Based on matplotlib.ticker.EngFormatter\n    \"\"\"\n\n    # The SI engineering prefixes\n    ENG_PREFIXES = {\n        -24: \"y\",\n        -21: \"z\",\n        -18: \"a\",\n        -15: \"f\",\n        -12: \"p\",\n        -9: \"n\",\n        -6: \"u\",\n        -3: \"m\",\n        0: \"\",\n        3: \"k\",\n        6: \"M\",\n        9: \"G\",\n        12: \"T\",\n        15: \"P\",\n        18: \"E\",\n        21: \"Z\",\n        24: \"Y\"\n    }\n\n    def __init__(self, accuracy=None, use_eng_prefix=False):\n        self.accuracy = accuracy\n        self.use_eng_prefix = use_eng_prefix\n\n    def __call__(self, num):\n        \"\"\" Formats a number in engineering notation, appending a letter\n        representing the power of 1000 of the original number. Some examples:\n\n        >>> format_eng(0)       # for self.accuracy = 0\n        ' 0'\n\n        >>> format_eng(1000000) # for self.accuracy = 1,\n                                #     self.use_eng_prefix = True\n        ' 1.0M'\n\n        >>> format_eng(\"-1e-6\") # for self.accuracy = 2\n                                #     self.use_eng_prefix = False\n        '-1.00E-06'\n\n        @param num: the value to represent\n        @type num: either a numeric value or a string that can be converted to\n                   a numeric value (as per decimal.Decimal constructor)\n\n        @return: engineering formatted string\n        \"\"\"\n        import decimal\n        import math\n        dnum = decimal.Decimal(str(num))\n\n        if decimal.Decimal.is_nan(dnum):\n            return 'NaN'\n\n        if decimal.Decimal.is_infinite(dnum):\n            return 'inf'\n\n        sign = 1\n\n        if dnum < 0:  # pragma: no cover\n            sign = -1\n            dnum = -dnum\n\n        if dnum != 0:\n            pow10 = decimal.Decimal(int(math.floor(dnum.log10() / 3) * 3))\n        else:\n            pow10 = decimal.Decimal(0)\n\n        pow10 = pow10.min(max(self.ENG_PREFIXES.keys()))\n        pow10 = pow10.max(min(self.ENG_PREFIXES.keys()))\n        int_pow10 = int(pow10)\n\n        if self.use_eng_prefix:\n            prefix = self.ENG_PREFIXES[int_pow10]\n        else:\n            if int_pow10 < 0:\n                prefix = 'E-{pow10:02d}'.format(pow10=-int_pow10)\n            else:\n                prefix = 'E+{pow10:02d}'.format(pow10=int_pow10)\n\n        mant = sign * dnum / (10**pow10)\n\n        if self.accuracy is None:  # pragma: no cover\n            format_str = u(\"{mant: g}{prefix}\")\n        else:\n            format_str = (u(\"{{mant: .{acc:d}f}}{{prefix}}\")\n                          .format(acc=self.accuracy))\n\n        formatted = format_str.format(mant=mant, prefix=prefix)\n\n        return formatted  # .strip()\n\n\ndef set_eng_float_format(accuracy=3, use_eng_prefix=False):\n    \"\"\"\n    Alter default behavior on how float is formatted in DataFrame.\n    Format float in engineering format. By accuracy, we mean the number of\n    decimal digits after the floating point.\n\n    See also EngFormatter.\n    \"\"\"\n\n    set_option(\"display.float_format\", EngFormatter(accuracy, use_eng_prefix))\n    set_option(\"display.column_space\", max(12, accuracy + 9))\n\n\ndef _put_lines(buf, lines):\n    if any(isinstance(x, compat.text_type) for x in lines):\n        lines = [compat.text_type(x) for x in lines]\n    buf.write('\\n'.join(lines))\n\n\ndef _binify(cols, line_width):\n    adjoin_width = 1\n    bins = []\n    curr_width = 0\n    i_last_column = len(cols) - 1\n    for i, w in enumerate(cols):\n        w_adjoined = w + adjoin_width\n        curr_width += w_adjoined\n        if i_last_column == i:\n            wrap = curr_width + 1 > line_width and i > 0\n        else:\n            wrap = curr_width + 2 > line_width and i > 0\n        if wrap:\n            bins.append(i)\n            curr_width = w_adjoined\n\n    bins.append(len(cols))\n    return bins\n"
    },
    {
      "filename": "pandas/tests/series/test_analytics.py",
      "content": "# coding=utf-8\n# pylint: disable-msg=E1101,W0612\n\nfrom itertools import product\nfrom distutils.version import LooseVersion\n\nimport pytest\n\nfrom numpy import nan\nimport numpy as np\nimport pandas as pd\n\nfrom pandas import (Series, Categorical, DataFrame, isna, notna,\n                    bdate_range, date_range, _np_version_under1p10)\nfrom pandas.core.index import MultiIndex\nfrom pandas.core.indexes.datetimes import Timestamp\nfrom pandas.core.indexes.timedeltas import Timedelta\nimport pandas.core.config as cf\n\nimport pandas.core.nanops as nanops\n\nfrom pandas.compat import lrange, range, is_platform_windows\nfrom pandas import compat\nfrom pandas.util.testing import (assert_series_equal, assert_almost_equal,\n                                 assert_frame_equal, assert_index_equal)\nimport pandas.util.testing as tm\n\nfrom .common import TestData\n\n\nskip_if_bottleneck_on_windows = (is_platform_windows() and\n                                 nanops._USE_BOTTLENECK)\n\n\nclass TestSeriesAnalytics(TestData):\n\n    def test_sum_zero(self):\n        arr = np.array([])\n        assert nanops.nansum(arr) == 0\n\n        arr = np.empty((10, 0))\n        assert (nanops.nansum(arr, axis=1) == 0).all()\n\n        # GH #844\n        s = Series([], index=[])\n        assert s.sum() == 0\n\n        df = DataFrame(np.empty((10, 0)))\n        assert (df.sum(1) == 0).all()\n\n    def test_nansum_buglet(self):\n        s = Series([1.0, np.nan], index=[0, 1])\n        result = np.nansum(s)\n        assert_almost_equal(result, 1)\n\n    def test_overflow(self):\n        # GH 6915\n        # overflowing on the smaller int dtypes\n        for dtype in ['int32', 'int64']:\n            v = np.arange(5000000, dtype=dtype)\n            s = Series(v)\n\n            # no bottleneck\n            result = s.sum(skipna=False)\n            assert int(result) == v.sum(dtype='int64')\n            result = s.min(skipna=False)\n            assert int(result) == 0\n            result = s.max(skipna=False)\n            assert int(result) == v[-1]\n\n        for dtype in ['float32', 'float64']:\n            v = np.arange(5000000, dtype=dtype)\n            s = Series(v)\n\n            # no bottleneck\n            result = s.sum(skipna=False)\n            assert result == v.sum(dtype=dtype)\n            result = s.min(skipna=False)\n            assert np.allclose(float(result), 0.0)\n            result = s.max(skipna=False)\n            assert np.allclose(float(result), v[-1])\n\n    @pytest.mark.xfail(\n        skip_if_bottleneck_on_windows,\n        reason=\"buggy bottleneck with sum overflow on windows\")\n    def test_overflow_with_bottleneck(self):\n        # GH 6915\n        # overflowing on the smaller int dtypes\n        for dtype in ['int32', 'int64']:\n            v = np.arange(5000000, dtype=dtype)\n            s = Series(v)\n\n            # use bottleneck if available\n            result = s.sum()\n            assert int(result) == v.sum(dtype='int64')\n            result = s.min()\n            assert int(result) == 0\n            result = s.max()\n            assert int(result) == v[-1]\n\n        for dtype in ['float32', 'float64']:\n            v = np.arange(5000000, dtype=dtype)\n            s = Series(v)\n\n            # use bottleneck if available\n            result = s.sum()\n            assert result == v.sum(dtype=dtype)\n            result = s.min()\n            assert np.allclose(float(result), 0.0)\n            result = s.max()\n            assert np.allclose(float(result), v[-1])\n\n    @pytest.mark.xfail(\n        skip_if_bottleneck_on_windows,\n        reason=\"buggy bottleneck with sum overflow on windows\")\n    def test_sum(self):\n        self._check_stat_op('sum', np.sum, check_allna=True)\n\n    def test_sum_inf(self):\n        import pandas.core.nanops as nanops\n\n        s = Series(np.random.randn(10))\n        s2 = s.copy()\n\n        s[5:8] = np.inf\n        s2[5:8] = np.nan\n\n        assert np.isinf(s.sum())\n\n        arr = np.random.randn(100, 100).astype('f4')\n        arr[:, 2] = np.inf\n\n        with cf.option_context(\"mode.use_inf_as_na\", True):\n            assert_almost_equal(s.sum(), s2.sum())\n\n        res = nanops.nansum(arr, axis=1)\n        assert np.isinf(res).all()\n\n    def test_mean(self):\n        self._check_stat_op('mean', np.mean)\n\n    def test_median(self):\n        self._check_stat_op('median', np.median)\n\n        # test with integers, test failure\n        int_ts = Series(np.ones(10, dtype=int), index=lrange(10))\n        tm.assert_almost_equal(np.median(int_ts), int_ts.median())\n\n    def test_mode(self):\n        # No mode should be found.\n        exp = Series([], dtype=np.float64)\n        tm.assert_series_equal(Series([]).mode(), exp)\n\n        exp = Series([1], dtype=np.int64)\n        tm.assert_series_equal(Series([1]).mode(), exp)\n\n        exp = Series(['a', 'b', 'c'], dtype=np.object)\n        tm.assert_series_equal(Series(['a', 'b', 'c']).mode(), exp)\n\n        # Test numerical data types.\n        exp_single = [1]\n        data_single = [1] * 5 + [2] * 3\n\n        exp_multi = [1, 3]\n        data_multi = [1] * 5 + [2] * 3 + [3] * 5\n\n        for dt in np.typecodes['AllInteger'] + np.typecodes['Float']:\n            s = Series(data_single, dtype=dt)\n            exp = Series(exp_single, dtype=dt)\n            tm.assert_series_equal(s.mode(), exp)\n\n            s = Series(data_multi, dtype=dt)\n            exp = Series(exp_multi, dtype=dt)\n            tm.assert_series_equal(s.mode(), exp)\n\n        # Test string and object types.\n        exp = ['b']\n        data = ['a'] * 2 + ['b'] * 3\n\n        s = Series(data, dtype='c')\n        exp = Series(exp, dtype='c')\n        tm.assert_series_equal(s.mode(), exp)\n\n        exp = ['bar']\n        data = ['foo'] * 2 + ['bar'] * 3\n\n        for dt in [str, object]:\n            s = Series(data, dtype=dt)\n            exp = Series(exp, dtype=dt)\n            tm.assert_series_equal(s.mode(), exp)\n\n        # Test datetime types.\n        exp = Series(['1900-05-03', '2011-01-03',\n                      '2013-01-02'], dtype='M8[ns]')\n        s = Series(['2011-01-03', '2013-01-02',\n                    '1900-05-03'], dtype='M8[ns]')\n        tm.assert_series_equal(s.mode(), exp)\n\n        exp = Series(['2011-01-03', '2013-01-02'], dtype='M8[ns]')\n        s = Series(['2011-01-03', '2013-01-02', '1900-05-03',\n                    '2011-01-03', '2013-01-02'], dtype='M8[ns]')\n        tm.assert_series_equal(s.mode(), exp)\n\n        # gh-5986: Test timedelta types.\n        exp = Series(['-1 days', '0 days', '1 days'], dtype='timedelta64[ns]')\n        s = Series(['1 days', '-1 days', '0 days'],\n                   dtype='timedelta64[ns]')\n        tm.assert_series_equal(s.mode(), exp)\n\n        exp = Series(['2 min', '1 day'], dtype='timedelta64[ns]')\n        s = Series(['1 day', '1 day', '-1 day', '-1 day 2 min',\n                    '2 min', '2 min'], dtype='timedelta64[ns]')\n        tm.assert_series_equal(s.mode(), exp)\n\n        # Test mixed dtype.\n        exp = Series(['foo'])\n        s = Series([1, 'foo', 'foo'])\n        tm.assert_series_equal(s.mode(), exp)\n\n        # Test for uint64 overflow.\n        exp = Series([2**63], dtype=np.uint64)\n        s = Series([1, 2**63, 2**63], dtype=np.uint64)\n        tm.assert_series_equal(s.mode(), exp)\n\n        exp = Series([1, 2**63], dtype=np.uint64)\n        s = Series([1, 2**63], dtype=np.uint64)\n        tm.assert_series_equal(s.mode(), exp)\n\n        # Test category dtype.\n        c = Categorical([1, 2])\n        exp = Categorical([1, 2], categories=[1, 2])\n        exp = Series(exp, dtype='category')\n        tm.assert_series_equal(Series(c).mode(), exp)\n\n        c = Categorical([1, 'a', 'a'])\n        exp = Categorical(['a'], categories=[1, 'a'])\n        exp = Series(exp, dtype='category')\n        tm.assert_series_equal(Series(c).mode(), exp)\n\n        c = Categorical([1, 1, 2, 3, 3])\n        exp = Categorical([1, 3], categories=[1, 2, 3])\n        exp = Series(exp, dtype='category')\n        tm.assert_series_equal(Series(c).mode(), exp)\n\n    def test_prod(self):\n        self._check_stat_op('prod', np.prod)\n\n    def test_min(self):\n        self._check_stat_op('min', np.min, check_objects=True)\n\n    def test_max(self):\n        self._check_stat_op('max', np.max, check_objects=True)\n\n    def test_var_std(self):\n        alt = lambda x: np.std(x, ddof=1)\n        self._check_stat_op('std', alt)\n\n        alt = lambda x: np.var(x, ddof=1)\n        self._check_stat_op('var', alt)\n\n        result = self.ts.std(ddof=4)\n        expected = np.std(self.ts.values, ddof=4)\n        assert_almost_equal(result, expected)\n\n        result = self.ts.var(ddof=4)\n        expected = np.var(self.ts.values, ddof=4)\n        assert_almost_equal(result, expected)\n\n        # 1 - element series with ddof=1\n        s = self.ts.iloc[[0]]\n        result = s.var(ddof=1)\n        assert isna(result)\n\n        result = s.std(ddof=1)\n        assert isna(result)\n\n    def test_sem(self):\n        alt = lambda x: np.std(x, ddof=1) / np.sqrt(len(x))\n        self._check_stat_op('sem', alt)\n\n        result = self.ts.sem(ddof=4)\n        expected = np.std(self.ts.values,\n                          ddof=4) / np.sqrt(len(self.ts.values))\n        assert_almost_equal(result, expected)\n\n        # 1 - element series with ddof=1\n        s = self.ts.iloc[[0]]\n        result = s.sem(ddof=1)\n        assert isna(result)\n\n    def test_skew(self):\n        tm._skip_if_no_scipy()\n\n        from scipy.stats import skew\n        alt = lambda x: skew(x, bias=False)\n        self._check_stat_op('skew', alt)\n\n        # test corner cases, skew() returns NaN unless there's at least 3\n        # values\n        min_N = 3\n        for i in range(1, min_N + 1):\n            s = Series(np.ones(i))\n            df = DataFrame(np.ones((i, i)))\n            if i < min_N:\n                assert np.isnan(s.skew())\n                assert np.isnan(df.skew()).all()\n            else:\n                assert 0 == s.skew()\n                assert (df.skew() == 0).all()\n\n    def test_kurt(self):\n        tm._skip_if_no_scipy()\n\n        from scipy.stats import kurtosis\n        alt = lambda x: kurtosis(x, bias=False)\n        self._check_stat_op('kurt', alt)\n\n        index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]],\n                           labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2],\n                                   [0, 1, 0, 1, 0, 1]])\n        s = Series(np.random.randn(6), index=index)\n        tm.assert_almost_equal(s.kurt(), s.kurt(level=0)['bar'])\n\n        # test corner cases, kurt() returns NaN unless there's at least 4\n        # values\n        min_N = 4\n        for i in range(1, min_N + 1):\n            s = Series(np.ones(i))\n            df = DataFrame(np.ones((i, i)))\n            if i < min_N:\n                assert np.isnan(s.kurt())\n                assert np.isnan(df.kurt()).all()\n            else:\n                assert 0 == s.kurt()\n                assert (df.kurt() == 0).all()\n\n    def test_describe(self):\n        s = Series([0, 1, 2, 3, 4], name='int_data')\n        result = s.describe()\n        expected = Series([5, 2, s.std(), 0, 1, 2, 3, 4],\n                          name='int_data',\n                          index=['count', 'mean', 'std', 'min', '25%',\n                                 '50%', '75%', 'max'])\n        tm.assert_series_equal(result, expected)\n\n        s = Series([True, True, False, False, False], name='bool_data')\n        result = s.describe()\n        expected = Series([5, 2, False, 3], name='bool_data',\n                          index=['count', 'unique', 'top', 'freq'])\n        tm.assert_series_equal(result, expected)\n\n        s = Series(['a', 'a', 'b', 'c', 'd'], name='str_data')\n        result = s.describe()\n        expected = Series([5, 4, 'a', 2], name='str_data',\n                          index=['count', 'unique', 'top', 'freq'])\n        tm.assert_series_equal(result, expected)\n\n    def test_argsort(self):\n        self._check_accum_op('argsort', check_dtype=False)\n        argsorted = self.ts.argsort()\n        assert issubclass(argsorted.dtype.type, np.integer)\n\n        # GH 2967 (introduced bug in 0.11-dev I think)\n        s = Series([Timestamp('201301%02d' % (i + 1)) for i in range(5)])\n        assert s.dtype == 'datetime64[ns]'\n        shifted = s.shift(-1)\n        assert shifted.dtype == 'datetime64[ns]'\n        assert isna(shifted[4])\n\n        result = s.argsort()\n        expected = Series(lrange(5), dtype='int64')\n        assert_series_equal(result, expected)\n\n        result = shifted.argsort()\n        expected = Series(lrange(4) + [-1], dtype='int64')\n        assert_series_equal(result, expected)\n\n    def test_argsort_stable(self):\n        s = Series(np.random.randint(0, 100, size=10000))\n        mindexer = s.argsort(kind='mergesort')\n        qindexer = s.argsort()\n\n        mexpected = np.argsort(s.values, kind='mergesort')\n        qexpected = np.argsort(s.values, kind='quicksort')\n\n        tm.assert_series_equal(mindexer, Series(mexpected),\n                               check_dtype=False)\n        tm.assert_series_equal(qindexer, Series(qexpected),\n                               check_dtype=False)\n        pytest.raises(AssertionError, tm.assert_numpy_array_equal,\n                      qindexer, mindexer)\n\n    def test_cumsum(self):\n        self._check_accum_op('cumsum')\n\n    def test_cumprod(self):\n        self._check_accum_op('cumprod')\n\n    def test_cummin(self):\n        tm.assert_numpy_array_equal(self.ts.cummin().values,\n                                    np.minimum.accumulate(np.array(self.ts)))\n        ts = self.ts.copy()\n        ts[::2] = np.NaN\n        result = ts.cummin()[1::2]\n        expected = np.minimum.accumulate(ts.valid())\n\n        tm.assert_series_equal(result, expected)\n\n    def test_cummax(self):\n        tm.assert_numpy_array_equal(self.ts.cummax().values,\n                                    np.maximum.accumulate(np.array(self.ts)))\n        ts = self.ts.copy()\n        ts[::2] = np.NaN\n        result = ts.cummax()[1::2]\n        expected = np.maximum.accumulate(ts.valid())\n\n        tm.assert_series_equal(result, expected)\n\n    def test_cummin_datetime64(self):\n        s = pd.Series(pd.to_datetime(['NaT', '2000-1-2', 'NaT', '2000-1-1',\n                                      'NaT', '2000-1-3']))\n\n        expected = pd.Series(pd.to_datetime(['NaT', '2000-1-2', 'NaT',\n                                             '2000-1-1', 'NaT', '2000-1-1']))\n        result = s.cummin(skipna=True)\n        tm.assert_series_equal(expected, result)\n\n        expected = pd.Series(pd.to_datetime(\n            ['NaT', '2000-1-2', '2000-1-2', '2000-1-1', '2000-1-1', '2000-1-1'\n             ]))\n        result = s.cummin(skipna=False)\n        tm.assert_series_equal(expected, result)\n\n    def test_cummax_datetime64(self):\n        s = pd.Series(pd.to_datetime(['NaT', '2000-1-2', 'NaT', '2000-1-1',\n                                      'NaT', '2000-1-3']))\n\n        expected = pd.Series(pd.to_datetime(['NaT', '2000-1-2', 'NaT',\n                                             '2000-1-2', 'NaT', '2000-1-3']))\n        result = s.cummax(skipna=True)\n        tm.assert_series_equal(expected, result)\n\n        expected = pd.Series(pd.to_datetime(\n            ['NaT', '2000-1-2', '2000-1-2', '2000-1-2', '2000-1-2', '2000-1-3'\n             ]))\n        result = s.cummax(skipna=False)\n        tm.assert_series_equal(expected, result)\n\n    def test_cummin_timedelta64(self):\n        s = pd.Series(pd.to_timedelta(['NaT',\n                                       '2 min',\n                                       'NaT',\n                                       '1 min',\n                                       'NaT',\n                                       '3 min', ]))\n\n        expected = pd.Series(pd.to_timedelta(['NaT',\n                                              '2 min',\n                                              'NaT',\n                                              '1 min',\n                                              'NaT',\n                                              '1 min', ]))\n        result = s.cummin(skipna=True)\n        tm.assert_series_equal(expected, result)\n\n        expected = pd.Series(pd.to_timedelta(['NaT',\n                                              '2 min',\n                                              '2 min',\n                                              '1 min',\n                                              '1 min',\n                                              '1 min', ]))\n        result = s.cummin(skipna=False)\n        tm.assert_series_equal(expected, result)\n\n    def test_cummax_timedelta64(self):\n        s = pd.Series(pd.to_timedelta(['NaT',\n                                       '2 min',\n                                       'NaT',\n                                       '1 min',\n                                       'NaT',\n                                       '3 min', ]))\n\n        expected = pd.Series(pd.to_timedelta(['NaT',\n                                              '2 min',\n                                              'NaT',\n                                              '2 min',\n                                              'NaT',\n                                              '3 min', ]))\n        result = s.cummax(skipna=True)\n        tm.assert_series_equal(expected, result)\n\n        expected = pd.Series(pd.to_timedelta(['NaT',\n                                              '2 min',\n                                              '2 min',\n                                              '2 min',\n                                              '2 min',\n                                              '3 min', ]))\n        result = s.cummax(skipna=False)\n        tm.assert_series_equal(expected, result)\n\n    def test_npdiff(self):\n        pytest.skip(\"skipping due to Series no longer being an \"\n                    \"ndarray\")\n\n        # no longer works as the return type of np.diff is now nd.array\n        s = Series(np.arange(5))\n\n        r = np.diff(s)\n        assert_series_equal(Series([nan, 0, 0, 0, nan]), r)\n\n    def _check_stat_op(self, name, alternate, check_objects=False,\n                       check_allna=False):\n        import pandas.core.nanops as nanops\n\n        def testit():\n            f = getattr(Series, name)\n\n            # add some NaNs\n            self.series[5:15] = np.NaN\n\n            # idxmax, idxmin, min, and max are valid for dates\n            if name not in ['max', 'min']:\n                ds = Series(date_range('1/1/2001', periods=10))\n                pytest.raises(TypeError, f, ds)\n\n            # skipna or no\n            assert notna(f(self.series))\n            assert isna(f(self.series, skipna=False))\n\n            # check the result is correct\n            nona = self.series.dropna()\n            assert_almost_equal(f(nona), alternate(nona.values))\n            assert_almost_equal(f(self.series), alternate(nona.values))\n\n            allna = self.series * nan\n\n            if check_allna:\n                # xref 9422\n                # bottleneck >= 1.0 give 0.0 for an allna Series sum\n                try:\n                    assert nanops._USE_BOTTLENECK\n                    import bottleneck as bn  # noqa\n                    assert bn.__version__ >= LooseVersion('1.0')\n                    assert f(allna) == 0.0\n                except:\n                    assert np.isnan(f(allna))\n\n            # dtype=object with None, it works!\n            s = Series([1, 2, 3, None, 5])\n            f(s)\n\n            # 2888\n            l = [0]\n            l.extend(lrange(2 ** 40, 2 ** 40 + 1000))\n            s = Series(l, dtype='int64')\n            assert_almost_equal(float(f(s)), float(alternate(s.values)))\n\n            # check date range\n            if check_objects:\n                s = Series(bdate_range('1/1/2000', periods=10))\n                res = f(s)\n                exp = alternate(s)\n                assert res == exp\n\n            # check on string data\n            if name not in ['sum', 'min', 'max']:\n                pytest.raises(TypeError, f, Series(list('abc')))\n\n            # Invalid axis.\n            pytest.raises(ValueError, f, self.series, axis=1)\n\n            # Unimplemented numeric_only parameter.\n            if 'numeric_only' in compat.signature(f).args:\n                tm.assert_raises_regex(NotImplementedError, name, f,\n                                       self.series, numeric_only=True)\n\n        testit()\n\n        try:\n            import bottleneck as bn  # noqa\n            nanops._USE_BOTTLENECK = False\n            testit()\n            nanops._USE_BOTTLENECK = True\n        except ImportError:\n            pass\n\n    def _check_accum_op(self, name, check_dtype=True):\n        func = getattr(np, name)\n        tm.assert_numpy_array_equal(func(self.ts).values,\n                                    func(np.array(self.ts)),\n                                    check_dtype=check_dtype)\n\n        # with missing values\n        ts = self.ts.copy()\n        ts[::2] = np.NaN\n\n        result = func(ts)[1::2]\n        expected = func(np.array(ts.valid()))\n\n        tm.assert_numpy_array_equal(result.values, expected,\n                                    check_dtype=False)\n\n    def test_compress(self):\n        cond = [True, False, True, False, False]\n        s = Series([1, -1, 5, 8, 7],\n                   index=list('abcde'), name='foo')\n        expected = Series(s.values.compress(cond),\n                          index=list('ac'), name='foo')\n        tm.assert_series_equal(s.compress(cond), expected)\n\n    def test_numpy_compress(self):\n        cond = [True, False, True, False, False]\n        s = Series([1, -1, 5, 8, 7],\n                   index=list('abcde'), name='foo')\n        expected = Series(s.values.compress(cond),\n                          index=list('ac'), name='foo')\n        tm.assert_series_equal(np.compress(cond, s), expected)\n\n        msg = \"the 'axis' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.compress,\n                               cond, s, axis=1)\n\n        msg = \"the 'out' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.compress,\n                               cond, s, out=s)\n\n    def test_round(self):\n        self.ts.index.name = \"index_name\"\n        result = self.ts.round(2)\n        expected = Series(np.round(self.ts.values, 2),\n                          index=self.ts.index, name='ts')\n        assert_series_equal(result, expected)\n        assert result.name == self.ts.name\n\n    def test_numpy_round(self):\n        # See gh-12600\n        s = Series([1.53, 1.36, 0.06])\n        out = np.round(s, decimals=0)\n        expected = Series([2., 1., 0.])\n        assert_series_equal(out, expected)\n\n        msg = \"the 'out' parameter is not supported\"\n        with tm.assert_raises_regex(ValueError, msg):\n            np.round(s, decimals=0, out=s)\n\n    def test_built_in_round(self):\n        if not compat.PY3:\n            pytest.skip(\n                'build in round cannot be overriden prior to Python 3')\n\n        s = Series([1.123, 2.123, 3.123], index=lrange(3))\n        result = round(s)\n        expected_rounded0 = Series([1., 2., 3.], index=lrange(3))\n        tm.assert_series_equal(result, expected_rounded0)\n\n        decimals = 2\n        expected_rounded = Series([1.12, 2.12, 3.12], index=lrange(3))\n        result = round(s, decimals)\n        tm.assert_series_equal(result, expected_rounded)\n\n    def test_prod_numpy16_bug(self):\n        s = Series([1., 1., 1.], index=lrange(3))\n        result = s.prod()\n\n        assert not isinstance(result, Series)\n\n    def test_all_any(self):\n        ts = tm.makeTimeSeries()\n        bool_series = ts > 0\n        assert not bool_series.all()\n        assert bool_series.any()\n\n        # Alternative types, with implicit 'object' dtype.\n        s = Series(['abc', True])\n        assert 'abc' == s.any()  # 'abc' || True => 'abc'\n\n    def test_all_any_params(self):\n        # Check skipna, with implicit 'object' dtype.\n        s1 = Series([np.nan, True])\n        s2 = Series([np.nan, False])\n        assert s1.all(skipna=False)  # nan && True => True\n        assert s1.all(skipna=True)\n        assert np.isnan(s2.any(skipna=False))  # nan || False => nan\n        assert not s2.any(skipna=True)\n\n        # Check level.\n        s = pd.Series([False, False, True, True, False, True],\n                      index=[0, 0, 1, 1, 2, 2])\n        assert_series_equal(s.all(level=0), Series([False, True, False]))\n        assert_series_equal(s.any(level=0), Series([False, True, True]))\n\n        # bool_only is not implemented with level option.\n        pytest.raises(NotImplementedError, s.any, bool_only=True, level=0)\n        pytest.raises(NotImplementedError, s.all, bool_only=True, level=0)\n\n        # bool_only is not implemented alone.\n        pytest.raises(NotImplementedError, s.any, bool_only=True)\n        pytest.raises(NotImplementedError, s.all, bool_only=True)\n\n    def test_modulo(self):\n        with np.errstate(all='ignore'):\n\n            # GH3590, modulo as ints\n            p = DataFrame({'first': [3, 4, 5, 8], 'second': [0, 0, 0, 3]})\n            result = p['first'] % p['second']\n            expected = Series(p['first'].values % p['second'].values,\n                              dtype='float64')\n            expected.iloc[0:3] = np.nan\n            assert_series_equal(result, expected)\n\n            result = p['first'] % 0\n            expected = Series(np.nan, index=p.index, name='first')\n            assert_series_equal(result, expected)\n\n            p = p.astype('float64')\n            result = p['first'] % p['second']\n            expected = Series(p['first'].values % p['second'].values)\n            assert_series_equal(result, expected)\n\n            p = p.astype('float64')\n            result = p['first'] % p['second']\n            result2 = p['second'] % p['first']\n            assert not np.array_equal(result, result2)\n\n            # GH 9144\n            s = Series([0, 1])\n\n            result = s % 0\n            expected = Series([nan, nan])\n            assert_series_equal(result, expected)\n\n            result = 0 % s\n            expected = Series([nan, 0.0])\n            assert_series_equal(result, expected)\n\n    def test_ops_consistency_on_empty(self):\n\n        # GH 7869\n        # consistency on empty\n\n        # float\n        result = Series(dtype=float).sum()\n        assert result == 0\n\n        result = Series(dtype=float).mean()\n        assert isna(result)\n\n        result = Series(dtype=float).median()\n        assert isna(result)\n\n        # timedelta64[ns]\n        result = Series(dtype='m8[ns]').sum()\n        assert result == Timedelta(0)\n\n        result = Series(dtype='m8[ns]').mean()\n        assert result is pd.NaT\n\n        result = Series(dtype='m8[ns]').median()\n        assert result is pd.NaT\n\n    def test_corr(self):\n        tm._skip_if_no_scipy()\n\n        import scipy.stats as stats\n\n        # full overlap\n        tm.assert_almost_equal(self.ts.corr(self.ts), 1)\n\n        # partial overlap\n        tm.assert_almost_equal(self.ts[:15].corr(self.ts[5:]), 1)\n\n        assert isna(self.ts[:15].corr(self.ts[5:], min_periods=12))\n\n        ts1 = self.ts[:15].reindex(self.ts.index)\n        ts2 = self.ts[5:].reindex(self.ts.index)\n        assert isna(ts1.corr(ts2, min_periods=12))\n\n        # No overlap\n        assert np.isnan(self.ts[::2].corr(self.ts[1::2]))\n\n        # all NA\n        cp = self.ts[:10].copy()\n        cp[:] = np.nan\n        assert isna(cp.corr(cp))\n\n        A = tm.makeTimeSeries()\n        B = tm.makeTimeSeries()\n        result = A.corr(B)\n        expected, _ = stats.pearsonr(A, B)\n        tm.assert_almost_equal(result, expected)\n\n    def test_corr_rank(self):\n        tm._skip_if_no_scipy()\n\n        import scipy\n        import scipy.stats as stats\n\n        # kendall and spearman\n        A = tm.makeTimeSeries()\n        B = tm.makeTimeSeries()\n        A[-5:] = A[:5]\n        result = A.corr(B, method='kendall')\n        expected = stats.kendalltau(A, B)[0]\n        tm.assert_almost_equal(result, expected)\n\n        result = A.corr(B, method='spearman')\n        expected = stats.spearmanr(A, B)[0]\n        tm.assert_almost_equal(result, expected)\n\n        # these methods got rewritten in 0.8\n        if scipy.__version__ < LooseVersion('0.9'):\n            pytest.skip(\"skipping corr rank because of scipy version \"\n                        \"{0}\".format(scipy.__version__))\n\n        # results from R\n        A = Series(\n            [-0.89926396, 0.94209606, -1.03289164, -0.95445587, 0.76910310, -\n             0.06430576, -2.09704447, 0.40660407, -0.89926396, 0.94209606])\n        B = Series(\n            [-1.01270225, -0.62210117, -1.56895827, 0.59592943, -0.01680292,\n             1.17258718, -1.06009347, -0.10222060, -0.89076239, 0.89372375])\n        kexp = 0.4319297\n        sexp = 0.5853767\n        tm.assert_almost_equal(A.corr(B, method='kendall'), kexp)\n        tm.assert_almost_equal(A.corr(B, method='spearman'), sexp)\n\n    def test_cov(self):\n        # full overlap\n        tm.assert_almost_equal(self.ts.cov(self.ts), self.ts.std() ** 2)\n\n        # partial overlap\n        tm.assert_almost_equal(self.ts[:15].cov(self.ts[5:]),\n                               self.ts[5:15].std() ** 2)\n\n        # No overlap\n        assert np.isnan(self.ts[::2].cov(self.ts[1::2]))\n\n        # all NA\n        cp = self.ts[:10].copy()\n        cp[:] = np.nan\n        assert isna(cp.cov(cp))\n\n        # min_periods\n        assert isna(self.ts[:15].cov(self.ts[5:], min_periods=12))\n\n        ts1 = self.ts[:15].reindex(self.ts.index)\n        ts2 = self.ts[5:].reindex(self.ts.index)\n        assert isna(ts1.cov(ts2, min_periods=12))\n\n    def test_count(self):\n        assert self.ts.count() == len(self.ts)\n\n        self.ts[::2] = np.NaN\n\n        assert self.ts.count() == np.isfinite(self.ts).sum()\n\n        mi = MultiIndex.from_arrays([list('aabbcc'), [1, 2, 2, nan, 1, 2]])\n        ts = Series(np.arange(len(mi)), index=mi)\n\n        left = ts.count(level=1)\n        right = Series([2, 3, 1], index=[1, 2, nan])\n        assert_series_equal(left, right)\n\n        ts.iloc[[0, 3, 5]] = nan\n        assert_series_equal(ts.count(level=1), right - 1)\n\n    def test_dot(self):\n        a = Series(np.random.randn(4), index=['p', 'q', 'r', 's'])\n        b = DataFrame(np.random.randn(3, 4), index=['1', '2', '3'],\n                      columns=['p', 'q', 'r', 's']).T\n\n        result = a.dot(b)\n        expected = Series(np.dot(a.values, b.values), index=['1', '2', '3'])\n        assert_series_equal(result, expected)\n\n        # Check index alignment\n        b2 = b.reindex(index=reversed(b.index))\n        result = a.dot(b)\n        assert_series_equal(result, expected)\n\n        # Check ndarray argument\n        result = a.dot(b.values)\n        assert np.all(result == expected.values)\n        assert_almost_equal(a.dot(b['2'].values), expected['2'])\n\n        # Check series argument\n        assert_almost_equal(a.dot(b['1']), expected['1'])\n        assert_almost_equal(a.dot(b2['1']), expected['1'])\n\n        pytest.raises(Exception, a.dot, a.values[:3])\n        pytest.raises(ValueError, a.dot, b.T)\n\n    def test_value_counts_nunique(self):\n\n        # basics.rst doc example\n        series = Series(np.random.randn(500))\n        series[20:500] = np.nan\n        series[10:20] = 5000\n        result = series.nunique()\n        assert result == 11\n\n    def test_unique(self):\n\n        # 714 also, dtype=float\n        s = Series([1.2345] * 100)\n        s[::2] = np.nan\n        result = s.unique()\n        assert len(result) == 2\n\n        s = Series([1.2345] * 100, dtype='f4')\n        s[::2] = np.nan\n        result = s.unique()\n        assert len(result) == 2\n\n        # NAs in object arrays #714\n        s = Series(['foo'] * 100, dtype='O')\n        s[::2] = np.nan\n        result = s.unique()\n        assert len(result) == 2\n\n        # decision about None\n        s = Series([1, 2, 3, None, None, None], dtype=object)\n        result = s.unique()\n        expected = np.array([1, 2, 3, None], dtype=object)\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_drop_duplicates(self):\n        # check both int and object\n        for s in [Series([1, 2, 3, 3]), Series(['1', '2', '3', '3'])]:\n            expected = Series([False, False, False, True])\n            assert_series_equal(s.duplicated(), expected)\n            assert_series_equal(s.drop_duplicates(), s[~expected])\n            sc = s.copy()\n            sc.drop_duplicates(inplace=True)\n            assert_series_equal(sc, s[~expected])\n\n            expected = Series([False, False, True, False])\n            assert_series_equal(s.duplicated(keep='last'), expected)\n            assert_series_equal(s.drop_duplicates(keep='last'), s[~expected])\n            sc = s.copy()\n            sc.drop_duplicates(keep='last', inplace=True)\n            assert_series_equal(sc, s[~expected])\n\n            expected = Series([False, False, True, True])\n            assert_series_equal(s.duplicated(keep=False), expected)\n            assert_series_equal(s.drop_duplicates(keep=False), s[~expected])\n            sc = s.copy()\n            sc.drop_duplicates(keep=False, inplace=True)\n            assert_series_equal(sc, s[~expected])\n\n        for s in [Series([1, 2, 3, 5, 3, 2, 4]),\n                  Series(['1', '2', '3', '5', '3', '2', '4'])]:\n            expected = Series([False, False, False, False, True, True, False])\n            assert_series_equal(s.duplicated(), expected)\n            assert_series_equal(s.drop_duplicates(), s[~expected])\n            sc = s.copy()\n            sc.drop_duplicates(inplace=True)\n            assert_series_equal(sc, s[~expected])\n\n            expected = Series([False, True, True, False, False, False, False])\n            assert_series_equal(s.duplicated(keep='last'), expected)\n            assert_series_equal(s.drop_duplicates(keep='last'), s[~expected])\n            sc = s.copy()\n            sc.drop_duplicates(keep='last', inplace=True)\n            assert_series_equal(sc, s[~expected])\n\n            expected = Series([False, True, True, False, True, True, False])\n            assert_series_equal(s.duplicated(keep=False), expected)\n            assert_series_equal(s.drop_duplicates(keep=False), s[~expected])\n            sc = s.copy()\n            sc.drop_duplicates(keep=False, inplace=True)\n            assert_series_equal(sc, s[~expected])\n\n    def test_clip(self):\n        val = self.ts.median()\n\n        assert self.ts.clip_lower(val).min() == val\n        assert self.ts.clip_upper(val).max() == val\n\n        assert self.ts.clip(lower=val).min() == val\n        assert self.ts.clip(upper=val).max() == val\n\n        result = self.ts.clip(-0.5, 0.5)\n        expected = np.clip(self.ts, -0.5, 0.5)\n        assert_series_equal(result, expected)\n        assert isinstance(expected, Series)\n\n    def test_clip_types_and_nulls(self):\n\n        sers = [Series([np.nan, 1.0, 2.0, 3.0]), Series([None, 'a', 'b', 'c']),\n                Series(pd.to_datetime(\n                    [np.nan, 1, 2, 3], unit='D'))]\n\n        for s in sers:\n            thresh = s[2]\n            l = s.clip_lower(thresh)\n            u = s.clip_upper(thresh)\n            assert l[notna(l)].min() == thresh\n            assert u[notna(u)].max() == thresh\n            assert list(isna(s)) == list(isna(l))\n            assert list(isna(s)) == list(isna(u))\n\n    def test_clip_with_na_args(self):\n        \"\"\"Should process np.nan argument as None \"\"\"\n        # GH # 17276\n        s = Series([1, 2, 3])\n\n        assert_series_equal(s.clip(np.nan), Series([1, 2, 3]))\n        assert_series_equal(s.clip(upper=[1, 1, np.nan]), Series([1, 2, 3]))\n        assert_series_equal(s.clip(lower=[1, np.nan, 1]), Series([1, 2, 3]))\n        assert_series_equal(s.clip(upper=np.nan, lower=np.nan),\n                            Series([1, 2, 3]))\n\n    def test_clip_against_series(self):\n        # GH #6966\n\n        s = Series([1.0, 1.0, 4.0])\n        threshold = Series([1.0, 2.0, 3.0])\n\n        assert_series_equal(s.clip_lower(threshold), Series([1.0, 2.0, 4.0]))\n        assert_series_equal(s.clip_upper(threshold), Series([1.0, 1.0, 3.0]))\n\n        lower = Series([1.0, 2.0, 3.0])\n        upper = Series([1.5, 2.5, 3.5])\n\n        assert_series_equal(s.clip(lower, upper), Series([1.0, 2.0, 3.5]))\n        assert_series_equal(s.clip(1.5, upper), Series([1.5, 1.5, 3.5]))\n\n    @pytest.mark.parametrize(\"inplace\", [True, False])\n    @pytest.mark.parametrize(\"upper\", [[1, 2, 3], np.asarray([1, 2, 3])])\n    def test_clip_against_list_like(self, inplace, upper):\n        # GH #15390\n        original = pd.Series([5, 6, 7])\n        result = original.clip(upper=upper, inplace=inplace)\n        expected = pd.Series([1, 2, 3])\n\n        if inplace:\n            result = original\n        tm.assert_series_equal(result, expected, check_exact=True)\n\n    def test_clip_with_datetimes(self):\n\n        # GH 11838\n        # naive and tz-aware datetimes\n\n        t = Timestamp('2015-12-01 09:30:30')\n        s = Series([Timestamp('2015-12-01 09:30:00'),\n                    Timestamp('2015-12-01 09:31:00')])\n        result = s.clip(upper=t)\n        expected = Series([Timestamp('2015-12-01 09:30:00'),\n                           Timestamp('2015-12-01 09:30:30')])\n        assert_series_equal(result, expected)\n\n        t = Timestamp('2015-12-01 09:30:30', tz='US/Eastern')\n        s = Series([Timestamp('2015-12-01 09:30:00', tz='US/Eastern'),\n                    Timestamp('2015-12-01 09:31:00', tz='US/Eastern')])\n        result = s.clip(upper=t)\n        expected = Series([Timestamp('2015-12-01 09:30:00', tz='US/Eastern'),\n                           Timestamp('2015-12-01 09:30:30', tz='US/Eastern')])\n        assert_series_equal(result, expected)\n\n    def test_cummethods_bool(self):\n        # GH 6270\n        # looks like a buggy np.maximum.accumulate for numpy 1.6.1, py 3.2\n        def cummin(x):\n            return np.minimum.accumulate(x)\n\n        def cummax(x):\n            return np.maximum.accumulate(x)\n\n        a = pd.Series([False, False, False, True, True, False, False])\n        b = ~a\n        c = pd.Series([False] * len(b))\n        d = ~c\n        methods = {'cumsum': np.cumsum,\n                   'cumprod': np.cumprod,\n                   'cummin': cummin,\n                   'cummax': cummax}\n        args = product((a, b, c, d), methods)\n        for s, method in args:\n            expected = Series(methods[method](s.values))\n            result = getattr(s, method)()\n            assert_series_equal(result, expected)\n\n        e = pd.Series([False, True, nan, False])\n        cse = pd.Series([0, 1, nan, 1], dtype=object)\n        cpe = pd.Series([False, 0, nan, 0])\n        cmin = pd.Series([False, False, nan, False])\n        cmax = pd.Series([False, True, nan, True])\n        expecteds = {'cumsum': cse,\n                     'cumprod': cpe,\n                     'cummin': cmin,\n                     'cummax': cmax}\n\n        for method in methods:\n            res = getattr(e, method)()\n            assert_series_equal(res, expecteds[method])\n\n    def test_isin(self):\n        s = Series(['A', 'B', 'C', 'a', 'B', 'B', 'A', 'C'])\n\n        result = s.isin(['A', 'C'])\n        expected = Series([True, False, True, False, False, False, True, True])\n        assert_series_equal(result, expected)\n\n        # GH: 16012\n        # This specific issue has to have a series over 1e6 in len, but the\n        # comparison array (in_list) must be large enough so that numpy doesn't\n        # do a manual masking trick that will avoid this issue altogether\n        s = Series(list('abcdefghijk' * 10 ** 5))\n        # If numpy doesn't do the manual comparison/mask, these\n        # unorderable mixed types are what cause the exception in numpy\n        in_list = [-1, 'a', 'b', 'G', 'Y', 'Z', 'E',\n                   'K', 'E', 'S', 'I', 'R', 'R'] * 6\n\n        assert s.isin(in_list).sum() == 200000\n\n    def test_isin_with_string_scalar(self):\n        # GH4763\n        s = Series(['A', 'B', 'C', 'a', 'B', 'B', 'A', 'C'])\n        with pytest.raises(TypeError):\n            s.isin('a')\n\n        with pytest.raises(TypeError):\n            s = Series(['aaa', 'b', 'c'])\n            s.isin('aaa')\n\n    def test_isin_with_i8(self):\n        # GH 5021\n\n        expected = Series([True, True, False, False, False])\n        expected2 = Series([False, True, False, False, False])\n\n        # datetime64[ns]\n        s = Series(date_range('jan-01-2013', 'jan-05-2013'))\n\n        result = s.isin(s[0:2])\n        assert_series_equal(result, expected)\n\n        result = s.isin(s[0:2].values)\n        assert_series_equal(result, expected)\n\n        # fails on dtype conversion in the first place\n        result = s.isin(s[0:2].values.astype('datetime64[D]'))\n        assert_series_equal(result, expected)\n\n        result = s.isin([s[1]])\n        assert_series_equal(result, expected2)\n\n        result = s.isin([np.datetime64(s[1])])\n        assert_series_equal(result, expected2)\n\n        result = s.isin(set(s[0:2]))\n        assert_series_equal(result, expected)\n\n        # timedelta64[ns]\n        s = Series(pd.to_timedelta(lrange(5), unit='d'))\n        result = s.isin(s[0:2])\n        assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"empty\", [[], Series(), np.array([])])\n    def test_isin_empty(self, empty):\n        # see gh-16991\n        s = Series([\"a\", \"b\"])\n        expected = Series([False, False])\n\n        result = s.isin(empty)\n        tm.assert_series_equal(expected, result)\n\n    def test_timedelta64_analytics(self):\n        from pandas import date_range\n\n        # index min/max\n        td = Series(date_range('2012-1-1', periods=3, freq='D')) - \\\n            Timestamp('20120101')\n\n        result = td.idxmin()\n        assert result == 0\n\n        result = td.idxmax()\n        assert result == 2\n\n        # GH 2982\n        # with NaT\n        td[0] = np.nan\n\n        result = td.idxmin()\n        assert result == 1\n\n        result = td.idxmax()\n        assert result == 2\n\n        # abs\n        s1 = Series(date_range('20120101', periods=3))\n        s2 = Series(date_range('20120102', periods=3))\n        expected = Series(s2 - s1)\n\n        # this fails as numpy returns timedelta64[us]\n        # result = np.abs(s1-s2)\n        # assert_frame_equal(result,expected)\n\n        result = (s1 - s2).abs()\n        assert_series_equal(result, expected)\n\n        # max/min\n        result = td.max()\n        expected = Timedelta('2 days')\n        assert result == expected\n\n        result = td.min()\n        expected = Timedelta('1 days')\n        assert result == expected\n\n    def test_idxmin(self):\n        # test idxmin\n        # _check_stat_op approach can not be used here because of isna check.\n\n        # add some NaNs\n        self.series[5:15] = np.NaN\n\n        # skipna or no\n        assert self.series[self.series.idxmin()] == self.series.min()\n        assert isna(self.series.idxmin(skipna=False))\n\n        # no NaNs\n        nona = self.series.dropna()\n        assert nona[nona.idxmin()] == nona.min()\n        assert (nona.index.values.tolist().index(nona.idxmin()) ==\n                nona.values.argmin())\n\n        # all NaNs\n        allna = self.series * nan\n        assert isna(allna.idxmin())\n\n        # datetime64[ns]\n        from pandas import date_range\n        s = Series(date_range('20130102', periods=6))\n        result = s.idxmin()\n        assert result == 0\n\n        s[0] = np.nan\n        result = s.idxmin()\n        assert result == 1\n\n    def test_numpy_argmin_deprecated(self):\n        # See gh-16830\n        data = np.arange(1, 11)\n\n        s = Series(data, index=data)\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            # The deprecation of Series.argmin also causes a deprecation\n            # warning when calling np.argmin. This behavior is temporary\n            # until the implemention of Series.argmin is corrected.\n            result = np.argmin(s)\n\n        assert result == 1\n\n        with tm.assert_produces_warning(FutureWarning):\n            # argmin is aliased to idxmin\n            result = s.argmin()\n\n        assert result == 1\n\n        if not _np_version_under1p10:\n            with tm.assert_produces_warning(FutureWarning,\n                                            check_stacklevel=False):\n                msg = \"the 'out' parameter is not supported\"\n                tm.assert_raises_regex(ValueError, msg, np.argmin,\n                                       s, out=data)\n\n    def test_idxmax(self):\n        # test idxmax\n        # _check_stat_op approach can not be used here because of isna check.\n\n        # add some NaNs\n        self.series[5:15] = np.NaN\n\n        # skipna or no\n        assert self.series[self.series.idxmax()] == self.series.max()\n        assert isna(self.series.idxmax(skipna=False))\n\n        # no NaNs\n        nona = self.series.dropna()\n        assert nona[nona.idxmax()] == nona.max()\n        assert (nona.index.values.tolist().index(nona.idxmax()) ==\n                nona.values.argmax())\n\n        # all NaNs\n        allna = self.series * nan\n        assert isna(allna.idxmax())\n\n        from pandas import date_range\n        s = Series(date_range('20130102', periods=6))\n        result = s.idxmax()\n        assert result == 5\n\n        s[5] = np.nan\n        result = s.idxmax()\n        assert result == 4\n\n        # Float64Index\n        # GH 5914\n        s = pd.Series([1, 2, 3], [1.1, 2.1, 3.1])\n        result = s.idxmax()\n        assert result == 3.1\n        result = s.idxmin()\n        assert result == 1.1\n\n        s = pd.Series(s.index, s.index)\n        result = s.idxmax()\n        assert result == 3.1\n        result = s.idxmin()\n        assert result == 1.1\n\n    def test_numpy_argmax_deprecated(self):\n        # See gh-16830\n        data = np.arange(1, 11)\n\n        s = Series(data, index=data)\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            # The deprecation of Series.argmax also causes a deprecation\n            # warning when calling np.argmax. This behavior is temporary\n            # until the implemention of Series.argmax is corrected.\n            result = np.argmax(s)\n        assert result == 10\n\n        with tm.assert_produces_warning(FutureWarning):\n            # argmax is aliased to idxmax\n            result = s.argmax()\n\n        assert result == 10\n\n        if not _np_version_under1p10:\n            with tm.assert_produces_warning(FutureWarning,\n                                            check_stacklevel=False):\n                msg = \"the 'out' parameter is not supported\"\n                tm.assert_raises_regex(ValueError, msg, np.argmax,\n                                       s, out=data)\n\n    def test_ptp(self):\n        N = 1000\n        arr = np.random.randn(N)\n        ser = Series(arr)\n        assert np.ptp(ser) == np.ptp(arr)\n\n        # GH11163\n        s = Series([3, 5, np.nan, -3, 10])\n        assert s.ptp() == 13\n        assert pd.isna(s.ptp(skipna=False))\n\n        mi = pd.MultiIndex.from_product([['a', 'b'], [1, 2, 3]])\n        s = pd.Series([1, np.nan, 7, 3, 5, np.nan], index=mi)\n\n        expected = pd.Series([6, 2], index=['a', 'b'], dtype=np.float64)\n        tm.assert_series_equal(s.ptp(level=0), expected)\n\n        expected = pd.Series([np.nan, np.nan], index=['a', 'b'])\n        tm.assert_series_equal(s.ptp(level=0, skipna=False), expected)\n\n        with pytest.raises(ValueError):\n            s.ptp(axis=1)\n\n        s = pd.Series(['a', 'b', 'c', 'd', 'e'])\n        with pytest.raises(TypeError):\n            s.ptp()\n\n        with pytest.raises(NotImplementedError):\n            s.ptp(numeric_only=True)\n\n    def test_empty_timeseries_redections_return_nat(self):\n        # covers #11245\n        for dtype in ('m8[ns]', 'm8[ns]', 'M8[ns]', 'M8[ns, UTC]'):\n            assert Series([], dtype=dtype).min() is pd.NaT\n            assert Series([], dtype=dtype).max() is pd.NaT\n\n    def test_unique_data_ownership(self):\n        # it works! #1807\n        Series(Series([\"a\", \"c\", \"b\"]).unique()).sort_values()\n\n    def test_repeat(self):\n        s = Series(np.random.randn(3), index=['a', 'b', 'c'])\n\n        reps = s.repeat(5)\n        exp = Series(s.values.repeat(5), index=s.index.values.repeat(5))\n        assert_series_equal(reps, exp)\n\n        with tm.assert_produces_warning(FutureWarning):\n            result = s.repeat(reps=5)\n            assert_series_equal(result, exp)\n\n        to_rep = [2, 3, 4]\n        reps = s.repeat(to_rep)\n        exp = Series(s.values.repeat(to_rep),\n                     index=s.index.values.repeat(to_rep))\n        assert_series_equal(reps, exp)\n\n    def test_numpy_repeat(self):\n        s = Series(np.arange(3), name='x')\n        expected = Series(s.values.repeat(2), name='x',\n                          index=s.index.values.repeat(2))\n        assert_series_equal(np.repeat(s, 2), expected)\n\n        msg = \"the 'axis' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.repeat, s, 2, axis=0)\n\n    def test_searchsorted(self):\n        s = Series([1, 2, 3])\n\n        idx = s.searchsorted(1, side='left')\n        tm.assert_numpy_array_equal(idx, np.array([0], dtype=np.intp))\n\n        idx = s.searchsorted(1, side='right')\n        tm.assert_numpy_array_equal(idx, np.array([1], dtype=np.intp))\n\n        with tm.assert_produces_warning(FutureWarning):\n            idx = s.searchsorted(v=1, side='left')\n            tm.assert_numpy_array_equal(idx, np.array([0], dtype=np.intp))\n\n    def test_searchsorted_numeric_dtypes_scalar(self):\n        s = Series([1, 2, 90, 1000, 3e9])\n        r = s.searchsorted(30)\n        e = 2\n        assert r == e\n\n        r = s.searchsorted([30])\n        e = np.array([2], dtype=np.intp)\n        tm.assert_numpy_array_equal(r, e)\n\n    def test_searchsorted_numeric_dtypes_vector(self):\n        s = Series([1, 2, 90, 1000, 3e9])\n        r = s.searchsorted([91, 2e6])\n        e = np.array([3, 4], dtype=np.intp)\n        tm.assert_numpy_array_equal(r, e)\n\n    def test_search_sorted_datetime64_scalar(self):\n        s = Series(pd.date_range('20120101', periods=10, freq='2D'))\n        v = pd.Timestamp('20120102')\n        r = s.searchsorted(v)\n        e = 1\n        assert r == e\n\n    def test_search_sorted_datetime64_list(self):\n        s = Series(pd.date_range('20120101', periods=10, freq='2D'))\n        v = [pd.Timestamp('20120102'), pd.Timestamp('20120104')]\n        r = s.searchsorted(v)\n        e = np.array([1, 2], dtype=np.intp)\n        tm.assert_numpy_array_equal(r, e)\n\n    def test_searchsorted_sorter(self):\n        # GH8490\n        s = Series([3, 1, 2])\n        r = s.searchsorted([0, 3], sorter=np.argsort(s))\n        e = np.array([0, 2], dtype=np.intp)\n        tm.assert_numpy_array_equal(r, e)\n\n    def test_is_unique(self):\n        # GH11946\n        s = Series(np.random.randint(0, 10, size=1000))\n        assert not s.is_unique\n        s = Series(np.arange(1000))\n        assert s.is_unique\n\n    def test_is_monotonic(self):\n\n        s = Series(np.random.randint(0, 10, size=1000))\n        assert not s.is_monotonic\n        s = Series(np.arange(1000))\n        assert s.is_monotonic\n        assert s.is_monotonic_increasing\n        s = Series(np.arange(1000, 0, -1))\n        assert s.is_monotonic_decreasing\n\n        s = Series(pd.date_range('20130101', periods=10))\n        assert s.is_monotonic\n        assert s.is_monotonic_increasing\n        s = Series(list(reversed(s.tolist())))\n        assert not s.is_monotonic\n        assert s.is_monotonic_decreasing\n\n    def test_sort_index_level(self):\n        mi = MultiIndex.from_tuples([[1, 1, 3], [1, 1, 1]], names=list('ABC'))\n        s = Series([1, 2], mi)\n        backwards = s.iloc[[1, 0]]\n\n        res = s.sort_index(level='A')\n        assert_series_equal(backwards, res)\n\n        res = s.sort_index(level=['A', 'B'])\n        assert_series_equal(backwards, res)\n\n        res = s.sort_index(level='A', sort_remaining=False)\n        assert_series_equal(s, res)\n\n        res = s.sort_index(level=['A', 'B'], sort_remaining=False)\n        assert_series_equal(s, res)\n\n    def test_apply_categorical(self):\n        values = pd.Categorical(list('ABBABCD'), categories=list('DCBA'),\n                                ordered=True)\n        s = pd.Series(values, name='XX', index=list('abcdefg'))\n        result = s.apply(lambda x: x.lower())\n\n        # should be categorical dtype when the number of categories are\n        # the same\n        values = pd.Categorical(list('abbabcd'), categories=list('dcba'),\n                                ordered=True)\n        exp = pd.Series(values, name='XX', index=list('abcdefg'))\n        tm.assert_series_equal(result, exp)\n        tm.assert_categorical_equal(result.values, exp.values)\n\n        result = s.apply(lambda x: 'A')\n        exp = pd.Series(['A'] * 7, name='XX', index=list('abcdefg'))\n        tm.assert_series_equal(result, exp)\n        assert result.dtype == np.object\n\n    def test_shift_int(self):\n        ts = self.ts.astype(int)\n        shifted = ts.shift(1)\n        expected = ts.astype(float).shift(1)\n        assert_series_equal(shifted, expected)\n\n    def test_shift_categorical(self):\n        # GH 9416\n        s = pd.Series(['a', 'b', 'c', 'd'], dtype='category')\n\n        assert_series_equal(s.iloc[:-1], s.shift(1).shift(-1).valid())\n\n        sp1 = s.shift(1)\n        assert_index_equal(s.index, sp1.index)\n        assert np.all(sp1.values.codes[:1] == -1)\n        assert np.all(s.values.codes[:-1] == sp1.values.codes[1:])\n\n        sn2 = s.shift(-2)\n        assert_index_equal(s.index, sn2.index)\n        assert np.all(sn2.values.codes[-2:] == -1)\n        assert np.all(s.values.codes[2:] == sn2.values.codes[:-2])\n\n        assert_index_equal(s.values.categories, sp1.values.categories)\n        assert_index_equal(s.values.categories, sn2.values.categories)\n\n    def test_reshape_deprecate(self):\n        x = Series(np.random.random(10), name='x')\n        tm.assert_produces_warning(FutureWarning, x.reshape, x.shape)\n\n    def test_reshape_non_2d(self):\n        # see gh-4554\n        with tm.assert_produces_warning(FutureWarning):\n            x = Series(np.random.random(201), name='x')\n            assert x.reshape(x.shape, ) is x\n\n        # see gh-2719\n        with tm.assert_produces_warning(FutureWarning):\n            a = Series([1, 2, 3, 4])\n            result = a.reshape(2, 2)\n            expected = a.values.reshape(2, 2)\n            tm.assert_numpy_array_equal(result, expected)\n            assert isinstance(result, type(expected))\n\n    def test_reshape_2d_return_array(self):\n        x = Series(np.random.random(201), name='x')\n\n        with tm.assert_produces_warning(FutureWarning):\n            result = x.reshape((-1, 1))\n            assert not isinstance(result, Series)\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result2 = np.reshape(x, (-1, 1))\n            assert not isinstance(result2, Series)\n\n        with tm.assert_produces_warning(FutureWarning):\n            result = x[:, None]\n            expected = x.reshape((-1, 1))\n            tm.assert_almost_equal(result, expected)\n\n    def test_reshape_bad_kwarg(self):\n        a = Series([1, 2, 3, 4])\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            msg = \"'foo' is an invalid keyword argument for this function\"\n            tm.assert_raises_regex(\n                TypeError, msg, a.reshape, (2, 2), foo=2)\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            msg = r\"reshape\\(\\) got an unexpected keyword argument 'foo'\"\n            tm.assert_raises_regex(\n                TypeError, msg, a.reshape, a.shape, foo=2)\n\n    def test_numpy_reshape(self):\n        a = Series([1, 2, 3, 4])\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = np.reshape(a, (2, 2))\n            expected = a.values.reshape(2, 2)\n            tm.assert_numpy_array_equal(result, expected)\n            assert isinstance(result, type(expected))\n\n        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):\n            result = np.reshape(a, a.shape)\n            tm.assert_series_equal(result, a)\n\n    def test_unstack(self):\n        from numpy import nan\n\n        index = MultiIndex(levels=[['bar', 'foo'], ['one', 'three', 'two']],\n                           labels=[[1, 1, 0, 0], [0, 1, 0, 2]])\n\n        s = Series(np.arange(4.), index=index)\n        unstacked = s.unstack()\n\n        expected = DataFrame([[2., nan, 3.], [0., 1., nan]],\n                             index=['bar', 'foo'],\n                             columns=['one', 'three', 'two'])\n\n        assert_frame_equal(unstacked, expected)\n\n        unstacked = s.unstack(level=0)\n        assert_frame_equal(unstacked, expected.T)\n\n        index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]],\n                           labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2],\n                                   [0, 1, 0, 1, 0, 1]])\n        s = Series(np.random.randn(6), index=index)\n        exp_index = MultiIndex(levels=[['one', 'two', 'three'], [0, 1]],\n                               labels=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]])\n        expected = DataFrame({'bar': s.values},\n                             index=exp_index).sort_index(level=0)\n        unstacked = s.unstack(0).sort_index()\n        assert_frame_equal(unstacked, expected)\n\n        # GH5873\n        idx = pd.MultiIndex.from_arrays([[101, 102], [3.5, np.nan]])\n        ts = pd.Series([1, 2], index=idx)\n        left = ts.unstack()\n        right = DataFrame([[nan, 1], [2, nan]], index=[101, 102],\n                          columns=[nan, 3.5])\n        assert_frame_equal(left, right)\n\n        idx = pd.MultiIndex.from_arrays([['cat', 'cat', 'cat', 'dog', 'dog'\n                                          ], ['a', 'a', 'b', 'a', 'b'],\n                                         [1, 2, 1, 1, np.nan]])\n        ts = pd.Series([1.0, 1.1, 1.2, 1.3, 1.4], index=idx)\n        right = DataFrame([[1.0, 1.3], [1.1, nan], [nan, 1.4], [1.2, nan]],\n                          columns=['cat', 'dog'])\n        tpls = [('a', 1), ('a', 2), ('b', nan), ('b', 1)]\n        right.index = pd.MultiIndex.from_tuples(tpls)\n        assert_frame_equal(ts.unstack(level=0), right)\n\n    def test_value_counts_datetime(self):\n        # most dtypes are tested in test_base.py\n        values = [pd.Timestamp('2011-01-01 09:00'),\n                  pd.Timestamp('2011-01-01 10:00'),\n                  pd.Timestamp('2011-01-01 11:00'),\n                  pd.Timestamp('2011-01-01 09:00'),\n                  pd.Timestamp('2011-01-01 09:00'),\n                  pd.Timestamp('2011-01-01 11:00')]\n\n        exp_idx = pd.DatetimeIndex(['2011-01-01 09:00', '2011-01-01 11:00',\n                                    '2011-01-01 10:00'])\n        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')\n\n        s = pd.Series(values, name='xxx')\n        tm.assert_series_equal(s.value_counts(), exp)\n        # check DatetimeIndex outputs the same result\n        idx = pd.DatetimeIndex(values, name='xxx')\n        tm.assert_series_equal(idx.value_counts(), exp)\n\n        # normalize\n        exp = pd.Series(np.array([3., 2., 1]) / 6.,\n                        index=exp_idx, name='xxx')\n        tm.assert_series_equal(s.value_counts(normalize=True), exp)\n        tm.assert_series_equal(idx.value_counts(normalize=True), exp)\n\n    def test_value_counts_datetime_tz(self):\n        values = [pd.Timestamp('2011-01-01 09:00', tz='US/Eastern'),\n                  pd.Timestamp('2011-01-01 10:00', tz='US/Eastern'),\n                  pd.Timestamp('2011-01-01 11:00', tz='US/Eastern'),\n                  pd.Timestamp('2011-01-01 09:00', tz='US/Eastern'),\n                  pd.Timestamp('2011-01-01 09:00', tz='US/Eastern'),\n                  pd.Timestamp('2011-01-01 11:00', tz='US/Eastern')]\n\n        exp_idx = pd.DatetimeIndex(['2011-01-01 09:00', '2011-01-01 11:00',\n                                    '2011-01-01 10:00'], tz='US/Eastern')\n        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')\n\n        s = pd.Series(values, name='xxx')\n        tm.assert_series_equal(s.value_counts(), exp)\n        idx = pd.DatetimeIndex(values, name='xxx')\n        tm.assert_series_equal(idx.value_counts(), exp)\n\n        exp = pd.Series(np.array([3., 2., 1]) / 6.,\n                        index=exp_idx, name='xxx')\n        tm.assert_series_equal(s.value_counts(normalize=True), exp)\n        tm.assert_series_equal(idx.value_counts(normalize=True), exp)\n\n    def test_value_counts_period(self):\n        values = [pd.Period('2011-01', freq='M'),\n                  pd.Period('2011-02', freq='M'),\n                  pd.Period('2011-03', freq='M'),\n                  pd.Period('2011-01', freq='M'),\n                  pd.Period('2011-01', freq='M'),\n                  pd.Period('2011-03', freq='M')]\n\n        exp_idx = pd.PeriodIndex(['2011-01', '2011-03', '2011-02'], freq='M')\n        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')\n\n        s = pd.Series(values, name='xxx')\n        tm.assert_series_equal(s.value_counts(), exp)\n        # check DatetimeIndex outputs the same result\n        idx = pd.PeriodIndex(values, name='xxx')\n        tm.assert_series_equal(idx.value_counts(), exp)\n\n        # normalize\n        exp = pd.Series(np.array([3., 2., 1]) / 6.,\n                        index=exp_idx, name='xxx')\n        tm.assert_series_equal(s.value_counts(normalize=True), exp)\n        tm.assert_series_equal(idx.value_counts(normalize=True), exp)\n\n    def test_value_counts_categorical_ordered(self):\n        # most dtypes are tested in test_base.py\n        values = pd.Categorical([1, 2, 3, 1, 1, 3], ordered=True)\n\n        exp_idx = pd.CategoricalIndex([1, 3, 2], categories=[1, 2, 3],\n                                      ordered=True)\n        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')\n\n        s = pd.Series(values, name='xxx')\n        tm.assert_series_equal(s.value_counts(), exp)\n        # check CategoricalIndex outputs the same result\n        idx = pd.CategoricalIndex(values, name='xxx')\n        tm.assert_series_equal(idx.value_counts(), exp)\n\n        # normalize\n        exp = pd.Series(np.array([3., 2., 1]) / 6.,\n                        index=exp_idx, name='xxx')\n        tm.assert_series_equal(s.value_counts(normalize=True), exp)\n        tm.assert_series_equal(idx.value_counts(normalize=True), exp)\n\n    def test_value_counts_categorical_not_ordered(self):\n        values = pd.Categorical([1, 2, 3, 1, 1, 3], ordered=False)\n\n        exp_idx = pd.CategoricalIndex([1, 3, 2], categories=[1, 2, 3],\n                                      ordered=False)\n        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')\n\n        s = pd.Series(values, name='xxx')\n        tm.assert_series_equal(s.value_counts(), exp)\n        # check CategoricalIndex outputs the same result\n        idx = pd.CategoricalIndex(values, name='xxx')\n        tm.assert_series_equal(idx.value_counts(), exp)\n\n        # normalize\n        exp = pd.Series(np.array([3., 2., 1]) / 6.,\n                        index=exp_idx, name='xxx')\n        tm.assert_series_equal(s.value_counts(normalize=True), exp)\n        tm.assert_series_equal(idx.value_counts(normalize=True), exp)\n\n\n@pytest.fixture\ndef s_main_dtypes():\n    df = pd.DataFrame(\n        {'datetime': pd.to_datetime(['2003', '2002',\n                                     '2001', '2002',\n                                     '2005']),\n         'datetimetz': pd.to_datetime(\n             ['2003', '2002',\n              '2001', '2002',\n              '2005']).tz_localize('US/Eastern'),\n         'timedelta': pd.to_timedelta(['3d', '2d', '1d',\n                                       '2d', '5d'])})\n\n    for dtype in ['int8', 'int16', 'int32', 'int64',\n                  'float32', 'float64',\n                  'uint8', 'uint16', 'uint32', 'uint64']:\n        df[dtype] = Series([3, 2, 1, 2, 5], dtype=dtype)\n\n    return df\n\n\nclass TestNLargestNSmallest(object):\n\n    @pytest.mark.parametrize(\n        \"r\", [Series([3., 2, 1, 2, '5'], dtype='object'),\n              Series([3., 2, 1, 2, 5], dtype='object'),\n              # not supported on some archs\n              # Series([3., 2, 1, 2, 5], dtype='complex256'),\n              Series([3., 2, 1, 2, 5], dtype='complex128'),\n              Series(list('abcde'))])\n    def test_error(self, r):\n        dt = r.dtype\n        msg = (\"Cannot use method 'n(larg|small)est' with \"\n               \"dtype {dt}\".format(dt=dt))\n        args = 2, len(r), 0, -1\n        methods = r.nlargest, r.nsmallest\n        for method, arg in product(methods, args):\n            with tm.assert_raises_regex(TypeError, msg):\n                method(arg)\n\n    def test_error_categorical_dtype(self):\n        # same as test_error, but regex hard to escape properly\n        msg = (\"Cannot use method 'n(larg|small)est' with dtype \"\n               \"CategoricalDtype.+\")\n        with tm.assert_raises_regex(TypeError, msg):\n            Series(list('ab'), dtype='category').nlargest(2)\n\n        with tm.assert_raises_regex(TypeError, msg):\n            Series(list('ab'), dtype='category').nsmallest(2)\n\n    @pytest.mark.parametrize(\n        \"s\",\n        [v for k, v in s_main_dtypes().iteritems()])\n    def test_nsmallest_nlargest(self, s):\n        # float, int, datetime64 (use i8), timedelts64 (same),\n        # object that are numbers, object that are strings\n\n        assert_series_equal(s.nsmallest(2), s.iloc[[2, 1]])\n        assert_series_equal(s.nsmallest(2, keep='last'), s.iloc[[2, 3]])\n\n        empty = s.iloc[0:0]\n        assert_series_equal(s.nsmallest(0), empty)\n        assert_series_equal(s.nsmallest(-1), empty)\n        assert_series_equal(s.nlargest(0), empty)\n        assert_series_equal(s.nlargest(-1), empty)\n\n        assert_series_equal(s.nsmallest(len(s)), s.sort_values())\n        assert_series_equal(s.nsmallest(len(s) + 1), s.sort_values())\n        assert_series_equal(s.nlargest(len(s)), s.iloc[[4, 0, 1, 3, 2]])\n        assert_series_equal(s.nlargest(len(s) + 1),\n                            s.iloc[[4, 0, 1, 3, 2]])\n\n    def test_misc(self):\n\n        s = Series([3., np.nan, 1, 2, 5])\n        assert_series_equal(s.nlargest(), s.iloc[[4, 0, 3, 2]])\n        assert_series_equal(s.nsmallest(), s.iloc[[2, 3, 0, 4]])\n\n        msg = 'keep must be either \"first\", \"last\"'\n        with tm.assert_raises_regex(ValueError, msg):\n            s.nsmallest(keep='invalid')\n        with tm.assert_raises_regex(ValueError, msg):\n            s.nlargest(keep='invalid')\n\n        # GH 15297\n        s = Series([1] * 5, index=[1, 2, 3, 4, 5])\n        expected_first = Series([1] * 3, index=[1, 2, 3])\n        expected_last = Series([1] * 3, index=[5, 4, 3])\n\n        result = s.nsmallest(3)\n        assert_series_equal(result, expected_first)\n\n        result = s.nsmallest(3, keep='last')\n        assert_series_equal(result, expected_last)\n\n        result = s.nlargest(3)\n        assert_series_equal(result, expected_first)\n\n        result = s.nlargest(3, keep='last')\n        assert_series_equal(result, expected_last)\n\n    @pytest.mark.parametrize('n', range(1, 5))\n    def test_n(self, n):\n\n        # GH 13412\n        s = Series([1, 4, 3, 2], index=[0, 0, 1, 1])\n        result = s.nlargest(n)\n        expected = s.sort_values(ascending=False).head(n)\n        assert_series_equal(result, expected)\n\n        result = s.nsmallest(n)\n        expected = s.sort_values().head(n)\n        assert_series_equal(result, expected)\n"
    },
    {
      "filename": "pandas/tests/series/test_api.py",
      "content": "# coding=utf-8\n# pylint: disable-msg=E1101,W0612\nfrom collections import OrderedDict\n\nimport pytest\n\nimport numpy as np\nimport pandas as pd\n\nfrom pandas import Index, Series, DataFrame, date_range\nfrom pandas.core.indexes.datetimes import Timestamp\n\nfrom pandas.compat import range\nfrom pandas import compat\nimport pandas.io.formats.printing as printing\nfrom pandas.util.testing import (assert_series_equal,\n                                 ensure_clean)\nimport pandas.util.testing as tm\n\nfrom .common import TestData\n\n\nclass SharedWithSparse(object):\n    \"\"\"\n    A collection of tests Series and SparseSeries can share.\n\n    In generic tests on this class, use ``self._assert_series_equal()``\n    which is implemented in sub-classes.\n    \"\"\"\n    def _assert_series_equal(self, left, right):\n        \"\"\"Dispatch to series class dependent assertion\"\"\"\n        raise NotImplementedError\n\n    def test_scalarop_preserve_name(self):\n        result = self.ts * 2\n        assert result.name == self.ts.name\n\n    def test_copy_name(self):\n        result = self.ts.copy()\n        assert result.name == self.ts.name\n\n    def test_copy_index_name_checking(self):\n        # don't want to be able to modify the index stored elsewhere after\n        # making a copy\n\n        self.ts.index.name = None\n        assert self.ts.index.name is None\n        assert self.ts is self.ts\n\n        cp = self.ts.copy()\n        cp.index.name = 'foo'\n        printing.pprint_thing(self.ts.index.name)\n        assert self.ts.index.name is None\n\n    def test_append_preserve_name(self):\n        result = self.ts[:5].append(self.ts[5:])\n        assert result.name == self.ts.name\n\n    def test_binop_maybe_preserve_name(self):\n        # names match, preserve\n        result = self.ts * self.ts\n        assert result.name == self.ts.name\n        result = self.ts.mul(self.ts)\n        assert result.name == self.ts.name\n\n        result = self.ts * self.ts[:-2]\n        assert result.name == self.ts.name\n\n        # names don't match, don't preserve\n        cp = self.ts.copy()\n        cp.name = 'something else'\n        result = self.ts + cp\n        assert result.name is None\n        result = self.ts.add(cp)\n        assert result.name is None\n\n        ops = ['add', 'sub', 'mul', 'div', 'truediv', 'floordiv', 'mod', 'pow']\n        ops = ops + ['r' + op for op in ops]\n        for op in ops:\n            # names match, preserve\n            s = self.ts.copy()\n            result = getattr(s, op)(s)\n            assert result.name == self.ts.name\n\n            # names don't match, don't preserve\n            cp = self.ts.copy()\n            cp.name = 'changed'\n            result = getattr(s, op)(cp)\n            assert result.name is None\n\n    def test_combine_first_name(self):\n        result = self.ts.combine_first(self.ts[:5])\n        assert result.name == self.ts.name\n\n    def test_getitem_preserve_name(self):\n        result = self.ts[self.ts > 0]\n        assert result.name == self.ts.name\n\n        result = self.ts[[0, 2, 4]]\n        assert result.name == self.ts.name\n\n        result = self.ts[5:10]\n        assert result.name == self.ts.name\n\n    def test_pickle(self):\n        unp_series = self._pickle_roundtrip(self.series)\n        unp_ts = self._pickle_roundtrip(self.ts)\n        assert_series_equal(unp_series, self.series)\n        assert_series_equal(unp_ts, self.ts)\n\n    def _pickle_roundtrip(self, obj):\n\n        with ensure_clean() as path:\n            obj.to_pickle(path)\n            unpickled = pd.read_pickle(path)\n            return unpickled\n\n    def test_argsort_preserve_name(self):\n        result = self.ts.argsort()\n        assert result.name == self.ts.name\n\n    def test_sort_index_name(self):\n        result = self.ts.sort_index(ascending=False)\n        assert result.name == self.ts.name\n\n    def test_to_sparse_pass_name(self):\n        result = self.ts.to_sparse()\n        assert result.name == self.ts.name\n\n    def test_constructor_dict(self):\n        d = {'a': 0., 'b': 1., 'c': 2.}\n        result = self.series_klass(d)\n        expected = self.series_klass(d, index=sorted(d.keys()))\n        self._assert_series_equal(result, expected)\n\n        result = self.series_klass(d, index=['b', 'c', 'd', 'a'])\n        expected = self.series_klass([1, 2, np.nan, 0],\n                                     index=['b', 'c', 'd', 'a'])\n        self._assert_series_equal(result, expected)\n\n    def test_constructor_subclass_dict(self):\n        data = tm.TestSubDict((x, 10.0 * x) for x in range(10))\n        series = self.series_klass(data)\n        expected = self.series_klass(dict(compat.iteritems(data)))\n        self._assert_series_equal(series, expected)\n\n    def test_constructor_ordereddict(self):\n        # GH3283\n        data = OrderedDict(\n            ('col%s' % i, np.random.random()) for i in range(12))\n\n        series = self.series_klass(data)\n        expected = self.series_klass(list(data.values()), list(data.keys()))\n        self._assert_series_equal(series, expected)\n\n        # Test with subclass\n        class A(OrderedDict):\n            pass\n\n        series = self.series_klass(A(data))\n        self._assert_series_equal(series, expected)\n\n    def test_constructor_dict_multiindex(self):\n        d = {('a', 'a'): 0., ('b', 'a'): 1., ('b', 'c'): 2.}\n        _d = sorted(d.items())\n        result = self.series_klass(d)\n        expected = self.series_klass(\n            [x[1] for x in _d],\n            index=pd.MultiIndex.from_tuples([x[0] for x in _d]))\n        self._assert_series_equal(result, expected)\n\n        d['z'] = 111.\n        _d.insert(0, ('z', d['z']))\n        result = self.series_klass(d)\n        expected = self.series_klass([x[1] for x in _d],\n                                     index=pd.Index([x[0] for x in _d],\n                                                    tupleize_cols=False))\n        result = result.reindex(index=expected.index)\n        self._assert_series_equal(result, expected)\n\n    def test_constructor_dict_timedelta_index(self):\n        # GH #12169 : Resample category data with timedelta index\n        # construct Series from dict as data and TimedeltaIndex as index\n        # will result NaN in result Series data\n        expected = self.series_klass(\n            data=['A', 'B', 'C'],\n            index=pd.to_timedelta([0, 10, 20], unit='s')\n        )\n\n        result = self.series_klass(\n            data={pd.to_timedelta(0, unit='s'): 'A',\n                  pd.to_timedelta(10, unit='s'): 'B',\n                  pd.to_timedelta(20, unit='s'): 'C'},\n            index=pd.to_timedelta([0, 10, 20], unit='s')\n        )\n        self._assert_series_equal(result, expected)\n\n\nclass TestSeriesMisc(TestData, SharedWithSparse):\n\n    series_klass = Series\n    # SharedWithSparse tests use generic, series_klass-agnostic assertion\n    _assert_series_equal = staticmethod(tm.assert_series_equal)\n\n    def test_tab_completion(self):\n        # GH 9910\n        s = Series(list('abcd'))\n        # Series of str values should have .str but not .dt/.cat in __dir__\n        assert 'str' in dir(s)\n        assert 'dt' not in dir(s)\n        assert 'cat' not in dir(s)\n\n        # similiarly for .dt\n        s = Series(date_range('1/1/2015', periods=5))\n        assert 'dt' in dir(s)\n        assert 'str' not in dir(s)\n        assert 'cat' not in dir(s)\n\n        # Similarly for .cat, but with the twist that str and dt should be\n        # there if the categories are of that type first cat and str.\n        s = Series(list('abbcd'), dtype=\"category\")\n        assert 'cat' in dir(s)\n        assert 'str' in dir(s)  # as it is a string categorical\n        assert 'dt' not in dir(s)\n\n        # similar to cat and str\n        s = Series(date_range('1/1/2015', periods=5)).astype(\"category\")\n        assert 'cat' in dir(s)\n        assert 'str' not in dir(s)\n        assert 'dt' in dir(s)  # as it is a datetime categorical\n\n    def test_not_hashable(self):\n        s_empty = Series()\n        s = Series([1])\n        pytest.raises(TypeError, hash, s_empty)\n        pytest.raises(TypeError, hash, s)\n\n    def test_contains(self):\n        tm.assert_contains_all(self.ts.index, self.ts)\n\n    def test_iter(self):\n        for i, val in enumerate(self.series):\n            assert val == self.series[i]\n\n        for i, val in enumerate(self.ts):\n            assert val == self.ts[i]\n\n    def test_keys(self):\n        # HACK: By doing this in two stages, we avoid 2to3 wrapping the call\n        # to .keys() in a list()\n        getkeys = self.ts.keys\n        assert getkeys() is self.ts.index\n\n    def test_values(self):\n        tm.assert_almost_equal(self.ts.values, self.ts, check_dtype=False)\n\n    def test_iteritems(self):\n        for idx, val in compat.iteritems(self.series):\n            assert val == self.series[idx]\n\n        for idx, val in compat.iteritems(self.ts):\n            assert val == self.ts[idx]\n\n        # assert is lazy (genrators don't define reverse, lists do)\n        assert not hasattr(self.series.iteritems(), 'reverse')\n\n    def test_items(self):\n        for idx, val in self.series.items():\n            assert val == self.series[idx]\n\n        for idx, val in self.ts.items():\n            assert val == self.ts[idx]\n\n        # assert is lazy (genrators don't define reverse, lists do)\n        assert not hasattr(self.series.items(), 'reverse')\n\n    def test_raise_on_info(self):\n        s = Series(np.random.randn(10))\n        with pytest.raises(AttributeError):\n            s.info()\n\n    def test_copy(self):\n\n        for deep in [None, False, True]:\n            s = Series(np.arange(10), dtype='float64')\n\n            # default deep is True\n            if deep is None:\n                s2 = s.copy()\n            else:\n                s2 = s.copy(deep=deep)\n\n            s2[::2] = np.NaN\n\n            if deep is None or deep is True:\n                # Did not modify original Series\n                assert np.isnan(s2[0])\n                assert not np.isnan(s[0])\n            else:\n                # we DID modify the original Series\n                assert np.isnan(s2[0])\n                assert np.isnan(s[0])\n\n        # GH 11794\n        # copy of tz-aware\n        expected = Series([Timestamp('2012/01/01', tz='UTC')])\n        expected2 = Series([Timestamp('1999/01/01', tz='UTC')])\n\n        for deep in [None, False, True]:\n\n            s = Series([Timestamp('2012/01/01', tz='UTC')])\n\n            if deep is None:\n                s2 = s.copy()\n            else:\n                s2 = s.copy(deep=deep)\n\n            s2[0] = pd.Timestamp('1999/01/01', tz='UTC')\n\n            # default deep is True\n            if deep is None or deep is True:\n                # Did not modify original Series\n                assert_series_equal(s2, expected2)\n                assert_series_equal(s, expected)\n            else:\n                # we DID modify the original Series\n                assert_series_equal(s2, expected2)\n                assert_series_equal(s, expected2)\n\n    def test_axis_alias(self):\n        s = Series([1, 2, np.nan])\n        assert_series_equal(s.dropna(axis='rows'), s.dropna(axis='index'))\n        assert s.dropna().sum('rows') == 3\n        assert s._get_axis_number('rows') == 0\n        assert s._get_axis_name('rows') == 'index'\n\n    def test_numpy_unique(self):\n        # it works!\n        np.unique(self.ts)\n\n    def test_ndarray_compat(self):\n\n        # test numpy compat with Series as sub-class of NDFrame\n        tsdf = DataFrame(np.random.randn(1000, 3), columns=['A', 'B', 'C'],\n                         index=date_range('1/1/2000', periods=1000))\n\n        def f(x):\n            return x[x.idxmax()]\n\n        result = tsdf.apply(f)\n        expected = tsdf.max()\n        tm.assert_series_equal(result, expected)\n\n        # .item()\n        s = Series([1])\n        result = s.item()\n        assert result == 1\n        assert s.item() == s.iloc[0]\n\n        # using an ndarray like function\n        s = Series(np.random.randn(10))\n        result = Series(np.ones_like(s))\n        expected = Series(1, index=range(10), dtype='float64')\n        tm.assert_series_equal(result, expected)\n\n        # ravel\n        s = Series(np.random.randn(10))\n        tm.assert_almost_equal(s.ravel(order='F'), s.values.ravel(order='F'))\n\n        # compress\n        # GH 6658\n        s = Series([0, 1., -1], index=list('abc'))\n        result = np.compress(s > 0, s)\n        tm.assert_series_equal(result, Series([1.], index=['b']))\n\n        result = np.compress(s < -1, s)\n        # result empty Index(dtype=object) as the same as original\n        exp = Series([], dtype='float64', index=Index([], dtype='object'))\n        tm.assert_series_equal(result, exp)\n\n        s = Series([0, 1., -1], index=[.1, .2, .3])\n        result = np.compress(s > 0, s)\n        tm.assert_series_equal(result, Series([1.], index=[.2]))\n\n        result = np.compress(s < -1, s)\n        # result empty Float64Index as the same as original\n        exp = Series([], dtype='float64', index=Index([], dtype='float64'))\n        tm.assert_series_equal(result, exp)\n\n    def test_str_attribute(self):\n        # GH9068\n        methods = ['strip', 'rstrip', 'lstrip']\n        s = Series([' jack', 'jill ', ' jesse ', 'frank'])\n        for method in methods:\n            expected = Series([getattr(str, method)(x) for x in s.values])\n            assert_series_equal(getattr(Series.str, method)(s.str), expected)\n\n        # str accessor only valid with string values\n        s = Series(range(5))\n        with tm.assert_raises_regex(AttributeError,\n                                    'only use .str accessor'):\n            s.str.repeat(2)\n\n    def test_empty_method(self):\n        s_empty = pd.Series()\n        assert s_empty.empty\n\n        for full_series in [pd.Series([1]), pd.Series(index=[1])]:\n            assert not full_series.empty\n"
    },
    {
      "filename": "pandas/tests/series/test_operators.py",
      "content": "# coding=utf-8\n# pylint: disable-msg=E1101,W0612\n\nimport pytest\nimport pytz\n\nfrom collections import Iterable\nfrom datetime import datetime, timedelta\nimport operator\nfrom itertools import product, starmap\n\nfrom numpy import nan, inf\nimport numpy as np\nimport pandas as pd\n\nfrom pandas import (Index, Series, DataFrame, isna, bdate_range,\n                    NaT, date_range, timedelta_range)\nfrom pandas.core.indexes.datetimes import Timestamp\nfrom pandas.core.indexes.timedeltas import Timedelta\nimport pandas.core.nanops as nanops\n\nfrom pandas.compat import range, zip\nfrom pandas import compat\nfrom pandas.util.testing import (assert_series_equal, assert_almost_equal,\n                                 assert_frame_equal, assert_index_equal)\nimport pandas.util.testing as tm\n\nfrom .common import TestData\n\n\nclass TestSeriesOperators(TestData):\n\n    def test_series_comparison_scalars(self):\n        series = Series(date_range('1/1/2000', periods=10))\n\n        val = datetime(2000, 1, 4)\n        result = series > val\n        expected = Series([x > val for x in series])\n        tm.assert_series_equal(result, expected)\n\n        val = series[5]\n        result = series > val\n        expected = Series([x > val for x in series])\n        tm.assert_series_equal(result, expected)\n\n    def test_comparisons(self):\n        left = np.random.randn(10)\n        right = np.random.randn(10)\n        left[:3] = np.nan\n\n        result = nanops.nangt(left, right)\n        with np.errstate(invalid='ignore'):\n            expected = (left > right).astype('O')\n        expected[:3] = np.nan\n\n        assert_almost_equal(result, expected)\n\n        s = Series(['a', 'b', 'c'])\n        s2 = Series([False, True, False])\n\n        # it works!\n        exp = Series([False, False, False])\n        assert_series_equal(s == s2, exp)\n        assert_series_equal(s2 == s, exp)\n\n    def test_op_method(self):\n        def check(series, other, check_reverse=False):\n            simple_ops = ['add', 'sub', 'mul', 'floordiv', 'truediv', 'pow']\n            if not compat.PY3:\n                simple_ops.append('div')\n\n            for opname in simple_ops:\n                op = getattr(Series, opname)\n\n                if op == 'div':\n                    alt = operator.truediv\n                else:\n                    alt = getattr(operator, opname)\n\n                result = op(series, other)\n                expected = alt(series, other)\n                assert_almost_equal(result, expected)\n                if check_reverse:\n                    rop = getattr(Series, \"r\" + opname)\n                    result = rop(series, other)\n                    expected = alt(other, series)\n                    assert_almost_equal(result, expected)\n\n        check(self.ts, self.ts * 2)\n        check(self.ts, self.ts[::2])\n        check(self.ts, 5, check_reverse=True)\n        check(tm.makeFloatSeries(), tm.makeFloatSeries(), check_reverse=True)\n\n    def test_neg(self):\n        assert_series_equal(-self.series, -1 * self.series)\n\n    def test_invert(self):\n        assert_series_equal(-(self.series < 0), ~(self.series < 0))\n\n    def test_div(self):\n        with np.errstate(all='ignore'):\n            # no longer do integer div for any ops, but deal with the 0's\n            p = DataFrame({'first': [3, 4, 5, 8], 'second': [0, 0, 0, 3]})\n            result = p['first'] / p['second']\n            expected = Series(\n                p['first'].values.astype(float) / p['second'].values,\n                dtype='float64')\n            expected.iloc[0:3] = np.inf\n            assert_series_equal(result, expected)\n\n            result = p['first'] / 0\n            expected = Series(np.inf, index=p.index, name='first')\n            assert_series_equal(result, expected)\n\n            p = p.astype('float64')\n            result = p['first'] / p['second']\n            expected = Series(p['first'].values / p['second'].values)\n            assert_series_equal(result, expected)\n\n            p = DataFrame({'first': [3, 4, 5, 8], 'second': [1, 1, 1, 1]})\n            result = p['first'] / p['second']\n            assert_series_equal(result, p['first'].astype('float64'),\n                                check_names=False)\n            assert result.name is None\n            assert not np.array_equal(result, p['second'] / p['first'])\n\n            # inf signing\n            s = Series([np.nan, 1., -1.])\n            result = s / 0\n            expected = Series([np.nan, np.inf, -np.inf])\n            assert_series_equal(result, expected)\n\n            # float/integer issue\n            # GH 7785\n            p = DataFrame({'first': (1, 0), 'second': (-0.01, -0.02)})\n            expected = Series([-0.01, -np.inf])\n\n            result = p['second'].div(p['first'])\n            assert_series_equal(result, expected, check_names=False)\n\n            result = p['second'] / p['first']\n            assert_series_equal(result, expected)\n\n            # GH 9144\n            s = Series([-1, 0, 1])\n\n            result = 0 / s\n            expected = Series([0.0, nan, 0.0])\n            assert_series_equal(result, expected)\n\n            result = s / 0\n            expected = Series([-inf, nan, inf])\n            assert_series_equal(result, expected)\n\n            result = s // 0\n            expected = Series([-inf, nan, inf])\n            assert_series_equal(result, expected)\n\n            # GH 8674\n            zero_array = np.array([0] * 5)\n            data = np.random.randn(5)\n            expected = pd.Series([0.] * 5)\n            result = zero_array / pd.Series(data)\n            assert_series_equal(result, expected)\n\n            result = pd.Series(zero_array) / data\n            assert_series_equal(result, expected)\n\n            result = pd.Series(zero_array) / pd.Series(data)\n            assert_series_equal(result, expected)\n\n    def test_operators(self):\n        def _check_op(series, other, op, pos_only=False,\n                      check_dtype=True):\n            left = np.abs(series) if pos_only else series\n            right = np.abs(other) if pos_only else other\n\n            cython_or_numpy = op(left, right)\n            python = left.combine(right, op)\n            assert_series_equal(cython_or_numpy, python,\n                                check_dtype=check_dtype)\n\n        def check(series, other):\n            simple_ops = ['add', 'sub', 'mul', 'truediv', 'floordiv', 'mod']\n\n            for opname in simple_ops:\n                _check_op(series, other, getattr(operator, opname))\n\n            _check_op(series, other, operator.pow, pos_only=True)\n\n            _check_op(series, other, lambda x, y: operator.add(y, x))\n            _check_op(series, other, lambda x, y: operator.sub(y, x))\n            _check_op(series, other, lambda x, y: operator.truediv(y, x))\n            _check_op(series, other, lambda x, y: operator.floordiv(y, x))\n            _check_op(series, other, lambda x, y: operator.mul(y, x))\n            _check_op(series, other, lambda x, y: operator.pow(y, x),\n                      pos_only=True)\n            _check_op(series, other, lambda x, y: operator.mod(y, x))\n\n        check(self.ts, self.ts * 2)\n        check(self.ts, self.ts * 0)\n        check(self.ts, self.ts[::2])\n        check(self.ts, 5)\n\n        def check_comparators(series, other, check_dtype=True):\n            _check_op(series, other, operator.gt, check_dtype=check_dtype)\n            _check_op(series, other, operator.ge, check_dtype=check_dtype)\n            _check_op(series, other, operator.eq, check_dtype=check_dtype)\n            _check_op(series, other, operator.lt, check_dtype=check_dtype)\n            _check_op(series, other, operator.le, check_dtype=check_dtype)\n\n        check_comparators(self.ts, 5)\n        check_comparators(self.ts, self.ts + 1, check_dtype=False)\n\n    def test_divmod(self):\n        def check(series, other):\n            results = divmod(series, other)\n            if isinstance(other, Iterable) and len(series) != len(other):\n                # if the lengths don't match, this is the test where we use\n                # `self.ts[::2]`. Pad every other value in `other_np` with nan.\n                other_np = []\n                for n in other:\n                    other_np.append(n)\n                    other_np.append(np.nan)\n            else:\n                other_np = other\n            other_np = np.asarray(other_np)\n            with np.errstate(all='ignore'):\n                expecteds = divmod(series.values, np.asarray(other_np))\n\n            for result, expected in zip(results, expecteds):\n                # check the values, name, and index separatly\n                assert_almost_equal(np.asarray(result), expected)\n\n                assert result.name == series.name\n                assert_index_equal(result.index, series.index)\n\n        check(self.ts, self.ts * 2)\n        check(self.ts, self.ts * 0)\n        check(self.ts, self.ts[::2])\n        check(self.ts, 5)\n\n    def test_operators_empty_int_corner(self):\n        s1 = Series([], [], dtype=np.int32)\n        s2 = Series({'x': 0.})\n        assert_series_equal(s1 * s2, Series([np.nan], index=['x']))\n\n    def test_operators_timedelta64(self):\n\n        # invalid ops\n        pytest.raises(Exception, self.objSeries.__add__, 1)\n        pytest.raises(Exception, self.objSeries.__add__,\n                      np.array(1, dtype=np.int64))\n        pytest.raises(Exception, self.objSeries.__sub__, 1)\n        pytest.raises(Exception, self.objSeries.__sub__,\n                      np.array(1, dtype=np.int64))\n\n        # seriese ops\n        v1 = date_range('2012-1-1', periods=3, freq='D')\n        v2 = date_range('2012-1-2', periods=3, freq='D')\n        rs = Series(v2) - Series(v1)\n        xp = Series(1e9 * 3600 * 24,\n                    rs.index).astype('int64').astype('timedelta64[ns]')\n        assert_series_equal(rs, xp)\n        assert rs.dtype == 'timedelta64[ns]'\n\n        df = DataFrame(dict(A=v1))\n        td = Series([timedelta(days=i) for i in range(3)])\n        assert td.dtype == 'timedelta64[ns]'\n\n        # series on the rhs\n        result = df['A'] - df['A'].shift()\n        assert result.dtype == 'timedelta64[ns]'\n\n        result = df['A'] + td\n        assert result.dtype == 'M8[ns]'\n\n        # scalar Timestamp on rhs\n        maxa = df['A'].max()\n        assert isinstance(maxa, Timestamp)\n\n        resultb = df['A'] - df['A'].max()\n        assert resultb.dtype == 'timedelta64[ns]'\n\n        # timestamp on lhs\n        result = resultb + df['A']\n        values = [Timestamp('20111230'), Timestamp('20120101'),\n                  Timestamp('20120103')]\n        expected = Series(values, name='A')\n        assert_series_equal(result, expected)\n\n        # datetimes on rhs\n        result = df['A'] - datetime(2001, 1, 1)\n        expected = Series(\n            [timedelta(days=4017 + i) for i in range(3)], name='A')\n        assert_series_equal(result, expected)\n        assert result.dtype == 'm8[ns]'\n\n        d = datetime(2001, 1, 1, 3, 4)\n        resulta = df['A'] - d\n        assert resulta.dtype == 'm8[ns]'\n\n        # roundtrip\n        resultb = resulta + d\n        assert_series_equal(df['A'], resultb)\n\n        # timedeltas on rhs\n        td = timedelta(days=1)\n        resulta = df['A'] + td\n        resultb = resulta - td\n        assert_series_equal(resultb, df['A'])\n        assert resultb.dtype == 'M8[ns]'\n\n        # roundtrip\n        td = timedelta(minutes=5, seconds=3)\n        resulta = df['A'] + td\n        resultb = resulta - td\n        assert_series_equal(df['A'], resultb)\n        assert resultb.dtype == 'M8[ns]'\n\n        # inplace\n        value = rs[2] + np.timedelta64(timedelta(minutes=5, seconds=1))\n        rs[2] += np.timedelta64(timedelta(minutes=5, seconds=1))\n        assert rs[2] == value\n\n    def test_operator_series_comparison_zerorank(self):\n        # GH 13006\n        result = np.float64(0) > pd.Series([1, 2, 3])\n        expected = 0.0 > pd.Series([1, 2, 3])\n        tm.assert_series_equal(result, expected)\n        result = pd.Series([1, 2, 3]) < np.float64(0)\n        expected = pd.Series([1, 2, 3]) < 0.0\n        tm.assert_series_equal(result, expected)\n        result = np.array([0, 1, 2])[0] > pd.Series([0, 1, 2])\n        expected = 0.0 > pd.Series([1, 2, 3])\n        tm.assert_series_equal(result, expected)\n\n    def test_timedeltas_with_DateOffset(self):\n\n        # GH 4532\n        # operate with pd.offsets\n        s = Series([Timestamp('20130101 9:01'), Timestamp('20130101 9:02')])\n\n        result = s + pd.offsets.Second(5)\n        result2 = pd.offsets.Second(5) + s\n        expected = Series([Timestamp('20130101 9:01:05'), Timestamp(\n            '20130101 9:02:05')])\n        assert_series_equal(result, expected)\n        assert_series_equal(result2, expected)\n\n        result = s - pd.offsets.Second(5)\n        result2 = -pd.offsets.Second(5) + s\n        expected = Series([Timestamp('20130101 9:00:55'), Timestamp(\n            '20130101 9:01:55')])\n        assert_series_equal(result, expected)\n        assert_series_equal(result2, expected)\n\n        result = s + pd.offsets.Milli(5)\n        result2 = pd.offsets.Milli(5) + s\n        expected = Series([Timestamp('20130101 9:01:00.005'), Timestamp(\n            '20130101 9:02:00.005')])\n        assert_series_equal(result, expected)\n        assert_series_equal(result2, expected)\n\n        result = s + pd.offsets.Minute(5) + pd.offsets.Milli(5)\n        expected = Series([Timestamp('20130101 9:06:00.005'), Timestamp(\n            '20130101 9:07:00.005')])\n        assert_series_equal(result, expected)\n\n        # operate with np.timedelta64 correctly\n        result = s + np.timedelta64(1, 's')\n        result2 = np.timedelta64(1, 's') + s\n        expected = Series([Timestamp('20130101 9:01:01'), Timestamp(\n            '20130101 9:02:01')])\n        assert_series_equal(result, expected)\n        assert_series_equal(result2, expected)\n\n        result = s + np.timedelta64(5, 'ms')\n        result2 = np.timedelta64(5, 'ms') + s\n        expected = Series([Timestamp('20130101 9:01:00.005'), Timestamp(\n            '20130101 9:02:00.005')])\n        assert_series_equal(result, expected)\n        assert_series_equal(result2, expected)\n\n        # valid DateOffsets\n        for do in ['Hour', 'Minute', 'Second', 'Day', 'Micro', 'Milli',\n                   'Nano']:\n            op = getattr(pd.offsets, do)\n            s + op(5)\n            op(5) + s\n\n    def test_timedelta_series_ops(self):\n        # GH11925\n\n        s = Series(timedelta_range('1 day', periods=3))\n        ts = Timestamp('2012-01-01')\n        expected = Series(date_range('2012-01-02', periods=3))\n        assert_series_equal(ts + s, expected)\n        assert_series_equal(s + ts, expected)\n\n        expected2 = Series(date_range('2011-12-31', periods=3, freq='-1D'))\n        assert_series_equal(ts - s, expected2)\n        assert_series_equal(ts + (-s), expected2)\n\n    def test_timedelta64_operations_with_DateOffset(self):\n        # GH 10699\n        td = Series([timedelta(minutes=5, seconds=3)] * 3)\n        result = td + pd.offsets.Minute(1)\n        expected = Series([timedelta(minutes=6, seconds=3)] * 3)\n        assert_series_equal(result, expected)\n\n        result = td - pd.offsets.Minute(1)\n        expected = Series([timedelta(minutes=4, seconds=3)] * 3)\n        assert_series_equal(result, expected)\n\n        result = td + Series([pd.offsets.Minute(1), pd.offsets.Second(3),\n                              pd.offsets.Hour(2)])\n        expected = Series([timedelta(minutes=6, seconds=3), timedelta(\n            minutes=5, seconds=6), timedelta(hours=2, minutes=5, seconds=3)])\n        assert_series_equal(result, expected)\n\n        result = td + pd.offsets.Minute(1) + pd.offsets.Second(12)\n        expected = Series([timedelta(minutes=6, seconds=15)] * 3)\n        assert_series_equal(result, expected)\n\n        # valid DateOffsets\n        for do in ['Hour', 'Minute', 'Second', 'Day', 'Micro', 'Milli',\n                   'Nano']:\n            op = getattr(pd.offsets, do)\n            td + op(5)\n            op(5) + td\n            td - op(5)\n            op(5) - td\n\n    def test_timedelta64_operations_with_timedeltas(self):\n\n        # td operate with td\n        td1 = Series([timedelta(minutes=5, seconds=3)] * 3)\n        td2 = timedelta(minutes=5, seconds=4)\n        result = td1 - td2\n        expected = Series([timedelta(seconds=0)] * 3) - Series([timedelta(\n            seconds=1)] * 3)\n        assert result.dtype == 'm8[ns]'\n        assert_series_equal(result, expected)\n\n        result2 = td2 - td1\n        expected = (Series([timedelta(seconds=1)] * 3) - Series([timedelta(\n            seconds=0)] * 3))\n        assert_series_equal(result2, expected)\n\n        # roundtrip\n        assert_series_equal(result + td2, td1)\n\n        # Now again, using pd.to_timedelta, which should build\n        # a Series or a scalar, depending on input.\n        td1 = Series(pd.to_timedelta(['00:05:03'] * 3))\n        td2 = pd.to_timedelta('00:05:04')\n        result = td1 - td2\n        expected = Series([timedelta(seconds=0)] * 3) - Series([timedelta(\n            seconds=1)] * 3)\n        assert result.dtype == 'm8[ns]'\n        assert_series_equal(result, expected)\n\n        result2 = td2 - td1\n        expected = (Series([timedelta(seconds=1)] * 3) - Series([timedelta(\n            seconds=0)] * 3))\n        assert_series_equal(result2, expected)\n\n        # roundtrip\n        assert_series_equal(result + td2, td1)\n\n    def test_timedelta64_operations_with_integers(self):\n\n        # GH 4521\n        # divide/multiply by integers\n        startdate = Series(date_range('2013-01-01', '2013-01-03'))\n        enddate = Series(date_range('2013-03-01', '2013-03-03'))\n\n        s1 = enddate - startdate\n        s1[2] = np.nan\n        s2 = Series([2, 3, 4])\n        expected = Series(s1.values.astype(np.int64) / s2, dtype='m8[ns]')\n        expected[2] = np.nan\n        result = s1 / s2\n        assert_series_equal(result, expected)\n\n        s2 = Series([20, 30, 40])\n        expected = Series(s1.values.astype(np.int64) / s2, dtype='m8[ns]')\n        expected[2] = np.nan\n        result = s1 / s2\n        assert_series_equal(result, expected)\n\n        result = s1 / 2\n        expected = Series(s1.values.astype(np.int64) / 2, dtype='m8[ns]')\n        expected[2] = np.nan\n        assert_series_equal(result, expected)\n\n        s2 = Series([20, 30, 40])\n        expected = Series(s1.values.astype(np.int64) * s2, dtype='m8[ns]')\n        expected[2] = np.nan\n        result = s1 * s2\n        assert_series_equal(result, expected)\n\n        for dtype in ['int32', 'int16', 'uint32', 'uint64', 'uint32', 'uint16',\n                      'uint8']:\n            s2 = Series([20, 30, 40], dtype=dtype)\n            expected = Series(\n                s1.values.astype(np.int64) * s2.astype(np.int64),\n                dtype='m8[ns]')\n            expected[2] = np.nan\n            result = s1 * s2\n            assert_series_equal(result, expected)\n\n        result = s1 * 2\n        expected = Series(s1.values.astype(np.int64) * 2, dtype='m8[ns]')\n        expected[2] = np.nan\n        assert_series_equal(result, expected)\n\n        result = s1 * -1\n        expected = Series(s1.values.astype(np.int64) * -1, dtype='m8[ns]')\n        expected[2] = np.nan\n        assert_series_equal(result, expected)\n\n        # invalid ops\n        assert_series_equal(s1 / s2.astype(float),\n                            Series([Timedelta('2 days 22:48:00'), Timedelta(\n                                '1 days 23:12:00'), Timedelta('NaT')]))\n        assert_series_equal(s1 / 2.0,\n                            Series([Timedelta('29 days 12:00:00'), Timedelta(\n                                '29 days 12:00:00'), Timedelta('NaT')]))\n\n        for op in ['__add__', '__sub__']:\n            sop = getattr(s1, op, None)\n            if sop is not None:\n                pytest.raises(TypeError, sop, 1)\n                pytest.raises(TypeError, sop, s2.values)\n\n    def test_timedelta64_conversions(self):\n        startdate = Series(date_range('2013-01-01', '2013-01-03'))\n        enddate = Series(date_range('2013-03-01', '2013-03-03'))\n\n        s1 = enddate - startdate\n        s1[2] = np.nan\n\n        for m in [1, 3, 10]:\n            for unit in ['D', 'h', 'm', 's', 'ms', 'us', 'ns']:\n\n                # op\n                expected = s1.apply(lambda x: x / np.timedelta64(m, unit))\n                result = s1 / np.timedelta64(m, unit)\n                assert_series_equal(result, expected)\n\n                if m == 1 and unit != 'ns':\n\n                    # astype\n                    result = s1.astype(\"timedelta64[{0}]\".format(unit))\n                    assert_series_equal(result, expected)\n\n                # reverse op\n                expected = s1.apply(\n                    lambda x: Timedelta(np.timedelta64(m, unit)) / x)\n                result = np.timedelta64(m, unit) / s1\n\n        # astype\n        s = Series(date_range('20130101', periods=3))\n        result = s.astype(object)\n        assert isinstance(result.iloc[0], datetime)\n        assert result.dtype == np.object_\n\n        result = s1.astype(object)\n        assert isinstance(result.iloc[0], timedelta)\n        assert result.dtype == np.object_\n\n    def test_timedelta64_equal_timedelta_supported_ops(self):\n        ser = Series([Timestamp('20130301'), Timestamp('20130228 23:00:00'),\n                      Timestamp('20130228 22:00:00'), Timestamp(\n                          '20130228 21:00:00')])\n\n        intervals = 'D', 'h', 'm', 's', 'us'\n\n        # TODO: unused\n        # npy16_mappings = {'D': 24 * 60 * 60 * 1000000,\n        #                   'h': 60 * 60 * 1000000,\n        #                   'm': 60 * 1000000,\n        #                   's': 1000000,\n        #                   'us': 1}\n\n        def timedelta64(*args):\n            return sum(starmap(np.timedelta64, zip(args, intervals)))\n\n        for op, d, h, m, s, us in product([operator.add, operator.sub],\n                                          *([range(2)] * 5)):\n            nptd = timedelta64(d, h, m, s, us)\n            pytd = timedelta(days=d, hours=h, minutes=m, seconds=s,\n                             microseconds=us)\n            lhs = op(ser, nptd)\n            rhs = op(ser, pytd)\n\n            try:\n                assert_series_equal(lhs, rhs)\n            except:\n                raise AssertionError(\n                    \"invalid comparsion [op->{0},d->{1},h->{2},m->{3},\"\n                    \"s->{4},us->{5}]\\n{6}\\n{7}\\n\".format(op, d, h, m, s,\n                                                         us, lhs, rhs))\n\n    def test_operators_datetimelike(self):\n        def run_ops(ops, get_ser, test_ser):\n\n            # check that we are getting a TypeError\n            # with 'operate' (from core/ops.py) for the ops that are not\n            # defined\n            for op_str in ops:\n                op = getattr(get_ser, op_str, None)\n                with tm.assert_raises_regex(TypeError, 'operate'):\n                    op(test_ser)\n\n        # ## timedelta64 ###\n        td1 = Series([timedelta(minutes=5, seconds=3)] * 3)\n        td1.iloc[2] = np.nan\n        td2 = timedelta(minutes=5, seconds=4)\n        ops = ['__mul__', '__floordiv__', '__pow__', '__rmul__',\n               '__rfloordiv__', '__rpow__']\n        run_ops(ops, td1, td2)\n        td1 + td2\n        td2 + td1\n        td1 - td2\n        td2 - td1\n        td1 / td2\n        td2 / td1\n\n        # ## datetime64 ###\n        dt1 = Series([Timestamp('20111230'), Timestamp('20120101'),\n                      Timestamp('20120103')])\n        dt1.iloc[2] = np.nan\n        dt2 = Series([Timestamp('20111231'), Timestamp('20120102'),\n                      Timestamp('20120104')])\n        ops = ['__add__', '__mul__', '__floordiv__', '__truediv__', '__div__',\n               '__pow__', '__radd__', '__rmul__', '__rfloordiv__',\n               '__rtruediv__', '__rdiv__', '__rpow__']\n        run_ops(ops, dt1, dt2)\n        dt1 - dt2\n        dt2 - dt1\n\n        # ## datetime64 with timetimedelta ###\n        ops = ['__mul__', '__floordiv__', '__truediv__', '__div__', '__pow__',\n               '__rmul__', '__rfloordiv__', '__rtruediv__', '__rdiv__',\n               '__rpow__']\n        run_ops(ops, dt1, td1)\n        dt1 + td1\n        td1 + dt1\n        dt1 - td1\n        # TODO: Decide if this ought to work.\n        # td1 - dt1\n\n        # ## timetimedelta with datetime64 ###\n        ops = ['__sub__', '__mul__', '__floordiv__', '__truediv__', '__div__',\n               '__pow__', '__rmul__', '__rfloordiv__', '__rtruediv__',\n               '__rdiv__', '__rpow__']\n        run_ops(ops, td1, dt1)\n        td1 + dt1\n        dt1 + td1\n\n        # 8260, 10763\n        # datetime64 with tz\n        ops = ['__mul__', '__floordiv__', '__truediv__', '__div__', '__pow__',\n               '__rmul__', '__rfloordiv__', '__rtruediv__', '__rdiv__',\n               '__rpow__']\n\n        tz = 'US/Eastern'\n        dt1 = Series(date_range('2000-01-01 09:00:00', periods=5,\n                                tz=tz), name='foo')\n        dt2 = dt1.copy()\n        dt2.iloc[2] = np.nan\n        td1 = Series(timedelta_range('1 days 1 min', periods=5, freq='H'))\n        td2 = td1.copy()\n        td2.iloc[1] = np.nan\n        run_ops(ops, dt1, td1)\n\n        result = dt1 + td1[0]\n        exp = (dt1.dt.tz_localize(None) + td1[0]).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n\n        result = dt2 + td2[0]\n        exp = (dt2.dt.tz_localize(None) + td2[0]).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n\n        # odd numpy behavior with scalar timedeltas\n        result = td1[0] + dt1\n        exp = (dt1.dt.tz_localize(None) + td1[0]).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n\n        result = td2[0] + dt2\n        exp = (dt2.dt.tz_localize(None) + td2[0]).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n\n        result = dt1 - td1[0]\n        exp = (dt1.dt.tz_localize(None) - td1[0]).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n        pytest.raises(TypeError, lambda: td1[0] - dt1)\n\n        result = dt2 - td2[0]\n        exp = (dt2.dt.tz_localize(None) - td2[0]).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n        pytest.raises(TypeError, lambda: td2[0] - dt2)\n\n        result = dt1 + td1\n        exp = (dt1.dt.tz_localize(None) + td1).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n\n        result = dt2 + td2\n        exp = (dt2.dt.tz_localize(None) + td2).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n\n        result = dt1 - td1\n        exp = (dt1.dt.tz_localize(None) - td1).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n\n        result = dt2 - td2\n        exp = (dt2.dt.tz_localize(None) - td2).dt.tz_localize(tz)\n        assert_series_equal(result, exp)\n\n        pytest.raises(TypeError, lambda: td1 - dt1)\n        pytest.raises(TypeError, lambda: td2 - dt2)\n\n    def test_sub_datetime_compat(self):\n        # see gh-14088\n        s = Series([datetime(2016, 8, 23, 12, tzinfo=pytz.utc), pd.NaT])\n        dt = datetime(2016, 8, 22, 12, tzinfo=pytz.utc)\n        exp = Series([Timedelta('1 days'), pd.NaT])\n        assert_series_equal(s - dt, exp)\n        assert_series_equal(s - Timestamp(dt), exp)\n\n    def test_sub_single_tz(self):\n        # GH12290\n        s1 = Series([pd.Timestamp('2016-02-10', tz='America/Sao_Paulo')])\n        s2 = Series([pd.Timestamp('2016-02-08', tz='America/Sao_Paulo')])\n        result = s1 - s2\n        expected = Series([Timedelta('2days')])\n        assert_series_equal(result, expected)\n        result = s2 - s1\n        expected = Series([Timedelta('-2days')])\n        assert_series_equal(result, expected)\n\n    def test_ops_nat(self):\n        # GH 11349\n        timedelta_series = Series([NaT, Timedelta('1s')])\n        datetime_series = Series([NaT, Timestamp('19900315')])\n        nat_series_dtype_timedelta = Series(\n            [NaT, NaT], dtype='timedelta64[ns]')\n        nat_series_dtype_timestamp = Series([NaT, NaT], dtype='datetime64[ns]')\n        single_nat_dtype_datetime = Series([NaT], dtype='datetime64[ns]')\n        single_nat_dtype_timedelta = Series([NaT], dtype='timedelta64[ns]')\n\n        # subtraction\n        assert_series_equal(timedelta_series - NaT, nat_series_dtype_timedelta)\n        assert_series_equal(-NaT + timedelta_series,\n                            nat_series_dtype_timedelta)\n\n        assert_series_equal(timedelta_series - single_nat_dtype_timedelta,\n                            nat_series_dtype_timedelta)\n        assert_series_equal(-single_nat_dtype_timedelta + timedelta_series,\n                            nat_series_dtype_timedelta)\n\n        assert_series_equal(datetime_series - NaT, nat_series_dtype_timestamp)\n        assert_series_equal(-NaT + datetime_series, nat_series_dtype_timestamp)\n\n        assert_series_equal(datetime_series - single_nat_dtype_datetime,\n                            nat_series_dtype_timedelta)\n        with pytest.raises(TypeError):\n            -single_nat_dtype_datetime + datetime_series\n\n        assert_series_equal(datetime_series - single_nat_dtype_timedelta,\n                            nat_series_dtype_timestamp)\n        assert_series_equal(-single_nat_dtype_timedelta + datetime_series,\n                            nat_series_dtype_timestamp)\n\n        # without a Series wrapping the NaT, it is ambiguous\n        # whether it is a datetime64 or timedelta64\n        # defaults to interpreting it as timedelta64\n        assert_series_equal(nat_series_dtype_timestamp - NaT,\n                            nat_series_dtype_timestamp)\n        assert_series_equal(-NaT + nat_series_dtype_timestamp,\n                            nat_series_dtype_timestamp)\n\n        assert_series_equal(nat_series_dtype_timestamp -\n                            single_nat_dtype_datetime,\n                            nat_series_dtype_timedelta)\n        with pytest.raises(TypeError):\n            -single_nat_dtype_datetime + nat_series_dtype_timestamp\n\n        assert_series_equal(nat_series_dtype_timestamp -\n                            single_nat_dtype_timedelta,\n                            nat_series_dtype_timestamp)\n        assert_series_equal(-single_nat_dtype_timedelta +\n                            nat_series_dtype_timestamp,\n                            nat_series_dtype_timestamp)\n\n        with pytest.raises(TypeError):\n            timedelta_series - single_nat_dtype_datetime\n\n        # addition\n        assert_series_equal(nat_series_dtype_timestamp + NaT,\n                            nat_series_dtype_timestamp)\n        assert_series_equal(NaT + nat_series_dtype_timestamp,\n                            nat_series_dtype_timestamp)\n\n        assert_series_equal(nat_series_dtype_timestamp +\n                            single_nat_dtype_timedelta,\n                            nat_series_dtype_timestamp)\n        assert_series_equal(single_nat_dtype_timedelta +\n                            nat_series_dtype_timestamp,\n                            nat_series_dtype_timestamp)\n\n        assert_series_equal(nat_series_dtype_timedelta + NaT,\n                            nat_series_dtype_timedelta)\n        assert_series_equal(NaT + nat_series_dtype_timedelta,\n                            nat_series_dtype_timedelta)\n\n        assert_series_equal(nat_series_dtype_timedelta +\n                            single_nat_dtype_timedelta,\n                            nat_series_dtype_timedelta)\n        assert_series_equal(single_nat_dtype_timedelta +\n                            nat_series_dtype_timedelta,\n                            nat_series_dtype_timedelta)\n\n        assert_series_equal(timedelta_series + NaT, nat_series_dtype_timedelta)\n        assert_series_equal(NaT + timedelta_series, nat_series_dtype_timedelta)\n\n        assert_series_equal(timedelta_series + single_nat_dtype_timedelta,\n                            nat_series_dtype_timedelta)\n        assert_series_equal(single_nat_dtype_timedelta + timedelta_series,\n                            nat_series_dtype_timedelta)\n\n        assert_series_equal(nat_series_dtype_timestamp + NaT,\n                            nat_series_dtype_timestamp)\n        assert_series_equal(NaT + nat_series_dtype_timestamp,\n                            nat_series_dtype_timestamp)\n\n        assert_series_equal(nat_series_dtype_timestamp +\n                            single_nat_dtype_timedelta,\n                            nat_series_dtype_timestamp)\n        assert_series_equal(single_nat_dtype_timedelta +\n                            nat_series_dtype_timestamp,\n                            nat_series_dtype_timestamp)\n\n        assert_series_equal(nat_series_dtype_timedelta + NaT,\n                            nat_series_dtype_timedelta)\n        assert_series_equal(NaT + nat_series_dtype_timedelta,\n                            nat_series_dtype_timedelta)\n\n        assert_series_equal(nat_series_dtype_timedelta +\n                            single_nat_dtype_timedelta,\n                            nat_series_dtype_timedelta)\n        assert_series_equal(single_nat_dtype_timedelta +\n                            nat_series_dtype_timedelta,\n                            nat_series_dtype_timedelta)\n\n        assert_series_equal(nat_series_dtype_timedelta +\n                            single_nat_dtype_datetime,\n                            nat_series_dtype_timestamp)\n        assert_series_equal(single_nat_dtype_datetime +\n                            nat_series_dtype_timedelta,\n                            nat_series_dtype_timestamp)\n\n        # multiplication\n        assert_series_equal(nat_series_dtype_timedelta * 1.0,\n                            nat_series_dtype_timedelta)\n        assert_series_equal(1.0 * nat_series_dtype_timedelta,\n                            nat_series_dtype_timedelta)\n\n        assert_series_equal(timedelta_series * 1, timedelta_series)\n        assert_series_equal(1 * timedelta_series, timedelta_series)\n\n        assert_series_equal(timedelta_series * 1.5,\n                            Series([NaT, Timedelta('1.5s')]))\n        assert_series_equal(1.5 * timedelta_series,\n                            Series([NaT, Timedelta('1.5s')]))\n\n        assert_series_equal(timedelta_series * nan, nat_series_dtype_timedelta)\n        assert_series_equal(nan * timedelta_series, nat_series_dtype_timedelta)\n\n        with pytest.raises(TypeError):\n            datetime_series * 1\n        with pytest.raises(TypeError):\n            nat_series_dtype_timestamp * 1\n        with pytest.raises(TypeError):\n            datetime_series * 1.0\n        with pytest.raises(TypeError):\n            nat_series_dtype_timestamp * 1.0\n\n        # division\n        assert_series_equal(timedelta_series / 2,\n                            Series([NaT, Timedelta('0.5s')]))\n        assert_series_equal(timedelta_series / 2.0,\n                            Series([NaT, Timedelta('0.5s')]))\n        assert_series_equal(timedelta_series / nan, nat_series_dtype_timedelta)\n        with pytest.raises(TypeError):\n            nat_series_dtype_timestamp / 1.0\n        with pytest.raises(TypeError):\n            nat_series_dtype_timestamp / 1\n\n    def test_ops_datetimelike_align(self):\n        # GH 7500\n        # datetimelike ops need to align\n        dt = Series(date_range('2012-1-1', periods=3, freq='D'))\n        dt.iloc[2] = np.nan\n        dt2 = dt[::-1]\n\n        expected = Series([timedelta(0), timedelta(0), pd.NaT])\n        # name is reset\n        result = dt2 - dt\n        assert_series_equal(result, expected)\n\n        expected = Series(expected, name=0)\n        result = (dt2.to_frame() - dt.to_frame())[0]\n        assert_series_equal(result, expected)\n\n    def test_object_comparisons(self):\n        s = Series(['a', 'b', np.nan, 'c', 'a'])\n\n        result = s == 'a'\n        expected = Series([True, False, False, False, True])\n        assert_series_equal(result, expected)\n\n        result = s < 'a'\n        expected = Series([False, False, False, False, False])\n        assert_series_equal(result, expected)\n\n        result = s != 'a'\n        expected = -(s == 'a')\n        assert_series_equal(result, expected)\n\n    def test_comparison_tuples(self):\n        # GH11339\n        # comparisons vs tuple\n        s = Series([(1, 1), (1, 2)])\n\n        result = s == (1, 2)\n        expected = Series([False, True])\n        assert_series_equal(result, expected)\n\n        result = s != (1, 2)\n        expected = Series([True, False])\n        assert_series_equal(result, expected)\n\n        result = s == (0, 0)\n        expected = Series([False, False])\n        assert_series_equal(result, expected)\n\n        result = s != (0, 0)\n        expected = Series([True, True])\n        assert_series_equal(result, expected)\n\n        s = Series([(1, 1), (1, 1)])\n\n        result = s == (1, 1)\n        expected = Series([True, True])\n        assert_series_equal(result, expected)\n\n        result = s != (1, 1)\n        expected = Series([False, False])\n        assert_series_equal(result, expected)\n\n        s = Series([frozenset([1]), frozenset([1, 2])])\n\n        result = s == frozenset([1])\n        expected = Series([True, False])\n        assert_series_equal(result, expected)\n\n    def test_comparison_operators_with_nas(self):\n        s = Series(bdate_range('1/1/2000', periods=10), dtype=object)\n        s[::2] = np.nan\n\n        # test that comparisons work\n        ops = ['lt', 'le', 'gt', 'ge', 'eq', 'ne']\n        for op in ops:\n            val = s[5]\n\n            f = getattr(operator, op)\n            result = f(s, val)\n\n            expected = f(s.dropna(), val).reindex(s.index)\n\n            if op == 'ne':\n                expected = expected.fillna(True).astype(bool)\n            else:\n                expected = expected.fillna(False).astype(bool)\n\n            assert_series_equal(result, expected)\n\n            # fffffffuuuuuuuuuuuu\n            # result = f(val, s)\n            # expected = f(val, s.dropna()).reindex(s.index)\n            # assert_series_equal(result, expected)\n\n            # boolean &, |, ^ should work with object arrays and propagate NAs\n\n        ops = ['and_', 'or_', 'xor']\n        mask = s.isna()\n        for bool_op in ops:\n            f = getattr(operator, bool_op)\n\n            filled = s.fillna(s[0])\n\n            result = f(s < s[9], s > s[3])\n\n            expected = f(filled < filled[9], filled > filled[3])\n            expected[mask] = False\n            assert_series_equal(result, expected)\n\n    def test_comparison_object_numeric_nas(self):\n        s = Series(np.random.randn(10), dtype=object)\n        shifted = s.shift(2)\n\n        ops = ['lt', 'le', 'gt', 'ge', 'eq', 'ne']\n        for op in ops:\n            f = getattr(operator, op)\n\n            result = f(s, shifted)\n            expected = f(s.astype(float), shifted.astype(float))\n            assert_series_equal(result, expected)\n\n    def test_comparison_invalid(self):\n\n        # GH4968\n        # invalid date/int comparisons\n        s = Series(range(5))\n        s2 = Series(date_range('20010101', periods=5))\n\n        for (x, y) in [(s, s2), (s2, s)]:\n            pytest.raises(TypeError, lambda: x == y)\n            pytest.raises(TypeError, lambda: x != y)\n            pytest.raises(TypeError, lambda: x >= y)\n            pytest.raises(TypeError, lambda: x > y)\n            pytest.raises(TypeError, lambda: x < y)\n            pytest.raises(TypeError, lambda: x <= y)\n\n    def test_more_na_comparisons(self):\n        for dtype in [None, object]:\n            left = Series(['a', np.nan, 'c'], dtype=dtype)\n            right = Series(['a', np.nan, 'd'], dtype=dtype)\n\n            result = left == right\n            expected = Series([True, False, False])\n            assert_series_equal(result, expected)\n\n            result = left != right\n            expected = Series([False, True, True])\n            assert_series_equal(result, expected)\n\n            result = left == np.nan\n            expected = Series([False, False, False])\n            assert_series_equal(result, expected)\n\n            result = left != np.nan\n            expected = Series([True, True, True])\n            assert_series_equal(result, expected)\n\n    def test_nat_comparisons(self):\n        data = [([pd.Timestamp('2011-01-01'), pd.NaT,\n                  pd.Timestamp('2011-01-03')],\n                 [pd.NaT, pd.NaT, pd.Timestamp('2011-01-03')]),\n\n                ([pd.Timedelta('1 days'), pd.NaT,\n                  pd.Timedelta('3 days')],\n                 [pd.NaT, pd.NaT, pd.Timedelta('3 days')]),\n\n                ([pd.Period('2011-01', freq='M'), pd.NaT,\n                  pd.Period('2011-03', freq='M')],\n                 [pd.NaT, pd.NaT, pd.Period('2011-03', freq='M')])]\n\n        # add lhs / rhs switched data\n        data = data + [(r, l) for l, r in data]\n\n        for l, r in data:\n            for dtype in [None, object]:\n                left = Series(l, dtype=dtype)\n\n                # Series, Index\n                for right in [Series(r, dtype=dtype), Index(r, dtype=dtype)]:\n                    expected = Series([False, False, True])\n                    assert_series_equal(left == right, expected)\n\n                    expected = Series([True, True, False])\n                    assert_series_equal(left != right, expected)\n\n                    expected = Series([False, False, False])\n                    assert_series_equal(left < right, expected)\n\n                    expected = Series([False, False, False])\n                    assert_series_equal(left > right, expected)\n\n                    expected = Series([False, False, True])\n                    assert_series_equal(left >= right, expected)\n\n                    expected = Series([False, False, True])\n                    assert_series_equal(left <= right, expected)\n\n    def test_nat_comparisons_scalar(self):\n        data = [[pd.Timestamp('2011-01-01'), pd.NaT,\n                 pd.Timestamp('2011-01-03')],\n\n                [pd.Timedelta('1 days'), pd.NaT, pd.Timedelta('3 days')],\n\n                [pd.Period('2011-01', freq='M'), pd.NaT,\n                 pd.Period('2011-03', freq='M')]]\n\n        for l in data:\n            for dtype in [None, object]:\n                left = Series(l, dtype=dtype)\n\n                expected = Series([False, False, False])\n                assert_series_equal(left == pd.NaT, expected)\n                assert_series_equal(pd.NaT == left, expected)\n\n                expected = Series([True, True, True])\n                assert_series_equal(left != pd.NaT, expected)\n                assert_series_equal(pd.NaT != left, expected)\n\n                expected = Series([False, False, False])\n                assert_series_equal(left < pd.NaT, expected)\n                assert_series_equal(pd.NaT > left, expected)\n                assert_series_equal(left <= pd.NaT, expected)\n                assert_series_equal(pd.NaT >= left, expected)\n\n                assert_series_equal(left > pd.NaT, expected)\n                assert_series_equal(pd.NaT < left, expected)\n                assert_series_equal(left >= pd.NaT, expected)\n                assert_series_equal(pd.NaT <= left, expected)\n\n    def test_comparison_different_length(self):\n        a = Series(['a', 'b', 'c'])\n        b = Series(['b', 'a'])\n        pytest.raises(ValueError, a.__lt__, b)\n\n        a = Series([1, 2])\n        b = Series([2, 3, 4])\n        pytest.raises(ValueError, a.__eq__, b)\n\n    def test_comparison_label_based(self):\n\n        # GH 4947\n        # comparisons should be label based\n\n        a = Series([True, False, True], list('bca'))\n        b = Series([False, True, False], list('abc'))\n\n        expected = Series([False, True, False], list('abc'))\n        result = a & b\n        assert_series_equal(result, expected)\n\n        expected = Series([True, True, False], list('abc'))\n        result = a | b\n        assert_series_equal(result, expected)\n\n        expected = Series([True, False, False], list('abc'))\n        result = a ^ b\n        assert_series_equal(result, expected)\n\n        # rhs is bigger\n        a = Series([True, False, True], list('bca'))\n        b = Series([False, True, False, True], list('abcd'))\n\n        expected = Series([False, True, False, False], list('abcd'))\n        result = a & b\n        assert_series_equal(result, expected)\n\n        expected = Series([True, True, False, False], list('abcd'))\n        result = a | b\n        assert_series_equal(result, expected)\n\n        # filling\n\n        # vs empty\n        result = a & Series([])\n        expected = Series([False, False, False], list('bca'))\n        assert_series_equal(result, expected)\n\n        result = a | Series([])\n        expected = Series([True, False, True], list('bca'))\n        assert_series_equal(result, expected)\n\n        # vs non-matching\n        result = a & Series([1], ['z'])\n        expected = Series([False, False, False, False], list('abcz'))\n        assert_series_equal(result, expected)\n\n        result = a | Series([1], ['z'])\n        expected = Series([True, True, False, False], list('abcz'))\n        assert_series_equal(result, expected)\n\n        # identity\n        # we would like s[s|e] == s to hold for any e, whether empty or not\n        for e in [Series([]), Series([1], ['z']),\n                  Series(np.nan, b.index), Series(np.nan, a.index)]:\n            result = a[a | e]\n            assert_series_equal(result, a[a])\n\n        for e in [Series(['z'])]:\n            if compat.PY3:\n                with tm.assert_produces_warning(RuntimeWarning):\n                    result = a[a | e]\n            else:\n                result = a[a | e]\n            assert_series_equal(result, a[a])\n\n        # vs scalars\n        index = list('bca')\n        t = Series([True, False, True])\n\n        for v in [True, 1, 2]:\n            result = Series([True, False, True], index=index) | v\n            expected = Series([True, True, True], index=index)\n            assert_series_equal(result, expected)\n\n        for v in [np.nan, 'foo']:\n            pytest.raises(TypeError, lambda: t | v)\n\n        for v in [False, 0]:\n            result = Series([True, False, True], index=index) | v\n            expected = Series([True, False, True], index=index)\n            assert_series_equal(result, expected)\n\n        for v in [True, 1]:\n            result = Series([True, False, True], index=index) & v\n            expected = Series([True, False, True], index=index)\n            assert_series_equal(result, expected)\n\n        for v in [False, 0]:\n            result = Series([True, False, True], index=index) & v\n            expected = Series([False, False, False], index=index)\n            assert_series_equal(result, expected)\n        for v in [np.nan]:\n            pytest.raises(TypeError, lambda: t & v)\n\n    def test_comparison_flex_basic(self):\n        left = pd.Series(np.random.randn(10))\n        right = pd.Series(np.random.randn(10))\n\n        assert_series_equal(left.eq(right), left == right)\n        assert_series_equal(left.ne(right), left != right)\n        assert_series_equal(left.le(right), left < right)\n        assert_series_equal(left.lt(right), left <= right)\n        assert_series_equal(left.gt(right), left > right)\n        assert_series_equal(left.ge(right), left >= right)\n\n        # axis\n        for axis in [0, None, 'index']:\n            assert_series_equal(left.eq(right, axis=axis), left == right)\n            assert_series_equal(left.ne(right, axis=axis), left != right)\n            assert_series_equal(left.le(right, axis=axis), left < right)\n            assert_series_equal(left.lt(right, axis=axis), left <= right)\n            assert_series_equal(left.gt(right, axis=axis), left > right)\n            assert_series_equal(left.ge(right, axis=axis), left >= right)\n\n        #\n        msg = 'No axis named 1 for object type'\n        for op in ['eq', 'ne', 'le', 'le', 'gt', 'ge']:\n            with tm.assert_raises_regex(ValueError, msg):\n                getattr(left, op)(right, axis=1)\n\n    def test_comparison_flex_alignment(self):\n        left = Series([1, 3, 2], index=list('abc'))\n        right = Series([2, 2, 2], index=list('bcd'))\n\n        exp = pd.Series([False, False, True, False], index=list('abcd'))\n        assert_series_equal(left.eq(right), exp)\n\n        exp = pd.Series([True, True, False, True], index=list('abcd'))\n        assert_series_equal(left.ne(right), exp)\n\n        exp = pd.Series([False, False, True, False], index=list('abcd'))\n        assert_series_equal(left.le(right), exp)\n\n        exp = pd.Series([False, False, False, False], index=list('abcd'))\n        assert_series_equal(left.lt(right), exp)\n\n        exp = pd.Series([False, True, True, False], index=list('abcd'))\n        assert_series_equal(left.ge(right), exp)\n\n        exp = pd.Series([False, True, False, False], index=list('abcd'))\n        assert_series_equal(left.gt(right), exp)\n\n    def test_comparison_flex_alignment_fill(self):\n        left = Series([1, 3, 2], index=list('abc'))\n        right = Series([2, 2, 2], index=list('bcd'))\n\n        exp = pd.Series([False, False, True, True], index=list('abcd'))\n        assert_series_equal(left.eq(right, fill_value=2), exp)\n\n        exp = pd.Series([True, True, False, False], index=list('abcd'))\n        assert_series_equal(left.ne(right, fill_value=2), exp)\n\n        exp = pd.Series([False, False, True, True], index=list('abcd'))\n        assert_series_equal(left.le(right, fill_value=0), exp)\n\n        exp = pd.Series([False, False, False, True], index=list('abcd'))\n        assert_series_equal(left.lt(right, fill_value=0), exp)\n\n        exp = pd.Series([True, True, True, False], index=list('abcd'))\n        assert_series_equal(left.ge(right, fill_value=0), exp)\n\n        exp = pd.Series([True, True, False, False], index=list('abcd'))\n        assert_series_equal(left.gt(right, fill_value=0), exp)\n\n    def test_return_dtypes_bool_op_costant(self):\n        # gh15115\n        s = pd.Series([1, 3, 2], index=range(3))\n        const = 2\n        for op in ['eq', 'ne', 'gt', 'lt', 'ge', 'le']:\n            result = getattr(s, op)(const).get_dtype_counts()\n            tm.assert_series_equal(result, Series([1], ['bool']))\n\n        # empty Series\n        empty = s.iloc[:0]\n        for op in ['eq', 'ne', 'gt', 'lt', 'ge', 'le']:\n            result = getattr(empty, op)(const).get_dtype_counts()\n            tm.assert_series_equal(result, Series([1], ['bool']))\n\n    def test_operators_bitwise(self):\n        # GH 9016: support bitwise op for integer types\n        index = list('bca')\n\n        s_tft = Series([True, False, True], index=index)\n        s_fff = Series([False, False, False], index=index)\n        s_tff = Series([True, False, False], index=index)\n        s_empty = Series([])\n\n        # TODO: unused\n        # s_0101 = Series([0, 1, 0, 1])\n\n        s_0123 = Series(range(4), dtype='int64')\n        s_3333 = Series([3] * 4)\n        s_4444 = Series([4] * 4)\n\n        res = s_tft & s_empty\n        expected = s_fff\n        assert_series_equal(res, expected)\n\n        res = s_tft | s_empty\n        expected = s_tft\n        assert_series_equal(res, expected)\n\n        res = s_0123 & s_3333\n        expected = Series(range(4), dtype='int64')\n        assert_series_equal(res, expected)\n\n        res = s_0123 | s_4444\n        expected = Series(range(4, 8), dtype='int64')\n        assert_series_equal(res, expected)\n\n        s_a0b1c0 = Series([1], list('b'))\n\n        res = s_tft & s_a0b1c0\n        expected = s_tff.reindex(list('abc'))\n        assert_series_equal(res, expected)\n\n        res = s_tft | s_a0b1c0\n        expected = s_tft.reindex(list('abc'))\n        assert_series_equal(res, expected)\n\n        n0 = 0\n        res = s_tft & n0\n        expected = s_fff\n        assert_series_equal(res, expected)\n\n        res = s_0123 & n0\n        expected = Series([0] * 4)\n        assert_series_equal(res, expected)\n\n        n1 = 1\n        res = s_tft & n1\n        expected = s_tft\n        assert_series_equal(res, expected)\n\n        res = s_0123 & n1\n        expected = Series([0, 1, 0, 1])\n        assert_series_equal(res, expected)\n\n        s_1111 = Series([1] * 4, dtype='int8')\n        res = s_0123 & s_1111\n        expected = Series([0, 1, 0, 1], dtype='int64')\n        assert_series_equal(res, expected)\n\n        res = s_0123.astype(np.int16) | s_1111.astype(np.int32)\n        expected = Series([1, 1, 3, 3], dtype='int32')\n        assert_series_equal(res, expected)\n\n        pytest.raises(TypeError, lambda: s_1111 & 'a')\n        pytest.raises(TypeError, lambda: s_1111 & ['a', 'b', 'c', 'd'])\n        pytest.raises(TypeError, lambda: s_0123 & np.NaN)\n        pytest.raises(TypeError, lambda: s_0123 & 3.14)\n        pytest.raises(TypeError, lambda: s_0123 & [0.1, 4, 3.14, 2])\n\n        # s_0123 will be all false now because of reindexing like s_tft\n        if compat.PY3:\n            # unable to sort incompatible object via .union.\n            exp = Series([False] * 7, index=['b', 'c', 'a', 0, 1, 2, 3])\n            with tm.assert_produces_warning(RuntimeWarning):\n                assert_series_equal(s_tft & s_0123, exp)\n        else:\n            exp = Series([False] * 7, index=[0, 1, 2, 3, 'a', 'b', 'c'])\n            assert_series_equal(s_tft & s_0123, exp)\n\n        # s_tft will be all false now because of reindexing like s_0123\n        if compat.PY3:\n            # unable to sort incompatible object via .union.\n            exp = Series([False] * 7, index=[0, 1, 2, 3, 'b', 'c', 'a'])\n            with tm.assert_produces_warning(RuntimeWarning):\n                assert_series_equal(s_0123 & s_tft, exp)\n        else:\n            exp = Series([False] * 7, index=[0, 1, 2, 3, 'a', 'b', 'c'])\n            assert_series_equal(s_0123 & s_tft, exp)\n\n        assert_series_equal(s_0123 & False, Series([False] * 4))\n        assert_series_equal(s_0123 ^ False, Series([False, True, True, True]))\n        assert_series_equal(s_0123 & [False], Series([False] * 4))\n        assert_series_equal(s_0123 & (False), Series([False] * 4))\n        assert_series_equal(s_0123 & Series([False, np.NaN, False, False]),\n                            Series([False] * 4))\n\n        s_ftft = Series([False, True, False, True])\n        assert_series_equal(s_0123 & Series([0.1, 4, -3.14, 2]), s_ftft)\n\n        s_abNd = Series(['a', 'b', np.NaN, 'd'])\n        res = s_0123 & s_abNd\n        expected = s_ftft\n        assert_series_equal(res, expected)\n\n    def test_scalar_na_cmp_corners(self):\n        s = Series([2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n        def tester(a, b):\n            return a & b\n\n        pytest.raises(TypeError, tester, s, datetime(2005, 1, 1))\n\n        s = Series([2, 3, 4, 5, 6, 7, 8, 9, datetime(2005, 1, 1)])\n        s[::2] = np.nan\n\n        expected = Series(True, index=s.index)\n        expected[::2] = False\n        assert_series_equal(tester(s, list(s)), expected)\n\n        d = DataFrame({'A': s})\n        # TODO: Fix this exception - needs to be fixed! (see GH5035)\n        # (previously this was a TypeError because series returned\n        # NotImplemented\n\n        # this is an alignment issue; these are equivalent\n        # https://github.com/pandas-dev/pandas/issues/5284\n\n        pytest.raises(ValueError, lambda: d.__and__(s, axis='columns'))\n        pytest.raises(ValueError, tester, s, d)\n\n        # this is wrong as its not a boolean result\n        # result = d.__and__(s,axis='index')\n\n    def test_operators_corner(self):\n        series = self.ts\n\n        empty = Series([], index=Index([]))\n\n        result = series + empty\n        assert np.isnan(result).all()\n\n        result = empty + Series([], index=Index([]))\n        assert len(result) == 0\n\n        # TODO: this returned NotImplemented earlier, what to do?\n        # deltas = Series([timedelta(1)] * 5, index=np.arange(5))\n        # sub_deltas = deltas[::2]\n        # deltas5 = deltas * 5\n        # deltas = deltas + sub_deltas\n\n        # float + int\n        int_ts = self.ts.astype(int)[:-5]\n        added = self.ts + int_ts\n        expected = Series(self.ts.values[:-5] + int_ts.values,\n                          index=self.ts.index[:-5], name='ts')\n        tm.assert_series_equal(added[:-5], expected)\n\n    def test_operators_reverse_object(self):\n        # GH 56\n        arr = Series(np.random.randn(10), index=np.arange(10), dtype=object)\n\n        def _check_op(arr, op):\n            result = op(1., arr)\n            expected = op(1., arr.astype(float))\n            assert_series_equal(result.astype(float), expected)\n\n        _check_op(arr, operator.add)\n        _check_op(arr, operator.sub)\n        _check_op(arr, operator.mul)\n        _check_op(arr, operator.truediv)\n        _check_op(arr, operator.floordiv)\n\n    def test_arith_ops_df_compat(self):\n        # GH 1134\n        s1 = pd.Series([1, 2, 3], index=list('ABC'), name='x')\n        s2 = pd.Series([2, 2, 2], index=list('ABD'), name='x')\n\n        exp = pd.Series([3.0, 4.0, np.nan, np.nan],\n                        index=list('ABCD'), name='x')\n        assert_series_equal(s1 + s2, exp)\n        assert_series_equal(s2 + s1, exp)\n\n        exp = pd.DataFrame({'x': [3.0, 4.0, np.nan, np.nan]},\n                           index=list('ABCD'))\n        assert_frame_equal(s1.to_frame() + s2.to_frame(), exp)\n        assert_frame_equal(s2.to_frame() + s1.to_frame(), exp)\n\n        # different length\n        s3 = pd.Series([1, 2, 3], index=list('ABC'), name='x')\n        s4 = pd.Series([2, 2, 2, 2], index=list('ABCD'), name='x')\n\n        exp = pd.Series([3, 4, 5, np.nan],\n                        index=list('ABCD'), name='x')\n        assert_series_equal(s3 + s4, exp)\n        assert_series_equal(s4 + s3, exp)\n\n        exp = pd.DataFrame({'x': [3, 4, 5, np.nan]},\n                           index=list('ABCD'))\n        assert_frame_equal(s3.to_frame() + s4.to_frame(), exp)\n        assert_frame_equal(s4.to_frame() + s3.to_frame(), exp)\n\n    def test_comp_ops_df_compat(self):\n        # GH 1134\n        s1 = pd.Series([1, 2, 3], index=list('ABC'), name='x')\n        s2 = pd.Series([2, 2, 2], index=list('ABD'), name='x')\n\n        s3 = pd.Series([1, 2, 3], index=list('ABC'), name='x')\n        s4 = pd.Series([2, 2, 2, 2], index=list('ABCD'), name='x')\n\n        for l, r in [(s1, s2), (s2, s1), (s3, s4), (s4, s3)]:\n\n            msg = \"Can only compare identically-labeled Series objects\"\n            with tm.assert_raises_regex(ValueError, msg):\n                l == r\n\n            with tm.assert_raises_regex(ValueError, msg):\n                l != r\n\n            with tm.assert_raises_regex(ValueError, msg):\n                l < r\n\n            msg = \"Can only compare identically-labeled DataFrame objects\"\n            with tm.assert_raises_regex(ValueError, msg):\n                l.to_frame() == r.to_frame()\n\n            with tm.assert_raises_regex(ValueError, msg):\n                l.to_frame() != r.to_frame()\n\n            with tm.assert_raises_regex(ValueError, msg):\n                l.to_frame() < r.to_frame()\n\n    def test_bool_ops_df_compat(self):\n        # GH 1134\n        s1 = pd.Series([True, False, True], index=list('ABC'), name='x')\n        s2 = pd.Series([True, True, False], index=list('ABD'), name='x')\n\n        exp = pd.Series([True, False, False, False],\n                        index=list('ABCD'), name='x')\n        assert_series_equal(s1 & s2, exp)\n        assert_series_equal(s2 & s1, exp)\n\n        # True | np.nan => True\n        exp = pd.Series([True, True, True, False],\n                        index=list('ABCD'), name='x')\n        assert_series_equal(s1 | s2, exp)\n        # np.nan | True => np.nan, filled with False\n        exp = pd.Series([True, True, False, False],\n                        index=list('ABCD'), name='x')\n        assert_series_equal(s2 | s1, exp)\n\n        # DataFrame doesn't fill nan with False\n        exp = pd.DataFrame({'x': [True, False, np.nan, np.nan]},\n                           index=list('ABCD'))\n        assert_frame_equal(s1.to_frame() & s2.to_frame(), exp)\n        assert_frame_equal(s2.to_frame() & s1.to_frame(), exp)\n\n        exp = pd.DataFrame({'x': [True, True, np.nan, np.nan]},\n                           index=list('ABCD'))\n        assert_frame_equal(s1.to_frame() | s2.to_frame(), exp)\n        assert_frame_equal(s2.to_frame() | s1.to_frame(), exp)\n\n        # different length\n        s3 = pd.Series([True, False, True], index=list('ABC'), name='x')\n        s4 = pd.Series([True, True, True, True], index=list('ABCD'), name='x')\n\n        exp = pd.Series([True, False, True, False],\n                        index=list('ABCD'), name='x')\n        assert_series_equal(s3 & s4, exp)\n        assert_series_equal(s4 & s3, exp)\n\n        # np.nan | True => np.nan, filled with False\n        exp = pd.Series([True, True, True, False],\n                        index=list('ABCD'), name='x')\n        assert_series_equal(s3 | s4, exp)\n        # True | np.nan => True\n        exp = pd.Series([True, True, True, True],\n                        index=list('ABCD'), name='x')\n        assert_series_equal(s4 | s3, exp)\n\n        exp = pd.DataFrame({'x': [True, False, True, np.nan]},\n                           index=list('ABCD'))\n        assert_frame_equal(s3.to_frame() & s4.to_frame(), exp)\n        assert_frame_equal(s4.to_frame() & s3.to_frame(), exp)\n\n        exp = pd.DataFrame({'x': [True, True, True, np.nan]},\n                           index=list('ABCD'))\n        assert_frame_equal(s3.to_frame() | s4.to_frame(), exp)\n        assert_frame_equal(s4.to_frame() | s3.to_frame(), exp)\n\n    def test_series_frame_radd_bug(self):\n        # GH 353\n        vals = Series(tm.rands_array(5, 10))\n        result = 'foo_' + vals\n        expected = vals.map(lambda x: 'foo_' + x)\n        assert_series_equal(result, expected)\n\n        frame = DataFrame({'vals': vals})\n        result = 'foo_' + frame\n        expected = DataFrame({'vals': vals.map(lambda x: 'foo_' + x)})\n        assert_frame_equal(result, expected)\n\n        # really raise this time\n        with pytest.raises(TypeError):\n            datetime.now() + self.ts\n\n        with pytest.raises(TypeError):\n            self.ts + datetime.now()\n\n    def test_series_radd_more(self):\n        data = [[1, 2, 3],\n                [1.1, 2.2, 3.3],\n                [pd.Timestamp('2011-01-01'), pd.Timestamp('2011-01-02'),\n                 pd.NaT],\n                ['x', 'y', 1]]\n\n        for d in data:\n            for dtype in [None, object]:\n                s = Series(d, dtype=dtype)\n                with pytest.raises(TypeError):\n                    'foo_' + s\n\n        for dtype in [None, object]:\n            res = 1 + pd.Series([1, 2, 3], dtype=dtype)\n            exp = pd.Series([2, 3, 4], dtype=dtype)\n            assert_series_equal(res, exp)\n            res = pd.Series([1, 2, 3], dtype=dtype) + 1\n            assert_series_equal(res, exp)\n\n            res = np.nan + pd.Series([1, 2, 3], dtype=dtype)\n            exp = pd.Series([np.nan, np.nan, np.nan], dtype=dtype)\n            assert_series_equal(res, exp)\n            res = pd.Series([1, 2, 3], dtype=dtype) + np.nan\n            assert_series_equal(res, exp)\n\n            s = pd.Series([pd.Timedelta('1 days'), pd.Timedelta('2 days'),\n                           pd.Timedelta('3 days')], dtype=dtype)\n            exp = pd.Series([pd.Timedelta('4 days'), pd.Timedelta('5 days'),\n                             pd.Timedelta('6 days')])\n            assert_series_equal(pd.Timedelta('3 days') + s, exp)\n            assert_series_equal(s + pd.Timedelta('3 days'), exp)\n\n        s = pd.Series(['x', np.nan, 'x'])\n        assert_series_equal('a' + s, pd.Series(['ax', np.nan, 'ax']))\n        assert_series_equal(s + 'a', pd.Series(['xa', np.nan, 'xa']))\n\n    def test_frame_radd_more(self):\n        data = [[1, 2, 3],\n                [1.1, 2.2, 3.3],\n                [pd.Timestamp('2011-01-01'), pd.Timestamp('2011-01-02'),\n                 pd.NaT],\n                ['x', 'y', 1]]\n\n        for d in data:\n            for dtype in [None, object]:\n                s = DataFrame(d, dtype=dtype)\n                with pytest.raises(TypeError):\n                    'foo_' + s\n\n        for dtype in [None, object]:\n            res = 1 + pd.DataFrame([1, 2, 3], dtype=dtype)\n            exp = pd.DataFrame([2, 3, 4], dtype=dtype)\n            assert_frame_equal(res, exp)\n            res = pd.DataFrame([1, 2, 3], dtype=dtype) + 1\n            assert_frame_equal(res, exp)\n\n            res = np.nan + pd.DataFrame([1, 2, 3], dtype=dtype)\n            exp = pd.DataFrame([np.nan, np.nan, np.nan], dtype=dtype)\n            assert_frame_equal(res, exp)\n            res = pd.DataFrame([1, 2, 3], dtype=dtype) + np.nan\n            assert_frame_equal(res, exp)\n\n        df = pd.DataFrame(['x', np.nan, 'x'])\n        assert_frame_equal('a' + df, pd.DataFrame(['ax', np.nan, 'ax']))\n        assert_frame_equal(df + 'a', pd.DataFrame(['xa', np.nan, 'xa']))\n\n    def test_operators_frame(self):\n        # rpow does not work with DataFrame\n        df = DataFrame({'A': self.ts})\n\n        assert_series_equal(self.ts + self.ts, self.ts + df['A'],\n                            check_names=False)\n        assert_series_equal(self.ts ** self.ts, self.ts ** df['A'],\n                            check_names=False)\n        assert_series_equal(self.ts < self.ts, self.ts < df['A'],\n                            check_names=False)\n        assert_series_equal(self.ts / self.ts, self.ts / df['A'],\n                            check_names=False)\n\n    def test_operators_combine(self):\n        def _check_fill(meth, op, a, b, fill_value=0):\n            exp_index = a.index.union(b.index)\n            a = a.reindex(exp_index)\n            b = b.reindex(exp_index)\n\n            amask = isna(a)\n            bmask = isna(b)\n\n            exp_values = []\n            for i in range(len(exp_index)):\n                with np.errstate(all='ignore'):\n                    if amask[i]:\n                        if bmask[i]:\n                            exp_values.append(nan)\n                            continue\n                        exp_values.append(op(fill_value, b[i]))\n                    elif bmask[i]:\n                        if amask[i]:\n                            exp_values.append(nan)\n                            continue\n                        exp_values.append(op(a[i], fill_value))\n                    else:\n                        exp_values.append(op(a[i], b[i]))\n\n            result = meth(a, b, fill_value=fill_value)\n            expected = Series(exp_values, exp_index)\n            assert_series_equal(result, expected)\n\n        a = Series([nan, 1., 2., 3., nan], index=np.arange(5))\n        b = Series([nan, 1, nan, 3, nan, 4.], index=np.arange(6))\n\n        pairings = []\n        for op in ['add', 'sub', 'mul', 'pow', 'truediv', 'floordiv']:\n            fv = 0\n            lop = getattr(Series, op)\n            lequiv = getattr(operator, op)\n            rop = getattr(Series, 'r' + op)\n            # bind op at definition time...\n            requiv = lambda x, y, op=op: getattr(operator, op)(y, x)\n            pairings.append((lop, lequiv, fv))\n            pairings.append((rop, requiv, fv))\n\n        if compat.PY3:\n            pairings.append((Series.div, operator.truediv, 1))\n            pairings.append((Series.rdiv, lambda x, y: operator.truediv(y, x),\n                             1))\n        else:\n            pairings.append((Series.div, operator.div, 1))\n            pairings.append((Series.rdiv, lambda x, y: operator.div(y, x), 1))\n\n        for op, equiv_op, fv in pairings:\n            result = op(a, b)\n            exp = equiv_op(a, b)\n            assert_series_equal(result, exp)\n            _check_fill(op, equiv_op, a, b, fill_value=fv)\n            # should accept axis=0 or axis='rows'\n            op(a, b, axis=0)\n\n    def test_ne(self):\n        ts = Series([3, 4, 5, 6, 7], [3, 4, 5, 6, 7], dtype=float)\n        expected = [True, True, False, True, True]\n        assert tm.equalContents(ts.index != 5, expected)\n        assert tm.equalContents(~(ts.index == 5), expected)\n\n    def test_operators_na_handling(self):\n        from decimal import Decimal\n        from datetime import date\n        s = Series([Decimal('1.3'), Decimal('2.3')],\n                   index=[date(2012, 1, 1), date(2012, 1, 2)])\n\n        result = s + s.shift(1)\n        result2 = s.shift(1) + s\n        assert isna(result[0])\n        assert isna(result2[0])\n\n        s = Series(['foo', 'bar', 'baz', np.nan])\n        result = 'prefix_' + s\n        expected = Series(['prefix_foo', 'prefix_bar', 'prefix_baz', np.nan])\n        assert_series_equal(result, expected)\n\n        result = s + '_suffix'\n        expected = Series(['foo_suffix', 'bar_suffix', 'baz_suffix', np.nan])\n        assert_series_equal(result, expected)\n\n    def test_divide_decimal(self):\n        \"\"\" resolves issue #9787 \"\"\"\n        from decimal import Decimal\n\n        expected = Series([Decimal(5)])\n\n        s = Series([Decimal(10)])\n        s = s / Decimal(2)\n\n        assert_series_equal(expected, s)\n\n        s = Series([Decimal(10)])\n        s = s // Decimal(2)\n\n        assert_series_equal(expected, s)\n\n    def test_datetime64_with_index(self):\n\n        # arithmetic integer ops with an index\n        s = Series(np.random.randn(5))\n        expected = s - s.index.to_series()\n        result = s - s.index\n        assert_series_equal(result, expected)\n\n        # GH 4629\n        # arithmetic datetime64 ops with an index\n        s = Series(date_range('20130101', periods=5),\n                   index=date_range('20130101', periods=5))\n        expected = s - s.index.to_series()\n        result = s - s.index\n        assert_series_equal(result, expected)\n\n        result = s - s.index.to_period()\n        assert_series_equal(result, expected)\n\n        df = DataFrame(np.random.randn(5, 2),\n                       index=date_range('20130101', periods=5))\n        df['date'] = Timestamp('20130102')\n        df['expected'] = df['date'] - df.index.to_series()\n        df['result'] = df['date'] - df.index\n        assert_series_equal(df['result'], df['expected'], check_names=False)\n\n    def test_dti_tz_convert_to_utc(self):\n        base = pd.DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'],\n                                tz='UTC')\n        idx1 = base.tz_convert('Asia/Tokyo')[:2]\n        idx2 = base.tz_convert('US/Eastern')[1:]\n\n        res = Series([1, 2], index=idx1) + Series([1, 1], index=idx2)\n        assert_series_equal(res, Series([np.nan, 3, np.nan], index=base))\n\n    def test_op_duplicate_index(self):\n        # GH14227\n        s1 = Series([1, 2], index=[1, 1])\n        s2 = Series([10, 10], index=[1, 2])\n        result = s1 + s2\n        expected = pd.Series([11, 12, np.nan], index=[1, 1, 2])\n        assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"test_input,error_type\",\n        [\n            (pd.Series([]), ValueError),\n\n            # For strings, or any Series with dtype 'O'\n            (pd.Series(['foo', 'bar', 'baz']), TypeError),\n            (pd.Series([(1,), (2,)]), TypeError),\n\n            # For mixed data types\n            (\n                pd.Series(['foo', 'foo', 'bar', 'bar', None, np.nan, 'baz']),\n                TypeError\n            ),\n        ]\n    )\n    def test_assert_idxminmax_raises(self, test_input, error_type):\n        \"\"\"\n        Cases where ``Series.argmax`` and related should raise an exception\n        \"\"\"\n        with pytest.raises(error_type):\n            test_input.idxmin()\n        with pytest.raises(error_type):\n            test_input.idxmin(skipna=False)\n        with pytest.raises(error_type):\n            test_input.idxmax()\n        with pytest.raises(error_type):\n            test_input.idxmax(skipna=False)\n\n    def test_idxminmax_with_inf(self):\n        # For numeric data with NA and Inf (GH #13595)\n        s = pd.Series([0, -np.inf, np.inf, np.nan])\n\n        assert s.idxmin() == 1\n        assert np.isnan(s.idxmin(skipna=False))\n\n        assert s.idxmax() == 2\n        assert np.isnan(s.idxmax(skipna=False))\n\n        # Using old-style behavior that treats floating point nan, -inf, and\n        # +inf as missing\n        with pd.option_context('mode.use_inf_as_na', True):\n            assert s.idxmin() == 0\n            assert np.isnan(s.idxmin(skipna=False))\n            assert s.idxmax() == 0\n            np.isnan(s.idxmax(skipna=False))\n"
    },
    {
      "filename": "pandas/tests/sparse/test_series.py",
      "content": "# pylint: disable-msg=E1101,W0612\n\nimport operator\nfrom datetime import datetime\n\nimport pytest\n\nfrom numpy import nan\nimport numpy as np\nimport pandas as pd\n\nfrom pandas import Series, DataFrame, bdate_range, isna, compat\nfrom pandas.tseries.offsets import BDay\nimport pandas.util.testing as tm\nfrom pandas.compat import range\nfrom pandas.core.reshape.util import cartesian_product\n\nimport pandas.core.sparse.frame as spf\n\nfrom pandas._libs.sparse import BlockIndex, IntIndex\nfrom pandas.core.sparse.api import SparseSeries\nfrom pandas.tests.series.test_api import SharedWithSparse\n\n\ndef _test_data1():\n    # nan-based\n    arr = np.arange(20, dtype=float)\n    index = np.arange(20)\n    arr[:2] = nan\n    arr[5:10] = nan\n    arr[-3:] = nan\n\n    return arr, index\n\n\ndef _test_data2():\n    # nan-based\n    arr = np.arange(15, dtype=float)\n    index = np.arange(15)\n    arr[7:12] = nan\n    arr[-1:] = nan\n    return arr, index\n\n\ndef _test_data1_zero():\n    # zero-based\n    arr, index = _test_data1()\n    arr[np.isnan(arr)] = 0\n    return arr, index\n\n\ndef _test_data2_zero():\n    # zero-based\n    arr, index = _test_data2()\n    arr[np.isnan(arr)] = 0\n    return arr, index\n\n\nclass TestSparseSeries(SharedWithSparse):\n\n    series_klass = SparseSeries\n    # SharedWithSparse tests use generic, series_klass-agnostic assertion\n    _assert_series_equal = staticmethod(tm.assert_sp_series_equal)\n\n    def setup_method(self, method):\n        arr, index = _test_data1()\n\n        date_index = bdate_range('1/1/2011', periods=len(index))\n\n        self.bseries = SparseSeries(arr, index=index, kind='block',\n                                    name='bseries')\n        self.ts = self.bseries\n\n        self.btseries = SparseSeries(arr, index=date_index, kind='block')\n\n        self.iseries = SparseSeries(arr, index=index, kind='integer',\n                                    name='iseries')\n\n        arr, index = _test_data2()\n        self.bseries2 = SparseSeries(arr, index=index, kind='block')\n        self.iseries2 = SparseSeries(arr, index=index, kind='integer')\n\n        arr, index = _test_data1_zero()\n        self.zbseries = SparseSeries(arr, index=index, kind='block',\n                                     fill_value=0, name='zbseries')\n        self.ziseries = SparseSeries(arr, index=index, kind='integer',\n                                     fill_value=0)\n\n        arr, index = _test_data2_zero()\n        self.zbseries2 = SparseSeries(arr, index=index, kind='block',\n                                      fill_value=0)\n        self.ziseries2 = SparseSeries(arr, index=index, kind='integer',\n                                      fill_value=0)\n\n    def test_constructor_dict_input(self):\n        # gh-16905\n        constructor_dict = {1: 1.}\n        index = [0, 1, 2]\n\n        # Series with index passed in\n        series = pd.Series(constructor_dict)\n        expected = SparseSeries(series, index=index)\n\n        result = SparseSeries(constructor_dict, index=index)\n        tm.assert_sp_series_equal(result, expected)\n\n        # Series with index and dictionary with no index\n        expected = SparseSeries(series)\n\n        result = SparseSeries(constructor_dict)\n        tm.assert_sp_series_equal(result, expected)\n\n    def test_constructor_dtype(self):\n        arr = SparseSeries([np.nan, 1, 2, np.nan])\n        assert arr.dtype == np.float64\n        assert np.isnan(arr.fill_value)\n\n        arr = SparseSeries([np.nan, 1, 2, np.nan], fill_value=0)\n        assert arr.dtype == np.float64\n        assert arr.fill_value == 0\n\n        arr = SparseSeries([0, 1, 2, 4], dtype=np.int64, fill_value=np.nan)\n        assert arr.dtype == np.int64\n        assert np.isnan(arr.fill_value)\n\n        arr = SparseSeries([0, 1, 2, 4], dtype=np.int64)\n        assert arr.dtype == np.int64\n        assert arr.fill_value == 0\n\n        arr = SparseSeries([0, 1, 2, 4], fill_value=0, dtype=np.int64)\n        assert arr.dtype == np.int64\n        assert arr.fill_value == 0\n\n    def test_iteration_and_str(self):\n        [x for x in self.bseries]\n        str(self.bseries)\n\n    def test_construct_DataFrame_with_sp_series(self):\n        # it works!\n        df = DataFrame({'col': self.bseries})\n\n        # printing & access\n        df.iloc[:1]\n        df['col']\n        df.dtypes\n        str(df)\n\n        tm.assert_sp_series_equal(df['col'], self.bseries, check_names=False)\n\n        result = df.iloc[:, 0]\n        tm.assert_sp_series_equal(result, self.bseries, check_names=False)\n\n        # blocking\n        expected = Series({'col': 'float64:sparse'})\n        result = df.ftypes\n        tm.assert_series_equal(expected, result)\n\n    def test_constructor_preserve_attr(self):\n        arr = pd.SparseArray([1, 0, 3, 0], dtype=np.int64, fill_value=0)\n        assert arr.dtype == np.int64\n        assert arr.fill_value == 0\n\n        s = pd.SparseSeries(arr, name='x')\n        assert s.dtype == np.int64\n        assert s.fill_value == 0\n\n    def test_series_density(self):\n        # GH2803\n        ts = Series(np.random.randn(10))\n        ts[2:-2] = nan\n        sts = ts.to_sparse()\n        density = sts.density  # don't die\n        assert density == 4 / 10.0\n\n    def test_sparse_to_dense(self):\n        arr, index = _test_data1()\n        series = self.bseries.to_dense()\n        tm.assert_series_equal(series, Series(arr, name='bseries'))\n\n        # see gh-14647\n        with tm.assert_produces_warning(FutureWarning,\n                                        check_stacklevel=False):\n            series = self.bseries.to_dense(sparse_only=True)\n\n        indexer = np.isfinite(arr)\n        exp = Series(arr[indexer], index=index[indexer], name='bseries')\n        tm.assert_series_equal(series, exp)\n\n        series = self.iseries.to_dense()\n        tm.assert_series_equal(series, Series(arr, name='iseries'))\n\n        arr, index = _test_data1_zero()\n        series = self.zbseries.to_dense()\n        tm.assert_series_equal(series, Series(arr, name='zbseries'))\n\n        series = self.ziseries.to_dense()\n        tm.assert_series_equal(series, Series(arr))\n\n    def test_to_dense_fill_value(self):\n        s = pd.Series([1, np.nan, np.nan, 3, np.nan])\n        res = SparseSeries(s).to_dense()\n        tm.assert_series_equal(res, s)\n\n        res = SparseSeries(s, fill_value=0).to_dense()\n        tm.assert_series_equal(res, s)\n\n        s = pd.Series([1, np.nan, 0, 3, 0])\n        res = SparseSeries(s, fill_value=0).to_dense()\n        tm.assert_series_equal(res, s)\n\n        res = SparseSeries(s, fill_value=0).to_dense()\n        tm.assert_series_equal(res, s)\n\n        s = pd.Series([np.nan, np.nan, np.nan, np.nan, np.nan])\n        res = SparseSeries(s).to_dense()\n        tm.assert_series_equal(res, s)\n\n        s = pd.Series([np.nan, np.nan, np.nan, np.nan, np.nan])\n        res = SparseSeries(s, fill_value=0).to_dense()\n        tm.assert_series_equal(res, s)\n\n    def test_dense_to_sparse(self):\n        series = self.bseries.to_dense()\n        bseries = series.to_sparse(kind='block')\n        iseries = series.to_sparse(kind='integer')\n        tm.assert_sp_series_equal(bseries, self.bseries)\n        tm.assert_sp_series_equal(iseries, self.iseries, check_names=False)\n        assert iseries.name == self.bseries.name\n\n        assert len(series) == len(bseries)\n        assert len(series) == len(iseries)\n        assert series.shape == bseries.shape\n        assert series.shape == iseries.shape\n\n        # non-NaN fill value\n        series = self.zbseries.to_dense()\n        zbseries = series.to_sparse(kind='block', fill_value=0)\n        ziseries = series.to_sparse(kind='integer', fill_value=0)\n        tm.assert_sp_series_equal(zbseries, self.zbseries)\n        tm.assert_sp_series_equal(ziseries, self.ziseries, check_names=False)\n        assert ziseries.name == self.zbseries.name\n\n        assert len(series) == len(zbseries)\n        assert len(series) == len(ziseries)\n        assert series.shape == zbseries.shape\n        assert series.shape == ziseries.shape\n\n    def test_to_dense_preserve_name(self):\n        assert (self.bseries.name is not None)\n        result = self.bseries.to_dense()\n        assert result.name == self.bseries.name\n\n    def test_constructor(self):\n        # test setup guys\n        assert np.isnan(self.bseries.fill_value)\n        assert isinstance(self.bseries.sp_index, BlockIndex)\n        assert np.isnan(self.iseries.fill_value)\n        assert isinstance(self.iseries.sp_index, IntIndex)\n\n        assert self.zbseries.fill_value == 0\n        tm.assert_numpy_array_equal(self.zbseries.values.values,\n                                    self.bseries.to_dense().fillna(0).values)\n\n        # pass SparseSeries\n        def _check_const(sparse, name):\n            # use passed series name\n            result = SparseSeries(sparse)\n            tm.assert_sp_series_equal(result, sparse)\n            assert sparse.name == name\n            assert result.name == name\n\n            # use passed name\n            result = SparseSeries(sparse, name='x')\n            tm.assert_sp_series_equal(result, sparse, check_names=False)\n            assert result.name == 'x'\n\n        _check_const(self.bseries, 'bseries')\n        _check_const(self.iseries, 'iseries')\n        _check_const(self.zbseries, 'zbseries')\n\n        # Sparse time series works\n        date_index = bdate_range('1/1/2000', periods=len(self.bseries))\n        s5 = SparseSeries(self.bseries, index=date_index)\n        assert isinstance(s5, SparseSeries)\n\n        # pass Series\n        bseries2 = SparseSeries(self.bseries.to_dense())\n        tm.assert_numpy_array_equal(self.bseries.sp_values, bseries2.sp_values)\n\n        # pass dict?\n\n        # don't copy the data by default\n        values = np.ones(self.bseries.npoints)\n        sp = SparseSeries(values, sparse_index=self.bseries.sp_index)\n        sp.sp_values[:5] = 97\n        assert values[0] == 97\n\n        assert len(sp) == 20\n        assert sp.shape == (20, )\n\n        # but can make it copy!\n        sp = SparseSeries(values, sparse_index=self.bseries.sp_index,\n                          copy=True)\n        sp.sp_values[:5] = 100\n        assert values[0] == 97\n\n        assert len(sp) == 20\n        assert sp.shape == (20, )\n\n    def test_constructor_scalar(self):\n        data = 5\n        sp = SparseSeries(data, np.arange(100))\n        sp = sp.reindex(np.arange(200))\n        assert (sp.loc[:99] == data).all()\n        assert isna(sp.loc[100:]).all()\n\n        data = np.nan\n        sp = SparseSeries(data, np.arange(100))\n        assert len(sp) == 100\n        assert sp.shape == (100, )\n\n    def test_constructor_ndarray(self):\n        pass\n\n    def test_constructor_nonnan(self):\n        arr = [0, 0, 0, nan, nan]\n        sp_series = SparseSeries(arr, fill_value=0)\n        tm.assert_numpy_array_equal(sp_series.values.values, np.array(arr))\n        assert len(sp_series) == 5\n        assert sp_series.shape == (5, )\n\n    def test_constructor_empty(self):\n        # see gh-9272\n        sp = SparseSeries()\n        assert len(sp.index) == 0\n        assert sp.shape == (0, )\n\n    def test_copy_astype(self):\n        cop = self.bseries.astype(np.float64)\n        assert cop is not self.bseries\n        assert cop.sp_index is self.bseries.sp_index\n        assert cop.dtype == np.float64\n\n        cop2 = self.iseries.copy()\n\n        tm.assert_sp_series_equal(cop, self.bseries)\n        tm.assert_sp_series_equal(cop2, self.iseries)\n\n        # test that data is copied\n        cop[:5] = 97\n        assert cop.sp_values[0] == 97\n        assert self.bseries.sp_values[0] != 97\n\n        # correct fill value\n        zbcop = self.zbseries.copy()\n        zicop = self.ziseries.copy()\n\n        tm.assert_sp_series_equal(zbcop, self.zbseries)\n        tm.assert_sp_series_equal(zicop, self.ziseries)\n\n        # no deep copy\n        view = self.bseries.copy(deep=False)\n        view.sp_values[:5] = 5\n        assert (self.bseries.sp_values[:5] == 5).all()\n\n    def test_shape(self):\n        # see gh-10452\n        assert self.bseries.shape == (20, )\n        assert self.btseries.shape == (20, )\n        assert self.iseries.shape == (20, )\n\n        assert self.bseries2.shape == (15, )\n        assert self.iseries2.shape == (15, )\n\n        assert self.zbseries2.shape == (15, )\n        assert self.ziseries2.shape == (15, )\n\n    def test_astype(self):\n        with pytest.raises(ValueError):\n            self.bseries.astype(np.int64)\n\n    def test_astype_all(self):\n        orig = pd.Series(np.array([1, 2, 3]))\n        s = SparseSeries(orig)\n\n        types = [np.float64, np.float32, np.int64,\n                 np.int32, np.int16, np.int8]\n        for typ in types:\n            res = s.astype(typ)\n            assert res.dtype == typ\n            tm.assert_series_equal(res.to_dense(), orig.astype(typ))\n\n    def test_kind(self):\n        assert self.bseries.kind == 'block'\n        assert self.iseries.kind == 'integer'\n\n    def test_to_frame(self):\n        # GH 9850\n        s = pd.SparseSeries([1, 2, 0, nan, 4, nan, 0], name='x')\n        exp = pd.SparseDataFrame({'x': [1, 2, 0, nan, 4, nan, 0]})\n        tm.assert_sp_frame_equal(s.to_frame(), exp)\n\n        exp = pd.SparseDataFrame({'y': [1, 2, 0, nan, 4, nan, 0]})\n        tm.assert_sp_frame_equal(s.to_frame(name='y'), exp)\n\n        s = pd.SparseSeries([1, 2, 0, nan, 4, nan, 0], name='x', fill_value=0)\n        exp = pd.SparseDataFrame({'x': [1, 2, 0, nan, 4, nan, 0]},\n                                 default_fill_value=0)\n\n        tm.assert_sp_frame_equal(s.to_frame(), exp)\n        exp = pd.DataFrame({'y': [1, 2, 0, nan, 4, nan, 0]})\n        tm.assert_frame_equal(s.to_frame(name='y').to_dense(), exp)\n\n    def test_pickle(self):\n        def _test_roundtrip(series):\n            unpickled = tm.round_trip_pickle(series)\n            tm.assert_sp_series_equal(series, unpickled)\n            tm.assert_series_equal(series.to_dense(), unpickled.to_dense())\n\n        self._check_all(_test_roundtrip)\n\n    def _check_all(self, check_func):\n        check_func(self.bseries)\n        check_func(self.iseries)\n        check_func(self.zbseries)\n        check_func(self.ziseries)\n\n    def test_getitem(self):\n        def _check_getitem(sp, dense):\n            for idx, val in compat.iteritems(dense):\n                tm.assert_almost_equal(val, sp[idx])\n\n            for i in range(len(dense)):\n                tm.assert_almost_equal(sp[i], dense[i])\n                # j = np.float64(i)\n                # assert_almost_equal(sp[j], dense[j])\n\n                # API change 1/6/2012\n                # negative getitem works\n                # for i in xrange(len(dense)):\n                #     assert_almost_equal(sp[-i], dense[-i])\n\n        _check_getitem(self.bseries, self.bseries.to_dense())\n        _check_getitem(self.btseries, self.btseries.to_dense())\n\n        _check_getitem(self.zbseries, self.zbseries.to_dense())\n        _check_getitem(self.iseries, self.iseries.to_dense())\n        _check_getitem(self.ziseries, self.ziseries.to_dense())\n\n        # exception handling\n        pytest.raises(Exception, self.bseries.__getitem__,\n                      len(self.bseries) + 1)\n\n        # index not contained\n        pytest.raises(Exception, self.btseries.__getitem__,\n                      self.btseries.index[-1] + BDay())\n\n    def test_get_get_value(self):\n        tm.assert_almost_equal(self.bseries.get(10), self.bseries[10])\n        assert self.bseries.get(len(self.bseries) + 1) is None\n\n        dt = self.btseries.index[10]\n        result = self.btseries.get(dt)\n        expected = self.btseries.to_dense()[dt]\n        tm.assert_almost_equal(result, expected)\n\n        tm.assert_almost_equal(self.bseries.get_value(10), self.bseries[10])\n\n    def test_set_value(self):\n\n        idx = self.btseries.index[7]\n        self.btseries.set_value(idx, 0)\n        assert self.btseries[idx] == 0\n\n        self.iseries.set_value('foobar', 0)\n        assert self.iseries.index[-1] == 'foobar'\n        assert self.iseries['foobar'] == 0\n\n    def test_getitem_slice(self):\n        idx = self.bseries.index\n        res = self.bseries[::2]\n        assert isinstance(res, SparseSeries)\n\n        expected = self.bseries.reindex(idx[::2])\n        tm.assert_sp_series_equal(res, expected)\n\n        res = self.bseries[:5]\n        assert isinstance(res, SparseSeries)\n        tm.assert_sp_series_equal(res, self.bseries.reindex(idx[:5]))\n\n        res = self.bseries[5:]\n        tm.assert_sp_series_equal(res, self.bseries.reindex(idx[5:]))\n\n        # negative indices\n        res = self.bseries[:-3]\n        tm.assert_sp_series_equal(res, self.bseries.reindex(idx[:-3]))\n\n    def test_take(self):\n        def _compare_with_dense(sp):\n            dense = sp.to_dense()\n\n            def _compare(idx):\n                dense_result = dense.take(idx).values\n                sparse_result = sp.take(idx)\n                assert isinstance(sparse_result, SparseSeries)\n                tm.assert_almost_equal(dense_result,\n                                       sparse_result.values.values)\n\n            _compare([1., 2., 3., 4., 5., 0.])\n            _compare([7, 2, 9, 0, 4])\n            _compare([3, 6, 3, 4, 7])\n\n        self._check_all(_compare_with_dense)\n\n        pytest.raises(Exception, self.bseries.take,\n                      [0, len(self.bseries) + 1])\n\n        # Corner case\n        sp = SparseSeries(np.ones(10) * nan)\n        exp = pd.Series(np.repeat(nan, 5))\n        tm.assert_series_equal(sp.take([0, 1, 2, 3, 4]), exp)\n\n    def test_numpy_take(self):\n        sp = SparseSeries([1.0, 2.0, 3.0])\n        indices = [1, 2]\n\n        tm.assert_series_equal(np.take(sp, indices, axis=0).to_dense(),\n                               np.take(sp.to_dense(), indices, axis=0))\n\n        msg = \"the 'out' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.take,\n                               sp, indices, out=np.empty(sp.shape))\n\n        msg = \"the 'mode' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.take,\n                               sp, indices, mode='clip')\n\n    def test_setitem(self):\n        self.bseries[5] = 7.\n        assert self.bseries[5] == 7.\n\n    def test_setslice(self):\n        self.bseries[5:10] = 7.\n        tm.assert_series_equal(self.bseries[5:10].to_dense(),\n                               Series(7., index=range(5, 10),\n                                      name=self.bseries.name))\n\n    def test_operators(self):\n\n        def _check_op(a, b, op):\n            sp_result = op(a, b)\n            adense = a.to_dense() if isinstance(a, SparseSeries) else a\n            bdense = b.to_dense() if isinstance(b, SparseSeries) else b\n            dense_result = op(adense, bdense)\n            tm.assert_almost_equal(sp_result.to_dense(), dense_result)\n\n        def check(a, b):\n            _check_op(a, b, operator.add)\n            _check_op(a, b, operator.sub)\n            _check_op(a, b, operator.truediv)\n            _check_op(a, b, operator.floordiv)\n            _check_op(a, b, operator.mul)\n\n            _check_op(a, b, lambda x, y: operator.add(y, x))\n            _check_op(a, b, lambda x, y: operator.sub(y, x))\n            _check_op(a, b, lambda x, y: operator.truediv(y, x))\n            _check_op(a, b, lambda x, y: operator.floordiv(y, x))\n            _check_op(a, b, lambda x, y: operator.mul(y, x))\n\n            # NaN ** 0 = 1 in C?\n            # _check_op(a, b, operator.pow)\n            # _check_op(a, b, lambda x, y: operator.pow(y, x))\n\n        check(self.bseries, self.bseries)\n        check(self.iseries, self.iseries)\n        check(self.bseries, self.iseries)\n\n        check(self.bseries, self.bseries2)\n        check(self.bseries, self.iseries2)\n        check(self.iseries, self.iseries2)\n\n        # scalar value\n        check(self.bseries, 5)\n\n        # zero-based\n        check(self.zbseries, self.zbseries * 2)\n        check(self.zbseries, self.zbseries2)\n        check(self.ziseries, self.ziseries2)\n\n        # with dense\n        result = self.bseries + self.bseries.to_dense()\n        tm.assert_sp_series_equal(result, self.bseries + self.bseries)\n\n    def test_binary_operators(self):\n\n        # skipping for now #####\n        import pytest\n        pytest.skip(\"skipping sparse binary operators test\")\n\n        def _check_inplace_op(iop, op):\n            tmp = self.bseries.copy()\n\n            expected = op(tmp, self.bseries)\n            iop(tmp, self.bseries)\n            tm.assert_sp_series_equal(tmp, expected)\n\n        inplace_ops = ['add', 'sub', 'mul', 'truediv', 'floordiv', 'pow']\n        for op in inplace_ops:\n            _check_inplace_op(getattr(operator, \"i%s\" % op),\n                              getattr(operator, op))\n\n    def test_abs(self):\n        s = SparseSeries([1, 2, -3], name='x')\n        expected = SparseSeries([1, 2, 3], name='x')\n        result = s.abs()\n        tm.assert_sp_series_equal(result, expected)\n        assert result.name == 'x'\n\n        result = abs(s)\n        tm.assert_sp_series_equal(result, expected)\n        assert result.name == 'x'\n\n        result = np.abs(s)\n        tm.assert_sp_series_equal(result, expected)\n        assert result.name == 'x'\n\n        s = SparseSeries([1, -2, 2, -3], fill_value=-2, name='x')\n        expected = SparseSeries([1, 2, 3], sparse_index=s.sp_index,\n                                fill_value=2, name='x')\n        result = s.abs()\n        tm.assert_sp_series_equal(result, expected)\n        assert result.name == 'x'\n\n        result = abs(s)\n        tm.assert_sp_series_equal(result, expected)\n        assert result.name == 'x'\n\n        result = np.abs(s)\n        tm.assert_sp_series_equal(result, expected)\n        assert result.name == 'x'\n\n    def test_reindex(self):\n        def _compare_with_series(sps, new_index):\n            spsre = sps.reindex(new_index)\n\n            series = sps.to_dense()\n            seriesre = series.reindex(new_index)\n            seriesre = seriesre.to_sparse(fill_value=sps.fill_value)\n\n            tm.assert_sp_series_equal(spsre, seriesre)\n            tm.assert_series_equal(spsre.to_dense(), seriesre.to_dense())\n\n        _compare_with_series(self.bseries, self.bseries.index[::2])\n        _compare_with_series(self.bseries, list(self.bseries.index[::2]))\n        _compare_with_series(self.bseries, self.bseries.index[:10])\n        _compare_with_series(self.bseries, self.bseries.index[5:])\n\n        _compare_with_series(self.zbseries, self.zbseries.index[::2])\n        _compare_with_series(self.zbseries, self.zbseries.index[:10])\n        _compare_with_series(self.zbseries, self.zbseries.index[5:])\n\n        # special cases\n        same_index = self.bseries.reindex(self.bseries.index)\n        tm.assert_sp_series_equal(self.bseries, same_index)\n        assert same_index is not self.bseries\n\n        # corner cases\n        sp = SparseSeries([], index=[])\n        # TODO: sp_zero is not used anywhere...remove?\n        sp_zero = SparseSeries([], index=[], fill_value=0)  # noqa\n        _compare_with_series(sp, np.arange(10))\n\n        # with copy=False\n        reindexed = self.bseries.reindex(self.bseries.index, copy=True)\n        reindexed.sp_values[:] = 1.\n        assert (self.bseries.sp_values != 1.).all()\n\n        reindexed = self.bseries.reindex(self.bseries.index, copy=False)\n        reindexed.sp_values[:] = 1.\n        tm.assert_numpy_array_equal(self.bseries.sp_values, np.repeat(1., 10))\n\n    def test_sparse_reindex(self):\n        length = 10\n\n        def _check(values, index1, index2, fill_value):\n            first_series = SparseSeries(values, sparse_index=index1,\n                                        fill_value=fill_value)\n            reindexed = first_series.sparse_reindex(index2)\n            assert reindexed.sp_index is index2\n\n            int_indices1 = index1.to_int_index().indices\n            int_indices2 = index2.to_int_index().indices\n\n            expected = Series(values, index=int_indices1)\n            expected = expected.reindex(int_indices2).fillna(fill_value)\n            tm.assert_almost_equal(expected.values, reindexed.sp_values)\n\n            # make sure level argument asserts\n            # TODO: expected is not used anywhere...remove?\n            expected = expected.reindex(int_indices2).fillna(fill_value)  # noqa\n\n        def _check_with_fill_value(values, first, second, fill_value=nan):\n            i_index1 = IntIndex(length, first)\n            i_index2 = IntIndex(length, second)\n\n            b_index1 = i_index1.to_block_index()\n            b_index2 = i_index2.to_block_index()\n\n            _check(values, i_index1, i_index2, fill_value)\n            _check(values, b_index1, b_index2, fill_value)\n\n        def _check_all(values, first, second):\n            _check_with_fill_value(values, first, second, fill_value=nan)\n            _check_with_fill_value(values, first, second, fill_value=0)\n\n        index1 = [2, 4, 5, 6, 8, 9]\n        values1 = np.arange(6.)\n\n        _check_all(values1, index1, [2, 4, 5])\n        _check_all(values1, index1, [2, 3, 4, 5, 6, 7, 8, 9])\n        _check_all(values1, index1, [0, 1])\n        _check_all(values1, index1, [0, 1, 7, 8, 9])\n        _check_all(values1, index1, [])\n\n        first_series = SparseSeries(values1,\n                                    sparse_index=IntIndex(length, index1),\n                                    fill_value=nan)\n        with tm.assert_raises_regex(TypeError,\n                                    'new index must be a SparseIndex'):\n            reindexed = first_series.sparse_reindex(0)  # noqa\n\n    def test_repr(self):\n        # TODO: These aren't used\n        bsrepr = repr(self.bseries)  # noqa\n        isrepr = repr(self.iseries)  # noqa\n\n    def test_iter(self):\n        pass\n\n    def test_truncate(self):\n        pass\n\n    def test_fillna(self):\n        pass\n\n    def test_groupby(self):\n        pass\n\n    def test_reductions(self):\n        def _compare_with_dense(obj, op):\n            sparse_result = getattr(obj, op)()\n            series = obj.to_dense()\n            dense_result = getattr(series, op)()\n            assert sparse_result == dense_result\n\n        to_compare = ['count', 'sum', 'mean', 'std', 'var', 'skew']\n\n        def _compare_all(obj):\n            for op in to_compare:\n                _compare_with_dense(obj, op)\n\n        _compare_all(self.bseries)\n\n        self.bseries.sp_values[5:10] = np.NaN\n        _compare_all(self.bseries)\n\n        _compare_all(self.zbseries)\n        self.zbseries.sp_values[5:10] = np.NaN\n        _compare_all(self.zbseries)\n\n        series = self.zbseries.copy()\n        series.fill_value = 2\n        _compare_all(series)\n\n        nonna = Series(np.random.randn(20)).to_sparse()\n        _compare_all(nonna)\n\n        nonna2 = Series(np.random.randn(20)).to_sparse(fill_value=0)\n        _compare_all(nonna2)\n\n    def test_dropna(self):\n        sp = SparseSeries([0, 0, 0, nan, nan, 5, 6], fill_value=0)\n\n        sp_valid = sp.valid()\n\n        expected = sp.to_dense().valid()\n        expected = expected[expected != 0]\n        exp_arr = pd.SparseArray(expected.values, fill_value=0, kind='block')\n        tm.assert_sp_array_equal(sp_valid.values, exp_arr)\n        tm.assert_index_equal(sp_valid.index, expected.index)\n        assert len(sp_valid.sp_values) == 2\n\n        result = self.bseries.dropna()\n        expected = self.bseries.to_dense().dropna()\n        assert not isinstance(result, SparseSeries)\n        tm.assert_series_equal(result, expected)\n\n    def test_homogenize(self):\n        def _check_matches(indices, expected):\n            data = {}\n            for i, idx in enumerate(indices):\n                data[i] = SparseSeries(idx.to_int_index().indices,\n                                       sparse_index=idx, fill_value=np.nan)\n            # homogenized is only valid with NaN fill values\n            homogenized = spf.homogenize(data)\n\n            for k, v in compat.iteritems(homogenized):\n                assert (v.sp_index.equals(expected))\n\n        indices1 = [BlockIndex(10, [2], [7]), BlockIndex(10, [1, 6], [3, 4]),\n                    BlockIndex(10, [0], [10])]\n        expected1 = BlockIndex(10, [2, 6], [2, 3])\n        _check_matches(indices1, expected1)\n\n        indices2 = [BlockIndex(10, [2], [7]), BlockIndex(10, [2], [7])]\n        expected2 = indices2[0]\n        _check_matches(indices2, expected2)\n\n        # must have NaN fill value\n        data = {'a': SparseSeries(np.arange(7), sparse_index=expected2,\n                                  fill_value=0)}\n        with tm.assert_raises_regex(TypeError, \"NaN fill value\"):\n            spf.homogenize(data)\n\n    def test_fill_value_corner(self):\n        cop = self.zbseries.copy()\n        cop.fill_value = 0\n        result = self.bseries / cop\n\n        assert np.isnan(result.fill_value)\n\n        cop2 = self.zbseries.copy()\n        cop2.fill_value = 1\n        result = cop2 / cop\n        # 1 / 0 is inf\n        assert np.isinf(result.fill_value)\n\n    def test_fill_value_when_combine_const(self):\n        # GH12723\n        s = SparseSeries([0, 1, np.nan, 3, 4, 5], index=np.arange(6))\n\n        exp = s.fillna(0).add(2)\n        res = s.add(2, fill_value=0)\n        tm.assert_series_equal(res, exp)\n\n    def test_shift(self):\n        series = SparseSeries([nan, 1., 2., 3., nan, nan], index=np.arange(6))\n\n        shifted = series.shift(0)\n        assert shifted is not series\n        tm.assert_sp_series_equal(shifted, series)\n\n        f = lambda s: s.shift(1)\n        _dense_series_compare(series, f)\n\n        f = lambda s: s.shift(-2)\n        _dense_series_compare(series, f)\n\n        series = SparseSeries([nan, 1., 2., 3., nan, nan],\n                              index=bdate_range('1/1/2000', periods=6))\n        f = lambda s: s.shift(2, freq='B')\n        _dense_series_compare(series, f)\n\n        f = lambda s: s.shift(2, freq=BDay())\n        _dense_series_compare(series, f)\n\n    def test_shift_nan(self):\n        # GH 12908\n        orig = pd.Series([np.nan, 2, np.nan, 4, 0, np.nan, 0])\n        sparse = orig.to_sparse()\n\n        tm.assert_sp_series_equal(sparse.shift(0), orig.shift(0).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(1), orig.shift(1).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(2), orig.shift(2).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(3), orig.shift(3).to_sparse())\n\n        tm.assert_sp_series_equal(sparse.shift(-1), orig.shift(-1).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(-2), orig.shift(-2).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(-3), orig.shift(-3).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(-4), orig.shift(-4).to_sparse())\n\n        sparse = orig.to_sparse(fill_value=0)\n        tm.assert_sp_series_equal(sparse.shift(0),\n                                  orig.shift(0).to_sparse(fill_value=0))\n        tm.assert_sp_series_equal(sparse.shift(1),\n                                  orig.shift(1).to_sparse(fill_value=0))\n        tm.assert_sp_series_equal(sparse.shift(2),\n                                  orig.shift(2).to_sparse(fill_value=0))\n        tm.assert_sp_series_equal(sparse.shift(3),\n                                  orig.shift(3).to_sparse(fill_value=0))\n\n        tm.assert_sp_series_equal(sparse.shift(-1),\n                                  orig.shift(-1).to_sparse(fill_value=0))\n        tm.assert_sp_series_equal(sparse.shift(-2),\n                                  orig.shift(-2).to_sparse(fill_value=0))\n        tm.assert_sp_series_equal(sparse.shift(-3),\n                                  orig.shift(-3).to_sparse(fill_value=0))\n        tm.assert_sp_series_equal(sparse.shift(-4),\n                                  orig.shift(-4).to_sparse(fill_value=0))\n\n    def test_shift_dtype(self):\n        # GH 12908\n        orig = pd.Series([1, 2, 3, 4], dtype=np.int64)\n\n        sparse = orig.to_sparse()\n        tm.assert_sp_series_equal(sparse.shift(0), orig.shift(0).to_sparse())\n\n        sparse = orig.to_sparse(fill_value=np.nan)\n        tm.assert_sp_series_equal(sparse.shift(0),\n                                  orig.shift(0).to_sparse(fill_value=np.nan))\n        # shift(1) or more span changes dtype to float64\n        tm.assert_sp_series_equal(sparse.shift(1), orig.shift(1).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(2), orig.shift(2).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(3), orig.shift(3).to_sparse())\n\n        tm.assert_sp_series_equal(sparse.shift(-1), orig.shift(-1).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(-2), orig.shift(-2).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(-3), orig.shift(-3).to_sparse())\n        tm.assert_sp_series_equal(sparse.shift(-4), orig.shift(-4).to_sparse())\n\n    def test_shift_dtype_fill_value(self):\n        # GH 12908\n        orig = pd.Series([1, 0, 0, 4], dtype=np.int64)\n\n        for v in [0, 1, np.nan]:\n            sparse = orig.to_sparse(fill_value=v)\n\n            tm.assert_sp_series_equal(sparse.shift(0),\n                                      orig.shift(0).to_sparse(fill_value=v))\n            tm.assert_sp_series_equal(sparse.shift(1),\n                                      orig.shift(1).to_sparse(fill_value=v))\n            tm.assert_sp_series_equal(sparse.shift(2),\n                                      orig.shift(2).to_sparse(fill_value=v))\n            tm.assert_sp_series_equal(sparse.shift(3),\n                                      orig.shift(3).to_sparse(fill_value=v))\n\n            tm.assert_sp_series_equal(sparse.shift(-1),\n                                      orig.shift(-1).to_sparse(fill_value=v))\n            tm.assert_sp_series_equal(sparse.shift(-2),\n                                      orig.shift(-2).to_sparse(fill_value=v))\n            tm.assert_sp_series_equal(sparse.shift(-3),\n                                      orig.shift(-3).to_sparse(fill_value=v))\n            tm.assert_sp_series_equal(sparse.shift(-4),\n                                      orig.shift(-4).to_sparse(fill_value=v))\n\n    def test_combine_first(self):\n        s = self.bseries\n\n        result = s[::2].combine_first(s)\n        result2 = s[::2].combine_first(s.to_dense())\n\n        expected = s[::2].to_dense().combine_first(s.to_dense())\n        expected = expected.to_sparse(fill_value=s.fill_value)\n\n        tm.assert_sp_series_equal(result, result2)\n        tm.assert_sp_series_equal(result, expected)\n\n\nclass TestSparseHandlingMultiIndexes(object):\n\n    def setup_method(self, method):\n        miindex = pd.MultiIndex.from_product(\n            [[\"x\", \"y\"], [\"10\", \"20\"]], names=['row-foo', 'row-bar'])\n        micol = pd.MultiIndex.from_product(\n            [['a', 'b', 'c'], [\"1\", \"2\"]], names=['col-foo', 'col-bar'])\n        dense_multiindex_frame = pd.DataFrame(\n            index=miindex, columns=micol).sort_index().sort_index(axis=1)\n        self.dense_multiindex_frame = dense_multiindex_frame.fillna(value=3.14)\n\n    def test_to_sparse_preserve_multiindex_names_columns(self):\n        sparse_multiindex_frame = self.dense_multiindex_frame.to_sparse()\n        sparse_multiindex_frame = sparse_multiindex_frame.copy()\n        tm.assert_index_equal(sparse_multiindex_frame.columns,\n                              self.dense_multiindex_frame.columns)\n\n    def test_round_trip_preserve_multiindex_names(self):\n        sparse_multiindex_frame = self.dense_multiindex_frame.to_sparse()\n        round_trip_multiindex_frame = sparse_multiindex_frame.to_dense()\n        tm.assert_frame_equal(self.dense_multiindex_frame,\n                              round_trip_multiindex_frame,\n                              check_column_type=True,\n                              check_names=True)\n\n\nclass TestSparseSeriesScipyInteraction(object):\n    # Issue 8048: add SparseSeries coo methods\n\n    def setup_method(self, method):\n        tm._skip_if_no_scipy()\n        import scipy.sparse\n        # SparseSeries inputs used in tests, the tests rely on the order\n        self.sparse_series = []\n        s = pd.Series([3.0, nan, 1.0, 2.0, nan, nan])\n        s.index = pd.MultiIndex.from_tuples([(1, 2, 'a', 0),\n                                             (1, 2, 'a', 1),\n                                             (1, 1, 'b', 0),\n                                             (1, 1, 'b', 1),\n                                             (2, 1, 'b', 0),\n                                             (2, 1, 'b', 1)],\n                                            names=['A', 'B', 'C', 'D'])\n        self.sparse_series.append(s.to_sparse())\n\n        ss = self.sparse_series[0].copy()\n        ss.index.names = [3, 0, 1, 2]\n        self.sparse_series.append(ss)\n\n        ss = pd.Series([\n            nan\n        ] * 12, index=cartesian_product((range(3), range(4)))).to_sparse()\n        for k, v in zip([(0, 0), (1, 2), (1, 3)], [3.0, 1.0, 2.0]):\n            ss[k] = v\n        self.sparse_series.append(ss)\n\n        # results used in tests\n        self.coo_matrices = []\n        self.coo_matrices.append(scipy.sparse.coo_matrix(\n            ([3.0, 1.0, 2.0], ([0, 1, 1], [0, 2, 3])), shape=(3, 4)))\n        self.coo_matrices.append(scipy.sparse.coo_matrix(\n            ([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(3, 4)))\n        self.coo_matrices.append(scipy.sparse.coo_matrix(\n            ([3.0, 1.0, 2.0], ([0, 1, 1], [0, 0, 1])), shape=(3, 2)))\n        self.ils = [[(1, 2), (1, 1), (2, 1)], [(1, 1), (1, 2), (2, 1)],\n                    [(1, 2, 'a'), (1, 1, 'b'), (2, 1, 'b')]]\n        self.jls = [[('a', 0), ('a', 1), ('b', 0), ('b', 1)], [0, 1]]\n\n    def test_to_coo_text_names_integer_row_levels_nosort(self):\n        ss = self.sparse_series[0]\n        kwargs = {'row_levels': [0, 1], 'column_levels': [2, 3]}\n        result = (self.coo_matrices[0], self.ils[0], self.jls[0])\n        self._run_test(ss, kwargs, result)\n\n    def test_to_coo_text_names_integer_row_levels_sort(self):\n        ss = self.sparse_series[0]\n        kwargs = {'row_levels': [0, 1],\n                  'column_levels': [2, 3],\n                  'sort_labels': True}\n        result = (self.coo_matrices[1], self.ils[1], self.jls[0])\n        self._run_test(ss, kwargs, result)\n\n    def test_to_coo_text_names_text_row_levels_nosort_col_level_single(self):\n        ss = self.sparse_series[0]\n        kwargs = {'row_levels': ['A', 'B', 'C'],\n                  'column_levels': ['D'],\n                  'sort_labels': False}\n        result = (self.coo_matrices[2], self.ils[2], self.jls[1])\n        self._run_test(ss, kwargs, result)\n\n    def test_to_coo_integer_names_integer_row_levels_nosort(self):\n        ss = self.sparse_series[1]\n        kwargs = {'row_levels': [3, 0], 'column_levels': [1, 2]}\n        result = (self.coo_matrices[0], self.ils[0], self.jls[0])\n        self._run_test(ss, kwargs, result)\n\n    def test_to_coo_text_names_text_row_levels_nosort(self):\n        ss = self.sparse_series[0]\n        kwargs = {'row_levels': ['A', 'B'], 'column_levels': ['C', 'D']}\n        result = (self.coo_matrices[0], self.ils[0], self.jls[0])\n        self._run_test(ss, kwargs, result)\n\n    def test_to_coo_bad_partition_nonnull_intersection(self):\n        ss = self.sparse_series[0]\n        pytest.raises(ValueError, ss.to_coo, ['A', 'B', 'C'], ['C', 'D'])\n\n    def test_to_coo_bad_partition_small_union(self):\n        ss = self.sparse_series[0]\n        pytest.raises(ValueError, ss.to_coo, ['A'], ['C', 'D'])\n\n    def test_to_coo_nlevels_less_than_two(self):\n        ss = self.sparse_series[0]\n        ss.index = np.arange(len(ss.index))\n        pytest.raises(ValueError, ss.to_coo)\n\n    def test_to_coo_bad_ilevel(self):\n        ss = self.sparse_series[0]\n        pytest.raises(KeyError, ss.to_coo, ['A', 'B'], ['C', 'D', 'E'])\n\n    def test_to_coo_duplicate_index_entries(self):\n        ss = pd.concat([self.sparse_series[0],\n                        self.sparse_series[0]]).to_sparse()\n        pytest.raises(ValueError, ss.to_coo, ['A', 'B'], ['C', 'D'])\n\n    def test_from_coo_dense_index(self):\n        ss = SparseSeries.from_coo(self.coo_matrices[0], dense_index=True)\n        check = self.sparse_series[2]\n        tm.assert_sp_series_equal(ss, check)\n\n    def test_from_coo_nodense_index(self):\n        ss = SparseSeries.from_coo(self.coo_matrices[0], dense_index=False)\n        check = self.sparse_series[2]\n        check = check.dropna().to_sparse()\n        tm.assert_sp_series_equal(ss, check)\n\n    def test_from_coo_long_repr(self):\n        # GH 13114\n        # test it doesn't raise error. Formatting is tested in test_format\n        tm._skip_if_no_scipy()\n        import scipy.sparse\n\n        sparse = SparseSeries.from_coo(scipy.sparse.rand(350, 18))\n        repr(sparse)\n\n    def _run_test(self, ss, kwargs, check):\n        results = ss.to_coo(**kwargs)\n        self._check_results_to_coo(results, check)\n        # for every test, also test symmetry property (transpose), switch\n        # row_levels and column_levels\n        d = kwargs.copy()\n        d['row_levels'] = kwargs['column_levels']\n        d['column_levels'] = kwargs['row_levels']\n        results = ss.to_coo(**d)\n        results = (results[0].T, results[2], results[1])\n        self._check_results_to_coo(results, check)\n\n    def _check_results_to_coo(self, results, check):\n        (A, il, jl) = results\n        (A_result, il_result, jl_result) = check\n        # convert to dense and compare\n        tm.assert_numpy_array_equal(A.todense(), A_result.todense())\n        # or compare directly as difference of sparse\n        # assert(abs(A - A_result).max() < 1e-12) # max is failing in python\n        # 2.6\n        assert il == il_result\n        assert jl == jl_result\n\n    def test_concat(self):\n        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])\n        val2 = np.array([3, np.nan, 4, 0, 0])\n\n        for kind in ['integer', 'block']:\n            sparse1 = pd.SparseSeries(val1, name='x', kind=kind)\n            sparse2 = pd.SparseSeries(val2, name='y', kind=kind)\n\n            res = pd.concat([sparse1, sparse2])\n            exp = pd.concat([pd.Series(val1), pd.Series(val2)])\n            exp = pd.SparseSeries(exp, kind=kind)\n            tm.assert_sp_series_equal(res, exp)\n\n            sparse1 = pd.SparseSeries(val1, fill_value=0, name='x', kind=kind)\n            sparse2 = pd.SparseSeries(val2, fill_value=0, name='y', kind=kind)\n\n            res = pd.concat([sparse1, sparse2])\n            exp = pd.concat([pd.Series(val1), pd.Series(val2)])\n            exp = pd.SparseSeries(exp, fill_value=0, kind=kind)\n            tm.assert_sp_series_equal(res, exp)\n\n    def test_concat_axis1(self):\n        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])\n        val2 = np.array([3, np.nan, 4, 0, 0])\n\n        sparse1 = pd.SparseSeries(val1, name='x')\n        sparse2 = pd.SparseSeries(val2, name='y')\n\n        res = pd.concat([sparse1, sparse2], axis=1)\n        exp = pd.concat([pd.Series(val1, name='x'),\n                         pd.Series(val2, name='y')], axis=1)\n        exp = pd.SparseDataFrame(exp)\n        tm.assert_sp_frame_equal(res, exp)\n\n    def test_concat_different_fill(self):\n        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])\n        val2 = np.array([3, np.nan, 4, 0, 0])\n\n        for kind in ['integer', 'block']:\n            sparse1 = pd.SparseSeries(val1, name='x', kind=kind)\n            sparse2 = pd.SparseSeries(val2, name='y', kind=kind, fill_value=0)\n\n            res = pd.concat([sparse1, sparse2])\n            exp = pd.concat([pd.Series(val1), pd.Series(val2)])\n            exp = pd.SparseSeries(exp, kind=kind)\n            tm.assert_sp_series_equal(res, exp)\n\n            res = pd.concat([sparse2, sparse1])\n            exp = pd.concat([pd.Series(val2), pd.Series(val1)])\n            exp = pd.SparseSeries(exp, kind=kind, fill_value=0)\n            tm.assert_sp_series_equal(res, exp)\n\n    def test_concat_axis1_different_fill(self):\n        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])\n        val2 = np.array([3, np.nan, 4, 0, 0])\n\n        sparse1 = pd.SparseSeries(val1, name='x')\n        sparse2 = pd.SparseSeries(val2, name='y', fill_value=0)\n\n        res = pd.concat([sparse1, sparse2], axis=1)\n        exp = pd.concat([pd.Series(val1, name='x'),\n                         pd.Series(val2, name='y')], axis=1)\n        assert isinstance(res, pd.SparseDataFrame)\n        tm.assert_frame_equal(res.to_dense(), exp)\n\n    def test_concat_different_kind(self):\n        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])\n        val2 = np.array([3, np.nan, 4, 0, 0])\n\n        sparse1 = pd.SparseSeries(val1, name='x', kind='integer')\n        sparse2 = pd.SparseSeries(val2, name='y', kind='block', fill_value=0)\n\n        res = pd.concat([sparse1, sparse2])\n        exp = pd.concat([pd.Series(val1), pd.Series(val2)])\n        exp = pd.SparseSeries(exp, kind='integer')\n        tm.assert_sp_series_equal(res, exp)\n\n        res = pd.concat([sparse2, sparse1])\n        exp = pd.concat([pd.Series(val2), pd.Series(val1)])\n        exp = pd.SparseSeries(exp, kind='block', fill_value=0)\n        tm.assert_sp_series_equal(res, exp)\n\n    def test_concat_sparse_dense(self):\n        # use first input's fill_value\n        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])\n        val2 = np.array([3, np.nan, 4, 0, 0])\n\n        for kind in ['integer', 'block']:\n            sparse = pd.SparseSeries(val1, name='x', kind=kind)\n            dense = pd.Series(val2, name='y')\n\n            res = pd.concat([sparse, dense])\n            exp = pd.concat([pd.Series(val1), dense])\n            exp = pd.SparseSeries(exp, kind=kind)\n            tm.assert_sp_series_equal(res, exp)\n\n            res = pd.concat([dense, sparse, dense])\n            exp = pd.concat([dense, pd.Series(val1), dense])\n            exp = pd.SparseSeries(exp, kind=kind)\n            tm.assert_sp_series_equal(res, exp)\n\n            sparse = pd.SparseSeries(val1, name='x', kind=kind, fill_value=0)\n            dense = pd.Series(val2, name='y')\n\n            res = pd.concat([sparse, dense])\n            exp = pd.concat([pd.Series(val1), dense])\n            exp = pd.SparseSeries(exp, kind=kind, fill_value=0)\n            tm.assert_sp_series_equal(res, exp)\n\n            res = pd.concat([dense, sparse, dense])\n            exp = pd.concat([dense, pd.Series(val1), dense])\n            exp = pd.SparseSeries(exp, kind=kind, fill_value=0)\n            tm.assert_sp_series_equal(res, exp)\n\n    def test_value_counts(self):\n        vals = [1, 2, nan, 0, nan, 1, 2, nan, nan, 1, 2, 0, 1, 1]\n        dense = pd.Series(vals, name='xx')\n\n        sparse = pd.SparseSeries(vals, name='xx')\n        tm.assert_series_equal(sparse.value_counts(),\n                               dense.value_counts())\n        tm.assert_series_equal(sparse.value_counts(dropna=False),\n                               dense.value_counts(dropna=False))\n\n        sparse = pd.SparseSeries(vals, name='xx', fill_value=0)\n        tm.assert_series_equal(sparse.value_counts(),\n                               dense.value_counts())\n        tm.assert_series_equal(sparse.value_counts(dropna=False),\n                               dense.value_counts(dropna=False))\n\n    def test_value_counts_dup(self):\n        vals = [1, 2, nan, 0, nan, 1, 2, nan, nan, 1, 2, 0, 1, 1]\n\n        # numeric op may cause sp_values to include the same value as\n        # fill_value\n        dense = pd.Series(vals, name='xx') / 0.\n        sparse = pd.SparseSeries(vals, name='xx') / 0.\n        tm.assert_series_equal(sparse.value_counts(),\n                               dense.value_counts())\n        tm.assert_series_equal(sparse.value_counts(dropna=False),\n                               dense.value_counts(dropna=False))\n\n        vals = [1, 2, 0, 0, 0, 1, 2, 0, 0, 1, 2, 0, 1, 1]\n\n        dense = pd.Series(vals, name='xx') * 0.\n        sparse = pd.SparseSeries(vals, name='xx') * 0.\n        tm.assert_series_equal(sparse.value_counts(),\n                               dense.value_counts())\n        tm.assert_series_equal(sparse.value_counts(dropna=False),\n                               dense.value_counts(dropna=False))\n\n    def test_value_counts_int(self):\n        vals = [1, 2, 0, 1, 2, 1, 2, 0, 1, 1]\n        dense = pd.Series(vals, name='xx')\n\n        # fill_value is np.nan, but should not be included in the result\n        sparse = pd.SparseSeries(vals, name='xx')\n        tm.assert_series_equal(sparse.value_counts(),\n                               dense.value_counts())\n        tm.assert_series_equal(sparse.value_counts(dropna=False),\n                               dense.value_counts(dropna=False))\n\n        sparse = pd.SparseSeries(vals, name='xx', fill_value=0)\n        tm.assert_series_equal(sparse.value_counts(),\n                               dense.value_counts())\n        tm.assert_series_equal(sparse.value_counts(dropna=False),\n                               dense.value_counts(dropna=False))\n\n    def test_isna(self):\n        # GH 8276\n        s = pd.SparseSeries([np.nan, np.nan, 1, 2, np.nan], name='xxx')\n\n        res = s.isna()\n        exp = pd.SparseSeries([True, True, False, False, True], name='xxx',\n                              fill_value=True)\n        tm.assert_sp_series_equal(res, exp)\n\n        # if fill_value is not nan, True can be included in sp_values\n        s = pd.SparseSeries([np.nan, 0., 1., 2., 0.], name='xxx',\n                            fill_value=0.)\n        res = s.isna()\n        assert isinstance(res, pd.SparseSeries)\n        exp = pd.Series([True, False, False, False, False], name='xxx')\n        tm.assert_series_equal(res.to_dense(), exp)\n\n    def test_notna(self):\n        # GH 8276\n        s = pd.SparseSeries([np.nan, np.nan, 1, 2, np.nan], name='xxx')\n\n        res = s.notna()\n        exp = pd.SparseSeries([False, False, True, True, False], name='xxx',\n                              fill_value=False)\n        tm.assert_sp_series_equal(res, exp)\n\n        # if fill_value is not nan, True can be included in sp_values\n        s = pd.SparseSeries([np.nan, 0., 1., 2., 0.], name='xxx',\n                            fill_value=0.)\n        res = s.notna()\n        assert isinstance(res, pd.SparseSeries)\n        exp = pd.Series([False, True, True, True, True], name='xxx')\n        tm.assert_series_equal(res.to_dense(), exp)\n\n\ndef _dense_series_compare(s, f):\n    result = f(s)\n    assert (isinstance(result, SparseSeries))\n    dense_result = f(s.to_dense())\n    tm.assert_series_equal(result.to_dense(), dense_result)\n\n\nclass TestSparseSeriesAnalytics(object):\n\n    def setup_method(self, method):\n        arr, index = _test_data1()\n        self.bseries = SparseSeries(arr, index=index, kind='block',\n                                    name='bseries')\n\n        arr, index = _test_data1_zero()\n        self.zbseries = SparseSeries(arr, index=index, kind='block',\n                                     fill_value=0, name='zbseries')\n\n    def test_cumsum(self):\n        result = self.bseries.cumsum()\n        expected = SparseSeries(self.bseries.to_dense().cumsum())\n        tm.assert_sp_series_equal(result, expected)\n\n        result = self.zbseries.cumsum()\n        expected = self.zbseries.to_dense().cumsum()\n        tm.assert_series_equal(result, expected)\n\n        axis = 1  # Series is 1-D, so only axis = 0 is valid.\n        msg = \"No axis named {axis}\".format(axis=axis)\n        with tm.assert_raises_regex(ValueError, msg):\n            self.bseries.cumsum(axis=axis)\n\n    def test_numpy_cumsum(self):\n        result = np.cumsum(self.bseries)\n        expected = SparseSeries(self.bseries.to_dense().cumsum())\n        tm.assert_sp_series_equal(result, expected)\n\n        result = np.cumsum(self.zbseries)\n        expected = self.zbseries.to_dense().cumsum()\n        tm.assert_series_equal(result, expected)\n\n        msg = \"the 'dtype' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.cumsum,\n                               self.bseries, dtype=np.int64)\n\n        msg = \"the 'out' parameter is not supported\"\n        tm.assert_raises_regex(ValueError, msg, np.cumsum,\n                               self.zbseries, out=result)\n\n    def test_numpy_func_call(self):\n        # no exception should be raised even though\n        # numpy passes in 'axis=None' or `axis=-1'\n        funcs = ['sum', 'cumsum', 'var', 'mean',\n                 'prod', 'cumprod', 'std', 'argsort',\n                 'min', 'max']\n        for func in funcs:\n            for series in ('bseries', 'zbseries'):\n                getattr(np, func)(getattr(self, series))\n\n    def test_deprecated_numpy_func_call(self):\n        # NOTE: These should be add to the 'test_numpy_func_call' test above\n        # once the behavior of argmin/argmax is corrected.\n        funcs = ['argmin', 'argmax']\n        for func in funcs:\n            for series in ('bseries', 'zbseries'):\n                with tm.assert_produces_warning(FutureWarning,\n                                                check_stacklevel=False):\n                    getattr(np, func)(getattr(self, series))\n\n                with tm.assert_produces_warning(FutureWarning,\n                                                check_stacklevel=False):\n                    getattr(getattr(self, series), func)()\n\n\n@pytest.mark.parametrize(\n    'datetime_type', (np.datetime64,\n                      pd.Timestamp,\n                      lambda x: datetime.strptime(x, '%Y-%m-%d')))\ndef test_constructor_dict_datetime64_index(datetime_type):\n    # GH 9456\n    dates = ['1984-02-19', '1988-11-06', '1989-12-03', '1990-03-15']\n    values = [42544017.198965244, 1234565, 40512335.181958228, -1]\n\n    result = SparseSeries(dict(zip(map(datetime_type, dates), values)))\n    expected = SparseSeries(values, map(pd.Timestamp, dates))\n\n    tm.assert_sp_series_equal(result, expected)\n"
    },
    {
      "filename": "pandas/util/_decorators.py",
      "content": "from pandas.compat import callable, signature\nfrom pandas._libs.properties import cache_readonly  # noqa\nimport types\nimport warnings\nfrom textwrap import dedent\nfrom functools import wraps, update_wrapper\n\n\ndef deprecate(name, alternative, alt_name=None, klass=None,\n              stacklevel=2, msg=None):\n    \"\"\"\n    Return a new function that emits a deprecation warning on use.\n\n    Parameters\n    ----------\n    name : str\n        Name of function to deprecate\n    alternative : str\n        Name of function to use instead\n    alt_name : str, optional\n        Name to use in preference of alternative.__name__\n    klass : Warning, default FutureWarning\n    stacklevel : int, default 2\n    msg : str\n          The message to display in the warning.\n          Default is '{name} is deprecated. Use {alt_name} instead.'\n    \"\"\"\n\n    alt_name = alt_name or alternative.__name__\n    klass = klass or FutureWarning\n    msg = msg or \"{} is deprecated. Use {} instead\".format(name, alt_name)\n\n    def wrapper(*args, **kwargs):\n        warnings.warn(msg, klass, stacklevel=stacklevel)\n        return alternative(*args, **kwargs)\n    return wrapper\n\n\ndef deprecate_kwarg(old_arg_name, new_arg_name, mapping=None, stacklevel=2):\n    \"\"\"\n    Decorator to deprecate a keyword argument of a function.\n\n    Parameters\n    ----------\n    old_arg_name : str\n        Name of argument in function to deprecate\n    new_arg_name : str\n        Name of preferred argument in function\n    mapping : dict or callable\n        If mapping is present, use it to translate old arguments to\n        new arguments. A callable must do its own value checking;\n        values not found in a dict will be forwarded unchanged.\n\n    Examples\n    --------\n    The following deprecates 'cols', using 'columns' instead\n\n    >>> @deprecate_kwarg(old_arg_name='cols', new_arg_name='columns')\n    ... def f(columns=''):\n    ...     print(columns)\n    ...\n    >>> f(columns='should work ok')\n    should work ok\n    >>> f(cols='should raise warning')\n    FutureWarning: cols is deprecated, use columns instead\n      warnings.warn(msg, FutureWarning)\n    should raise warning\n    >>> f(cols='should error', columns=\"can\\'t pass do both\")\n    TypeError: Can only specify 'cols' or 'columns', not both\n    >>> @deprecate_kwarg('old', 'new', {'yes': True, 'no': False})\n    ... def f(new=False):\n    ...     print('yes!' if new else 'no!')\n    ...\n    >>> f(old='yes')\n    FutureWarning: old='yes' is deprecated, use new=True instead\n      warnings.warn(msg, FutureWarning)\n    yes!\n    \"\"\"\n\n    if mapping is not None and not hasattr(mapping, 'get') and \\\n            not callable(mapping):\n        raise TypeError(\"mapping from old to new argument values \"\n                        \"must be dict or callable!\")\n\n    def _deprecate_kwarg(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            old_arg_value = kwargs.pop(old_arg_name, None)\n            if old_arg_value is not None:\n                if mapping is not None:\n                    if hasattr(mapping, 'get'):\n                        new_arg_value = mapping.get(old_arg_value,\n                                                    old_arg_value)\n                    else:\n                        new_arg_value = mapping(old_arg_value)\n                    msg = (\"the {old_name}={old_val!r} keyword is deprecated, \"\n                           \"use {new_name}={new_val!r} instead\"\n                           ).format(old_name=old_arg_name,\n                                    old_val=old_arg_value,\n                                    new_name=new_arg_name,\n                                    new_val=new_arg_value)\n                else:\n                    new_arg_value = old_arg_value\n                    msg = (\"the '{old_name}' keyword is deprecated, \"\n                           \"use '{new_name}' instead\"\n                           ).format(old_name=old_arg_name,\n                                    new_name=new_arg_name)\n\n                warnings.warn(msg, FutureWarning, stacklevel=stacklevel)\n                if kwargs.get(new_arg_name, None) is not None:\n                    msg = (\"Can only specify '{old_name}' or '{new_name}', \"\n                           \"not both\").format(old_name=old_arg_name,\n                                              new_name=new_arg_name)\n                    raise TypeError(msg)\n                else:\n                    kwargs[new_arg_name] = new_arg_value\n            return func(*args, **kwargs)\n        return wrapper\n    return _deprecate_kwarg\n\n\n# Substitution and Appender are derived from matplotlib.docstring (1.1.0)\n# module http://matplotlib.org/users/license.html\n\n\nclass Substitution(object):\n    \"\"\"\n    A decorator to take a function's docstring and perform string\n    substitution on it.\n\n    This decorator should be robust even if func.__doc__ is None\n    (for example, if -OO was passed to the interpreter)\n\n    Usage: construct a docstring.Substitution with a sequence or\n    dictionary suitable for performing substitution; then\n    decorate a suitable function with the constructed object. e.g.\n\n    sub_author_name = Substitution(author='Jason')\n\n    @sub_author_name\n    def some_function(x):\n        \"%(author)s wrote this function\"\n\n    # note that some_function.__doc__ is now \"Jason wrote this function\"\n\n    One can also use positional arguments.\n\n    sub_first_last_names = Substitution('Edgar Allen', 'Poe')\n\n    @sub_first_last_names\n    def some_function(x):\n        \"%s %s wrote the Raven\"\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        if (args and kwargs):\n            raise AssertionError(\"Only positional or keyword args are allowed\")\n\n        self.params = args or kwargs\n\n    def __call__(self, func):\n        func.__doc__ = func.__doc__ and func.__doc__ % self.params\n        return func\n\n    def update(self, *args, **kwargs):\n        \"\"\"\n        Update self.params with supplied args.\n\n        If called, we assume self.params is a dict.\n        \"\"\"\n\n        self.params.update(*args, **kwargs)\n\n    @classmethod\n    def from_params(cls, params):\n        \"\"\"\n        In the case where the params is a mutable sequence (list or dictionary)\n        and it may change before this class is called, one may explicitly use a\n        reference to the params rather than using *args or **kwargs which will\n        copy the values and not reference them.\n        \"\"\"\n        result = cls()\n        result.params = params\n        return result\n\n\nclass Appender(object):\n    \"\"\"\n    A function decorator that will append an addendum to the docstring\n    of the target function.\n\n    This decorator should be robust even if func.__doc__ is None\n    (for example, if -OO was passed to the interpreter).\n\n    Usage: construct a docstring.Appender with a string to be joined to\n    the original docstring. An optional 'join' parameter may be supplied\n    which will be used to join the docstring and addendum. e.g.\n\n    add_copyright = Appender(\"Copyright (c) 2009\", join='\\n')\n\n    @add_copyright\n    def my_dog(has='fleas'):\n        \"This docstring will have a copyright below\"\n        pass\n    \"\"\"\n\n    def __init__(self, addendum, join='', indents=0):\n        if indents > 0:\n            self.addendum = indent(addendum, indents=indents)\n        else:\n            self.addendum = addendum\n        self.join = join\n\n    def __call__(self, func):\n        func.__doc__ = func.__doc__ if func.__doc__ else ''\n        self.addendum = self.addendum if self.addendum else ''\n        docitems = [func.__doc__, self.addendum]\n        func.__doc__ = dedent(self.join.join(docitems))\n        return func\n\n\ndef indent(text, indents=1):\n    if not text or not isinstance(text, str):\n        return ''\n    jointext = ''.join(['\\n'] + ['    '] * indents)\n    return jointext.join(text.split('\\n'))\n\n\ndef make_signature(func):\n    \"\"\"\n    Returns a string repr of the arg list of a func call, with any defaults.\n\n    Examples\n    --------\n    >>> def f(a,b,c=2) :\n    >>>     return a*b*c\n    >>> print(_make_signature(f))\n    a,b,c=2\n    \"\"\"\n\n    spec = signature(func)\n    if spec.defaults is None:\n        n_wo_defaults = len(spec.args)\n        defaults = ('',) * n_wo_defaults\n    else:\n        n_wo_defaults = len(spec.args) - len(spec.defaults)\n        defaults = ('',) * n_wo_defaults + tuple(spec.defaults)\n    args = []\n    for i, (var, default) in enumerate(zip(spec.args, defaults)):\n        args.append(var if default == '' else var + '=' + repr(default))\n    if spec.varargs:\n        args.append('*' + spec.varargs)\n    if spec.keywords:\n        args.append('**' + spec.keywords)\n    return args, spec.args\n\n\nclass docstring_wrapper(object):\n    \"\"\"\n    Decorator to wrap a function and provide\n    a dynamically evaluated doc-string.\n\n    Parameters\n    ----------\n    func : callable\n    creator : callable\n        return the doc-string\n    default : str, optional\n        return this doc-string on error\n    \"\"\"\n    _attrs = ['__module__', '__name__',\n              '__qualname__', '__annotations__']\n\n    def __init__(self, func, creator, default=None):\n        self.func = func\n        self.creator = creator\n        self.default = default\n        update_wrapper(\n            self, func, [attr for attr in self._attrs\n                         if hasattr(func, attr)])\n\n    def __get__(self, instance, cls=None):\n\n        # we are called with a class\n        if instance is None:\n            return self\n\n        # we want to return the actual passed instance\n        return types.MethodType(self, instance)\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n    @property\n    def __doc__(self):\n        try:\n            return self.creator()\n        except Exception as exc:\n            msg = self.default or str(exc)\n            return msg\n"
    }
  ],
  "questions": [
    "In https://github.com/pandas-dev/pandas/blob/master/pandas/core/series.py, the DocStrings for `idxmax` and `idxmin` are misleading (in my opinion).  They say they return the index of the max/min, but they actually return the label (if I've got my Pandas lingo right).\r\n\r\nSame with `DataFrame.idxmax` and `DataFrame.idxmin`.  \r\n\r\nAlso, is there a reason `Series` provides `argmax` as a synonym for `idxmax`, and `DataFrame` does not?\r\n\r\nOne more thought: I find it surprising that `idxmax` and `argmax` do the same thing.  I expected `idxmax` to return an index and `argmax` to return a label.  I suppose it's too late to make that change."
  ],
  "golden_answers": [
    "Agreed that changing to label would be clearer.\r\n\r\nFor the `idxmax` vs. `argmax`, I think that was accidentally broken once `Series` no longer inherited from `ndarray`, see https://github.com/pandas-dev/pandas/issues/6214.\r\nI think we could issue a `FutureWarning` in 0.21.0, saying to use `.idxmax` instead of `.argmax`, and then followup to change `.argmax` to always be positional."
  ],
  "questions_generated": [
    "What is the issue with the documentation of the `idxmax` and `idxmin` methods in pandas' Series and DataFrame classes?",
    "Why was there a discrepancy between the behavior of `Series.argmax` and `DataFrame.argmax` in pandas?",
    "What was the proposed solution to address the confusion between `idxmax` and `argmax` in the Series class?",
    "What was the concern raised about making changes to the `idxmax` and `argmax` methods, and how was it decided to proceed?",
    "How was the decision made regarding whether to address the documentation issue despite an ongoing related PR?"
  ],
  "golden_answers_generated": [
    "The issue with the documentation is that it claims `idxmax` and `idxmin` return the index of the max/min value, but they actually return the label of the max/min value. This can be misleading for users who expect a numerical index instead of a label.",
    "The discrepancy arose because `Series.argmax` was kept as a synonym for `idxmax` after `Series` no longer inherited from `ndarray`, but this was not done for `DataFrame`. This inconsistency was noted, and there was a suggestion to issue a `FutureWarning` to encourage using `idxmax` over `argmax`.",
    "The proposed solution was to issue a `FutureWarning` in version 0.21.0 to inform users to use `.idxmax` instead of `.argmax`. Subsequently, `argmax` could be changed to always return a positional index, thus aligning its behavior with typical expectations.",
    "The concern was that changing the behavior of `idxmax` and `argmax` might be too late since it could break existing code relying on the current behavior. It was decided to handle the documentation issue separately and consider a gradual deprecation process for `argmax` to maintain backward compatibility.",
    "The decision was influenced by the status of the related PR #16955. It was considered that if the PR was not progressing, the documentation issue should still be addressed to avoid misleading users. However, if the PR was close to completion, it might render the documentation changes unnecessary."
  ]
}