{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "60084",
  "issue_description": "# BUG: arithmetic can break equality-hash invariant for Timestamp during DST transition\n\n### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\n# this is immediately following a DST transition\r\ndt_str = \"2023-11-05 01:00-08:00\"\r\ntz_str = \"America/Los_Angeles\"\r\n\r\nts1 = pd.Timestamp(dt_str, tz=tz_str)\r\n\r\n# This is the shortest way I know of to reproduce--more complex math or `pd.date_range` can also\r\n# create this behavior.\r\nts2 = ts1 + pd.Timedelta(hours=0)\r\n                                                                                                                                                                                                                               \r\nassert ts1 == ts2  # True\r\nassert hash(ts1) == hash(ts2)  # False\n```\n\n\n### Issue Description\n\nDatetime arithmetic on a `Timestamp` instance on a DST transition can create two `Timestamps` that compare equal but have different hash values. Inspecting the example, the cause seems to be that `ts1.fold == 0` and `ts2.fold == 1`, despite otherwise being equal and representing the same instant in time. This breaks the invariant documented in [`object.__hash__`](https://docs.python.org/3/reference/datamodel.html#object.__hash__):\r\n\r\n> object.\\_\\_hash\\_\\_(self)\r\n> ... The only required property is that objects which compare equal have the same hash value; ...\r\n\r\nConverting to UTC or using `pytz`'s normalization as `ts.tz.normalize(ts)` both serve as workarounds for this issue.\n\n### Expected Behavior\n\nI expected `Timestamp` instances that compare equal to have the same hash value.\n\n### Installed Versions\n\n<details>\r\n                                                                                                                                                                                                                                           \r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 0691c5cf90477d3503834d983f69350f250a6ff7\r\npython                : 3.12.6\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.153.1-microsoft-standard-WSL2\r\nVersion               : #1 SMP Fri Mar 29 23:14:13 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : C.UTF-8\r\n                                                                                                                                                                                                                                           \r\npandas                : 2.2.3\r\nnumpy                 : 2.1.2\r\npytz                  : 2024.2\r\ndateutil              : 2.9.0.post0\r\npip                   : 24.1.2\r\nCython                : None\r\nsphinx                : None\r\nIPython               : 8.28.0\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nblosc                 : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\nhtml5lib              : None\r\nhypothesis            : None\r\ngcsfs                 : None\r\njinja2                : None\r\nlxml.etree            : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npsycopg2              : None\r\npymysql               : None\r\npyarrow               : None\r\npyreadstat            : None\r\npytest                : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nxlsxwriter            : None\r\nzstandard             : None\r\ntzdata                : 2024.2\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 2430453355,
      "user": "rhshadrach",
      "body": "Thanks for the report! Agreed this would be a bug, I cannot reproduce on main but can on 2.3.x. Need to run a git bisect to see which PR fixed this and if tests need to be added."
    },
    {
      "id": 2433426184,
      "user": "Kxnr",
      "body": "Thanks for taking a look so quickly! I bisected the last 1000 commits and found 96d732e3ccaf5b10373747664392fa231afd3c3a as the first fixed commit, corresponding to https://github.com/pandas-dev/pandas/pull/59089. On both this commit and `main`, I can reproduce by installing with `pip install \".[timezone]\"` but not with a default build"
    },
    {
      "id": 2436355254,
      "user": "rhshadrach",
      "body": "Thanks! I think this could use a test. Contributions welcome!"
    },
    {
      "id": 2437984766,
      "user": "vgauraha62",
      "body": "is this issue resolved?"
    },
    {
      "id": 2438580180,
      "user": "rhshadrach",
      "body": "@vgauraha62 - a test needs to be added in order for the issue to be resolved."
    },
    {
      "id": 2438958467,
      "user": "Kxnr",
      "body": "I'll take a look at adding a test this weekend; I'll need to get up to speed on how things work around here, so others are welcome to pick this up if they have the bandwidth.\r\n\r\n@rhshadrach is there any additional action needed to correct this with the `timezone` extra?"
    },
    {
      "id": 2439036315,
      "user": "rhshadrach",
      "body": "> is there any additional action needed to correct this with the `timezone` extra?\r\n\r\nNot that I can see, but if you believe so, could you elaborate?"
    },
    {
      "id": 2439189676,
      "user": "vgauraha62",
      "body": "Please assign this issue to me, I'd add a test in a couple of hours and get this resolved"
    },
    {
      "id": 2439510416,
      "user": "rhshadrach",
      "body": "@vgauraha62 - see here: https://pandas.pydata.org/docs/dev/development/contributing.html#finding-an-issue-to-contribute-to"
    },
    {
      "id": 2439526881,
      "user": "vgauraha62",
      "body": "@rhshadrach sir are you suggesting that I should create a branch and submit a pull request?"
    },
    {
      "id": 2440407832,
      "user": "saldanhad",
      "body": "Hi @vgauraha62, you can comment `take` to have this issue self-assigned to you. Please follow the applicable requirements in the PR checklist and remember to reference this issue in your PR."
    },
    {
      "id": 2440688105,
      "user": "vgauraha62",
      "body": "take"
    },
    {
      "id": 2442268828,
      "user": "Kxnr",
      "body": "> > is there any additional action needed to correct this with the `timezone` extra?\r\n> \r\n> Not that I can see, but if you believe so, could you elaborate?\r\n\r\nApologies, I thought the initial test script replicated the issue but I have to explicitly use a `pytz` timezone on latest main. The test script below replicates the original issue on main when the `timezone` extra is installed. This isn't a \"real\" failure from my usecase, just dotting my i's.\r\n\r\n<details>\r\n\r\n```python\r\nimport pandas as pd\r\nimport pytz\r\n\r\n# this is immediately following a DST transition\r\ndt_str = \"2023-11-05 01:00-08:00\"\r\ntz_str = \"America/Los_Angeles\"\r\npytz_tz = pytz.timezone(tz_str)\r\n\r\nts1 = pd.Timestamp(dt_str, tz=pytz_tz)\r\nts2 = ts1 + pd.Timedelta(hours=0)\r\n                                                                                                                                                                                                                               \r\nassert ts1 == ts2\r\nassert hash(ts1) == hash(ts2) \r\nprint(\"hash-equality invariant upheld\")\r\n```\r\n\r\n</details>\r\n"
    },
    {
      "id": 2452789757,
      "user": "eightyseven",
      "body": "> take\r\n\r\nare you still working on this issue?I'm also interested in this one ."
    },
    {
      "id": 2481125214,
      "user": "KevsterAmp",
      "body": "take"
    }
  ],
  "text_context": "# BUG: arithmetic can break equality-hash invariant for Timestamp during DST transition\n\n### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\n# this is immediately following a DST transition\r\ndt_str = \"2023-11-05 01:00-08:00\"\r\ntz_str = \"America/Los_Angeles\"\r\n\r\nts1 = pd.Timestamp(dt_str, tz=tz_str)\r\n\r\n# This is the shortest way I know of to reproduce--more complex math or `pd.date_range` can also\r\n# create this behavior.\r\nts2 = ts1 + pd.Timedelta(hours=0)\r\n                                                                                                                                                                                                                               \r\nassert ts1 == ts2  # True\r\nassert hash(ts1) == hash(ts2)  # False\n```\n\n\n### Issue Description\n\nDatetime arithmetic on a `Timestamp` instance on a DST transition can create two `Timestamps` that compare equal but have different hash values. Inspecting the example, the cause seems to be that `ts1.fold == 0` and `ts2.fold == 1`, despite otherwise being equal and representing the same instant in time. This breaks the invariant documented in [`object.__hash__`](https://docs.python.org/3/reference/datamodel.html#object.__hash__):\r\n\r\n> object.\\_\\_hash\\_\\_(self)\r\n> ... The only required property is that objects which compare equal have the same hash value; ...\r\n\r\nConverting to UTC or using `pytz`'s normalization as `ts.tz.normalize(ts)` both serve as workarounds for this issue.\n\n### Expected Behavior\n\nI expected `Timestamp` instances that compare equal to have the same hash value.\n\n### Installed Versions\n\n<details>\r\n                                                                                                                                                                                                                                           \r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 0691c5cf90477d3503834d983f69350f250a6ff7\r\npython                : 3.12.6\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.153.1-microsoft-standard-WSL2\r\nVersion               : #1 SMP Fri Mar 29 23:14:13 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : C.UTF-8\r\n                                                                                                                                                                                                                                           \r\npandas                : 2.2.3\r\nnumpy                 : 2.1.2\r\npytz                  : 2024.2\r\ndateutil              : 2.9.0.post0\r\npip                   : 24.1.2\r\nCython                : None\r\nsphinx                : None\r\nIPython               : 8.28.0\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nblosc                 : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\nhtml5lib              : None\r\nhypothesis            : None\r\ngcsfs                 : None\r\njinja2                : None\r\nlxml.etree            : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npsycopg2              : None\r\npymysql               : None\r\npyarrow               : None\r\npyreadstat            : None\r\npytest                : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nxlsxwriter            : None\r\nzstandard             : None\r\ntzdata                : 2024.2\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n</details>\r\n\n\nThanks for the report! Agreed this would be a bug, I cannot reproduce on main but can on 2.3.x. Need to run a git bisect to see which PR fixed this and if tests need to be added.\n\nThanks for taking a look so quickly! I bisected the last 1000 commits and found 96d732e3ccaf5b10373747664392fa231afd3c3a as the first fixed commit, corresponding to https://github.com/pandas-dev/pandas/pull/59089. On both this commit and `main`, I can reproduce by installing with `pip install \".[timezone]\"` but not with a default build\n\nThanks! I think this could use a test. Contributions welcome!\n\nis this issue resolved?\n\n@vgauraha62 - a test needs to be added in order for the issue to be resolved.\n\nI'll take a look at adding a test this weekend; I'll need to get up to speed on how things work around here, so others are welcome to pick this up if they have the bandwidth.\r\n\r\n@rhshadrach is there any additional action needed to correct this with the `timezone` extra?\n\n> is there any additional action needed to correct this with the `timezone` extra?\r\n\r\nNot that I can see, but if you believe so, could you elaborate?\n\nPlease assign this issue to me, I'd add a test in a couple of hours and get this resolved\n\n@vgauraha62 - see here: https://pandas.pydata.org/docs/dev/development/contributing.html#finding-an-issue-to-contribute-to\n\n@rhshadrach sir are you suggesting that I should create a branch and submit a pull request?\n\nHi @vgauraha62, you can comment `take` to have this issue self-assigned to you. Please follow the applicable requirements in the PR checklist and remember to reference this issue in your PR.\n\ntake\n\n> > is there any additional action needed to correct this with the `timezone` extra?\r\n> \r\n> Not that I can see, but if you believe so, could you elaborate?\r\n\r\nApologies, I thought the initial test script replicated the issue but I have to explicitly use a `pytz` timezone on latest main. The test script below replicates the original issue on main when the `timezone` extra is installed. This isn't a \"real\" failure from my usecase, just dotting my i's.\r\n\r\n<details>\r\n\r\n```python\r\nimport pandas as pd\r\nimport pytz\r\n\r\n# this is immediately following a DST transition\r\ndt_str = \"2023-11-05 01:00-08:00\"\r\ntz_str = \"America/Los_Angeles\"\r\npytz_tz = pytz.timezone(tz_str)\r\n\r\nts1 = pd.Timestamp(dt_str, tz=pytz_tz)\r\nts2 = ts1 + pd.Timedelta(hours=0)\r\n                                                                                                                                                                                                                               \r\nassert ts1 == ts2\r\nassert hash(ts1) == hash(ts2) \r\nprint(\"hash-equality invariant upheld\")\r\n```\r\n\r\n</details>\r\n\n\n> take\r\n\r\nare you still working on this issue?I'm also interested in this one .\n\ntake",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/59089",
  "code_context": [
    {
      "filename": "pandas/__init__.py",
      "content": "from __future__ import annotations\n\n__docformat__ = \"restructuredtext\"\n\n# Let users know if they're missing any of our hard dependencies\n_hard_dependencies = (\"numpy\", \"dateutil\")\n_missing_dependencies = []\n\nfor _dependency in _hard_dependencies:\n    try:\n        __import__(_dependency)\n    except ImportError as _e:  # pragma: no cover\n        _missing_dependencies.append(f\"{_dependency}: {_e}\")\n\nif _missing_dependencies:  # pragma: no cover\n    raise ImportError(\n        \"Unable to import required dependencies:\\n\" + \"\\n\".join(_missing_dependencies)\n    )\ndel _hard_dependencies, _dependency, _missing_dependencies\n\ntry:\n    # numpy compat\n    from pandas.compat import (\n        is_numpy_dev as _is_numpy_dev,  # pyright: ignore[reportUnusedImport] # noqa: F401\n    )\nexcept ImportError as _err:  # pragma: no cover\n    _module = _err.name\n    raise ImportError(\n        f\"C extension: {_module} not built. If you want to import \"\n        \"pandas from the source directory, you may need to run \"\n        \"'python -m pip install -ve . --no-build-isolation --config-settings \"\n        \"editable-verbose=true' to build the C extensions first.\"\n    ) from _err\n\nfrom pandas._config import (\n    get_option,\n    set_option,\n    reset_option,\n    describe_option,\n    option_context,\n    options,\n)\n\n# let init-time option registration happen\nimport pandas.core.config_init  # pyright: ignore[reportUnusedImport] # noqa: F401\n\nfrom pandas.core.api import (\n    # dtype\n    ArrowDtype,\n    Int8Dtype,\n    Int16Dtype,\n    Int32Dtype,\n    Int64Dtype,\n    UInt8Dtype,\n    UInt16Dtype,\n    UInt32Dtype,\n    UInt64Dtype,\n    Float32Dtype,\n    Float64Dtype,\n    CategoricalDtype,\n    PeriodDtype,\n    IntervalDtype,\n    DatetimeTZDtype,\n    StringDtype,\n    BooleanDtype,\n    # missing\n    NA,\n    isna,\n    isnull,\n    notna,\n    notnull,\n    # indexes\n    Index,\n    CategoricalIndex,\n    RangeIndex,\n    MultiIndex,\n    IntervalIndex,\n    TimedeltaIndex,\n    DatetimeIndex,\n    PeriodIndex,\n    IndexSlice,\n    # tseries\n    NaT,\n    Period,\n    period_range,\n    Timedelta,\n    timedelta_range,\n    Timestamp,\n    date_range,\n    bdate_range,\n    Interval,\n    interval_range,\n    DateOffset,\n    # conversion\n    to_numeric,\n    to_datetime,\n    to_timedelta,\n    # misc\n    Flags,\n    Grouper,\n    factorize,\n    unique,\n    NamedAgg,\n    array,\n    Categorical,\n    set_eng_float_format,\n    Series,\n    DataFrame,\n)\n\nfrom pandas.core.dtypes.dtypes import SparseDtype\n\nfrom pandas.tseries.api import infer_freq\nfrom pandas.tseries import offsets\n\nfrom pandas.core.computation.api import eval\n\nfrom pandas.core.reshape.api import (\n    concat,\n    lreshape,\n    melt,\n    wide_to_long,\n    merge,\n    merge_asof,\n    merge_ordered,\n    crosstab,\n    pivot,\n    pivot_table,\n    get_dummies,\n    from_dummies,\n    cut,\n    qcut,\n)\n\nfrom pandas import api, arrays, errors, io, plotting, tseries\nfrom pandas import testing\nfrom pandas.util._print_versions import show_versions\n\nfrom pandas.io.api import (\n    # excel\n    ExcelFile,\n    ExcelWriter,\n    read_excel,\n    # parsers\n    read_csv,\n    read_fwf,\n    read_table,\n    # pickle\n    read_pickle,\n    to_pickle,\n    # pytables\n    HDFStore,\n    read_hdf,\n    # sql\n    read_sql,\n    read_sql_query,\n    read_sql_table,\n    # misc\n    read_clipboard,\n    read_parquet,\n    read_orc,\n    read_feather,\n    read_html,\n    read_xml,\n    read_json,\n    read_stata,\n    read_sas,\n    read_spss,\n)\n\nfrom pandas.io.json._normalize import json_normalize\n\nfrom pandas.util._tester import test\n\n# use the closest tagged version if possible\n_built_with_meson = False\ntry:\n    from pandas._version_meson import (  # pyright: ignore [reportMissingImports]\n        __version__,\n        __git_version__,\n    )\n\n    _built_with_meson = True\nexcept ImportError:\n    from pandas._version import get_versions\n\n    v = get_versions()\n    __version__ = v.get(\"closest-tag\", v[\"version\"])\n    __git_version__ = v.get(\"full-revisionid\")\n    del get_versions, v\n\n\n# module level doc-string\n__doc__ = \"\"\"\npandas - a powerful data analysis and manipulation library for Python\n=====================================================================\n\n**pandas** is a Python package providing fast, flexible, and expressive data\nstructures designed to make working with \"relational\" or \"labeled\" data both\neasy and intuitive. It aims to be the fundamental high-level building block for\ndoing practical, **real world** data analysis in Python. Additionally, it has\nthe broader goal of becoming **the most powerful and flexible open source data\nanalysis / manipulation tool available in any language**. It is already well on\nits way toward this goal.\n\nMain Features\n-------------\nHere are just a few of the things that pandas does well:\n\n  - Easy handling of missing data in floating point as well as non-floating\n    point data.\n  - Size mutability: columns can be inserted and deleted from DataFrame and\n    higher dimensional objects\n  - Automatic and explicit data alignment: objects can be explicitly aligned\n    to a set of labels, or the user can simply ignore the labels and let\n    `Series`, `DataFrame`, etc. automatically align the data for you in\n    computations.\n  - Powerful, flexible group by functionality to perform split-apply-combine\n    operations on data sets, for both aggregating and transforming data.\n  - Make it easy to convert ragged, differently-indexed data in other Python\n    and NumPy data structures into DataFrame objects.\n  - Intelligent label-based slicing, fancy indexing, and subsetting of large\n    data sets.\n  - Intuitive merging and joining data sets.\n  - Flexible reshaping and pivoting of data sets.\n  - Hierarchical labeling of axes (possible to have multiple labels per tick).\n  - Robust IO tools for loading data from flat files (CSV and delimited),\n    Excel files, databases, and saving/loading data from the ultrafast HDF5\n    format.\n  - Time series-specific functionality: date range generation and frequency\n    conversion, moving window statistics, date shifting and lagging.\n\"\"\"\n\n# Use __all__ to let type checkers know what is part of the public API.\n# Pandas is not (yet) a py.typed library: the public API is determined\n# based on the documentation.\n__all__ = [\n    \"ArrowDtype\",\n    \"BooleanDtype\",\n    \"Categorical\",\n    \"CategoricalDtype\",\n    \"CategoricalIndex\",\n    \"DataFrame\",\n    \"DateOffset\",\n    \"DatetimeIndex\",\n    \"DatetimeTZDtype\",\n    \"ExcelFile\",\n    \"ExcelWriter\",\n    \"Flags\",\n    \"Float32Dtype\",\n    \"Float64Dtype\",\n    \"Grouper\",\n    \"HDFStore\",\n    \"Index\",\n    \"IndexSlice\",\n    \"Int16Dtype\",\n    \"Int32Dtype\",\n    \"Int64Dtype\",\n    \"Int8Dtype\",\n    \"Interval\",\n    \"IntervalDtype\",\n    \"IntervalIndex\",\n    \"MultiIndex\",\n    \"NA\",\n    \"NaT\",\n    \"NamedAgg\",\n    \"Period\",\n    \"PeriodDtype\",\n    \"PeriodIndex\",\n    \"RangeIndex\",\n    \"Series\",\n    \"SparseDtype\",\n    \"StringDtype\",\n    \"Timedelta\",\n    \"TimedeltaIndex\",\n    \"Timestamp\",\n    \"UInt16Dtype\",\n    \"UInt32Dtype\",\n    \"UInt64Dtype\",\n    \"UInt8Dtype\",\n    \"api\",\n    \"array\",\n    \"arrays\",\n    \"bdate_range\",\n    \"concat\",\n    \"crosstab\",\n    \"cut\",\n    \"date_range\",\n    \"describe_option\",\n    \"errors\",\n    \"eval\",\n    \"factorize\",\n    \"get_dummies\",\n    \"from_dummies\",\n    \"get_option\",\n    \"infer_freq\",\n    \"interval_range\",\n    \"io\",\n    \"isna\",\n    \"isnull\",\n    \"json_normalize\",\n    \"lreshape\",\n    \"melt\",\n    \"merge\",\n    \"merge_asof\",\n    \"merge_ordered\",\n    \"notna\",\n    \"notnull\",\n    \"offsets\",\n    \"option_context\",\n    \"options\",\n    \"period_range\",\n    \"pivot\",\n    \"pivot_table\",\n    \"plotting\",\n    \"qcut\",\n    \"read_clipboard\",\n    \"read_csv\",\n    \"read_excel\",\n    \"read_feather\",\n    \"read_fwf\",\n    \"read_hdf\",\n    \"read_html\",\n    \"read_json\",\n    \"read_orc\",\n    \"read_parquet\",\n    \"read_pickle\",\n    \"read_sas\",\n    \"read_spss\",\n    \"read_sql\",\n    \"read_sql_query\",\n    \"read_sql_table\",\n    \"read_stata\",\n    \"read_table\",\n    \"read_xml\",\n    \"reset_option\",\n    \"set_eng_float_format\",\n    \"set_option\",\n    \"show_versions\",\n    \"test\",\n    \"testing\",\n    \"timedelta_range\",\n    \"to_datetime\",\n    \"to_numeric\",\n    \"to_pickle\",\n    \"to_timedelta\",\n    \"tseries\",\n    \"unique\",\n    \"wide_to_long\",\n]\n"
    },
    {
      "filename": "pandas/compat/__init__.py",
      "content": "\"\"\"\ncompat\n======\n\nCross-compatible functions for different versions of Python.\n\nOther items:\n* platform checker\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport platform\nimport sys\nfrom typing import TYPE_CHECKING\n\nfrom pandas.compat._constants import (\n    IS64,\n    ISMUSL,\n    PY311,\n    PY312,\n    PYPY,\n    WASM,\n)\nfrom pandas.compat.numpy import is_numpy_dev\nfrom pandas.compat.pyarrow import (\n    HAS_PYARROW,\n    pa_version_under10p1,\n    pa_version_under11p0,\n    pa_version_under13p0,\n    pa_version_under14p0,\n    pa_version_under14p1,\n    pa_version_under16p0,\n    pa_version_under17p0,\n    pa_version_under18p0,\n)\n\nif TYPE_CHECKING:\n    from pandas._typing import F\n\n\ndef set_function_name(f: F, name: str, cls: type) -> F:\n    \"\"\"\n    Bind the name/qualname attributes of the function.\n    \"\"\"\n    f.__name__ = name\n    f.__qualname__ = f\"{cls.__name__}.{name}\"\n    f.__module__ = cls.__module__\n    return f\n\n\ndef is_platform_little_endian() -> bool:\n    \"\"\"\n    Checking if the running platform is little endian.\n\n    Returns\n    -------\n    bool\n        True if the running platform is little endian.\n    \"\"\"\n    return sys.byteorder == \"little\"\n\n\ndef is_platform_windows() -> bool:\n    \"\"\"\n    Checking if the running platform is windows.\n\n    Returns\n    -------\n    bool\n        True if the running platform is windows.\n    \"\"\"\n    return sys.platform in [\"win32\", \"cygwin\"]\n\n\ndef is_platform_linux() -> bool:\n    \"\"\"\n    Checking if the running platform is linux.\n\n    Returns\n    -------\n    bool\n        True if the running platform is linux.\n    \"\"\"\n    return sys.platform == \"linux\"\n\n\ndef is_platform_mac() -> bool:\n    \"\"\"\n    Checking if the running platform is mac.\n\n    Returns\n    -------\n    bool\n        True if the running platform is mac.\n    \"\"\"\n    return sys.platform == \"darwin\"\n\n\ndef is_platform_arm() -> bool:\n    \"\"\"\n    Checking if the running platform use ARM architecture.\n\n    Returns\n    -------\n    bool\n        True if the running platform uses ARM architecture.\n    \"\"\"\n    return platform.machine() in (\"arm64\", \"aarch64\") or platform.machine().startswith(\n        \"armv\"\n    )\n\n\ndef is_platform_power() -> bool:\n    \"\"\"\n    Checking if the running platform use Power architecture.\n\n    Returns\n    -------\n    bool\n        True if the running platform uses ARM architecture.\n    \"\"\"\n    return platform.machine() in (\"ppc64\", \"ppc64le\")\n\n\ndef is_platform_riscv64() -> bool:\n    \"\"\"\n    Checking if the running platform use riscv64 architecture.\n\n    Returns\n    -------\n    bool\n        True if the running platform uses riscv64 architecture.\n    \"\"\"\n    return platform.machine() == \"riscv64\"\n\n\ndef is_ci_environment() -> bool:\n    \"\"\"\n    Checking if running in a continuous integration environment by checking\n    the PANDAS_CI environment variable.\n\n    Returns\n    -------\n    bool\n        True if the running in a continuous integration environment.\n    \"\"\"\n    return os.environ.get(\"PANDAS_CI\", \"0\") == \"1\"\n\n\n__all__ = [\n    \"is_numpy_dev\",\n    \"pa_version_under10p1\",\n    \"pa_version_under11p0\",\n    \"pa_version_under13p0\",\n    \"pa_version_under14p0\",\n    \"pa_version_under14p1\",\n    \"pa_version_under16p0\",\n    \"pa_version_under17p0\",\n    \"pa_version_under18p0\",\n    \"HAS_PYARROW\",\n    \"IS64\",\n    \"ISMUSL\",\n    \"PY311\",\n    \"PY312\",\n    \"PYPY\",\n    \"WASM\",\n]\n"
    },
    {
      "filename": "pandas/compat/_optional.py",
      "content": "from __future__ import annotations\n\nimport importlib\nimport sys\nfrom typing import (\n    TYPE_CHECKING,\n    Literal,\n    overload,\n)\nimport warnings\n\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.util.version import Version\n\nif TYPE_CHECKING:\n    import types\n\n# Update install.rst, actions-310-minimum_versions.yaml,\n# deps_minimum.toml & pyproject.toml when updating versions!\n\nVERSIONS = {\n    \"adbc-driver-postgresql\": \"0.10.0\",\n    \"adbc-driver-sqlite\": \"0.8.0\",\n    \"bs4\": \"4.11.2\",\n    \"blosc\": \"1.21.3\",\n    \"bottleneck\": \"1.3.6\",\n    \"fastparquet\": \"2023.10.0\",\n    \"fsspec\": \"2022.11.0\",\n    \"html5lib\": \"1.1\",\n    \"hypothesis\": \"6.84.0\",\n    \"gcsfs\": \"2022.11.0\",\n    \"jinja2\": \"3.1.2\",\n    \"lxml.etree\": \"4.9.2\",\n    \"matplotlib\": \"3.6.3\",\n    \"numba\": \"0.56.4\",\n    \"numexpr\": \"2.8.4\",\n    \"odfpy\": \"1.4.1\",\n    \"openpyxl\": \"3.1.0\",\n    \"psycopg2\": \"2.9.6\",  # (dt dec pq3 ext lo64)\n    \"pymysql\": \"1.0.2\",\n    \"pyarrow\": \"10.0.1\",\n    \"pyreadstat\": \"1.2.0\",\n    \"pytest\": \"7.3.2\",\n    \"python-calamine\": \"0.1.7\",\n    \"pytz\": \"2023.4\",\n    \"pyxlsb\": \"1.0.10\",\n    \"s3fs\": \"2022.11.0\",\n    \"scipy\": \"1.10.0\",\n    \"sqlalchemy\": \"2.0.0\",\n    \"tables\": \"3.8.0\",\n    \"tabulate\": \"0.9.0\",\n    \"xarray\": \"2022.12.0\",\n    \"xlrd\": \"2.0.1\",\n    \"xlsxwriter\": \"3.0.5\",\n    \"zstandard\": \"0.19.0\",\n    \"tzdata\": \"2022.7\",\n    \"qtpy\": \"2.3.0\",\n    \"pyqt5\": \"5.15.9\",\n}\n\n# A mapping from import name to package name (on PyPI) for packages where\n# these two names are different.\n\nINSTALL_MAPPING = {\n    \"bs4\": \"beautifulsoup4\",\n    \"bottleneck\": \"Bottleneck\",\n    \"jinja2\": \"Jinja2\",\n    \"lxml.etree\": \"lxml\",\n    \"odf\": \"odfpy\",\n    \"python_calamine\": \"python-calamine\",\n    \"sqlalchemy\": \"SQLAlchemy\",\n    \"tables\": \"pytables\",\n}\n\n\ndef get_version(module: types.ModuleType) -> str:\n    version = getattr(module, \"__version__\", None)\n\n    if version is None:\n        raise ImportError(f\"Can't determine version for {module.__name__}\")\n    if module.__name__ == \"psycopg2\":\n        # psycopg2 appends \" (dt dec pq3 ext lo64)\" to it's version\n        version = version.split()[0]\n    return version\n\n\n@overload\ndef import_optional_dependency(\n    name: str,\n    extra: str = ...,\n    min_version: str | None = ...,\n    *,\n    errors: Literal[\"raise\"] = ...,\n) -> types.ModuleType: ...\n\n\n@overload\ndef import_optional_dependency(\n    name: str,\n    extra: str = ...,\n    min_version: str | None = ...,\n    *,\n    errors: Literal[\"warn\", \"ignore\"],\n) -> types.ModuleType | None: ...\n\n\ndef import_optional_dependency(\n    name: str,\n    extra: str = \"\",\n    min_version: str | None = None,\n    *,\n    errors: Literal[\"raise\", \"warn\", \"ignore\"] = \"raise\",\n) -> types.ModuleType | None:\n    \"\"\"\n    Import an optional dependency.\n\n    By default, if a dependency is missing an ImportError with a nice\n    message will be raised. If a dependency is present, but too old,\n    we raise.\n\n    Parameters\n    ----------\n    name : str\n        The module name.\n    extra : str\n        Additional text to include in the ImportError message.\n    errors : str {'raise', 'warn', 'ignore'}\n        What to do when a dependency is not found or its version is too old.\n\n        * raise : Raise an ImportError\n        * warn : Only applicable when a module's version is to old.\n          Warns that the version is too old and returns None\n        * ignore: If the module is not installed, return None, otherwise,\n          return the module, even if the version is too old.\n          It's expected that users validate the version locally when\n          using ``errors=\"ignore\"`` (see. ``io/html.py``)\n    min_version : str, default None\n        Specify a minimum version that is different from the global pandas\n        minimum version required.\n    Returns\n    -------\n    maybe_module : Optional[ModuleType]\n        The imported module, when found and the version is correct.\n        None is returned when the package is not found and `errors`\n        is False, or when the package's version is too old and `errors`\n        is ``'warn'`` or ``'ignore'``.\n    \"\"\"\n    assert errors in {\"warn\", \"raise\", \"ignore\"}\n\n    package_name = INSTALL_MAPPING.get(name)\n    install_name = package_name if package_name is not None else name\n\n    msg = (\n        f\"Missing optional dependency '{install_name}'. {extra} \"\n        f\"Use pip or conda to install {install_name}.\"\n    )\n    try:\n        module = importlib.import_module(name)\n    except ImportError as err:\n        if errors == \"raise\":\n            raise ImportError(msg) from err\n        return None\n\n    # Handle submodules: if we have submodule, grab parent module from sys.modules\n    parent = name.split(\".\")[0]\n    if parent != name:\n        install_name = parent\n        module_to_get = sys.modules[install_name]\n    else:\n        module_to_get = module\n    minimum_version = min_version if min_version is not None else VERSIONS.get(parent)\n    if minimum_version:\n        version = get_version(module_to_get)\n        if version and Version(version) < Version(minimum_version):\n            msg = (\n                f\"Pandas requires version '{minimum_version}' or newer of '{parent}' \"\n                f\"(version '{version}' currently installed).\"\n            )\n            if errors == \"warn\":\n                warnings.warn(\n                    msg,\n                    UserWarning,\n                    stacklevel=find_stack_level(),\n                )\n                return None\n            elif errors == \"raise\":\n                raise ImportError(msg)\n            else:\n                return None\n\n    return module\n"
    },
    {
      "filename": "pandas/compat/pyarrow.py",
      "content": "\"\"\"support pyarrow compatibility across versions\"\"\"\n\nfrom __future__ import annotations\n\nfrom pandas.util.version import Version\n\ntry:\n    import pyarrow as pa\n\n    _palv = Version(Version(pa.__version__).base_version)\n    pa_version_under10p1 = _palv < Version(\"10.0.1\")\n    pa_version_under11p0 = _palv < Version(\"11.0.0\")\n    pa_version_under12p0 = _palv < Version(\"12.0.0\")\n    pa_version_under13p0 = _palv < Version(\"13.0.0\")\n    pa_version_under14p0 = _palv < Version(\"14.0.0\")\n    pa_version_under14p1 = _palv < Version(\"14.0.1\")\n    pa_version_under15p0 = _palv < Version(\"15.0.0\")\n    pa_version_under16p0 = _palv < Version(\"16.0.0\")\n    pa_version_under17p0 = _palv < Version(\"17.0.0\")\n    pa_version_under18p0 = _palv < Version(\"18.0.0\")\n    HAS_PYARROW = True\nexcept ImportError:\n    pa_version_under10p1 = True\n    pa_version_under11p0 = True\n    pa_version_under12p0 = True\n    pa_version_under13p0 = True\n    pa_version_under14p0 = True\n    pa_version_under14p1 = True\n    pa_version_under15p0 = True\n    pa_version_under16p0 = True\n    pa_version_under17p0 = True\n    pa_version_under18p0 = True\n    HAS_PYARROW = False\n"
    },
    {
      "filename": "pandas/conftest.py",
      "content": "\"\"\"\nThis file is very long and growing, but it was decided to not split it yet, as\nit's still manageable (2020-03-17, ~1.1k LoC). See gh-31989\n\nInstead of splitting it was decided to define sections here:\n- Configuration / Settings\n- Autouse fixtures\n- Common arguments\n- Missing values & co.\n- Classes\n- Indices\n- Series'\n- DataFrames\n- Operators & Operations\n- Data sets/files\n- Time zones\n- Dtypes\n- Misc\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections import abc\nfrom datetime import (\n    date,\n    datetime,\n    time,\n    timedelta,\n    timezone,\n)\nfrom decimal import Decimal\nimport gc\nimport operator\nimport os\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n)\nimport uuid\n\nfrom dateutil.tz import (\n    tzlocal,\n    tzutc,\n)\nimport hypothesis\nfrom hypothesis import strategies as st\nimport numpy as np\nimport pytest\n\nfrom pandas.compat._optional import import_optional_dependency\nimport pandas.util._test_decorators as td\n\nfrom pandas.core.dtypes.dtypes import (\n    DatetimeTZDtype,\n    IntervalDtype,\n)\n\nimport pandas as pd\nfrom pandas import (\n    CategoricalIndex,\n    DataFrame,\n    Interval,\n    IntervalIndex,\n    Period,\n    RangeIndex,\n    Series,\n    Timedelta,\n    Timestamp,\n    date_range,\n    period_range,\n    timedelta_range,\n)\nimport pandas._testing as tm\nfrom pandas.core import ops\nfrom pandas.core.indexes.api import (\n    Index,\n    MultiIndex,\n)\n\nif TYPE_CHECKING:\n    from collections.abc import (\n        Callable,\n        Hashable,\n        Iterator,\n    )\n\ntry:\n    import pyarrow as pa\nexcept ImportError:\n    has_pyarrow = False\nelse:\n    del pa\n    has_pyarrow = True\n\npytz = import_optional_dependency(\"pytz\", errors=\"ignore\")\n\n\n# ----------------------------------------------------------------\n# Configuration / Settings\n# ----------------------------------------------------------------\n# pytest\n\n\ndef pytest_addoption(parser) -> None:\n    parser.addoption(\n        \"--no-strict-data-files\",\n        action=\"store_false\",\n        help=\"Don't fail if a test is skipped for missing data file.\",\n    )\n\n\ndef ignore_doctest_warning(item: pytest.Item, path: str, message: str) -> None:\n    \"\"\"Ignore doctest warning.\n\n    Parameters\n    ----------\n    item : pytest.Item\n        pytest test item.\n    path : str\n        Module path to Python object, e.g. \"pandas.DataFrame.append\". A\n        warning will be filtered when item.name ends with in given path. So it is\n        sufficient to specify e.g. \"DataFrame.append\".\n    message : str\n        Message to be filtered.\n    \"\"\"\n    if item.name.endswith(path):\n        item.add_marker(pytest.mark.filterwarnings(f\"ignore:{message}\"))\n\n\ndef pytest_collection_modifyitems(items, config) -> None:\n    is_doctest = config.getoption(\"--doctest-modules\") or config.getoption(\n        \"--doctest-cython\", default=False\n    )\n\n    # Warnings from doctests that can be ignored; place reason in comment above.\n    # Each entry specifies (path, message) - see the ignore_doctest_warning function\n    ignored_doctest_warnings = [\n        (\"is_int64_dtype\", \"is_int64_dtype is deprecated\"),\n        (\"is_interval_dtype\", \"is_interval_dtype is deprecated\"),\n        (\"is_period_dtype\", \"is_period_dtype is deprecated\"),\n        (\"is_datetime64tz_dtype\", \"is_datetime64tz_dtype is deprecated\"),\n        (\"is_categorical_dtype\", \"is_categorical_dtype is deprecated\"),\n        (\"is_sparse\", \"is_sparse is deprecated\"),\n        (\"DataFrameGroupBy.fillna\", \"DataFrameGroupBy.fillna is deprecated\"),\n        (\"DataFrameGroupBy.corrwith\", \"DataFrameGroupBy.corrwith is deprecated\"),\n        (\"NDFrame.replace\", \"Series.replace without 'value'\"),\n        (\"NDFrame.clip\", \"Downcasting behavior in Series and DataFrame methods\"),\n        (\"Series.idxmin\", \"The behavior of Series.idxmin\"),\n        (\"Series.idxmax\", \"The behavior of Series.idxmax\"),\n        (\"SeriesGroupBy.fillna\", \"SeriesGroupBy.fillna is deprecated\"),\n        (\"SeriesGroupBy.idxmin\", \"The behavior of Series.idxmin\"),\n        (\"SeriesGroupBy.idxmax\", \"The behavior of Series.idxmax\"),\n        (\"to_pytimedelta\", \"The behavior of TimedeltaProperties.to_pytimedelta\"),\n        (\"NDFrame.reindex_like\", \"keyword argument 'method' is deprecated\"),\n        # Docstring divides by zero to show behavior difference\n        (\"missing.mask_zero_div_zero\", \"divide by zero encountered\"),\n        (\n            \"pandas.core.generic.NDFrame.first\",\n            \"first is deprecated and will be removed in a future version. \"\n            \"Please create a mask and filter using `.loc` instead\",\n        ),\n        (\n            \"Resampler.fillna\",\n            \"DatetimeIndexResampler.fillna is deprecated\",\n        ),\n        (\n            \"DataFrameGroupBy.fillna\",\n            \"DataFrameGroupBy.fillna with 'method' is deprecated\",\n        ),\n        (\"read_parquet\", \"Passing a BlockManager to DataFrame is deprecated\"),\n    ]\n\n    if is_doctest:\n        for item in items:\n            for path, message in ignored_doctest_warnings:\n                ignore_doctest_warning(item, path, message)\n\n\nhypothesis_health_checks = [\n    hypothesis.HealthCheck.too_slow,\n    hypothesis.HealthCheck.differing_executors,\n]\n\n# Hypothesis\nhypothesis.settings.register_profile(\n    \"ci\",\n    # Hypothesis timing checks are tuned for scalars by default, so we bump\n    # them from 200ms to 500ms per test case as the global default.  If this\n    # is too short for a specific test, (a) try to make it faster, and (b)\n    # if it really is slow add `@settings(deadline=...)` with a working value,\n    # or `deadline=None` to entirely disable timeouts for that test.\n    # 2022-02-09: Changed deadline from 500 -> None. Deadline leads to\n    # non-actionable, flaky CI failures (# GH 24641, 44969, 45118, 44969)\n    deadline=None,\n    suppress_health_check=tuple(hypothesis_health_checks),\n)\nhypothesis.settings.load_profile(\"ci\")\n\n# Registering these strategies makes them globally available via st.from_type,\n# which is use for offsets in tests/tseries/offsets/test_offsets_properties.py\nfor name in \"MonthBegin MonthEnd BMonthBegin BMonthEnd\".split():\n    cls = getattr(pd.tseries.offsets, name)\n    st.register_type_strategy(\n        cls, st.builds(cls, n=st.integers(-99, 99), normalize=st.booleans())\n    )\n\nfor name in \"YearBegin YearEnd BYearBegin BYearEnd\".split():\n    cls = getattr(pd.tseries.offsets, name)\n    st.register_type_strategy(\n        cls,\n        st.builds(\n            cls,\n            n=st.integers(-5, 5),\n            normalize=st.booleans(),\n            month=st.integers(min_value=1, max_value=12),\n        ),\n    )\n\nfor name in \"QuarterBegin QuarterEnd BQuarterBegin BQuarterEnd\".split():\n    cls = getattr(pd.tseries.offsets, name)\n    st.register_type_strategy(\n        cls,\n        st.builds(\n            cls,\n            n=st.integers(-24, 24),\n            normalize=st.booleans(),\n            startingMonth=st.integers(min_value=1, max_value=12),\n        ),\n    )\n\n\n# ----------------------------------------------------------------\n# Autouse fixtures\n# ----------------------------------------------------------------\n\n\n# https://github.com/pytest-dev/pytest/issues/11873\n# Would like to avoid autouse=True, but cannot as of pytest 8.0.0\n@pytest.fixture(autouse=True)\ndef add_doctest_imports(doctest_namespace) -> None:\n    \"\"\"\n    Make `np` and `pd` names available for doctests.\n    \"\"\"\n    doctest_namespace[\"np\"] = np\n    doctest_namespace[\"pd\"] = pd\n\n\n@pytest.fixture(autouse=True)\ndef configure_tests() -> None:\n    \"\"\"\n    Configure settings for all tests and test modules.\n    \"\"\"\n    pd.set_option(\"chained_assignment\", \"raise\")\n\n\n# ----------------------------------------------------------------\n# Common arguments\n# ----------------------------------------------------------------\n@pytest.fixture(params=[0, 1, \"index\", \"columns\"], ids=lambda x: f\"axis={x!r}\")\ndef axis(request):\n    \"\"\"\n    Fixture for returning the axis numbers of a DataFrame.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef observed(request):\n    \"\"\"\n    Pass in the observed keyword to groupby for [True, False]\n    This indicates whether categoricals should return values for\n    values which are not in the grouper [False / None], or only values which\n    appear in the grouper [True]. [None] is supported for future compatibility\n    if we decide to change the default (and would need to warn if this\n    parameter is not passed).\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False, None])\ndef ordered(request):\n    \"\"\"\n    Boolean 'ordered' parameter for Categorical.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef dropna(request):\n    \"\"\"\n    Boolean 'dropna' parameter.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef sort(request):\n    \"\"\"\n    Boolean 'sort' parameter.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef skipna(request):\n    \"\"\"\n    Boolean 'skipna' parameter.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[\"first\", \"last\", False])\ndef keep(request):\n    \"\"\"\n    Valid values for the 'keep' parameter used in\n    .duplicated or .drop_duplicates\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[\"both\", \"neither\", \"left\", \"right\"])\ndef inclusive_endpoints_fixture(request):\n    \"\"\"\n    Fixture for trying all interval 'inclusive' parameters.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[\"left\", \"right\", \"both\", \"neither\"])\ndef closed(request):\n    \"\"\"\n    Fixture for trying all interval closed parameters.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[\"left\", \"right\", \"both\", \"neither\"])\ndef other_closed(request):\n    \"\"\"\n    Secondary closed fixture to allow parametrizing over all pairs of closed.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        None,\n        \"gzip\",\n        \"bz2\",\n        \"zip\",\n        \"xz\",\n        \"tar\",\n        pytest.param(\"zstd\", marks=td.skip_if_no(\"zstandard\")),\n    ]\n)\ndef compression(request):\n    \"\"\"\n    Fixture for trying common compression types in compression tests.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        \"gzip\",\n        \"bz2\",\n        \"zip\",\n        \"xz\",\n        \"tar\",\n        pytest.param(\"zstd\", marks=td.skip_if_no(\"zstandard\")),\n    ]\n)\ndef compression_only(request):\n    \"\"\"\n    Fixture for trying common compression types in compression tests excluding\n    uncompressed case.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef writable(request):\n    \"\"\"\n    Fixture that an array is writable.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[\"inner\", \"outer\", \"left\", \"right\"])\ndef join_type(request):\n    \"\"\"\n    Fixture for trying all types of join operations.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[\"nlargest\", \"nsmallest\"])\ndef nselect_method(request):\n    \"\"\"\n    Fixture for trying all nselect methods.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[None, \"ignore\"])\ndef na_action(request):\n    \"\"\"\n    Fixture for 'na_action' argument in map.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef ascending(request):\n    \"\"\"\n    Fixture for 'na_action' argument in sort_values/sort_index/rank.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[\"average\", \"min\", \"max\", \"first\", \"dense\"])\ndef rank_method(request):\n    \"\"\"\n    Fixture for 'rank' argument in rank.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef as_index(request):\n    \"\"\"\n    Fixture for 'as_index' argument in groupby.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef cache(request):\n    \"\"\"\n    Fixture for 'cache' argument in to_datetime.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef parallel(request):\n    \"\"\"\n    Fixture for parallel keyword argument for numba.jit.\n    \"\"\"\n    return request.param\n\n\n# Can parameterize nogil & nopython over True | False, but limiting per\n# https://github.com/pandas-dev/pandas/pull/41971#issuecomment-860607472\n\n\n@pytest.fixture(params=[False])\ndef nogil(request):\n    \"\"\"\n    Fixture for nogil keyword argument for numba.jit.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True])\ndef nopython(request):\n    \"\"\"\n    Fixture for nopython keyword argument for numba.jit.\n    \"\"\"\n    return request.param\n\n\n# ----------------------------------------------------------------\n# Missing values & co.\n# ----------------------------------------------------------------\n@pytest.fixture(params=tm.NULL_OBJECTS, ids=lambda x: type(x).__name__)\ndef nulls_fixture(request):\n    \"\"\"\n    Fixture for each null type in pandas.\n    \"\"\"\n    return request.param\n\n\nnulls_fixture2 = nulls_fixture  # Generate cartesian product of nulls_fixture\n\n\n@pytest.fixture(params=[None, np.nan, pd.NaT])\ndef unique_nulls_fixture(request):\n    \"\"\"\n    Fixture for each null type in pandas, each null type exactly once.\n    \"\"\"\n    return request.param\n\n\n# Generate cartesian product of unique_nulls_fixture:\nunique_nulls_fixture2 = unique_nulls_fixture\n\n\n@pytest.fixture(params=tm.NP_NAT_OBJECTS, ids=lambda x: type(x).__name__)\ndef np_nat_fixture(request):\n    \"\"\"\n    Fixture for each NaT type in numpy.\n    \"\"\"\n    return request.param\n\n\n# Generate cartesian product of np_nat_fixture:\nnp_nat_fixture2 = np_nat_fixture\n\n\n# ----------------------------------------------------------------\n# Classes\n# ----------------------------------------------------------------\n\n\n@pytest.fixture(params=[DataFrame, Series])\ndef frame_or_series(request):\n    \"\"\"\n    Fixture to parametrize over DataFrame and Series.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[Index, Series], ids=[\"index\", \"series\"])\ndef index_or_series(request):\n    \"\"\"\n    Fixture to parametrize over Index and Series, made necessary by a mypy\n    bug, giving an error:\n\n    List item 0 has incompatible type \"Type[Series]\"; expected \"Type[PandasObject]\"\n\n    See GH#29725\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[Index, Series, pd.array], ids=[\"index\", \"series\", \"array\"])\ndef index_or_series_or_array(request):\n    \"\"\"\n    Fixture to parametrize over Index, Series, and ExtensionArray\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[Index, Series, DataFrame, pd.array], ids=lambda x: x.__name__)\ndef box_with_array(request):\n    \"\"\"\n    Fixture to test behavior for Index, Series, DataFrame, and pandas Array\n    classes\n    \"\"\"\n    return request.param\n\n\nbox_with_array2 = box_with_array\n\n\n@pytest.fixture\ndef dict_subclass() -> type[dict]:\n    \"\"\"\n    Fixture for a dictionary subclass.\n    \"\"\"\n\n    class TestSubDict(dict):\n        def __init__(self, *args, **kwargs) -> None:\n            dict.__init__(self, *args, **kwargs)\n\n    return TestSubDict\n\n\n@pytest.fixture\ndef non_dict_mapping_subclass() -> type[abc.Mapping]:\n    \"\"\"\n    Fixture for a non-mapping dictionary subclass.\n    \"\"\"\n\n    class TestNonDictMapping(abc.Mapping):\n        def __init__(self, underlying_dict) -> None:\n            self._data = underlying_dict\n\n        def __getitem__(self, key):\n            return self._data.__getitem__(key)\n\n        def __iter__(self) -> Iterator:\n            return self._data.__iter__()\n\n        def __len__(self) -> int:\n            return self._data.__len__()\n\n    return TestNonDictMapping\n\n\n# ----------------------------------------------------------------\n# Indices\n# ----------------------------------------------------------------\n@pytest.fixture\ndef multiindex_year_month_day_dataframe_random_data():\n    \"\"\"\n    DataFrame with 3 level MultiIndex (year, month, day) covering\n    first 100 business days from 2000-01-01 with random data\n    \"\"\"\n    tdf = DataFrame(\n        np.random.default_rng(2).standard_normal((100, 4)),\n        columns=Index(list(\"ABCD\"), dtype=object),\n        index=date_range(\"2000-01-01\", periods=100, freq=\"B\"),\n    )\n    ymd = tdf.groupby([lambda x: x.year, lambda x: x.month, lambda x: x.day]).sum()\n    # use int64 Index, to make sure things work\n    ymd.index = ymd.index.set_levels([lev.astype(\"i8\") for lev in ymd.index.levels])\n    ymd.index.set_names([\"year\", \"month\", \"day\"], inplace=True)\n    return ymd\n\n\n@pytest.fixture\ndef lexsorted_two_level_string_multiindex() -> MultiIndex:\n    \"\"\"\n    2-level MultiIndex, lexsorted, with string names.\n    \"\"\"\n    return MultiIndex(\n        levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n        codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n        names=[\"first\", \"second\"],\n    )\n\n\n@pytest.fixture\ndef multiindex_dataframe_random_data(\n    lexsorted_two_level_string_multiindex,\n) -> DataFrame:\n    \"\"\"DataFrame with 2 level MultiIndex with random data\"\"\"\n    index = lexsorted_two_level_string_multiindex\n    return DataFrame(\n        np.random.default_rng(2).standard_normal((10, 3)),\n        index=index,\n        columns=Index([\"A\", \"B\", \"C\"], name=\"exp\"),\n    )\n\n\ndef _create_multiindex():\n    \"\"\"\n    MultiIndex used to test the general functionality of this object\n    \"\"\"\n\n    # See Also: tests.multi.conftest.idx\n    major_axis = Index([\"foo\", \"bar\", \"baz\", \"qux\"])\n    minor_axis = Index([\"one\", \"two\"])\n\n    major_codes = np.array([0, 0, 1, 2, 3, 3])\n    minor_codes = np.array([0, 1, 0, 1, 0, 1])\n    index_names = [\"first\", \"second\"]\n    return MultiIndex(\n        levels=[major_axis, minor_axis],\n        codes=[major_codes, minor_codes],\n        names=index_names,\n        verify_integrity=False,\n    )\n\n\ndef _create_mi_with_dt64tz_level():\n    \"\"\"\n    MultiIndex with a level that is a tzaware DatetimeIndex.\n    \"\"\"\n    # GH#8367 round trip with pickle\n    return MultiIndex.from_product(\n        [[1, 2], [\"a\", \"b\"], date_range(\"20130101\", periods=3, tz=\"US/Eastern\")],\n        names=[\"one\", \"two\", \"three\"],\n    )\n\n\nindices_dict = {\n    \"string\": Index([f\"pandas_{i}\" for i in range(10)]),\n    \"datetime\": date_range(\"2020-01-01\", periods=10),\n    \"datetime-tz\": date_range(\"2020-01-01\", periods=10, tz=\"US/Pacific\"),\n    \"period\": period_range(\"2020-01-01\", periods=10, freq=\"D\"),\n    \"timedelta\": timedelta_range(start=\"1 day\", periods=10, freq=\"D\"),\n    \"range\": RangeIndex(10),\n    \"int8\": Index(np.arange(10), dtype=\"int8\"),\n    \"int16\": Index(np.arange(10), dtype=\"int16\"),\n    \"int32\": Index(np.arange(10), dtype=\"int32\"),\n    \"int64\": Index(np.arange(10), dtype=\"int64\"),\n    \"uint8\": Index(np.arange(10), dtype=\"uint8\"),\n    \"uint16\": Index(np.arange(10), dtype=\"uint16\"),\n    \"uint32\": Index(np.arange(10), dtype=\"uint32\"),\n    \"uint64\": Index(np.arange(10), dtype=\"uint64\"),\n    \"float32\": Index(np.arange(10), dtype=\"float32\"),\n    \"float64\": Index(np.arange(10), dtype=\"float64\"),\n    \"bool-object\": Index([True, False] * 5, dtype=object),\n    \"bool-dtype\": Index([True, False] * 5, dtype=bool),\n    \"complex64\": Index(\n        np.arange(10, dtype=\"complex64\") + 1.0j * np.arange(10, dtype=\"complex64\")\n    ),\n    \"complex128\": Index(\n        np.arange(10, dtype=\"complex128\") + 1.0j * np.arange(10, dtype=\"complex128\")\n    ),\n    \"categorical\": CategoricalIndex(list(\"abcd\") * 2),\n    \"interval\": IntervalIndex.from_breaks(np.linspace(0, 100, num=11)),\n    \"empty\": Index([]),\n    \"tuples\": MultiIndex.from_tuples(zip([\"foo\", \"bar\", \"baz\"], [1, 2, 3])),\n    \"mi-with-dt64tz-level\": _create_mi_with_dt64tz_level(),\n    \"multi\": _create_multiindex(),\n    \"repeats\": Index([0, 0, 1, 1, 2, 2]),\n    \"nullable_int\": Index(np.arange(10), dtype=\"Int64\"),\n    \"nullable_uint\": Index(np.arange(10), dtype=\"UInt16\"),\n    \"nullable_float\": Index(np.arange(10), dtype=\"Float32\"),\n    \"nullable_bool\": Index(np.arange(10).astype(bool), dtype=\"boolean\"),\n    \"string-python\": Index(\n        pd.array([f\"pandas_{i}\" for i in range(10)], dtype=\"string[python]\")\n    ),\n}\nif has_pyarrow:\n    idx = Index(pd.array([f\"pandas_{i}\" for i in range(10)], dtype=\"string[pyarrow]\"))\n    indices_dict[\"string-pyarrow\"] = idx\n\n\n@pytest.fixture(params=indices_dict.keys())\ndef index(request):\n    \"\"\"\n    Fixture for many \"simple\" kinds of indices.\n\n    These indices are unlikely to cover corner cases, e.g.\n        - no names\n        - no NaTs/NaNs\n        - no values near implementation bounds\n        - ...\n    \"\"\"\n    # copy to avoid mutation, e.g. setting .name\n    return indices_dict[request.param].copy()\n\n\n@pytest.fixture(\n    params=[\n        key for key, value in indices_dict.items() if not isinstance(value, MultiIndex)\n    ]\n)\ndef index_flat(request):\n    \"\"\"\n    index fixture, but excluding MultiIndex cases.\n    \"\"\"\n    key = request.param\n    return indices_dict[key].copy()\n\n\n@pytest.fixture(\n    params=[\n        key\n        for key, value in indices_dict.items()\n        if not (\n            key.startswith((\"int\", \"uint\", \"float\"))\n            or key in [\"range\", \"empty\", \"repeats\", \"bool-dtype\"]\n        )\n        and not isinstance(value, MultiIndex)\n    ]\n)\ndef index_with_missing(request):\n    \"\"\"\n    Fixture for indices with missing values.\n\n    Integer-dtype and empty cases are excluded because they cannot hold missing\n    values.\n\n    MultiIndex is excluded because isna() is not defined for MultiIndex.\n    \"\"\"\n\n    # GH 35538. Use deep copy to avoid illusive bug on np-dev\n    # GHA pipeline that writes into indices_dict despite copy\n    ind = indices_dict[request.param].copy(deep=True)\n    vals = ind.values.copy()\n    if request.param in [\"tuples\", \"mi-with-dt64tz-level\", \"multi\"]:\n        # For setting missing values in the top level of MultiIndex\n        vals = ind.tolist()\n        vals[0] = (None,) + vals[0][1:]\n        vals[-1] = (None,) + vals[-1][1:]\n        return MultiIndex.from_tuples(vals)\n    else:\n        vals[0] = None\n        vals[-1] = None\n        return type(ind)(vals)\n\n\n# ----------------------------------------------------------------\n# Series'\n# ----------------------------------------------------------------\n@pytest.fixture\ndef string_series() -> Series:\n    \"\"\"\n    Fixture for Series of floats with Index of unique strings\n    \"\"\"\n    return Series(\n        np.arange(30, dtype=np.float64) * 1.1,\n        index=Index([f\"i_{i}\" for i in range(30)], dtype=object),\n        name=\"series\",\n    )\n\n\n@pytest.fixture\ndef object_series() -> Series:\n    \"\"\"\n    Fixture for Series of dtype object with Index of unique strings\n    \"\"\"\n    data = [f\"foo_{i}\" for i in range(30)]\n    index = Index([f\"bar_{i}\" for i in range(30)], dtype=object)\n    return Series(data, index=index, name=\"objects\", dtype=object)\n\n\n@pytest.fixture\ndef datetime_series() -> Series:\n    \"\"\"\n    Fixture for Series of floats with DatetimeIndex\n    \"\"\"\n    return Series(\n        np.random.default_rng(2).standard_normal(30),\n        index=date_range(\"2000-01-01\", periods=30, freq=\"B\"),\n        name=\"ts\",\n    )\n\n\ndef _create_series(index):\n    \"\"\"Helper for the _series dict\"\"\"\n    size = len(index)\n    data = np.random.default_rng(2).standard_normal(size)\n    return Series(data, index=index, name=\"a\", copy=False)\n\n\n_series = {\n    f\"series-with-{index_id}-index\": _create_series(index)\n    for index_id, index in indices_dict.items()\n}\n\n\n@pytest.fixture\ndef series_with_simple_index(index) -> Series:\n    \"\"\"\n    Fixture for tests on series with changing types of indices.\n    \"\"\"\n    return _create_series(index)\n\n\n_narrow_series = {\n    f\"{dtype.__name__}-series\": Series(\n        range(30), index=[f\"i-{i}\" for i in range(30)], name=\"a\", dtype=dtype\n    )\n    for dtype in tm.NARROW_NP_DTYPES\n}\n\n\n_index_or_series_objs = {**indices_dict, **_series, **_narrow_series}\n\n\n@pytest.fixture(params=_index_or_series_objs.keys())\ndef index_or_series_obj(request):\n    \"\"\"\n    Fixture for tests on indexes, series and series with a narrow dtype\n    copy to avoid mutation, e.g. setting .name\n    \"\"\"\n    return _index_or_series_objs[request.param].copy(deep=True)\n\n\n_typ_objects_series = {\n    f\"{dtype.__name__}-series\": Series(dtype) for dtype in tm.PYTHON_DATA_TYPES\n}\n\n\n_index_or_series_memory_objs = {\n    **indices_dict,\n    **_series,\n    **_narrow_series,\n    **_typ_objects_series,\n}\n\n\n@pytest.fixture(params=_index_or_series_memory_objs.keys())\ndef index_or_series_memory_obj(request):\n    \"\"\"\n    Fixture for tests on indexes, series, series with a narrow dtype and\n    series with empty objects type\n    copy to avoid mutation, e.g. setting .name\n    \"\"\"\n    return _index_or_series_memory_objs[request.param].copy(deep=True)\n\n\n# ----------------------------------------------------------------\n# DataFrames\n# ----------------------------------------------------------------\n@pytest.fixture\ndef int_frame() -> DataFrame:\n    \"\"\"\n    Fixture for DataFrame of ints with index of unique strings\n\n    Columns are ['A', 'B', 'C', 'D']\n    \"\"\"\n    return DataFrame(\n        np.ones((30, 4), dtype=np.int64),\n        index=Index([f\"foo_{i}\" for i in range(30)], dtype=object),\n        columns=Index(list(\"ABCD\"), dtype=object),\n    )\n\n\n@pytest.fixture\ndef float_frame() -> DataFrame:\n    \"\"\"\n    Fixture for DataFrame of floats with index of unique strings\n\n    Columns are ['A', 'B', 'C', 'D'].\n    \"\"\"\n    return DataFrame(\n        np.random.default_rng(2).standard_normal((30, 4)),\n        index=Index([f\"foo_{i}\" for i in range(30)]),\n        columns=Index(list(\"ABCD\")),\n    )\n\n\n@pytest.fixture\ndef rand_series_with_duplicate_datetimeindex() -> Series:\n    \"\"\"\n    Fixture for Series with a DatetimeIndex that has duplicates.\n    \"\"\"\n    dates = [\n        datetime(2000, 1, 2),\n        datetime(2000, 1, 2),\n        datetime(2000, 1, 2),\n        datetime(2000, 1, 3),\n        datetime(2000, 1, 3),\n        datetime(2000, 1, 3),\n        datetime(2000, 1, 4),\n        datetime(2000, 1, 4),\n        datetime(2000, 1, 4),\n        datetime(2000, 1, 5),\n    ]\n\n    return Series(np.random.default_rng(2).standard_normal(len(dates)), index=dates)\n\n\n# ----------------------------------------------------------------\n# Scalars\n# ----------------------------------------------------------------\n@pytest.fixture(\n    params=[\n        (Interval(left=0, right=5), IntervalDtype(\"int64\", \"right\")),\n        (Interval(left=0.1, right=0.5), IntervalDtype(\"float64\", \"right\")),\n        (Period(\"2012-01\", freq=\"M\"), \"period[M]\"),\n        (Period(\"2012-02-01\", freq=\"D\"), \"period[D]\"),\n        (\n            Timestamp(\"2011-01-01\", tz=\"US/Eastern\"),\n            DatetimeTZDtype(unit=\"s\", tz=\"US/Eastern\"),\n        ),\n        (Timedelta(seconds=500), \"timedelta64[ns]\"),\n    ]\n)\ndef ea_scalar_and_dtype(request):\n    \"\"\"\n    Fixture that tests each scalar and datetime type.\n    \"\"\"\n    return request.param\n\n\n# ----------------------------------------------------------------\n# Operators & Operations\n# ----------------------------------------------------------------\n\n\n@pytest.fixture(params=tm.arithmetic_dunder_methods)\ndef all_arithmetic_operators(request):\n    \"\"\"\n    Fixture for dunder names for common arithmetic operations.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        operator.add,\n        ops.radd,\n        operator.sub,\n        ops.rsub,\n        operator.mul,\n        ops.rmul,\n        operator.truediv,\n        ops.rtruediv,\n        operator.floordiv,\n        ops.rfloordiv,\n        operator.mod,\n        ops.rmod,\n        operator.pow,\n        ops.rpow,\n        operator.eq,\n        operator.ne,\n        operator.lt,\n        operator.le,\n        operator.gt,\n        operator.ge,\n        operator.and_,\n        ops.rand_,\n        operator.xor,\n        ops.rxor,\n        operator.or_,\n        ops.ror_,\n    ]\n)\ndef all_binary_operators(request):\n    \"\"\"\n    Fixture for operator and roperator arithmetic, comparison, and logical ops.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        operator.add,\n        ops.radd,\n        operator.sub,\n        ops.rsub,\n        operator.mul,\n        ops.rmul,\n        operator.truediv,\n        ops.rtruediv,\n        operator.floordiv,\n        ops.rfloordiv,\n        operator.mod,\n        ops.rmod,\n        operator.pow,\n        ops.rpow,\n    ]\n)\ndef all_arithmetic_functions(request):\n    \"\"\"\n    Fixture for operator and roperator arithmetic functions.\n\n    Notes\n    -----\n    This includes divmod and rdivmod, whereas all_arithmetic_operators\n    does not.\n    \"\"\"\n    return request.param\n\n\n_all_numeric_reductions = [\n    \"count\",\n    \"sum\",\n    \"max\",\n    \"min\",\n    \"mean\",\n    \"prod\",\n    \"std\",\n    \"var\",\n    \"median\",\n    \"kurt\",\n    \"skew\",\n    \"sem\",\n]\n\n\n@pytest.fixture(params=_all_numeric_reductions)\ndef all_numeric_reductions(request):\n    \"\"\"\n    Fixture for numeric reduction names.\n    \"\"\"\n    return request.param\n\n\n_all_boolean_reductions = [\"all\", \"any\"]\n\n\n@pytest.fixture(params=_all_boolean_reductions)\ndef all_boolean_reductions(request):\n    \"\"\"\n    Fixture for boolean reduction names.\n    \"\"\"\n    return request.param\n\n\n_all_reductions = _all_numeric_reductions + _all_boolean_reductions\n\n\n@pytest.fixture(params=_all_reductions)\ndef all_reductions(request):\n    \"\"\"\n    Fixture for all (boolean + numeric) reduction names.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        operator.eq,\n        operator.ne,\n        operator.gt,\n        operator.ge,\n        operator.lt,\n        operator.le,\n    ]\n)\ndef comparison_op(request):\n    \"\"\"\n    Fixture for operator module comparison functions.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[\"__le__\", \"__lt__\", \"__ge__\", \"__gt__\"])\ndef compare_operators_no_eq_ne(request):\n    \"\"\"\n    Fixture for dunder names for compare operations except == and !=\n\n    * >=\n    * >\n    * <\n    * <=\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\"__and__\", \"__rand__\", \"__or__\", \"__ror__\", \"__xor__\", \"__rxor__\"]\n)\ndef all_logical_operators(request):\n    \"\"\"\n    Fixture for dunder names for common logical operations\n\n    * |\n    * &\n    * ^\n    \"\"\"\n    return request.param\n\n\n_all_numeric_accumulations = [\"cumsum\", \"cumprod\", \"cummin\", \"cummax\"]\n\n\n@pytest.fixture(params=_all_numeric_accumulations)\ndef all_numeric_accumulations(request):\n    \"\"\"\n    Fixture for numeric accumulation names\n    \"\"\"\n    return request.param\n\n\n# ----------------------------------------------------------------\n# Data sets/files\n# ----------------------------------------------------------------\n@pytest.fixture\ndef strict_data_files(pytestconfig):\n    \"\"\"\n    Returns the configuration for the test setting `--no-strict-data-files`.\n    \"\"\"\n    return pytestconfig.getoption(\"--no-strict-data-files\")\n\n\n@pytest.fixture\ndef datapath(strict_data_files: str) -> Callable[..., str]:\n    \"\"\"\n    Get the path to a data file.\n\n    Parameters\n    ----------\n    path : str\n        Path to the file, relative to ``pandas/tests/``\n\n    Returns\n    -------\n    path including ``pandas/tests``.\n\n    Raises\n    ------\n    ValueError\n        If the path doesn't exist and the --no-strict-data-files option is not set.\n    \"\"\"\n    BASE_PATH = os.path.join(os.path.dirname(__file__), \"tests\")\n\n    def deco(*args):\n        path = os.path.join(BASE_PATH, *args)\n        if not os.path.exists(path):\n            if strict_data_files:\n                raise ValueError(\n                    f\"Could not find file {path} and --no-strict-data-files is not set.\"\n                )\n            pytest.skip(f\"Could not find {path}.\")\n        return path\n\n    return deco\n\n\n# ----------------------------------------------------------------\n# Time zones\n# ----------------------------------------------------------------\nTIMEZONES = [\n    None,\n    \"UTC\",\n    \"US/Eastern\",\n    \"Asia/Tokyo\",\n    \"dateutil/US/Pacific\",\n    \"dateutil/Asia/Singapore\",\n    \"+01:15\",\n    \"-02:15\",\n    \"UTC+01:15\",\n    \"UTC-02:15\",\n    tzutc(),\n    tzlocal(),\n    timezone.utc,\n    timezone(timedelta(hours=1)),\n    timezone(timedelta(hours=-1), name=\"foo\"),\n]\nif pytz is not None:\n    TIMEZONES.extend(\n        (\n            pytz.FixedOffset(300),\n            pytz.FixedOffset(0),\n            pytz.FixedOffset(-300),\n            pytz.timezone(\"US/Pacific\"),\n            pytz.timezone(\"UTC\"),\n        )\n    )\nTIMEZONE_IDS = [repr(i) for i in TIMEZONES]\n\n\n@td.parametrize_fixture_doc(str(TIMEZONE_IDS))\n@pytest.fixture(params=TIMEZONES, ids=TIMEZONE_IDS)\ndef tz_naive_fixture(request):\n    \"\"\"\n    Fixture for trying timezones including default (None): {0}\n    \"\"\"\n    return request.param\n\n\n@td.parametrize_fixture_doc(str(TIMEZONE_IDS[1:]))\n@pytest.fixture(params=TIMEZONES[1:], ids=TIMEZONE_IDS[1:])\ndef tz_aware_fixture(request):\n    \"\"\"\n    Fixture for trying explicit timezones: {0}\n    \"\"\"\n    return request.param\n\n\n_UTCS = [\"utc\", \"dateutil/UTC\", tzutc(), timezone.utc]\n\nif pytz is not None:\n    _UTCS.append(pytz.utc)\n\n\n@pytest.fixture(params=_UTCS)\ndef utc_fixture(request):\n    \"\"\"\n    Fixture to provide variants of UTC timezone strings and tzinfo objects.\n    \"\"\"\n    return request.param\n\n\nutc_fixture2 = utc_fixture\n\n\n@pytest.fixture(params=[\"s\", \"ms\", \"us\", \"ns\"])\ndef unit(request):\n    \"\"\"\n    datetime64 units we support.\n    \"\"\"\n    return request.param\n\n\nunit2 = unit\n\n\n# ----------------------------------------------------------------\n# Dtypes\n# ----------------------------------------------------------------\n@pytest.fixture(params=tm.STRING_DTYPES)\ndef string_dtype(request):\n    \"\"\"\n    Parametrized fixture for string dtypes.\n\n    * str\n    * 'str'\n    * 'U'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        \"string[python]\",\n        pytest.param(\"string[pyarrow]\", marks=td.skip_if_no(\"pyarrow\")),\n    ]\n)\ndef nullable_string_dtype(request):\n    \"\"\"\n    Parametrized fixture for string dtypes.\n\n    * 'string[python]'\n    * 'string[pyarrow]'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        \"python\",\n        pytest.param(\"pyarrow\", marks=td.skip_if_no(\"pyarrow\")),\n    ]\n)\ndef string_storage(request):\n    \"\"\"\n    Parametrized fixture for pd.options.mode.string_storage.\n\n    * 'python'\n    * 'pyarrow'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        (\"python\", pd.NA),\n        pytest.param((\"pyarrow\", pd.NA), marks=td.skip_if_no(\"pyarrow\")),\n        pytest.param((\"pyarrow\", np.nan), marks=td.skip_if_no(\"pyarrow\")),\n        (\"python\", np.nan),\n    ]\n)\ndef string_dtype_arguments(request):\n    \"\"\"\n    Parametrized fixture for StringDtype storage and na_value.\n\n    * 'python' + pd.NA\n    * 'pyarrow' + pd.NA\n    * 'pyarrow' + np.nan\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        \"numpy_nullable\",\n        pytest.param(\"pyarrow\", marks=td.skip_if_no(\"pyarrow\")),\n    ]\n)\ndef dtype_backend(request):\n    \"\"\"\n    Parametrized fixture for pd.options.mode.string_storage.\n\n    * 'python'\n    * 'pyarrow'\n    \"\"\"\n    return request.param\n\n\n# Alias so we can test with cartesian product of string_storage\nstring_storage2 = string_storage\n\n\n@pytest.fixture(params=tm.BYTES_DTYPES)\ndef bytes_dtype(request):\n    \"\"\"\n    Parametrized fixture for bytes dtypes.\n\n    * bytes\n    * 'bytes'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.OBJECT_DTYPES)\ndef object_dtype(request):\n    \"\"\"\n    Parametrized fixture for object dtypes.\n\n    * object\n    * 'object'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        np.dtype(\"object\"),\n        (\"python\", pd.NA),\n        pytest.param((\"pyarrow\", pd.NA), marks=td.skip_if_no(\"pyarrow\")),\n        pytest.param((\"pyarrow\", np.nan), marks=td.skip_if_no(\"pyarrow\")),\n        (\"python\", np.nan),\n    ],\n    ids=[\n        \"string=object\",\n        \"string=string[python]\",\n        \"string=string[pyarrow]\",\n        \"string=str[pyarrow]\",\n        \"string=str[python]\",\n    ],\n)\ndef any_string_dtype(request):\n    \"\"\"\n    Parametrized fixture for string dtypes.\n    * 'object'\n    * 'string[python]' (NA variant)\n    * 'string[pyarrow]' (NA variant)\n    * 'str' (NaN variant, with pyarrow)\n    * 'str' (NaN variant, without pyarrow)\n    \"\"\"\n    if isinstance(request.param, np.dtype):\n        return request.param\n    else:\n        # need to instantiate the StringDtype here instead of in the params\n        # to avoid importing pyarrow during test collection\n        storage, na_value = request.param\n        return pd.StringDtype(storage, na_value)\n\n\n@pytest.fixture(params=tm.DATETIME64_DTYPES)\ndef datetime64_dtype(request):\n    \"\"\"\n    Parametrized fixture for datetime64 dtypes.\n\n    * 'datetime64[ns]'\n    * 'M8[ns]'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.TIMEDELTA64_DTYPES)\ndef timedelta64_dtype(request):\n    \"\"\"\n    Parametrized fixture for timedelta64 dtypes.\n\n    * 'timedelta64[ns]'\n    * 'm8[ns]'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture\ndef fixed_now_ts() -> Timestamp:\n    \"\"\"\n    Fixture emits fixed Timestamp.now()\n    \"\"\"\n    return Timestamp(  # pyright: ignore[reportReturnType]\n        year=2021, month=1, day=1, hour=12, minute=4, second=13, microsecond=22\n    )\n\n\n@pytest.fixture(params=tm.FLOAT_NUMPY_DTYPES)\ndef float_numpy_dtype(request):\n    \"\"\"\n    Parameterized fixture for float dtypes.\n\n    * float\n    * 'float32'\n    * 'float64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.FLOAT_EA_DTYPES)\ndef float_ea_dtype(request):\n    \"\"\"\n    Parameterized fixture for float dtypes.\n\n    * 'Float32'\n    * 'Float64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_FLOAT_DTYPES)\ndef any_float_dtype(request):\n    \"\"\"\n    Parameterized fixture for float dtypes.\n\n    * float\n    * 'float32'\n    * 'float64'\n    * 'Float32'\n    * 'Float64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.COMPLEX_DTYPES)\ndef complex_dtype(request):\n    \"\"\"\n    Parameterized fixture for complex dtypes.\n\n    * complex\n    * 'complex64'\n    * 'complex128'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.COMPLEX_FLOAT_DTYPES)\ndef complex_or_float_dtype(request):\n    \"\"\"\n    Parameterized fixture for complex and numpy float dtypes.\n\n    * complex\n    * 'complex64'\n    * 'complex128'\n    * float\n    * 'float32'\n    * 'float64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.SIGNED_INT_NUMPY_DTYPES)\ndef any_signed_int_numpy_dtype(request):\n    \"\"\"\n    Parameterized fixture for signed integer dtypes.\n\n    * int\n    * 'int8'\n    * 'int16'\n    * 'int32'\n    * 'int64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.UNSIGNED_INT_NUMPY_DTYPES)\ndef any_unsigned_int_numpy_dtype(request):\n    \"\"\"\n    Parameterized fixture for unsigned integer dtypes.\n\n    * 'uint8'\n    * 'uint16'\n    * 'uint32'\n    * 'uint64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_INT_NUMPY_DTYPES)\ndef any_int_numpy_dtype(request):\n    \"\"\"\n    Parameterized fixture for any integer dtype.\n\n    * int\n    * 'int8'\n    * 'uint8'\n    * 'int16'\n    * 'uint16'\n    * 'int32'\n    * 'uint32'\n    * 'int64'\n    * 'uint64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_INT_EA_DTYPES)\ndef any_int_ea_dtype(request):\n    \"\"\"\n    Parameterized fixture for any nullable integer dtype.\n\n    * 'UInt8'\n    * 'Int8'\n    * 'UInt16'\n    * 'Int16'\n    * 'UInt32'\n    * 'Int32'\n    * 'UInt64'\n    * 'Int64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_INT_DTYPES)\ndef any_int_dtype(request):\n    \"\"\"\n    Parameterized fixture for any nullable integer dtype.\n\n    * int\n    * 'int8'\n    * 'uint8'\n    * 'int16'\n    * 'uint16'\n    * 'int32'\n    * 'uint32'\n    * 'int64'\n    * 'uint64'\n    * 'UInt8'\n    * 'Int8'\n    * 'UInt16'\n    * 'Int16'\n    * 'UInt32'\n    * 'Int32'\n    * 'UInt64'\n    * 'Int64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_INT_EA_DTYPES + tm.FLOAT_EA_DTYPES)\ndef any_numeric_ea_dtype(request):\n    \"\"\"\n    Parameterized fixture for any nullable integer dtype and\n    any float ea dtypes.\n\n    * 'UInt8'\n    * 'Int8'\n    * 'UInt16'\n    * 'Int16'\n    * 'UInt32'\n    * 'Int32'\n    * 'UInt64'\n    * 'Int64'\n    * 'Float32'\n    * 'Float64'\n    \"\"\"\n    return request.param\n\n\n#  Unsupported operand types for + (\"List[Union[str, ExtensionDtype, dtype[Any],\n#  Type[object]]]\" and \"List[str]\")\n@pytest.fixture(\n    params=tm.ALL_INT_EA_DTYPES\n    + tm.FLOAT_EA_DTYPES\n    + tm.ALL_INT_PYARROW_DTYPES_STR_REPR\n    + tm.FLOAT_PYARROW_DTYPES_STR_REPR  # type: ignore[operator]\n)\ndef any_numeric_ea_and_arrow_dtype(request):\n    \"\"\"\n    Parameterized fixture for any nullable integer dtype and\n    any float ea dtypes.\n\n    * 'UInt8'\n    * 'Int8'\n    * 'UInt16'\n    * 'Int16'\n    * 'UInt32'\n    * 'Int32'\n    * 'UInt64'\n    * 'Int64'\n    * 'Float32'\n    * 'Float64'\n    * 'uint8[pyarrow]'\n    * 'int8[pyarrow]'\n    * 'uint16[pyarrow]'\n    * 'int16[pyarrow]'\n    * 'uint32[pyarrow]'\n    * 'int32[pyarrow]'\n    * 'uint64[pyarrow]'\n    * 'int64[pyarrow]'\n    * 'float32[pyarrow]'\n    * 'float64[pyarrow]'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.SIGNED_INT_EA_DTYPES)\ndef any_signed_int_ea_dtype(request):\n    \"\"\"\n    Parameterized fixture for any signed nullable integer dtype.\n\n    * 'Int8'\n    * 'Int16'\n    * 'Int32'\n    * 'Int64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_REAL_NUMPY_DTYPES)\ndef any_real_numpy_dtype(request):\n    \"\"\"\n    Parameterized fixture for any (purely) real numeric dtype.\n\n    * int\n    * 'int8'\n    * 'uint8'\n    * 'int16'\n    * 'uint16'\n    * 'int32'\n    * 'uint32'\n    * 'int64'\n    * 'uint64'\n    * float\n    * 'float32'\n    * 'float64'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_REAL_DTYPES)\ndef any_real_numeric_dtype(request):\n    \"\"\"\n    Parameterized fixture for any (purely) real numeric dtype.\n\n    * int\n    * 'int8'\n    * 'uint8'\n    * 'int16'\n    * 'uint16'\n    * 'int32'\n    * 'uint32'\n    * 'int64'\n    * 'uint64'\n    * float\n    * 'float32'\n    * 'float64'\n\n    and associated ea dtypes.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_NUMPY_DTYPES)\ndef any_numpy_dtype(request):\n    \"\"\"\n    Parameterized fixture for all numpy dtypes.\n\n    * bool\n    * 'bool'\n    * int\n    * 'int8'\n    * 'uint8'\n    * 'int16'\n    * 'uint16'\n    * 'int32'\n    * 'uint32'\n    * 'int64'\n    * 'uint64'\n    * float\n    * 'float32'\n    * 'float64'\n    * complex\n    * 'complex64'\n    * 'complex128'\n    * str\n    * 'str'\n    * 'U'\n    * bytes\n    * 'bytes'\n    * 'datetime64[ns]'\n    * 'M8[ns]'\n    * 'timedelta64[ns]'\n    * 'm8[ns]'\n    * object\n    * 'object'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_REAL_NULLABLE_DTYPES)\ndef any_real_nullable_dtype(request):\n    \"\"\"\n    Parameterized fixture for all real dtypes that can hold NA.\n\n    * float\n    * 'float32'\n    * 'float64'\n    * 'Float32'\n    * 'Float64'\n    * 'UInt8'\n    * 'UInt16'\n    * 'UInt32'\n    * 'UInt64'\n    * 'Int8'\n    * 'Int16'\n    * 'Int32'\n    * 'Int64'\n    * 'uint8[pyarrow]'\n    * 'uint16[pyarrow]'\n    * 'uint32[pyarrow]'\n    * 'uint64[pyarrow]'\n    * 'int8[pyarrow]'\n    * 'int16[pyarrow]'\n    * 'int32[pyarrow]'\n    * 'int64[pyarrow]'\n    * 'float[pyarrow]'\n    * 'double[pyarrow]'\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=tm.ALL_NUMERIC_DTYPES)\ndef any_numeric_dtype(request):\n    \"\"\"\n    Parameterized fixture for all numeric dtypes.\n\n    * int\n    * 'int8'\n    * 'uint8'\n    * 'int16'\n    * 'uint16'\n    * 'int32'\n    * 'uint32'\n    * 'int64'\n    * 'uint64'\n    * float\n    * 'float32'\n    * 'float64'\n    * complex\n    * 'complex64'\n    * 'complex128'\n    * 'UInt8'\n    * 'Int8'\n    * 'UInt16'\n    * 'Int16'\n    * 'UInt32'\n    * 'Int32'\n    * 'UInt64'\n    * 'Int64'\n    * 'Float32'\n    * 'Float64'\n    \"\"\"\n    return request.param\n\n\n# categoricals are handled separately\n_any_skipna_inferred_dtype = [\n    (\"string\", [\"a\", np.nan, \"c\"]),\n    (\"string\", [\"a\", pd.NA, \"c\"]),\n    (\"mixed\", [\"a\", pd.NaT, \"c\"]),  # pd.NaT not considered valid by is_string_array\n    (\"bytes\", [b\"a\", np.nan, b\"c\"]),\n    (\"empty\", [np.nan, np.nan, np.nan]),\n    (\"empty\", []),\n    (\"mixed-integer\", [\"a\", np.nan, 2]),\n    (\"mixed\", [\"a\", np.nan, 2.0]),\n    (\"floating\", [1.0, np.nan, 2.0]),\n    (\"integer\", [1, np.nan, 2]),\n    (\"mixed-integer-float\", [1, np.nan, 2.0]),\n    (\"decimal\", [Decimal(1), np.nan, Decimal(2)]),\n    (\"boolean\", [True, np.nan, False]),\n    (\"boolean\", [True, pd.NA, False]),\n    (\"datetime64\", [np.datetime64(\"2013-01-01\"), np.nan, np.datetime64(\"2018-01-01\")]),\n    (\"datetime\", [Timestamp(\"20130101\"), np.nan, Timestamp(\"20180101\")]),\n    (\"date\", [date(2013, 1, 1), np.nan, date(2018, 1, 1)]),\n    (\"complex\", [1 + 1j, np.nan, 2 + 2j]),\n    # The following dtype is commented out due to GH 23554\n    # ('timedelta64', [np.timedelta64(1, 'D'),\n    #                  np.nan, np.timedelta64(2, 'D')]),\n    (\"timedelta\", [timedelta(1), np.nan, timedelta(2)]),\n    (\"time\", [time(1), np.nan, time(2)]),\n    (\"period\", [Period(2013), pd.NaT, Period(2018)]),\n    (\"interval\", [Interval(0, 1), np.nan, Interval(0, 2)]),\n]\nids, _ = zip(*_any_skipna_inferred_dtype)  # use inferred type as fixture-id\n\n\n@pytest.fixture(params=_any_skipna_inferred_dtype, ids=ids)\ndef any_skipna_inferred_dtype(request):\n    \"\"\"\n    Fixture for all inferred dtypes from _libs.lib.infer_dtype\n\n    The covered (inferred) types are:\n    * 'string'\n    * 'empty'\n    * 'bytes'\n    * 'mixed'\n    * 'mixed-integer'\n    * 'mixed-integer-float'\n    * 'floating'\n    * 'integer'\n    * 'decimal'\n    * 'boolean'\n    * 'datetime64'\n    * 'datetime'\n    * 'date'\n    * 'timedelta'\n    * 'time'\n    * 'period'\n    * 'interval'\n\n    Returns\n    -------\n    inferred_dtype : str\n        The string for the inferred dtype from _libs.lib.infer_dtype\n    values : np.ndarray\n        An array of object dtype that will be inferred to have\n        `inferred_dtype`\n\n    Examples\n    --------\n    >>> from pandas._libs import lib\n    >>>\n    >>> def test_something(any_skipna_inferred_dtype):\n    ...     inferred_dtype, values = any_skipna_inferred_dtype\n    ...     # will pass\n    ...     assert lib.infer_dtype(values, skipna=True) == inferred_dtype\n    \"\"\"\n    inferred_dtype, values = request.param\n    values = np.array(values, dtype=object)  # object dtype to avoid casting\n\n    # correctness of inference tested in tests/dtypes/test_inference.py\n    return inferred_dtype, values\n\n\n# ----------------------------------------------------------------\n# Misc\n# ----------------------------------------------------------------\n@pytest.fixture\ndef ip():\n    \"\"\"\n    Get an instance of IPython.InteractiveShell.\n\n    Will raise a skip if IPython is not installed.\n    \"\"\"\n    pytest.importorskip(\"IPython\", minversion=\"6.0.0\")\n    from IPython.core.interactiveshell import InteractiveShell\n\n    # GH#35711 make sure sqlite history file handle is not leaked\n    from traitlets.config import Config  # isort:skip\n\n    c = Config()\n    c.HistoryManager.hist_file = \":memory:\"\n\n    return InteractiveShell(config=c)\n\n\n@pytest.fixture\ndef mpl_cleanup():\n    \"\"\"\n    Ensure Matplotlib is cleaned up around a test.\n\n    Before a test is run:\n\n    1) Set the backend to \"template\" to avoid requiring a GUI.\n\n    After a test is run:\n\n    1) Reset units registry\n    2) Reset rc_context\n    3) Close all figures\n\n    See matplotlib/testing/decorators.py#L24.\n    \"\"\"\n    mpl = pytest.importorskip(\"matplotlib\")\n    mpl_units = pytest.importorskip(\"matplotlib.units\")\n    plt = pytest.importorskip(\"matplotlib.pyplot\")\n    orig_units_registry = mpl_units.registry.copy()\n    try:\n        with mpl.rc_context():\n            mpl.use(\"template\")\n            yield\n    finally:\n        mpl_units.registry.clear()\n        mpl_units.registry.update(orig_units_registry)\n        plt.close(\"all\")\n        # https://matplotlib.org/stable/users/prev_whats_new/whats_new_3.6.0.html#garbage-collection-is-no-longer-run-on-figure-close  # noqa: E501\n        gc.collect(1)\n\n\n@pytest.fixture(\n    params=[\n        getattr(pd.offsets, o)\n        for o in pd.offsets.__all__\n        if issubclass(getattr(pd.offsets, o), pd.offsets.Tick) and o != \"Tick\"\n    ]\n)\ndef tick_classes(request):\n    \"\"\"\n    Fixture for Tick based datetime offsets available for a time series.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[None, lambda x: x])\ndef sort_by_key(request):\n    \"\"\"\n    Simple fixture for testing keys in sorting methods.\n    Tests None (no key) and the identity key.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(\n    params=[\n        (\"foo\", None, None),\n        (\"Egon\", \"Venkman\", None),\n        (\"NCC1701D\", \"NCC1701D\", \"NCC1701D\"),\n        # possibly-matching NAs\n        (np.nan, np.nan, np.nan),\n        (np.nan, pd.NaT, None),\n        (np.nan, pd.NA, None),\n        (pd.NA, pd.NA, pd.NA),\n    ]\n)\ndef names(request) -> tuple[Hashable, Hashable, Hashable]:\n    \"\"\"\n    A 3-tuple of names, the first two for operands, the last for a result.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[tm.setitem, tm.loc, tm.iloc])\ndef indexer_sli(request):\n    \"\"\"\n    Parametrize over __setitem__, loc.__setitem__, iloc.__setitem__\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[tm.loc, tm.iloc])\ndef indexer_li(request):\n    \"\"\"\n    Parametrize over loc.__getitem__, iloc.__getitem__\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[tm.setitem, tm.iloc])\ndef indexer_si(request):\n    \"\"\"\n    Parametrize over __setitem__, iloc.__setitem__\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[tm.setitem, tm.loc])\ndef indexer_sl(request):\n    \"\"\"\n    Parametrize over __setitem__, loc.__setitem__\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[tm.at, tm.loc])\ndef indexer_al(request):\n    \"\"\"\n    Parametrize over at.__setitem__, loc.__setitem__\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[tm.iat, tm.iloc])\ndef indexer_ial(request):\n    \"\"\"\n    Parametrize over iat.__setitem__, iloc.__setitem__\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[True, False])\ndef performance_warning(request) -> Iterator[bool | type[Warning]]:\n    \"\"\"\n    Fixture to check if performance warnings are enabled. Either produces\n    ``PerformanceWarning`` if they are enabled, otherwise ``False``.\n    \"\"\"\n    with pd.option_context(\"mode.performance_warnings\", request.param):\n        yield pd.errors.PerformanceWarning if request.param else False\n\n\n@pytest.fixture\ndef using_infer_string() -> bool:\n    \"\"\"\n    Fixture to check if infer string option is enabled.\n    \"\"\"\n    return pd.options.future.infer_string is True\n\n\n_warsaws: list[Any] = [\"Europe/Warsaw\", \"dateutil/Europe/Warsaw\"]\nif pytz is not None:\n    _warsaws.append(pytz.timezone(\"Europe/Warsaw\"))\n\n\n@pytest.fixture(params=_warsaws)\ndef warsaw(request) -> str:\n    \"\"\"\n    tzinfo for Europe/Warsaw using pytz, dateutil, or zoneinfo.\n    \"\"\"\n    return request.param\n\n\n@pytest.fixture\ndef temp_file(tmp_path):\n    \"\"\"\n    Generate a unique file for testing use. See link for removal policy.\n    https://docs.pytest.org/en/7.1.x/how-to/tmp_path.html#the-default-base-temporary-directory\n    \"\"\"\n    file_path = tmp_path / str(uuid.uuid4())\n    file_path.touch()\n    return file_path\n"
    },
    {
      "filename": "pandas/core/arrays/datetimelike.py",
      "content": "from __future__ import annotations\n\nfrom datetime import (\n    datetime,\n    timedelta,\n)\nfrom functools import wraps\nimport operator\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Literal,\n    Union,\n    cast,\n    final,\n    overload,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config.config import get_option\n\nfrom pandas._libs import (\n    algos,\n    lib,\n)\nfrom pandas._libs.tslibs import (\n    BaseOffset,\n    IncompatibleFrequency,\n    NaT,\n    NaTType,\n    Period,\n    Resolution,\n    Tick,\n    Timedelta,\n    Timestamp,\n    add_overflowsafe,\n    astype_overflowsafe,\n    get_unit_from_dtype,\n    iNaT,\n    ints_to_pydatetime,\n    ints_to_pytimedelta,\n    periods_per_day,\n    to_offset,\n)\nfrom pandas._libs.tslibs.fields import (\n    RoundTo,\n    round_nsint64,\n)\nfrom pandas._libs.tslibs.np_datetime import compare_mismatched_resolutions\nfrom pandas._libs.tslibs.timedeltas import get_unit_for_round\nfrom pandas._libs.tslibs.timestamps import integer_op_not_supported\nfrom pandas._typing import (\n    ArrayLike,\n    AxisInt,\n    DatetimeLikeScalar,\n    Dtype,\n    DtypeObj,\n    F,\n    InterpolateOptions,\n    NpDtype,\n    PositionalIndexer2D,\n    PositionalIndexerTuple,\n    ScalarIndexer,\n    Self,\n    SequenceIndexer,\n    TakeIndexer,\n    TimeAmbiguous,\n    TimeNonexistent,\n    npt,\n)\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import (\n    AbstractMethodError,\n    InvalidComparison,\n    PerformanceWarning,\n)\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    cache_readonly,\n)\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.cast import construct_1d_object_array_from_listlike\nfrom pandas.core.dtypes.common import (\n    is_all_strings,\n    is_integer_dtype,\n    is_list_like,\n    is_object_dtype,\n    is_string_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import (\n    ArrowDtype,\n    CategoricalDtype,\n    DatetimeTZDtype,\n    ExtensionDtype,\n    PeriodDtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCCategorical,\n    ABCMultiIndex,\n)\nfrom pandas.core.dtypes.missing import (\n    is_valid_na_for_dtype,\n    isna,\n)\n\nfrom pandas.core import (\n    algorithms,\n    missing,\n    nanops,\n    ops,\n)\nfrom pandas.core.algorithms import (\n    isin,\n    map_array,\n    unique1d,\n)\nfrom pandas.core.array_algos import datetimelike_accumulations\nfrom pandas.core.arraylike import OpsMixin\nfrom pandas.core.arrays._mixins import (\n    NDArrayBackedExtensionArray,\n    ravel_compat,\n)\nfrom pandas.core.arrays.arrow.array import ArrowExtensionArray\nfrom pandas.core.arrays.base import ExtensionArray\nfrom pandas.core.arrays.integer import IntegerArray\nimport pandas.core.common as com\nfrom pandas.core.construction import (\n    array as pd_array,\n    ensure_wrapped_if_datetimelike,\n    extract_array,\n)\nfrom pandas.core.indexers import (\n    check_array_indexer,\n    check_setitem_lengths,\n)\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\nfrom pandas.core.ops.invalid import (\n    invalid_comparison,\n    make_invalid_op,\n)\n\nfrom pandas.tseries import frequencies\n\nif TYPE_CHECKING:\n    from collections.abc import (\n        Callable,\n        Iterator,\n        Sequence,\n    )\n\n    from pandas import Index\n    from pandas.core.arrays import (\n        DatetimeArray,\n        PeriodArray,\n        TimedeltaArray,\n    )\n\nDTScalarOrNaT = Union[DatetimeLikeScalar, NaTType]\n\n\ndef _make_unpacked_invalid_op(op_name: str):\n    op = make_invalid_op(op_name)\n    return unpack_zerodim_and_defer(op_name)(op)\n\n\ndef _period_dispatch(meth: F) -> F:\n    \"\"\"\n    For PeriodArray methods, dispatch to DatetimeArray and re-wrap the results\n    in PeriodArray.  We cannot use ._ndarray directly for the affected\n    methods because the i8 data has different semantics on NaT values.\n    \"\"\"\n\n    @wraps(meth)\n    def new_meth(self, *args, **kwargs):\n        if not isinstance(self.dtype, PeriodDtype):\n            return meth(self, *args, **kwargs)\n\n        arr = self.view(\"M8[ns]\")\n        result = meth(arr, *args, **kwargs)\n        if result is NaT:\n            return NaT\n        elif isinstance(result, Timestamp):\n            return self._box_func(result._value)\n\n        res_i8 = result.view(\"i8\")\n        return self._from_backing_data(res_i8)\n\n    return cast(F, new_meth)\n\n\n# error: Definition of \"_concat_same_type\" in base class \"NDArrayBacked\" is\n# incompatible with definition in base class \"ExtensionArray\"\nclass DatetimeLikeArrayMixin(  # type: ignore[misc]\n    OpsMixin, NDArrayBackedExtensionArray\n):\n    \"\"\"\n    Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray\n\n    Assumes that __new__/__init__ defines:\n        _ndarray\n\n    and that inheriting subclass implements:\n        freq\n    \"\"\"\n\n    # _infer_matches -> which infer_dtype strings are close enough to our own\n    _infer_matches: tuple[str, ...]\n    _is_recognized_dtype: Callable[[DtypeObj], bool]\n    _recognized_scalars: tuple[type, ...]\n    _ndarray: np.ndarray\n    freq: BaseOffset | None\n\n    @cache_readonly\n    def _can_hold_na(self) -> bool:\n        return True\n\n    def __init__(\n        self, data, dtype: Dtype | None = None, freq=None, copy: bool = False\n    ) -> None:\n        raise AbstractMethodError(self)\n\n    @property\n    def _scalar_type(self) -> type[DatetimeLikeScalar]:\n        \"\"\"\n        The scalar associated with this datelike\n\n        * PeriodArray : Period\n        * DatetimeArray : Timestamp\n        * TimedeltaArray : Timedelta\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _scalar_from_string(self, value: str) -> DTScalarOrNaT:\n        \"\"\"\n        Construct a scalar type from a string.\n\n        Parameters\n        ----------\n        value : str\n\n        Returns\n        -------\n        Period, Timestamp, or Timedelta, or NaT\n            Whatever the type of ``self._scalar_type`` is.\n\n        Notes\n        -----\n        This should call ``self._check_compatible_with`` before\n        unboxing the result.\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _unbox_scalar(\n        self, value: DTScalarOrNaT\n    ) -> np.int64 | np.datetime64 | np.timedelta64:\n        \"\"\"\n        Unbox the integer value of a scalar `value`.\n\n        Parameters\n        ----------\n        value : Period, Timestamp, Timedelta, or NaT\n            Depending on subclass.\n\n        Returns\n        -------\n        int\n\n        Examples\n        --------\n        >>> arr = pd.array(np.array([\"1970-01-01\"], \"datetime64[ns]\"))\n        >>> arr._unbox_scalar(arr[0])\n        numpy.datetime64('1970-01-01T00:00:00.000000000')\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _check_compatible_with(self, other: DTScalarOrNaT) -> None:\n        \"\"\"\n        Verify that `self` and `other` are compatible.\n\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n\n        In each case, NaT is considered compatible.\n\n        Parameters\n        ----------\n        other\n\n        Raises\n        ------\n        Exception\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    # ------------------------------------------------------------------\n\n    def _box_func(self, x):\n        \"\"\"\n        box function to get object from internal representation\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _box_values(self, values) -> np.ndarray:\n        \"\"\"\n        apply box func to passed values\n        \"\"\"\n        return lib.map_infer(values, self._box_func, convert=False)\n\n    def __iter__(self) -> Iterator:\n        if self.ndim > 1:\n            return (self[n] for n in range(len(self)))\n        else:\n            return (self._box_func(v) for v in self.asi8)\n\n    @property\n    def asi8(self) -> npt.NDArray[np.int64]:\n        \"\"\"\n        Integer representation of the values.\n\n        Returns\n        -------\n        ndarray\n            An ndarray with int64 dtype.\n        \"\"\"\n        # do not cache or you'll create a memory leak\n        return self._ndarray.view(\"i8\")\n\n    # ----------------------------------------------------------------\n    # Rendering Methods\n\n    def _format_native_types(\n        self, *, na_rep: str | float = \"NaT\", date_format=None\n    ) -> npt.NDArray[np.object_]:\n        \"\"\"\n        Helper method for astype when converting to strings.\n\n        Returns\n        -------\n        ndarray[str]\n        \"\"\"\n        raise AbstractMethodError(self)\n\n    def _formatter(self, boxed: bool = False) -> Callable[[object], str]:\n        # TODO: Remove Datetime & DatetimeTZ formatters.\n        return \"'{}'\".format\n\n    # ----------------------------------------------------------------\n    # Array-Like / EA-Interface Methods\n\n    def __array__(\n        self, dtype: NpDtype | None = None, copy: bool | None = None\n    ) -> np.ndarray:\n        # used for Timedelta/DatetimeArray, overwritten by PeriodArray\n        if is_object_dtype(dtype):\n            return np.array(list(self), dtype=object)\n        return self._ndarray\n\n    @overload\n    def __getitem__(self, key: ScalarIndexer) -> DTScalarOrNaT: ...\n\n    @overload\n    def __getitem__(\n        self,\n        key: SequenceIndexer | PositionalIndexerTuple,\n    ) -> Self: ...\n\n    def __getitem__(self, key: PositionalIndexer2D) -> Self | DTScalarOrNaT:\n        \"\"\"\n        This getitem defers to the underlying array, which by-definition can\n        only handle list-likes, slices, and integer scalars\n        \"\"\"\n        # Use cast as we know we will get back a DatetimeLikeArray or DTScalar,\n        # but skip evaluating the Union at runtime for performance\n        # (see https://github.com/pandas-dev/pandas/pull/44624)\n        result = cast(\"Union[Self, DTScalarOrNaT]\", super().__getitem__(key))\n        if lib.is_scalar(result):\n            return result\n        else:\n            # At this point we know the result is an array.\n            result = cast(Self, result)\n        result._freq = self._get_getitem_freq(key)\n        return result\n\n    def _get_getitem_freq(self, key) -> BaseOffset | None:\n        \"\"\"\n        Find the `freq` attribute to assign to the result of a __getitem__ lookup.\n        \"\"\"\n        is_period = isinstance(self.dtype, PeriodDtype)\n        if is_period:\n            freq = self.freq\n        elif self.ndim != 1:\n            freq = None\n        else:\n            key = check_array_indexer(self, key)  # maybe ndarray[bool] -> slice\n            freq = None\n            if isinstance(key, slice):\n                if self.freq is not None and key.step is not None:\n                    freq = key.step * self.freq\n                else:\n                    freq = self.freq\n            elif key is Ellipsis:\n                # GH#21282 indexing with Ellipsis is similar to a full slice,\n                #  should preserve `freq` attribute\n                freq = self.freq\n            elif com.is_bool_indexer(key):\n                new_key = lib.maybe_booleans_to_slice(key.view(np.uint8))\n                if isinstance(new_key, slice):\n                    return self._get_getitem_freq(new_key)\n        return freq\n\n    # error: Argument 1 of \"__setitem__\" is incompatible with supertype\n    # \"ExtensionArray\"; supertype defines the argument type as \"Union[int,\n    # ndarray]\"\n    def __setitem__(\n        self,\n        key: int | Sequence[int] | Sequence[bool] | slice,\n        value: NaTType | Any | Sequence[Any],\n    ) -> None:\n        # I'm fudging the types a bit here. \"Any\" above really depends\n        # on type(self). For PeriodArray, it's Period (or stuff coercible\n        # to a period in from_sequence). For DatetimeArray, it's Timestamp...\n        # I don't know if mypy can do that, possibly with Generics.\n        # https://mypy.readthedocs.io/en/latest/generics.html\n\n        no_op = check_setitem_lengths(key, value, self)\n\n        # Calling super() before the no_op short-circuit means that we raise\n        #  on invalid 'value' even if this is a no-op, e.g. wrong-dtype empty array.\n        super().__setitem__(key, value)\n\n        if no_op:\n            return\n\n        self._maybe_clear_freq()\n\n    def _maybe_clear_freq(self) -> None:\n        # inplace operations like __setitem__ may invalidate the freq of\n        # DatetimeArray and TimedeltaArray\n        pass\n\n    def astype(self, dtype, copy: bool = True):\n        # Some notes on cases we don't have to handle here in the base class:\n        #   1. PeriodArray.astype handles period -> period\n        #   2. DatetimeArray.astype handles conversion between tz.\n        #   3. DatetimeArray.astype handles datetime -> period\n        dtype = pandas_dtype(dtype)\n\n        if dtype == object:\n            if self.dtype.kind == \"M\":\n                self = cast(\"DatetimeArray\", self)\n                # *much* faster than self._box_values\n                #  for e.g. test_get_loc_tuple_monotonic_above_size_cutoff\n                i8data = self.asi8\n                converted = ints_to_pydatetime(\n                    i8data,\n                    tz=self.tz,\n                    box=\"timestamp\",\n                    reso=self._creso,\n                )\n                return converted\n\n            elif self.dtype.kind == \"m\":\n                return ints_to_pytimedelta(self._ndarray, box=True)\n\n            return self._box_values(self.asi8.ravel()).reshape(self.shape)\n\n        elif isinstance(dtype, ExtensionDtype):\n            return super().astype(dtype, copy=copy)\n        elif is_string_dtype(dtype):\n            return self._format_native_types()\n        elif dtype.kind in \"iu\":\n            # we deliberately ignore int32 vs. int64 here.\n            # See https://github.com/pandas-dev/pandas/issues/24381 for more.\n            values = self.asi8\n            if dtype != np.int64:\n                raise TypeError(\n                    f\"Converting from {self.dtype} to {dtype} is not supported. \"\n                    \"Do obj.astype('int64').astype(dtype) instead\"\n                )\n\n            if copy:\n                values = values.copy()\n            return values\n        elif (dtype.kind in \"mM\" and self.dtype != dtype) or dtype.kind == \"f\":\n            # disallow conversion between datetime/timedelta,\n            # and conversions for any datetimelike to float\n            msg = f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n            raise TypeError(msg)\n        else:\n            return np.asarray(self, dtype=dtype)\n\n    @overload\n    def view(self) -> Self: ...\n\n    @overload\n    def view(self, dtype: Literal[\"M8[ns]\"]) -> DatetimeArray: ...\n\n    @overload\n    def view(self, dtype: Literal[\"m8[ns]\"]) -> TimedeltaArray: ...\n\n    @overload\n    def view(self, dtype: Dtype | None = ...) -> ArrayLike: ...\n\n    def view(self, dtype: Dtype | None = None) -> ArrayLike:\n        # we need to explicitly call super() method as long as the `@overload`s\n        #  are present in this file.\n        return super().view(dtype)\n\n    # ------------------------------------------------------------------\n    # Validation Methods\n    # TODO: try to de-duplicate these, ensure identical behavior\n\n    def _validate_comparison_value(self, other):\n        if isinstance(other, str):\n            try:\n                # GH#18435 strings get a pass from tzawareness compat\n                other = self._scalar_from_string(other)\n            except (ValueError, IncompatibleFrequency) as err:\n                # failed to parse as Timestamp/Timedelta/Period\n                raise InvalidComparison(other) from err\n\n        if isinstance(other, self._recognized_scalars) or other is NaT:\n            other = self._scalar_type(other)\n            try:\n                self._check_compatible_with(other)\n            except (TypeError, IncompatibleFrequency) as err:\n                # e.g. tzawareness mismatch\n                raise InvalidComparison(other) from err\n\n        elif not is_list_like(other):\n            raise InvalidComparison(other)\n\n        elif len(other) != len(self):\n            raise ValueError(\"Lengths must match\")\n\n        else:\n            try:\n                other = self._validate_listlike(other, allow_object=True)\n                self._check_compatible_with(other)\n            except (TypeError, IncompatibleFrequency) as err:\n                if is_object_dtype(getattr(other, \"dtype\", None)):\n                    # We will have to operate element-wise\n                    pass\n                else:\n                    raise InvalidComparison(other) from err\n\n        return other\n\n    def _validate_scalar(\n        self,\n        value,\n        *,\n        allow_listlike: bool = False,\n        unbox: bool = True,\n    ):\n        \"\"\"\n        Validate that the input value can be cast to our scalar_type.\n\n        Parameters\n        ----------\n        value : object\n        allow_listlike: bool, default False\n            When raising an exception, whether the message should say\n            listlike inputs are allowed.\n        unbox : bool, default True\n            Whether to unbox the result before returning.  Note: unbox=False\n            skips the setitem compatibility check.\n\n        Returns\n        -------\n        self._scalar_type or NaT\n        \"\"\"\n        if isinstance(value, self._scalar_type):\n            pass\n\n        elif isinstance(value, str):\n            # NB: Careful about tzawareness\n            try:\n                value = self._scalar_from_string(value)\n            except ValueError as err:\n                msg = self._validation_error_message(value, allow_listlike)\n                raise TypeError(msg) from err\n\n        elif is_valid_na_for_dtype(value, self.dtype):\n            # GH#18295\n            value = NaT\n\n        elif isna(value):\n            # if we are dt64tz and value is dt64(\"NaT\"), dont cast to NaT,\n            #  or else we'll fail to raise in _unbox_scalar\n            msg = self._validation_error_message(value, allow_listlike)\n            raise TypeError(msg)\n\n        elif isinstance(value, self._recognized_scalars):\n            # error: Argument 1 to \"Timestamp\" has incompatible type \"object\"; expected\n            # \"integer[Any] | float | str | date | datetime | datetime64\"\n            value = self._scalar_type(value)  # type: ignore[arg-type]\n\n        else:\n            msg = self._validation_error_message(value, allow_listlike)\n            raise TypeError(msg)\n\n        if not unbox:\n            # NB: In general NDArrayBackedExtensionArray will unbox here;\n            #  this option exists to prevent a performance hit in\n            #  TimedeltaIndex.get_loc\n            return value\n        return self._unbox_scalar(value)\n\n    def _validation_error_message(self, value, allow_listlike: bool = False) -> str:\n        \"\"\"\n        Construct an exception message on validation error.\n\n        Some methods allow only scalar inputs, while others allow either scalar\n        or listlike.\n\n        Parameters\n        ----------\n        allow_listlike: bool, default False\n\n        Returns\n        -------\n        str\n        \"\"\"\n        if hasattr(value, \"dtype\") and getattr(value, \"ndim\", 0) > 0:\n            msg_got = f\"{value.dtype} array\"\n        else:\n            msg_got = f\"'{type(value).__name__}'\"\n        if allow_listlike:\n            msg = (\n                f\"value should be a '{self._scalar_type.__name__}', 'NaT', \"\n                f\"or array of those. Got {msg_got} instead.\"\n            )\n        else:\n            msg = (\n                f\"value should be a '{self._scalar_type.__name__}' or 'NaT'. \"\n                f\"Got {msg_got} instead.\"\n            )\n        return msg\n\n    def _validate_listlike(self, value, allow_object: bool = False):\n        if isinstance(value, type(self)):\n            if self.dtype.kind in \"mM\" and not allow_object and self.unit != value.unit:  # type: ignore[attr-defined]\n                # error: \"DatetimeLikeArrayMixin\" has no attribute \"as_unit\"\n                value = value.as_unit(self.unit, round_ok=False)  # type: ignore[attr-defined]\n            return value\n\n        if isinstance(value, list) and len(value) == 0:\n            # We treat empty list as our own dtype.\n            return type(self)._from_sequence([], dtype=self.dtype)\n\n        if hasattr(value, \"dtype\") and value.dtype == object:\n            # `array` below won't do inference if value is an Index or Series.\n            #  so do so here.  in the Index case, inferred_type may be cached.\n            if lib.infer_dtype(value) in self._infer_matches:\n                try:\n                    value = type(self)._from_sequence(value)\n                except (ValueError, TypeError) as err:\n                    if allow_object:\n                        return value\n                    msg = self._validation_error_message(value, True)\n                    raise TypeError(msg) from err\n\n        # Do type inference if necessary up front (after unpacking\n        # NumpyExtensionArray)\n        # e.g. we passed PeriodIndex.values and got an ndarray of Periods\n        value = extract_array(value, extract_numpy=True)\n        value = pd_array(value)\n        value = extract_array(value, extract_numpy=True)\n\n        if is_all_strings(value):\n            # We got a StringArray\n            try:\n                # TODO: Could use from_sequence_of_strings if implemented\n                # Note: passing dtype is necessary for PeriodArray tests\n                value = type(self)._from_sequence(value, dtype=self.dtype)\n            except ValueError:\n                pass\n\n        if isinstance(value.dtype, CategoricalDtype):\n            # e.g. we have a Categorical holding self.dtype\n            if value.categories.dtype == self.dtype:\n                # TODO: do we need equal dtype or just comparable?\n                value = value._internal_get_values()\n                value = extract_array(value, extract_numpy=True)\n\n        if allow_object and is_object_dtype(value.dtype):\n            pass\n\n        elif not type(self)._is_recognized_dtype(value.dtype):\n            msg = self._validation_error_message(value, True)\n            raise TypeError(msg)\n\n        if self.dtype.kind in \"mM\" and not allow_object:\n            # error: \"DatetimeLikeArrayMixin\" has no attribute \"as_unit\"\n            value = value.as_unit(self.unit, round_ok=False)  # type: ignore[attr-defined]\n        return value\n\n    def _validate_setitem_value(self, value):\n        if is_list_like(value):\n            value = self._validate_listlike(value)\n        else:\n            return self._validate_scalar(value, allow_listlike=True)\n\n        return self._unbox(value)\n\n    @final\n    def _unbox(self, other) -> np.int64 | np.datetime64 | np.timedelta64 | np.ndarray:\n        \"\"\"\n        Unbox either a scalar with _unbox_scalar or an instance of our own type.\n        \"\"\"\n        if lib.is_scalar(other):\n            other = self._unbox_scalar(other)\n        else:\n            # same type as self\n            self._check_compatible_with(other)\n            other = other._ndarray\n        return other\n\n    # ------------------------------------------------------------------\n    # Additional array methods\n    #  These are not part of the EA API, but we implement them because\n    #  pandas assumes they're there.\n\n    @ravel_compat\n    def map(self, mapper, na_action: Literal[\"ignore\"] | None = None):\n        from pandas import Index\n\n        result = map_array(self, mapper, na_action=na_action)\n        result = Index(result)\n\n        if isinstance(result, ABCMultiIndex):\n            return result.to_numpy()\n        else:\n            return result.array\n\n    def isin(self, values: ArrayLike) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        Compute boolean array of whether each value is found in the\n        passed set of values.\n\n        Parameters\n        ----------\n        values : np.ndarray or ExtensionArray\n\n        Returns\n        -------\n        ndarray[bool]\n        \"\"\"\n        if values.dtype.kind in \"fiuc\":\n            # TODO: de-duplicate with equals, validate_comparison_value\n            return np.zeros(self.shape, dtype=bool)\n\n        values = ensure_wrapped_if_datetimelike(values)\n\n        if not isinstance(values, type(self)):\n            if values.dtype == object:\n                values = lib.maybe_convert_objects(\n                    values,  # type: ignore[arg-type]\n                    convert_non_numeric=True,\n                    dtype_if_all_nat=self.dtype,\n                )\n                if values.dtype != object:\n                    return self.isin(values)\n                else:\n                    # TODO: Deprecate this case\n                    # https://github.com/pandas-dev/pandas/pull/58645/files#r1604055791\n                    return isin(self.astype(object), values)\n            return np.zeros(self.shape, dtype=bool)\n\n        if self.dtype.kind in \"mM\":\n            self = cast(\"DatetimeArray | TimedeltaArray\", self)\n            # error: \"DatetimeLikeArrayMixin\" has no attribute \"as_unit\"\n            values = values.as_unit(self.unit)  # type: ignore[attr-defined]\n\n        try:\n            # error: Argument 1 to \"_check_compatible_with\" of \"DatetimeLikeArrayMixin\"\n            # has incompatible type \"ExtensionArray | ndarray[Any, Any]\"; expected\n            # \"Period | Timestamp | Timedelta | NaTType\"\n            self._check_compatible_with(values)  # type: ignore[arg-type]\n        except (TypeError, ValueError):\n            # Includes tzawareness mismatch and IncompatibleFrequencyError\n            return np.zeros(self.shape, dtype=bool)\n\n        # error: Item \"ExtensionArray\" of \"ExtensionArray | ndarray[Any, Any]\"\n        # has no attribute \"asi8\"\n        return isin(self.asi8, values.asi8)  # type: ignore[union-attr]\n\n    # ------------------------------------------------------------------\n    # Null Handling\n\n    def isna(self) -> npt.NDArray[np.bool_]:\n        return self._isnan\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _isnan(self) -> npt.NDArray[np.bool_]:\n        \"\"\"\n        return if each value is nan\n        \"\"\"\n        return self.asi8 == iNaT\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _hasna(self) -> bool:\n        \"\"\"\n        return if I have any nans; enables various perf speedups\n        \"\"\"\n        return bool(self._isnan.any())\n\n    def _maybe_mask_results(\n        self, result: np.ndarray, fill_value=iNaT, convert=None\n    ) -> np.ndarray:\n        \"\"\"\n        Parameters\n        ----------\n        result : np.ndarray\n        fill_value : object, default iNaT\n        convert : str, dtype or None\n\n        Returns\n        -------\n        result : ndarray with values replace by the fill_value\n\n        mask the result if needed, convert to the provided dtype if its not\n        None\n\n        This is an internal routine.\n        \"\"\"\n        if self._hasna:\n            if convert:\n                result = result.astype(convert)\n            if fill_value is None:\n                fill_value = np.nan\n            np.putmask(result, self._isnan, fill_value)\n        return result\n\n    # ------------------------------------------------------------------\n    # Frequency Properties/Methods\n\n    @property\n    def freqstr(self) -> str | None:\n        \"\"\"\n        Return the frequency object as a string if it's set, otherwise None.\n\n        See Also\n        --------\n        DatetimeIndex.inferred_freq : Returns a string representing a frequency\n            generated by infer_freq.\n\n        Examples\n        --------\n        For DatetimeIndex:\n\n        >>> idx = pd.DatetimeIndex([\"1/1/2020 10:00:00+00:00\"], freq=\"D\")\n        >>> idx.freqstr\n        'D'\n\n        The frequency can be inferred if there are more than 2 points:\n\n        >>> idx = pd.DatetimeIndex(\n        ...     [\"2018-01-01\", \"2018-01-03\", \"2018-01-05\"], freq=\"infer\"\n        ... )\n        >>> idx.freqstr\n        '2D'\n\n        For PeriodIndex:\n\n        >>> idx = pd.PeriodIndex([\"2023-1\", \"2023-2\", \"2023-3\"], freq=\"M\")\n        >>> idx.freqstr\n        'M'\n        \"\"\"\n        if self.freq is None:\n            return None\n        return self.freq.freqstr\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def inferred_freq(self) -> str | None:\n        \"\"\"\n        Tries to return a string representing a frequency generated by infer_freq.\n\n        Returns None if it can't autodetect the frequency.\n\n        See Also\n        --------\n        DatetimeIndex.freqstr : Return the frequency object as a string if it's set,\n            otherwise None.\n\n        Examples\n        --------\n        For DatetimeIndex:\n\n        >>> idx = pd.DatetimeIndex([\"2018-01-01\", \"2018-01-03\", \"2018-01-05\"])\n        >>> idx.inferred_freq\n        '2D'\n\n        For TimedeltaIndex:\n\n        >>> tdelta_idx = pd.to_timedelta([\"0 days\", \"10 days\", \"20 days\"])\n        >>> tdelta_idx\n        TimedeltaIndex(['0 days', '10 days', '20 days'],\n                       dtype='timedelta64[ns]', freq=None)\n        >>> tdelta_idx.inferred_freq\n        '10D'\n        \"\"\"\n        if self.ndim != 1:\n            return None\n        try:\n            return frequencies.infer_freq(self)\n        except ValueError:\n            return None\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _resolution_obj(self) -> Resolution | None:\n        freqstr = self.freqstr\n        if freqstr is None:\n            return None\n        try:\n            return Resolution.get_reso_from_freqstr(freqstr)\n        except KeyError:\n            return None\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def resolution(self) -> str:\n        \"\"\"\n        Returns day, hour, minute, second, millisecond or microsecond\n        \"\"\"\n        # error: Item \"None\" of \"Optional[Any]\" has no attribute \"attrname\"\n        return self._resolution_obj.attrname  # type: ignore[union-attr]\n\n    # monotonicity/uniqueness properties are called via frequencies.infer_freq,\n    #  see GH#23789\n\n    @property\n    def _is_monotonic_increasing(self) -> bool:\n        return algos.is_monotonic(self.asi8, timelike=True)[0]\n\n    @property\n    def _is_monotonic_decreasing(self) -> bool:\n        return algos.is_monotonic(self.asi8, timelike=True)[1]\n\n    @property\n    def _is_unique(self) -> bool:\n        return len(unique1d(self.asi8.ravel(\"K\"))) == self.size\n\n    # ------------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _cmp_method(self, other, op):\n        if self.ndim > 1 and getattr(other, \"shape\", None) == self.shape:\n            # TODO: handle 2D-like listlikes\n            return op(self.ravel(), other.ravel()).reshape(self.shape)\n\n        try:\n            other = self._validate_comparison_value(other)\n        except InvalidComparison:\n            return invalid_comparison(self, other, op)\n\n        dtype = getattr(other, \"dtype\", None)\n        if is_object_dtype(dtype):\n            # We have to use comp_method_OBJECT_ARRAY instead of numpy\n            #  comparison otherwise it would raise when comparing to None\n            result = ops.comp_method_OBJECT_ARRAY(\n                op, np.asarray(self.astype(object)), other\n            )\n            return result\n        if other is NaT:\n            if op is operator.ne:\n                result = np.ones(self.shape, dtype=bool)\n            else:\n                result = np.zeros(self.shape, dtype=bool)\n            return result\n\n        if not isinstance(self.dtype, PeriodDtype):\n            self = cast(TimelikeOps, self)\n            if self._creso != other._creso:\n                if not isinstance(other, type(self)):\n                    # i.e. Timedelta/Timestamp, cast to ndarray and let\n                    #  compare_mismatched_resolutions handle broadcasting\n                    try:\n                        # GH#52080 see if we can losslessly cast to shared unit\n                        other = other.as_unit(self.unit, round_ok=False)\n                    except ValueError:\n                        other_arr = np.array(other.asm8)\n                        return compare_mismatched_resolutions(\n                            self._ndarray, other_arr, op\n                        )\n                else:\n                    other_arr = other._ndarray\n                    return compare_mismatched_resolutions(self._ndarray, other_arr, op)\n\n        other_vals = self._unbox(other)\n        # GH#37462 comparison on i8 values is almost 2x faster than M8/m8\n        result = op(self._ndarray.view(\"i8\"), other_vals.view(\"i8\"))\n\n        o_mask = isna(other)\n        mask = self._isnan | o_mask\n        if mask.any():\n            nat_result = op is operator.ne\n            np.putmask(result, mask, nat_result)\n\n        return result\n\n    # pow is invalid for all three subclasses; TimedeltaArray will override\n    #  the multiplication and division ops\n    __pow__ = _make_unpacked_invalid_op(\"__pow__\")\n    __rpow__ = _make_unpacked_invalid_op(\"__rpow__\")\n    __mul__ = _make_unpacked_invalid_op(\"__mul__\")\n    __rmul__ = _make_unpacked_invalid_op(\"__rmul__\")\n    __truediv__ = _make_unpacked_invalid_op(\"__truediv__\")\n    __rtruediv__ = _make_unpacked_invalid_op(\"__rtruediv__\")\n    __floordiv__ = _make_unpacked_invalid_op(\"__floordiv__\")\n    __rfloordiv__ = _make_unpacked_invalid_op(\"__rfloordiv__\")\n    __mod__ = _make_unpacked_invalid_op(\"__mod__\")\n    __rmod__ = _make_unpacked_invalid_op(\"__rmod__\")\n    __divmod__ = _make_unpacked_invalid_op(\"__divmod__\")\n    __rdivmod__ = _make_unpacked_invalid_op(\"__rdivmod__\")\n\n    @final\n    def _get_i8_values_and_mask(\n        self, other\n    ) -> tuple[int | npt.NDArray[np.int64], None | npt.NDArray[np.bool_]]:\n        \"\"\"\n        Get the int64 values and b_mask to pass to add_overflowsafe.\n        \"\"\"\n        if isinstance(other, Period):\n            i8values = other.ordinal\n            mask = None\n        elif isinstance(other, (Timestamp, Timedelta)):\n            i8values = other._value\n            mask = None\n        else:\n            # PeriodArray, DatetimeArray, TimedeltaArray\n            mask = other._isnan\n            i8values = other.asi8\n        return i8values, mask\n\n    @final\n    def _get_arithmetic_result_freq(self, other) -> BaseOffset | None:\n        \"\"\"\n        Check if we can preserve self.freq in addition or subtraction.\n        \"\"\"\n        # Adding or subtracting a Timedelta/Timestamp scalar is freq-preserving\n        #  whenever self.freq is a Tick\n        if isinstance(self.dtype, PeriodDtype):\n            return self.freq\n        elif not lib.is_scalar(other):\n            return None\n        elif isinstance(self.freq, Tick):\n            # In these cases\n            return self.freq\n        return None\n\n    @final\n    def _add_datetimelike_scalar(self, other) -> DatetimeArray:\n        if not lib.is_np_dtype(self.dtype, \"m\"):\n            raise TypeError(\n                f\"cannot add {type(self).__name__} and {type(other).__name__}\"\n            )\n\n        self = cast(\"TimedeltaArray\", self)\n\n        from pandas.core.arrays import DatetimeArray\n        from pandas.core.arrays.datetimes import tz_to_dtype\n\n        assert other is not NaT\n        if isna(other):\n            # i.e. np.datetime64(\"NaT\")\n            # In this case we specifically interpret NaT as a datetime, not\n            # the timedelta interpretation we would get by returning self + NaT\n            result = self._ndarray + NaT.to_datetime64().astype(f\"M8[{self.unit}]\")\n            # Preserve our resolution\n            return DatetimeArray._simple_new(result, dtype=result.dtype)\n\n        other = Timestamp(other)\n        self, other = self._ensure_matching_resos(other)\n        self = cast(\"TimedeltaArray\", self)\n\n        other_i8, o_mask = self._get_i8_values_and_mask(other)\n        result = add_overflowsafe(self.asi8, np.asarray(other_i8, dtype=\"i8\"))\n        res_values = result.view(f\"M8[{self.unit}]\")\n\n        dtype = tz_to_dtype(tz=other.tz, unit=self.unit)\n        res_values = result.view(f\"M8[{self.unit}]\")\n        new_freq = self._get_arithmetic_result_freq(other)\n        return DatetimeArray._simple_new(res_values, dtype=dtype, freq=new_freq)\n\n    @final\n    def _add_datetime_arraylike(self, other: DatetimeArray) -> DatetimeArray:\n        if not lib.is_np_dtype(self.dtype, \"m\"):\n            raise TypeError(\n                f\"cannot add {type(self).__name__} and {type(other).__name__}\"\n            )\n\n        # defer to DatetimeArray.__add__\n        return other + self\n\n    @final\n    def _sub_datetimelike_scalar(\n        self, other: datetime | np.datetime64\n    ) -> TimedeltaArray:\n        if self.dtype.kind != \"M\":\n            raise TypeError(f\"cannot subtract a datelike from a {type(self).__name__}\")\n\n        self = cast(\"DatetimeArray\", self)\n        # subtract a datetime from myself, yielding a ndarray[timedelta64[ns]]\n\n        if isna(other):\n            # i.e. np.datetime64(\"NaT\")\n            return self - NaT\n\n        ts = Timestamp(other)\n\n        self, ts = self._ensure_matching_resos(ts)\n        return self._sub_datetimelike(ts)\n\n    @final\n    def _sub_datetime_arraylike(self, other: DatetimeArray) -> TimedeltaArray:\n        if self.dtype.kind != \"M\":\n            raise TypeError(f\"cannot subtract a datelike from a {type(self).__name__}\")\n\n        if len(self) != len(other):\n            raise ValueError(\"cannot add indices of unequal length\")\n\n        self = cast(\"DatetimeArray\", self)\n\n        self, other = self._ensure_matching_resos(other)\n        return self._sub_datetimelike(other)\n\n    @final\n    def _sub_datetimelike(self, other: Timestamp | DatetimeArray) -> TimedeltaArray:\n        self = cast(\"DatetimeArray\", self)\n\n        from pandas.core.arrays import TimedeltaArray\n\n        try:\n            self._assert_tzawareness_compat(other)\n        except TypeError as err:\n            new_message = str(err).replace(\"compare\", \"subtract\")\n            raise type(err)(new_message) from err\n\n        other_i8, o_mask = self._get_i8_values_and_mask(other)\n        res_values = add_overflowsafe(self.asi8, np.asarray(-other_i8, dtype=\"i8\"))\n        res_m8 = res_values.view(f\"timedelta64[{self.unit}]\")\n\n        new_freq = self._get_arithmetic_result_freq(other)\n        new_freq = cast(\"Tick | None\", new_freq)\n        return TimedeltaArray._simple_new(res_m8, dtype=res_m8.dtype, freq=new_freq)\n\n    @final\n    def _add_period(self, other: Period) -> PeriodArray:\n        if not lib.is_np_dtype(self.dtype, \"m\"):\n            raise TypeError(f\"cannot add Period to a {type(self).__name__}\")\n\n        # We will wrap in a PeriodArray and defer to the reversed operation\n        from pandas.core.arrays.period import PeriodArray\n\n        i8vals = np.broadcast_to(other.ordinal, self.shape)\n        dtype = PeriodDtype(other.freq)\n        parr = PeriodArray(i8vals, dtype=dtype)\n        return parr + self\n\n    def _add_offset(self, offset):\n        raise AbstractMethodError(self)\n\n    def _add_timedeltalike_scalar(self, other):\n        \"\"\"\n        Add a delta of a timedeltalike\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        if isna(other):\n            # i.e np.timedelta64(\"NaT\")\n            new_values = np.empty(self.shape, dtype=\"i8\").view(self._ndarray.dtype)\n            new_values.fill(iNaT)\n            return type(self)._simple_new(new_values, dtype=self.dtype)\n\n        # PeriodArray overrides, so we only get here with DTA/TDA\n        self = cast(\"DatetimeArray | TimedeltaArray\", self)\n        other = Timedelta(other)\n        self, other = self._ensure_matching_resos(other)\n        return self._add_timedeltalike(other)\n\n    def _add_timedelta_arraylike(self, other: TimedeltaArray) -> Self:\n        \"\"\"\n        Add a delta of a TimedeltaIndex\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        # overridden by PeriodArray\n\n        if len(self) != len(other):\n            raise ValueError(\"cannot add indices of unequal length\")\n\n        self, other = cast(\n            \"DatetimeArray | TimedeltaArray\", self\n        )._ensure_matching_resos(other)\n        return self._add_timedeltalike(other)\n\n    @final\n    def _add_timedeltalike(self, other: Timedelta | TimedeltaArray) -> Self:\n        other_i8, o_mask = self._get_i8_values_and_mask(other)\n        new_values = add_overflowsafe(self.asi8, np.asarray(other_i8, dtype=\"i8\"))\n        res_values = new_values.view(self._ndarray.dtype)\n\n        new_freq = self._get_arithmetic_result_freq(other)\n\n        # error: Unexpected keyword argument \"freq\" for \"_simple_new\" of \"NDArrayBacked\"\n        return type(self)._simple_new(\n            res_values,\n            dtype=self.dtype,\n            freq=new_freq,  # type: ignore[call-arg]\n        )\n\n    @final\n    def _add_nat(self) -> Self:\n        \"\"\"\n        Add pd.NaT to self\n        \"\"\"\n        if isinstance(self.dtype, PeriodDtype):\n            raise TypeError(\n                f\"Cannot add {type(self).__name__} and {type(NaT).__name__}\"\n            )\n\n        # GH#19124 pd.NaT is treated like a timedelta for both timedelta\n        # and datetime dtypes\n        result = np.empty(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        result = result.view(self._ndarray.dtype)  # preserve reso\n        # error: Unexpected keyword argument \"freq\" for \"_simple_new\" of \"NDArrayBacked\"\n        return type(self)._simple_new(\n            result,\n            dtype=self.dtype,\n            freq=None,  # type: ignore[call-arg]\n        )\n\n    @final\n    def _sub_nat(self) -> np.ndarray:\n        \"\"\"\n        Subtract pd.NaT from self\n        \"\"\"\n        # GH#19124 Timedelta - datetime is not in general well-defined.\n        # We make an exception for pd.NaT, which in this case quacks\n        # like a timedelta.\n        # For datetime64 dtypes by convention we treat NaT as a datetime, so\n        # this subtraction returns a timedelta64 dtype.\n        # For period dtype, timedelta64 is a close-enough return dtype.\n        result = np.empty(self.shape, dtype=np.int64)\n        result.fill(iNaT)\n        if self.dtype.kind in \"mM\":\n            # We can retain unit in dtype\n            self = cast(\"DatetimeArray| TimedeltaArray\", self)\n            return result.view(f\"timedelta64[{self.unit}]\")\n        else:\n            return result.view(\"timedelta64[ns]\")\n\n    @final\n    def _sub_periodlike(self, other: Period | PeriodArray) -> npt.NDArray[np.object_]:\n        # If the operation is well-defined, we return an object-dtype ndarray\n        # of DateOffsets.  Null entries are filled with pd.NaT\n        if not isinstance(self.dtype, PeriodDtype):\n            raise TypeError(\n                f\"cannot subtract {type(other).__name__} from {type(self).__name__}\"\n            )\n\n        self = cast(\"PeriodArray\", self)\n        self._check_compatible_with(other)\n\n        other_i8, o_mask = self._get_i8_values_and_mask(other)\n        new_i8_data = add_overflowsafe(self.asi8, np.asarray(-other_i8, dtype=\"i8\"))\n        new_data = np.array([self.freq.base * x for x in new_i8_data])\n\n        if o_mask is None:\n            # i.e. Period scalar\n            mask = self._isnan\n        else:\n            # i.e. PeriodArray\n            mask = self._isnan | o_mask\n        new_data[mask] = NaT\n        return new_data\n\n    @final\n    def _addsub_object_array(self, other: npt.NDArray[np.object_], op) -> np.ndarray:\n        \"\"\"\n        Add or subtract array-like of DateOffset objects\n\n        Parameters\n        ----------\n        other : np.ndarray[object]\n        op : {operator.add, operator.sub}\n\n        Returns\n        -------\n        np.ndarray[object]\n            Except in fastpath case with length 1 where we operate on the\n            contained scalar.\n        \"\"\"\n        assert op in [operator.add, operator.sub]\n        if len(other) == 1 and self.ndim == 1:\n            # Note: without this special case, we could annotate return type\n            #  as ndarray[object]\n            # If both 1D then broadcasting is unambiguous\n            return op(self, other[0])\n\n        if get_option(\"performance_warnings\"):\n            warnings.warn(\n                \"Adding/subtracting object-dtype array to \"\n                f\"{type(self).__name__} not vectorized.\",\n                PerformanceWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        # Caller is responsible for broadcasting if necessary\n        assert self.shape == other.shape, (self.shape, other.shape)\n\n        res_values = op(self.astype(\"O\"), np.asarray(other))\n        return res_values\n\n    def _accumulate(self, name: str, *, skipna: bool = True, **kwargs) -> Self:\n        if name not in {\"cummin\", \"cummax\"}:\n            raise TypeError(f\"Accumulation {name} not supported for {type(self)}\")\n\n        op = getattr(datetimelike_accumulations, name)\n        result = op(self.copy(), skipna=skipna, **kwargs)\n\n        return type(self)._simple_new(result, dtype=self.dtype)\n\n    @unpack_zerodim_and_defer(\"__add__\")\n    def __add__(self, other):\n        other_dtype = getattr(other, \"dtype\", None)\n        other = ensure_wrapped_if_datetimelike(other)\n\n        # scalar others\n        if other is NaT:\n            result: np.ndarray | DatetimeLikeArrayMixin = self._add_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_timedeltalike_scalar(other)\n        elif isinstance(other, BaseOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._add_datetimelike_scalar(other)\n        elif isinstance(other, Period) and lib.is_np_dtype(self.dtype, \"m\"):\n            result = self._add_period(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not isinstance(self.dtype, PeriodDtype):\n                raise integer_op_not_supported(self)\n            obj = cast(\"PeriodArray\", self)\n            result = obj._addsub_int_array_or_scalar(other * obj.dtype._n, operator.add)\n\n        # array-like others\n        elif lib.is_np_dtype(other_dtype, \"m\"):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_timedelta_arraylike(other)\n        elif is_object_dtype(other_dtype):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.add)\n        elif lib.is_np_dtype(other_dtype, \"M\") or isinstance(\n            other_dtype, DatetimeTZDtype\n        ):\n            # DatetimeIndex, ndarray[datetime64]\n            return self._add_datetime_arraylike(other)\n        elif is_integer_dtype(other_dtype):\n            if not isinstance(self.dtype, PeriodDtype):\n                raise integer_op_not_supported(self)\n            obj = cast(\"PeriodArray\", self)\n            result = obj._addsub_int_array_or_scalar(other * obj.dtype._n, operator.add)\n        else:\n            # Includes Categorical, other ExtensionArrays\n            # For PeriodDtype, if self is a TimedeltaArray and other is a\n            #  PeriodArray with  a timedelta-like (i.e. Tick) freq, this\n            #  operation is valid.  Defer to the PeriodArray implementation.\n            #  In remaining cases, this will end up raising TypeError.\n            return NotImplemented\n\n        if isinstance(result, np.ndarray) and lib.is_np_dtype(result.dtype, \"m\"):\n            from pandas.core.arrays import TimedeltaArray\n\n            return TimedeltaArray._from_sequence(result)\n        return result\n\n    def __radd__(self, other):\n        # alias for __add__\n        return self.__add__(other)\n\n    @unpack_zerodim_and_defer(\"__sub__\")\n    def __sub__(self, other):\n        other_dtype = getattr(other, \"dtype\", None)\n        other = ensure_wrapped_if_datetimelike(other)\n\n        # scalar others\n        if other is NaT:\n            result: np.ndarray | DatetimeLikeArrayMixin = self._sub_nat()\n        elif isinstance(other, (Tick, timedelta, np.timedelta64)):\n            result = self._add_timedeltalike_scalar(-other)\n        elif isinstance(other, BaseOffset):\n            # specifically _not_ a Tick\n            result = self._add_offset(-other)\n        elif isinstance(other, (datetime, np.datetime64)):\n            result = self._sub_datetimelike_scalar(other)\n        elif lib.is_integer(other):\n            # This check must come after the check for np.timedelta64\n            # as is_integer returns True for these\n            if not isinstance(self.dtype, PeriodDtype):\n                raise integer_op_not_supported(self)\n            obj = cast(\"PeriodArray\", self)\n            result = obj._addsub_int_array_or_scalar(other * obj.dtype._n, operator.sub)\n\n        elif isinstance(other, Period):\n            result = self._sub_periodlike(other)\n\n        # array-like others\n        elif lib.is_np_dtype(other_dtype, \"m\"):\n            # TimedeltaIndex, ndarray[timedelta64]\n            result = self._add_timedelta_arraylike(-other)\n        elif is_object_dtype(other_dtype):\n            # e.g. Array/Index of DateOffset objects\n            result = self._addsub_object_array(other, operator.sub)\n        elif lib.is_np_dtype(other_dtype, \"M\") or isinstance(\n            other_dtype, DatetimeTZDtype\n        ):\n            # DatetimeIndex, ndarray[datetime64]\n            result = self._sub_datetime_arraylike(other)\n        elif isinstance(other_dtype, PeriodDtype):\n            # PeriodIndex\n            result = self._sub_periodlike(other)\n        elif is_integer_dtype(other_dtype):\n            if not isinstance(self.dtype, PeriodDtype):\n                raise integer_op_not_supported(self)\n            obj = cast(\"PeriodArray\", self)\n            result = obj._addsub_int_array_or_scalar(other * obj.dtype._n, operator.sub)\n        else:\n            # Includes ExtensionArrays, float_dtype\n            return NotImplemented\n\n        if isinstance(result, np.ndarray) and lib.is_np_dtype(result.dtype, \"m\"):\n            from pandas.core.arrays import TimedeltaArray\n\n            return TimedeltaArray._from_sequence(result)\n        return result\n\n    def __rsub__(self, other):\n        other_dtype = getattr(other, \"dtype\", None)\n        other_is_dt64 = lib.is_np_dtype(other_dtype, \"M\") or isinstance(\n            other_dtype, DatetimeTZDtype\n        )\n\n        if other_is_dt64 and lib.is_np_dtype(self.dtype, \"m\"):\n            # ndarray[datetime64] cannot be subtracted from self, so\n            # we need to wrap in DatetimeArray/Index and flip the operation\n            if lib.is_scalar(other):\n                # i.e. np.datetime64 object\n                return Timestamp(other) - self\n            if not isinstance(other, DatetimeLikeArrayMixin):\n                # Avoid down-casting DatetimeIndex\n                from pandas.core.arrays import DatetimeArray\n\n                other = DatetimeArray._from_sequence(other)\n            return other - self\n        elif self.dtype.kind == \"M\" and hasattr(other, \"dtype\") and not other_is_dt64:\n            # GH#19959 datetime - datetime is well-defined as timedelta,\n            # but any other type - datetime is not well-defined.\n            raise TypeError(\n                f\"cannot subtract {type(self).__name__} from {type(other).__name__}\"\n            )\n        elif isinstance(self.dtype, PeriodDtype) and lib.is_np_dtype(other_dtype, \"m\"):\n            # TODO: Can we simplify/generalize these cases at all?\n            raise TypeError(f\"cannot subtract {type(self).__name__} from {other.dtype}\")\n        elif lib.is_np_dtype(self.dtype, \"m\"):\n            self = cast(\"TimedeltaArray\", self)\n            return (-self) + other\n\n        # We get here with e.g. datetime objects\n        return -(self - other)\n\n    def __iadd__(self, other) -> Self:\n        result = self + other\n        self[:] = result[:]\n\n        if not isinstance(self.dtype, PeriodDtype):\n            # restore freq, which is invalidated by setitem\n            self._freq = result.freq\n        return self\n\n    def __isub__(self, other) -> Self:\n        result = self - other\n        self[:] = result[:]\n\n        if not isinstance(self.dtype, PeriodDtype):\n            # restore freq, which is invalidated by setitem\n            self._freq = result.freq\n        return self\n\n    # --------------------------------------------------------------\n    # Reductions\n\n    @_period_dispatch\n    def _quantile(\n        self,\n        qs: npt.NDArray[np.float64],\n        interpolation: str,\n    ) -> Self:\n        return super()._quantile(qs=qs, interpolation=interpolation)\n\n    @_period_dispatch\n    def min(self, *, axis: AxisInt | None = None, skipna: bool = True, **kwargs):\n        \"\"\"\n        Return the minimum value of the Array or minimum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.min\n        Index.min : Return the minimum value in an Index.\n        Series.min : Return the minimum value in a Series.\n        \"\"\"\n        nv.validate_min((), kwargs)\n        nv.validate_minmax_axis(axis, self.ndim)\n\n        result = nanops.nanmin(self._ndarray, axis=axis, skipna=skipna)\n        return self._wrap_reduction_result(axis, result)\n\n    @_period_dispatch\n    def max(self, *, axis: AxisInt | None = None, skipna: bool = True, **kwargs):\n        \"\"\"\n        Return the maximum value of the Array or maximum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.max\n        Index.max : Return the maximum value in an Index.\n        Series.max : Return the maximum value in a Series.\n        \"\"\"\n        nv.validate_max((), kwargs)\n        nv.validate_minmax_axis(axis, self.ndim)\n\n        result = nanops.nanmax(self._ndarray, axis=axis, skipna=skipna)\n        return self._wrap_reduction_result(axis, result)\n\n    def mean(self, *, skipna: bool = True, axis: AxisInt | None = 0):\n        \"\"\"\n        Return the mean value of the Array.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Whether to ignore any NaT elements.\n        axis : int, optional, default 0\n            Axis for the function to be applied on.\n\n        Returns\n        -------\n        scalar\n            Timestamp or Timedelta.\n\n        See Also\n        --------\n        numpy.ndarray.mean : Returns the average of array elements along a given axis.\n        Series.mean : Return the mean value in a Series.\n\n        Notes\n        -----\n        mean is only defined for Datetime and Timedelta dtypes, not for Period.\n\n        Examples\n        --------\n        For :class:`pandas.DatetimeIndex`:\n\n        >>> idx = pd.date_range(\"2001-01-01 00:00\", periods=3)\n        >>> idx\n        DatetimeIndex(['2001-01-01', '2001-01-02', '2001-01-03'],\n                      dtype='datetime64[ns]', freq='D')\n        >>> idx.mean()\n        Timestamp('2001-01-02 00:00:00')\n\n        For :class:`pandas.TimedeltaIndex`:\n\n        >>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit=\"D\")\n        >>> tdelta_idx\n        TimedeltaIndex(['1 days', '2 days', '3 days'],\n                        dtype='timedelta64[ns]', freq=None)\n        >>> tdelta_idx.mean()\n        Timedelta('2 days 00:00:00')\n        \"\"\"\n        if isinstance(self.dtype, PeriodDtype):\n            # See discussion in GH#24757\n            raise TypeError(\n                f\"mean is not implemented for {type(self).__name__} since the \"\n                \"meaning is ambiguous.  An alternative is \"\n                \"obj.to_timestamp(how='start').mean()\"\n            )\n\n        result = nanops.nanmean(\n            self._ndarray, axis=axis, skipna=skipna, mask=self.isna()\n        )\n        return self._wrap_reduction_result(axis, result)\n\n    @_period_dispatch\n    def median(self, *, axis: AxisInt | None = None, skipna: bool = True, **kwargs):\n        nv.validate_median((), kwargs)\n\n        if axis is not None and abs(axis) >= self.ndim:\n            raise ValueError(\"abs(axis) must be less than ndim\")\n\n        result = nanops.nanmedian(self._ndarray, axis=axis, skipna=skipna)\n        return self._wrap_reduction_result(axis, result)\n\n    def _mode(self, dropna: bool = True):\n        mask = None\n        if dropna:\n            mask = self.isna()\n\n        i8modes = algorithms.mode(self.view(\"i8\"), mask=mask)\n        npmodes = i8modes.view(self._ndarray.dtype)\n        npmodes = cast(np.ndarray, npmodes)\n        return self._from_backing_data(npmodes)\n\n    # ------------------------------------------------------------------\n    # GroupBy Methods\n\n    def _groupby_op(\n        self,\n        *,\n        how: str,\n        has_dropped_na: bool,\n        min_count: int,\n        ngroups: int,\n        ids: npt.NDArray[np.intp],\n        **kwargs,\n    ):\n        dtype = self.dtype\n        if dtype.kind == \"M\":\n            # Adding/multiplying datetimes is not valid\n            if how in [\"sum\", \"prod\", \"cumsum\", \"cumprod\", \"var\", \"skew\"]:\n                raise TypeError(f\"datetime64 type does not support operation '{how}'\")\n            if how in [\"any\", \"all\"]:\n                # GH#34479\n                raise TypeError(\n                    f\"'{how}' with datetime64 dtypes is no longer supported. \"\n                    f\"Use (obj != pd.Timestamp(0)).{how}() instead.\"\n                )\n\n        elif isinstance(dtype, PeriodDtype):\n            # Adding/multiplying Periods is not valid\n            if how in [\"sum\", \"prod\", \"cumsum\", \"cumprod\", \"var\", \"skew\"]:\n                raise TypeError(f\"Period type does not support {how} operations\")\n            if how in [\"any\", \"all\"]:\n                # GH#34479\n                raise TypeError(\n                    f\"'{how}' with PeriodDtype is no longer supported. \"\n                    f\"Use (obj != pd.Period(0, freq)).{how}() instead.\"\n                )\n        else:\n            # timedeltas we can add but not multiply\n            if how in [\"prod\", \"cumprod\", \"skew\", \"var\"]:\n                raise TypeError(f\"timedelta64 type does not support {how} operations\")\n\n        # All of the functions implemented here are ordinal, so we can\n        #  operate on the tz-naive equivalents\n        npvalues = self._ndarray.view(\"M8[ns]\")\n\n        from pandas.core.groupby.ops import WrappedCythonOp\n\n        kind = WrappedCythonOp.get_kind_from_how(how)\n        op = WrappedCythonOp(how=how, kind=kind, has_dropped_na=has_dropped_na)\n\n        res_values = op._cython_op_ndim_compat(\n            npvalues,\n            min_count=min_count,\n            ngroups=ngroups,\n            comp_ids=ids,\n            mask=None,\n            **kwargs,\n        )\n\n        if op.how in op.cast_blocklist:\n            # i.e. how in [\"rank\"], since other cast_blocklist methods don't go\n            #  through cython_operation\n            return res_values\n\n        # We did a view to M8[ns] above, now we go the other direction\n        assert res_values.dtype == \"M8[ns]\"\n        if how in [\"std\", \"sem\"]:\n            from pandas.core.arrays import TimedeltaArray\n\n            if isinstance(self.dtype, PeriodDtype):\n                raise TypeError(\"'std' and 'sem' are not valid for PeriodDtype\")\n            self = cast(\"DatetimeArray | TimedeltaArray\", self)\n            new_dtype = f\"m8[{self.unit}]\"\n            res_values = res_values.view(new_dtype)\n            return TimedeltaArray._simple_new(res_values, dtype=res_values.dtype)\n\n        res_values = res_values.view(self._ndarray.dtype)\n        return self._from_backing_data(res_values)\n\n\nclass DatelikeOps(DatetimeLikeArrayMixin):\n    \"\"\"\n    Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.\n    \"\"\"\n\n    @Substitution(\n        URL=\"https://docs.python.org/3/library/datetime.html\"\n        \"#strftime-and-strptime-behavior\"\n    )\n    def strftime(self, date_format: str) -> npt.NDArray[np.object_]:\n        \"\"\"\n        Convert to Index using specified date_format.\n\n        Return an Index of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format\n        doc <%(URL)s>`__.\n\n        Formats supported by the C `strftime` API but not by the python string format\n        doc (such as `\"%%R\"`, `\"%%r\"`) are not officially supported and should be\n        preferably replaced with their supported equivalents (such as `\"%%H:%%M\"`,\n        `\"%%I:%%M:%%S %%p\"`).\n\n        Note that `PeriodIndex` support additional directives, detailed in\n        `Period.strftime`.\n\n        Parameters\n        ----------\n        date_format : str\n            Date format string (e.g. \"%%Y-%%m-%%d\").\n\n        Returns\n        -------\n        ndarray[object]\n            NumPy ndarray of formatted strings.\n\n        See Also\n        --------\n        to_datetime : Convert the given argument to datetime.\n        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\n        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.\n        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\n        Timestamp.strftime : Format a single Timestamp.\n        Period.strftime : Format a single Period.\n\n        Examples\n        --------\n        >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"), periods=3, freq=\"s\")\n        >>> rng.strftime(\"%%B %%d, %%Y, %%r\")\n        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n               'March 10, 2018, 09:00:02 AM'],\n              dtype='object')\n        \"\"\"\n        result = self._format_native_types(date_format=date_format, na_rep=np.nan)\n        return result.astype(object, copy=False)\n\n\n_round_doc = \"\"\"\n    Perform {op} operation on the data to the specified `freq`.\n\n    Parameters\n    ----------\n    freq : str or Offset\n        The frequency level to {op} the index to. Must be a fixed\n        frequency like 's' (second) not 'ME' (month end). See\n        :ref:`frequency aliases <timeseries.offset_aliases>` for\n        a list of possible `freq` values.\n    ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n        Only relevant for DatetimeIndex:\n\n        - 'infer' will attempt to infer fall dst-transition hours based on\n          order\n        - bool-ndarray where True signifies a DST time, False designates\n          a non-DST time (note that this flag is only applicable for\n          ambiguous times)\n        - 'NaT' will return NaT where there are ambiguous times\n        - 'raise' will raise a ValueError if there are ambiguous\n          times.\n\n    nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, default 'raise'\n        A nonexistent time does not exist in a particular timezone\n        where clocks moved forward due to DST.\n\n        - 'shift_forward' will shift the nonexistent time forward to the\n          closest existing time\n        - 'shift_backward' will shift the nonexistent time backward to the\n          closest existing time\n        - 'NaT' will return NaT where there are nonexistent times\n        - timedelta objects will shift nonexistent times by the timedelta\n        - 'raise' will raise a ValueError if there are\n          nonexistent times.\n\n    Returns\n    -------\n    DatetimeIndex, TimedeltaIndex, or Series\n        Index of the same type for a DatetimeIndex or TimedeltaIndex,\n        or a Series with the same index for a Series.\n\n    Raises\n    ------\n    ValueError if the `freq` cannot be converted.\n\n    See Also\n    --------\n    DatetimeIndex.floor : Perform floor operation on the data to the specified `freq`.\n    DatetimeIndex.snap : Snap time stamps to nearest occurring frequency.\n\n    Notes\n    -----\n    If the timestamps have a timezone, {op}ing will take place relative to the\n    local (\"wall\") time and re-localized to the same timezone. When {op}ing\n    near daylight savings time, use ``nonexistent`` and ``ambiguous`` to\n    control the re-localization behavior.\n\n    Examples\n    --------\n    **DatetimeIndex**\n\n    >>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n    >>> rng\n    DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n                   '2018-01-01 12:01:00'],\n                  dtype='datetime64[ns]', freq='min')\n    \"\"\"\n\n_round_example = \"\"\">>> rng.round('h')\n    DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                   '2018-01-01 12:00:00'],\n                  dtype='datetime64[ns]', freq=None)\n\n    **Series**\n\n    >>> pd.Series(rng).dt.round(\"h\")\n    0   2018-01-01 12:00:00\n    1   2018-01-01 12:00:00\n    2   2018-01-01 12:00:00\n    dtype: datetime64[ns]\n\n    When rounding near a daylight savings time transition, use ``ambiguous`` or\n    ``nonexistent`` to control how the timestamp should be re-localized.\n\n    >>> rng_tz = pd.DatetimeIndex([\"2021-10-31 03:30:00\"], tz=\"Europe/Amsterdam\")\n\n    >>> rng_tz.floor(\"2h\", ambiguous=False)\n    DatetimeIndex(['2021-10-31 02:00:00+01:00'],\n                  dtype='datetime64[s, Europe/Amsterdam]', freq=None)\n\n    >>> rng_tz.floor(\"2h\", ambiguous=True)\n    DatetimeIndex(['2021-10-31 02:00:00+02:00'],\n                  dtype='datetime64[s, Europe/Amsterdam]', freq=None)\n    \"\"\"\n\n_floor_example = \"\"\">>> rng.floor('h')\n    DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',\n                   '2018-01-01 12:00:00'],\n                  dtype='datetime64[ns]', freq=None)\n\n    **Series**\n\n    >>> pd.Series(rng).dt.floor(\"h\")\n    0   2018-01-01 11:00:00\n    1   2018-01-01 12:00:00\n    2   2018-01-01 12:00:00\n    dtype: datetime64[ns]\n\n    When rounding near a daylight savings time transition, use ``ambiguous`` or\n    ``nonexistent`` to control how the timestamp should be re-localized.\n\n    >>> rng_tz = pd.DatetimeIndex([\"2021-10-31 03:30:00\"], tz=\"Europe/Amsterdam\")\n\n    >>> rng_tz.floor(\"2h\", ambiguous=False)\n    DatetimeIndex(['2021-10-31 02:00:00+01:00'],\n                 dtype='datetime64[s, Europe/Amsterdam]', freq=None)\n\n    >>> rng_tz.floor(\"2h\", ambiguous=True)\n    DatetimeIndex(['2021-10-31 02:00:00+02:00'],\n                  dtype='datetime64[s, Europe/Amsterdam]', freq=None)\n    \"\"\"\n\n_ceil_example = \"\"\">>> rng.ceil('h')\n    DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n                   '2018-01-01 13:00:00'],\n                  dtype='datetime64[ns]', freq=None)\n\n    **Series**\n\n    >>> pd.Series(rng).dt.ceil(\"h\")\n    0   2018-01-01 12:00:00\n    1   2018-01-01 12:00:00\n    2   2018-01-01 13:00:00\n    dtype: datetime64[ns]\n\n    When rounding near a daylight savings time transition, use ``ambiguous`` or\n    ``nonexistent`` to control how the timestamp should be re-localized.\n\n    >>> rng_tz = pd.DatetimeIndex([\"2021-10-31 01:30:00\"], tz=\"Europe/Amsterdam\")\n\n    >>> rng_tz.ceil(\"h\", ambiguous=False)\n    DatetimeIndex(['2021-10-31 02:00:00+01:00'],\n                  dtype='datetime64[s, Europe/Amsterdam]', freq=None)\n\n    >>> rng_tz.ceil(\"h\", ambiguous=True)\n    DatetimeIndex(['2021-10-31 02:00:00+02:00'],\n                  dtype='datetime64[s, Europe/Amsterdam]', freq=None)\n    \"\"\"\n\n\nclass TimelikeOps(DatetimeLikeArrayMixin):\n    \"\"\"\n    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.\n    \"\"\"\n\n    @classmethod\n    def _validate_dtype(cls, values, dtype):\n        raise AbstractMethodError(cls)\n\n    @property\n    def freq(self):\n        \"\"\"\n        Return the frequency object if it is set, otherwise None.\n\n        To learn more about the frequency strings, please see\n        :ref:`this link<timeseries.offset_aliases>`.\n\n        See Also\n        --------\n        DatetimeIndex.freq : Return the frequency object if it is set, otherwise None.\n        PeriodIndex.freq : Return the frequency object if it is set, otherwise None.\n\n        Examples\n        --------\n        >>> datetimeindex = pd.date_range(\n        ...     \"2022-02-22 02:22:22\", periods=10, tz=\"America/Chicago\", freq=\"h\"\n        ... )\n        >>> datetimeindex\n        DatetimeIndex(['2022-02-22 02:22:22-06:00', '2022-02-22 03:22:22-06:00',\n                       '2022-02-22 04:22:22-06:00', '2022-02-22 05:22:22-06:00',\n                       '2022-02-22 06:22:22-06:00', '2022-02-22 07:22:22-06:00',\n                       '2022-02-22 08:22:22-06:00', '2022-02-22 09:22:22-06:00',\n                       '2022-02-22 10:22:22-06:00', '2022-02-22 11:22:22-06:00'],\n                      dtype='datetime64[ns, America/Chicago]', freq='h')\n        >>> datetimeindex.freq\n        <Hour>\n        \"\"\"\n        return self._freq\n\n    @freq.setter\n    def freq(self, value) -> None:\n        if value is not None:\n            value = to_offset(value)\n            self._validate_frequency(self, value)\n            if self.dtype.kind == \"m\" and not isinstance(value, Tick):\n                raise TypeError(\"TimedeltaArray/Index freq must be a Tick\")\n\n            if self.ndim > 1:\n                raise ValueError(\"Cannot set freq with ndim > 1\")\n\n        self._freq = value\n\n    @final\n    def _maybe_pin_freq(self, freq, validate_kwds: dict) -> None:\n        \"\"\"\n        Constructor helper to pin the appropriate `freq` attribute.  Assumes\n        that self._freq is currently set to any freq inferred in\n        _from_sequence_not_strict.\n        \"\"\"\n        if freq is None:\n            # user explicitly passed None -> override any inferred_freq\n            self._freq = None\n        elif freq == \"infer\":\n            # if self._freq is *not* None then we already inferred a freq\n            #  and there is nothing left to do\n            if self._freq is None:\n                # Set _freq directly to bypass duplicative _validate_frequency\n                # check.\n                self._freq = to_offset(self.inferred_freq)\n        elif freq is lib.no_default:\n            # user did not specify anything, keep inferred freq if the original\n            #  data had one, otherwise do nothing\n            pass\n        elif self._freq is None:\n            # We cannot inherit a freq from the data, so we need to validate\n            #  the user-passed freq\n            freq = to_offset(freq)\n            type(self)._validate_frequency(self, freq, **validate_kwds)\n            self._freq = freq\n        else:\n            # Otherwise we just need to check that the user-passed freq\n            #  doesn't conflict with the one we already have.\n            freq = to_offset(freq)\n            _validate_inferred_freq(freq, self._freq)\n\n    @final\n    @classmethod\n    def _validate_frequency(cls, index, freq: BaseOffset, **kwargs) -> None:\n        \"\"\"\n        Validate that a frequency is compatible with the values of a given\n        Datetime Array/Index or Timedelta Array/Index\n\n        Parameters\n        ----------\n        index : DatetimeIndex or TimedeltaIndex\n            The index on which to determine if the given frequency is valid\n        freq : DateOffset\n            The frequency to validate\n        \"\"\"\n        inferred = index.inferred_freq\n        if index.size == 0 or inferred == freq.freqstr:\n            return None\n\n        try:\n            on_freq = cls._generate_range(\n                start=index[0],\n                end=None,\n                periods=len(index),\n                freq=freq,\n                unit=index.unit,\n                **kwargs,\n            )\n            if not np.array_equal(index.asi8, on_freq.asi8):\n                raise ValueError\n        except ValueError as err:\n            if \"non-fixed\" in str(err):\n                # non-fixed frequencies are not meaningful for timedelta64;\n                #  we retain that error message\n                raise err\n            # GH#11587 the main way this is reached is if the `np.array_equal`\n            #  check above is False.  This can also be reached if index[0]\n            #  is `NaT`, in which case the call to `cls._generate_range` will\n            #  raise a ValueError, which we re-raise with a more targeted\n            #  message.\n            raise ValueError(\n                f\"Inferred frequency {inferred} from passed values \"\n                f\"does not conform to passed frequency {freq.freqstr}\"\n            ) from err\n\n    @classmethod\n    def _generate_range(\n        cls, start, end, periods: int | None, freq, *args, **kwargs\n    ) -> Self:\n        raise AbstractMethodError(cls)\n\n    # --------------------------------------------------------------\n\n    @cache_readonly\n    def _creso(self) -> int:\n        return get_unit_from_dtype(self._ndarray.dtype)\n\n    @cache_readonly\n    def unit(self) -> str:\n        # e.g. \"ns\", \"us\", \"ms\"\n        # error: Argument 1 to \"dtype_to_unit\" has incompatible type\n        # \"ExtensionDtype\"; expected \"Union[DatetimeTZDtype, dtype[Any]]\"\n        return dtype_to_unit(self.dtype)  # type: ignore[arg-type]\n\n    def as_unit(self, unit: str, round_ok: bool = True) -> Self:\n        \"\"\"\n        Convert to a dtype with the given unit resolution.\n\n        The limits of timestamp representation depend on the chosen resolution.\n        Different resolutions can be converted to each other through as_unit.\n\n        Parameters\n        ----------\n        unit : {'s', 'ms', 'us', 'ns'}\n        round_ok : bool, default True\n            If False and the conversion requires rounding, raise ValueError.\n\n        Returns\n        -------\n        same type as self\n            Converted to the specified unit.\n\n        See Also\n        --------\n        Timestamp.as_unit : Convert to the given unit.\n\n        Examples\n        --------\n        For :class:`pandas.DatetimeIndex`:\n\n        >>> idx = pd.DatetimeIndex([\"2020-01-02 01:02:03.004005006\"])\n        >>> idx\n        DatetimeIndex(['2020-01-02 01:02:03.004005006'],\n                      dtype='datetime64[ns]', freq=None)\n        >>> idx.as_unit(\"s\")\n        DatetimeIndex(['2020-01-02 01:02:03'], dtype='datetime64[s]', freq=None)\n\n        For :class:`pandas.TimedeltaIndex`:\n\n        >>> tdelta_idx = pd.to_timedelta([\"1 day 3 min 2 us 42 ns\"])\n        >>> tdelta_idx\n        TimedeltaIndex(['1 days 00:03:00.000002042'],\n                        dtype='timedelta64[ns]', freq=None)\n        >>> tdelta_idx.as_unit(\"s\")\n        TimedeltaIndex(['1 days 00:03:00'], dtype='timedelta64[s]', freq=None)\n        \"\"\"\n        if unit not in [\"s\", \"ms\", \"us\", \"ns\"]:\n            raise ValueError(\"Supported units are 's', 'ms', 'us', 'ns'\")\n\n        dtype = np.dtype(f\"{self.dtype.kind}8[{unit}]\")\n        new_values = astype_overflowsafe(self._ndarray, dtype, round_ok=round_ok)\n\n        if isinstance(self.dtype, np.dtype):\n            new_dtype = new_values.dtype\n        else:\n            tz = cast(\"DatetimeArray\", self).tz\n            new_dtype = DatetimeTZDtype(tz=tz, unit=unit)\n\n        # error: Unexpected keyword argument \"freq\" for \"_simple_new\" of\n        # \"NDArrayBacked\"  [call-arg]\n        return type(self)._simple_new(\n            new_values,\n            dtype=new_dtype,\n            freq=self.freq,  # type: ignore[call-arg]\n        )\n\n    # TODO: annotate other as DatetimeArray | TimedeltaArray | Timestamp | Timedelta\n    #  with the return type matching input type.  TypeVar?\n    def _ensure_matching_resos(self, other):\n        if self._creso != other._creso:\n            # Just as with Timestamp/Timedelta, we cast to the higher resolution\n            if self._creso < other._creso:\n                self = self.as_unit(other.unit)\n            else:\n                other = other.as_unit(self.unit)\n        return self, other\n\n    # --------------------------------------------------------------\n\n    def __array_ufunc__(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):\n        if (\n            ufunc in [np.isnan, np.isinf, np.isfinite]\n            and len(inputs) == 1\n            and inputs[0] is self\n        ):\n            # numpy 1.18 changed isinf and isnan to not raise on dt64/td64\n            return getattr(ufunc, method)(self._ndarray, **kwargs)\n\n        return super().__array_ufunc__(ufunc, method, *inputs, **kwargs)\n\n    def _round(self, freq, mode, ambiguous, nonexistent):\n        # round the local times\n        if isinstance(self.dtype, DatetimeTZDtype):\n            # operate on naive timestamps, then convert back to aware\n            self = cast(\"DatetimeArray\", self)\n            naive = self.tz_localize(None)\n            result = naive._round(freq, mode, ambiguous, nonexistent)\n            return result.tz_localize(\n                self.tz, ambiguous=ambiguous, nonexistent=nonexistent\n            )\n\n        values = self.view(\"i8\")\n        values = cast(np.ndarray, values)\n        nanos = get_unit_for_round(freq, self._creso)\n        if nanos == 0:\n            # GH 52761\n            return self.copy()\n        result_i8 = round_nsint64(values, mode, nanos)\n        result = self._maybe_mask_results(result_i8, fill_value=iNaT)\n        result = result.view(self._ndarray.dtype)\n        return self._simple_new(result, dtype=self.dtype)\n\n    @Appender((_round_doc + _round_example).format(op=\"round\"))\n    def round(\n        self,\n        freq,\n        ambiguous: TimeAmbiguous = \"raise\",\n        nonexistent: TimeNonexistent = \"raise\",\n    ) -> Self:\n        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)\n\n    @Appender((_round_doc + _floor_example).format(op=\"floor\"))\n    def floor(\n        self,\n        freq,\n        ambiguous: TimeAmbiguous = \"raise\",\n        nonexistent: TimeNonexistent = \"raise\",\n    ) -> Self:\n        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)\n\n    @Appender((_round_doc + _ceil_example).format(op=\"ceil\"))\n    def ceil(\n        self,\n        freq,\n        ambiguous: TimeAmbiguous = \"raise\",\n        nonexistent: TimeNonexistent = \"raise\",\n    ) -> Self:\n        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)\n\n    # --------------------------------------------------------------\n    # Reductions\n\n    def any(self, *, axis: AxisInt | None = None, skipna: bool = True) -> bool:\n        # GH#34479 the nanops call will raise a TypeError for non-td64 dtype\n        return nanops.nanany(self._ndarray, axis=axis, skipna=skipna, mask=self.isna())\n\n    def all(self, *, axis: AxisInt | None = None, skipna: bool = True) -> bool:\n        # GH#34479 the nanops call will raise a TypeError for non-td64 dtype\n\n        return nanops.nanall(self._ndarray, axis=axis, skipna=skipna, mask=self.isna())\n\n    # --------------------------------------------------------------\n    # Frequency Methods\n\n    def _maybe_clear_freq(self) -> None:\n        self._freq = None\n\n    def _with_freq(self, freq) -> Self:\n        \"\"\"\n        Helper to get a view on the same data, with a new freq.\n\n        Parameters\n        ----------\n        freq : DateOffset, None, or \"infer\"\n\n        Returns\n        -------\n        Same type as self\n        \"\"\"\n        # GH#29843\n        if freq is None:\n            # Always valid\n            pass\n        elif len(self) == 0 and isinstance(freq, BaseOffset):\n            # Always valid.  In the TimedeltaArray case, we require a Tick offset\n            if self.dtype.kind == \"m\" and not isinstance(freq, Tick):\n                raise TypeError(\"TimedeltaArray/Index freq must be a Tick\")\n        else:\n            # As an internal method, we can ensure this assertion always holds\n            assert freq == \"infer\"\n            freq = to_offset(self.inferred_freq)\n\n        arr = self.view()\n        arr._freq = freq\n        return arr\n\n    # --------------------------------------------------------------\n    # ExtensionArray Interface\n\n    def _values_for_json(self) -> np.ndarray:\n        # Small performance bump vs the base class which calls np.asarray(self)\n        if isinstance(self.dtype, np.dtype):\n            return self._ndarray\n        return super()._values_for_json()\n\n    def factorize(\n        self,\n        use_na_sentinel: bool = True,\n        sort: bool = False,\n    ):\n        if self.freq is not None:\n            # We must be unique, so can short-circuit (and retain freq)\n            if sort and self.freq.n < 0:\n                codes = np.arange(len(self) - 1, -1, -1, dtype=np.intp)\n                uniques = self[::-1]\n            else:\n                codes = np.arange(len(self), dtype=np.intp)\n                uniques = self.copy()  # TODO: copy or view?\n            return codes, uniques\n\n        if sort:\n            # algorithms.factorize only passes sort=True here when freq is\n            #  not None, so this should not be reached.\n            raise NotImplementedError(\n                f\"The 'sort' keyword in {type(self).__name__}.factorize is \"\n                \"ignored unless arr.freq is not None. To factorize with sort, \"\n                \"call pd.factorize(obj, sort=True) instead.\"\n            )\n        return super().factorize(use_na_sentinel=use_na_sentinel)\n\n    @classmethod\n    def _concat_same_type(\n        cls,\n        to_concat: Sequence[Self],\n        axis: AxisInt = 0,\n    ) -> Self:\n        new_obj = super()._concat_same_type(to_concat, axis)\n\n        obj = to_concat[0]\n\n        if axis == 0:\n            # GH 3232: If the concat result is evenly spaced, we can retain the\n            # original frequency\n            to_concat = [x for x in to_concat if len(x)]\n\n            if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):\n                pairs = zip(to_concat[:-1], to_concat[1:])\n                if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):\n                    new_freq = obj.freq\n                    new_obj._freq = new_freq\n        return new_obj\n\n    def copy(self, order: str = \"C\") -> Self:\n        new_obj = super().copy(order=order)\n        new_obj._freq = self.freq\n        return new_obj\n\n    def interpolate(\n        self,\n        *,\n        method: InterpolateOptions,\n        axis: int,\n        index: Index,\n        limit,\n        limit_direction,\n        limit_area,\n        copy: bool,\n        **kwargs,\n    ) -> Self:\n        \"\"\"\n        See NDFrame.interpolate.__doc__.\n        \"\"\"\n        # NB: we return type(self) even if copy=False\n        if method != \"linear\":\n            raise NotImplementedError\n\n        if not copy:\n            out_data = self._ndarray\n        else:\n            out_data = self._ndarray.copy()\n\n        missing.interpolate_2d_inplace(\n            out_data,\n            method=method,\n            axis=axis,\n            index=index,\n            limit=limit,\n            limit_direction=limit_direction,\n            limit_area=limit_area,\n            **kwargs,\n        )\n        if not copy:\n            return self\n        return type(self)._simple_new(out_data, dtype=self.dtype)\n\n    def take(\n        self,\n        indices: TakeIndexer,\n        *,\n        allow_fill: bool = False,\n        fill_value: Any = None,\n        axis: AxisInt = 0,\n    ) -> Self:\n        result = super().take(\n            indices=indices, allow_fill=allow_fill, fill_value=fill_value, axis=axis\n        )\n\n        indices = np.asarray(indices, dtype=np.intp)\n        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))\n\n        if isinstance(maybe_slice, slice):\n            freq = self._get_getitem_freq(maybe_slice)\n            result._freq = freq\n\n        return result\n\n    # --------------------------------------------------------------\n    # Unsorted\n\n    @property\n    def _is_dates_only(self) -> bool:\n        \"\"\"\n        Check if we are round times at midnight (and no timezone), which will\n        be given a more compact __repr__ than other cases. For TimedeltaArray\n        we are checking for multiples of 24H.\n        \"\"\"\n        if not lib.is_np_dtype(self.dtype):\n            # i.e. we have a timezone\n            return False\n\n        values_int = self.asi8\n        consider_values = values_int != iNaT\n        reso = get_unit_from_dtype(self.dtype)\n        ppd = periods_per_day(reso)\n\n        # TODO: can we reuse is_date_array_normalized?  would need a skipna kwd\n        #  (first attempt at this was less performant than this implementation)\n        even_days = np.logical_and(consider_values, values_int % ppd != 0).sum() == 0\n        return even_days\n\n\n# -------------------------------------------------------------------\n# Shared Constructor Helpers\n\n\ndef ensure_arraylike_for_datetimelike(\n    data, copy: bool, cls_name: str\n) -> tuple[ArrayLike, bool]:\n    if not hasattr(data, \"dtype\"):\n        # e.g. list, tuple\n        if not isinstance(data, (list, tuple)) and np.ndim(data) == 0:\n            # i.e. generator\n            data = list(data)\n\n        data = construct_1d_object_array_from_listlike(data)\n        copy = False\n    elif isinstance(data, ABCMultiIndex):\n        raise TypeError(f\"Cannot create a {cls_name} from a MultiIndex.\")\n    else:\n        data = extract_array(data, extract_numpy=True)\n\n    if isinstance(data, IntegerArray) or (\n        isinstance(data, ArrowExtensionArray) and data.dtype.kind in \"iu\"\n    ):\n        data = data.to_numpy(\"int64\", na_value=iNaT)\n        copy = False\n    elif isinstance(data, ArrowExtensionArray):\n        data = data._maybe_convert_datelike_array()\n        data = data.to_numpy()\n        copy = False\n    elif not isinstance(data, (np.ndarray, ExtensionArray)):\n        # GH#24539 e.g. xarray, dask object\n        data = np.asarray(data)\n\n    elif isinstance(data, ABCCategorical):\n        # GH#18664 preserve tz in going DTI->Categorical->DTI\n        # TODO: cases where we need to do another pass through maybe_convert_dtype,\n        #  e.g. the categories are timedelta64s\n        data = data.categories.take(data.codes, fill_value=NaT)._values\n        copy = False\n\n    return data, copy\n\n\n@overload\ndef validate_periods(periods: None) -> None: ...\n\n\n@overload\ndef validate_periods(periods: int) -> int: ...\n\n\ndef validate_periods(periods: int | None) -> int | None:\n    \"\"\"\n    If a `periods` argument is passed to the Datetime/Timedelta Array/Index\n    constructor, cast it to an integer.\n\n    Parameters\n    ----------\n    periods : None, int\n\n    Returns\n    -------\n    periods : None or int\n\n    Raises\n    ------\n    TypeError\n        if periods is not None or int\n    \"\"\"\n    if periods is not None and not lib.is_integer(periods):\n        raise TypeError(f\"periods must be an integer, got {periods}\")\n    # error: Incompatible return value type (got \"int | integer[Any] | None\",\n    # expected \"int | None\")\n    return periods  # type: ignore[return-value]\n\n\ndef _validate_inferred_freq(\n    freq: BaseOffset | None, inferred_freq: BaseOffset | None\n) -> BaseOffset | None:\n    \"\"\"\n    If the user passes a freq and another freq is inferred from passed data,\n    require that they match.\n\n    Parameters\n    ----------\n    freq : DateOffset or None\n    inferred_freq : DateOffset or None\n\n    Returns\n    -------\n    freq : DateOffset or None\n    \"\"\"\n    if inferred_freq is not None:\n        if freq is not None and freq != inferred_freq:\n            raise ValueError(\n                f\"Inferred frequency {inferred_freq} from passed \"\n                \"values does not conform to passed frequency \"\n                f\"{freq.freqstr}\"\n            )\n        if freq is None:\n            freq = inferred_freq\n\n    return freq\n\n\ndef dtype_to_unit(dtype: DatetimeTZDtype | np.dtype | ArrowDtype) -> str:\n    \"\"\"\n    Return the unit str corresponding to the dtype's resolution.\n\n    Parameters\n    ----------\n    dtype : DatetimeTZDtype or np.dtype\n        If np.dtype, we assume it is a datetime64 dtype.\n\n    Returns\n    -------\n    str\n    \"\"\"\n    if isinstance(dtype, DatetimeTZDtype):\n        return dtype.unit\n    elif isinstance(dtype, ArrowDtype):\n        if dtype.kind not in \"mM\":\n            raise ValueError(f\"{dtype=} does not have a resolution.\")\n        return dtype.pyarrow_dtype.unit\n    return np.datetime_data(dtype)[0]\n"
    },
    {
      "filename": "pandas/core/arrays/datetimes.py",
      "content": "from __future__ import annotations\n\nfrom datetime import (\n    datetime,\n    timedelta,\n    tzinfo,\n)\nfrom typing import (\n    TYPE_CHECKING,\n    TypeVar,\n    cast,\n    overload,\n)\nimport warnings\n\nimport numpy as np\n\nfrom pandas._config.config import get_option\n\nfrom pandas._libs import (\n    lib,\n    tslib,\n)\nfrom pandas._libs.tslibs import (\n    BaseOffset,\n    NaT,\n    NaTType,\n    Resolution,\n    Timestamp,\n    astype_overflowsafe,\n    fields,\n    get_resolution,\n    get_supported_dtype,\n    get_unit_from_dtype,\n    ints_to_pydatetime,\n    is_date_array_normalized,\n    is_supported_dtype,\n    is_unitless,\n    normalize_i8_timestamps,\n    timezones,\n    to_offset,\n    tz_convert_from_utc,\n    tzconversion,\n)\nfrom pandas._libs.tslibs.dtypes import abbrev_to_npy_unit\nfrom pandas.errors import PerformanceWarning\nfrom pandas.util._exceptions import find_stack_level\nfrom pandas.util._validators import validate_inclusive\n\nfrom pandas.core.dtypes.common import (\n    DT64NS_DTYPE,\n    INT64_DTYPE,\n    is_bool_dtype,\n    is_float_dtype,\n    is_string_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import (\n    DatetimeTZDtype,\n    ExtensionDtype,\n    PeriodDtype,\n)\nfrom pandas.core.dtypes.missing import isna\n\nfrom pandas.core.arrays import datetimelike as dtl\nfrom pandas.core.arrays._ranges import generate_regular_range\nimport pandas.core.common as com\n\nfrom pandas.tseries.frequencies import get_period_alias\nfrom pandas.tseries.offsets import (\n    Day,\n    Tick,\n)\n\nif TYPE_CHECKING:\n    from collections.abc import (\n        Generator,\n        Iterator,\n    )\n\n    from pandas._typing import (\n        ArrayLike,\n        DateTimeErrorChoices,\n        DtypeObj,\n        IntervalClosedType,\n        Self,\n        TimeAmbiguous,\n        TimeNonexistent,\n        npt,\n    )\n\n    from pandas import (\n        DataFrame,\n        Timedelta,\n    )\n    from pandas.core.arrays import PeriodArray\n\n    _TimestampNoneT1 = TypeVar(\"_TimestampNoneT1\", Timestamp, None)\n    _TimestampNoneT2 = TypeVar(\"_TimestampNoneT2\", Timestamp, None)\n\n\n_ITER_CHUNKSIZE = 10_000\n\n\n@overload\ndef tz_to_dtype(tz: tzinfo, unit: str = ...) -> DatetimeTZDtype: ...\n\n\n@overload\ndef tz_to_dtype(tz: None, unit: str = ...) -> np.dtype[np.datetime64]: ...\n\n\ndef tz_to_dtype(\n    tz: tzinfo | None, unit: str = \"ns\"\n) -> np.dtype[np.datetime64] | DatetimeTZDtype:\n    \"\"\"\n    Return a datetime64[ns] dtype appropriate for the given timezone.\n\n    Parameters\n    ----------\n    tz : tzinfo or None\n    unit : str, default \"ns\"\n\n    Returns\n    -------\n    np.dtype or Datetime64TZDType\n    \"\"\"\n    if tz is None:\n        return np.dtype(f\"M8[{unit}]\")\n    else:\n        return DatetimeTZDtype(tz=tz, unit=unit)\n\n\ndef _field_accessor(name: str, field: str, docstring: str | None = None):\n    def f(self):\n        values = self._local_timestamps()\n\n        if field in self._bool_ops:\n            result: np.ndarray\n\n            if field.endswith((\"start\", \"end\")):\n                freq = self.freq\n                month_kw = 12\n                if freq:\n                    kwds = freq.kwds\n                    month_kw = kwds.get(\"startingMonth\", kwds.get(\"month\", month_kw))\n\n                if freq is not None:\n                    freq_name = freq.name\n                else:\n                    freq_name = None\n                result = fields.get_start_end_field(\n                    values, field, freq_name, month_kw, reso=self._creso\n                )\n            else:\n                result = fields.get_date_field(values, field, reso=self._creso)\n\n            # these return a boolean by-definition\n            return result\n\n        if field in self._object_ops:\n            result = fields.get_date_name_field(values, field, reso=self._creso)\n            result = self._maybe_mask_results(result, fill_value=None)\n\n        else:\n            result = fields.get_date_field(values, field, reso=self._creso)\n            result = self._maybe_mask_results(\n                result, fill_value=None, convert=\"float64\"\n            )\n\n        return result\n\n    f.__name__ = name\n    f.__doc__ = docstring\n    return property(f)\n\n\n# error: Definition of \"_concat_same_type\" in base class \"NDArrayBacked\" is\n# incompatible with definition in base class \"ExtensionArray\"\nclass DatetimeArray(dtl.TimelikeOps, dtl.DatelikeOps):  # type: ignore[misc]\n    \"\"\"\n    Pandas ExtensionArray for tz-naive or tz-aware datetime data.\n\n    .. warning::\n\n       DatetimeArray is currently experimental, and its API may change\n       without warning. In particular, :attr:`DatetimeArray.dtype` is\n       expected to change to always be an instance of an ``ExtensionDtype``\n       subclass.\n\n    Parameters\n    ----------\n    data : Series, Index, DatetimeArray, ndarray\n        The datetime data.\n\n        For DatetimeArray `values` (or a Series or Index boxing one),\n        `dtype` and `freq` will be extracted from `values`.\n\n    dtype : numpy.dtype or DatetimeTZDtype\n        Note that the only NumPy dtype allowed is 'datetime64[ns]'.\n    freq : str or Offset, optional\n        The frequency.\n    copy : bool, default False\n        Whether to copy the underlying array of values.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> pd.arrays.DatetimeArray._from_sequence(\n    ...     pd.DatetimeIndex([\"2023-01-01\", \"2023-01-02\"], freq=\"D\")\n    ... )\n    <DatetimeArray>\n    ['2023-01-01 00:00:00', '2023-01-02 00:00:00']\n    Length: 2, dtype: datetime64[s]\n    \"\"\"\n\n    _typ = \"datetimearray\"\n    _internal_fill_value = np.datetime64(\"NaT\", \"ns\")\n    _recognized_scalars = (datetime, np.datetime64)\n    _is_recognized_dtype = lambda x: lib.is_np_dtype(x, \"M\") or isinstance(\n        x, DatetimeTZDtype\n    )\n    _infer_matches = (\"datetime\", \"datetime64\", \"date\")\n\n    @property\n    def _scalar_type(self) -> type[Timestamp]:\n        return Timestamp\n\n    # define my properties & methods for delegation\n    _bool_ops: list[str] = [\n        \"is_month_start\",\n        \"is_month_end\",\n        \"is_quarter_start\",\n        \"is_quarter_end\",\n        \"is_year_start\",\n        \"is_year_end\",\n        \"is_leap_year\",\n    ]\n    _object_ops: list[str] = [\"freq\", \"tz\"]\n    _field_ops: list[str] = [\n        \"year\",\n        \"month\",\n        \"day\",\n        \"hour\",\n        \"minute\",\n        \"second\",\n        \"weekday\",\n        \"dayofweek\",\n        \"day_of_week\",\n        \"dayofyear\",\n        \"day_of_year\",\n        \"quarter\",\n        \"days_in_month\",\n        \"daysinmonth\",\n        \"microsecond\",\n        \"nanosecond\",\n    ]\n    _other_ops: list[str] = [\"date\", \"time\", \"timetz\"]\n    _datetimelike_ops: list[str] = (\n        _field_ops + _object_ops + _bool_ops + _other_ops + [\"unit\"]\n    )\n    _datetimelike_methods: list[str] = [\n        \"to_period\",\n        \"tz_localize\",\n        \"tz_convert\",\n        \"normalize\",\n        \"strftime\",\n        \"round\",\n        \"floor\",\n        \"ceil\",\n        \"month_name\",\n        \"day_name\",\n        \"as_unit\",\n    ]\n\n    # ndim is inherited from ExtensionArray, must exist to ensure\n    #  Timestamp.__richcmp__(DateTimeArray) operates pointwise\n\n    # ensure that operations with numpy arrays defer to our implementation\n    __array_priority__ = 1000\n\n    # -----------------------------------------------------------------\n    # Constructors\n\n    _dtype: np.dtype[np.datetime64] | DatetimeTZDtype\n    _freq: BaseOffset | None = None\n\n    @classmethod\n    def _from_scalars(cls, scalars, *, dtype: DtypeObj) -> Self:\n        if lib.infer_dtype(scalars, skipna=True) not in [\"datetime\", \"datetime64\"]:\n            # TODO: require any NAs be valid-for-DTA\n            # TODO: if dtype is passed, check for tzawareness compat?\n            raise ValueError\n        return cls._from_sequence(scalars, dtype=dtype)\n\n    @classmethod\n    def _validate_dtype(cls, values, dtype):\n        # used in TimeLikeOps.__init__\n        dtype = _validate_dt64_dtype(dtype)\n        _validate_dt64_dtype(values.dtype)\n        if isinstance(dtype, np.dtype):\n            if values.dtype != dtype:\n                raise ValueError(\"Values resolution does not match dtype.\")\n        else:\n            vunit = np.datetime_data(values.dtype)[0]\n            if vunit != dtype.unit:\n                raise ValueError(\"Values resolution does not match dtype.\")\n        return dtype\n\n    # error: Signature of \"_simple_new\" incompatible with supertype \"NDArrayBacked\"\n    @classmethod\n    def _simple_new(  # type: ignore[override]\n        cls,\n        values: npt.NDArray[np.datetime64],\n        freq: BaseOffset | None = None,\n        dtype: np.dtype[np.datetime64] | DatetimeTZDtype = DT64NS_DTYPE,\n    ) -> Self:\n        assert isinstance(values, np.ndarray)\n        assert dtype.kind == \"M\"\n        if isinstance(dtype, np.dtype):\n            assert dtype == values.dtype\n            assert not is_unitless(dtype)\n        else:\n            # DatetimeTZDtype. If we have e.g. DatetimeTZDtype[us, UTC],\n            #  then values.dtype should be M8[us].\n            assert dtype._creso == get_unit_from_dtype(values.dtype)\n\n        result = super()._simple_new(values, dtype)\n        result._freq = freq\n        return result\n\n    @classmethod\n    def _from_sequence(cls, scalars, *, dtype=None, copy: bool = False) -> Self:\n        return cls._from_sequence_not_strict(scalars, dtype=dtype, copy=copy)\n\n    @classmethod\n    def _from_sequence_not_strict(\n        cls,\n        data,\n        *,\n        dtype=None,\n        copy: bool = False,\n        tz=lib.no_default,\n        freq: str | BaseOffset | lib.NoDefault | None = lib.no_default,\n        dayfirst: bool = False,\n        yearfirst: bool = False,\n        ambiguous: TimeAmbiguous = \"raise\",\n    ) -> Self:\n        \"\"\"\n        A non-strict version of _from_sequence, called from DatetimeIndex.__new__.\n        \"\"\"\n\n        # if the user either explicitly passes tz=None or a tz-naive dtype, we\n        #  disallows inferring a tz.\n        explicit_tz_none = tz is None\n        if tz is lib.no_default:\n            tz = None\n        else:\n            tz = timezones.maybe_get_tz(tz)\n\n        dtype = _validate_dt64_dtype(dtype)\n        # if dtype has an embedded tz, capture it\n        tz = _validate_tz_from_dtype(dtype, tz, explicit_tz_none)\n\n        unit = None\n        if dtype is not None:\n            unit = dtl.dtype_to_unit(dtype)\n\n        data, copy = dtl.ensure_arraylike_for_datetimelike(\n            data, copy, cls_name=\"DatetimeArray\"\n        )\n        inferred_freq = None\n        if isinstance(data, DatetimeArray):\n            inferred_freq = data.freq\n\n        subarr, tz = _sequence_to_dt64(\n            data,\n            copy=copy,\n            tz=tz,\n            dayfirst=dayfirst,\n            yearfirst=yearfirst,\n            ambiguous=ambiguous,\n            out_unit=unit,\n        )\n        # We have to call this again after possibly inferring a tz above\n        _validate_tz_from_dtype(dtype, tz, explicit_tz_none)\n        if tz is not None and explicit_tz_none:\n            raise ValueError(\n                \"Passed data is timezone-aware, incompatible with 'tz=None'. \"\n                \"Use obj.tz_localize(None) instead.\"\n            )\n\n        data_unit = np.datetime_data(subarr.dtype)[0]\n        data_dtype = tz_to_dtype(tz, data_unit)\n        result = cls._simple_new(subarr, freq=inferred_freq, dtype=data_dtype)\n        if unit is not None and unit != result.unit:\n            # If unit was specified in user-passed dtype, cast to it here\n            result = result.as_unit(unit)\n\n        validate_kwds = {\"ambiguous\": ambiguous}\n        result._maybe_pin_freq(freq, validate_kwds)\n        return result\n\n    @classmethod\n    def _generate_range(\n        cls,\n        start,\n        end,\n        periods: int | None,\n        freq,\n        tz=None,\n        normalize: bool = False,\n        ambiguous: TimeAmbiguous = \"raise\",\n        nonexistent: TimeNonexistent = \"raise\",\n        inclusive: IntervalClosedType = \"both\",\n        *,\n        unit: str | None = None,\n    ) -> Self:\n        periods = dtl.validate_periods(periods)\n        if freq is None and any(x is None for x in [periods, start, end]):\n            raise ValueError(\"Must provide freq argument if no data is supplied\")\n\n        if com.count_not_none(start, end, periods, freq) != 3:\n            raise ValueError(\n                \"Of the four parameters: start, end, periods, \"\n                \"and freq, exactly three must be specified\"\n            )\n        freq = to_offset(freq)\n\n        if start is not None:\n            start = Timestamp(start)\n\n        if end is not None:\n            end = Timestamp(end)\n\n        if start is NaT or end is NaT:\n            raise ValueError(\"Neither `start` nor `end` can be NaT\")\n\n        if unit is not None:\n            if unit not in [\"s\", \"ms\", \"us\", \"ns\"]:\n                raise ValueError(\"'unit' must be one of 's', 'ms', 'us', 'ns'\")\n        else:\n            unit = \"ns\"\n\n        if start is not None:\n            start = start.as_unit(unit, round_ok=False)\n        if end is not None:\n            end = end.as_unit(unit, round_ok=False)\n\n        left_inclusive, right_inclusive = validate_inclusive(inclusive)\n        start, end = _maybe_normalize_endpoints(start, end, normalize)\n        tz = _infer_tz_from_endpoints(start, end, tz)\n\n        if tz is not None:\n            # Localize the start and end arguments\n            start = _maybe_localize_point(start, freq, tz, ambiguous, nonexistent)\n            end = _maybe_localize_point(end, freq, tz, ambiguous, nonexistent)\n\n        if freq is not None:\n            # We break Day arithmetic (fixed 24 hour) here and opt for\n            # Day to mean calendar day (23/24/25 hour). Therefore, strip\n            # tz info from start and day to avoid DST arithmetic\n            if isinstance(freq, Day):\n                if start is not None:\n                    start = start.tz_localize(None)\n                if end is not None:\n                    end = end.tz_localize(None)\n\n            if isinstance(freq, Tick):\n                i8values = generate_regular_range(start, end, periods, freq, unit=unit)\n            else:\n                xdr = _generate_range(\n                    start=start, end=end, periods=periods, offset=freq, unit=unit\n                )\n                i8values = np.array([x._value for x in xdr], dtype=np.int64)\n\n            endpoint_tz = start.tz if start is not None else end.tz\n\n            if tz is not None and endpoint_tz is None:\n                if not timezones.is_utc(tz):\n                    # short-circuit tz_localize_to_utc which would make\n                    #  an unnecessary copy with UTC but be a no-op.\n                    creso = abbrev_to_npy_unit(unit)\n                    i8values = tzconversion.tz_localize_to_utc(\n                        i8values,\n                        tz,\n                        ambiguous=ambiguous,\n                        nonexistent=nonexistent,\n                        creso=creso,\n                    )\n\n                # i8values is localized datetime64 array -> have to convert\n                # start/end as well to compare\n                if start is not None:\n                    start = start.tz_localize(tz, ambiguous, nonexistent)\n                if end is not None:\n                    end = end.tz_localize(tz, ambiguous, nonexistent)\n        else:\n            # Create a linearly spaced date_range in local time\n            # Nanosecond-granularity timestamps aren't always correctly\n            # representable with doubles, so we limit the range that we\n            # pass to np.linspace as much as possible\n            periods = cast(int, periods)\n            i8values = (\n                np.linspace(0, end._value - start._value, periods, dtype=\"int64\")\n                + start._value\n            )\n            if i8values.dtype != \"i8\":\n                # 2022-01-09 I (brock) am not sure if it is possible for this\n                #  to overflow and cast to e.g. f8, but if it does we need to cast\n                i8values = i8values.astype(\"i8\")\n\n        if start == end:\n            if not left_inclusive and not right_inclusive:\n                i8values = i8values[1:-1]\n        else:\n            start_i8 = Timestamp(start)._value\n            end_i8 = Timestamp(end)._value\n            if not left_inclusive or not right_inclusive:\n                if not left_inclusive and len(i8values) and i8values[0] == start_i8:\n                    i8values = i8values[1:]\n                if not right_inclusive and len(i8values) and i8values[-1] == end_i8:\n                    i8values = i8values[:-1]\n\n        dt64_values = i8values.view(f\"datetime64[{unit}]\")\n        dtype = tz_to_dtype(tz, unit=unit)\n        return cls._simple_new(dt64_values, freq=freq, dtype=dtype)\n\n    # -----------------------------------------------------------------\n    # DatetimeLike Interface\n\n    def _unbox_scalar(self, value) -> np.datetime64:\n        if not isinstance(value, self._scalar_type) and value is not NaT:\n            raise ValueError(\"'value' should be a Timestamp.\")\n        self._check_compatible_with(value)\n        if value is NaT:\n            return np.datetime64(value._value, self.unit)\n        else:\n            return value.as_unit(self.unit, round_ok=False).asm8\n\n    def _scalar_from_string(self, value) -> Timestamp | NaTType:\n        return Timestamp(value, tz=self.tz)\n\n    def _check_compatible_with(self, other) -> None:\n        if other is NaT:\n            return\n        self._assert_tzawareness_compat(other)\n\n    # -----------------------------------------------------------------\n    # Descriptive Properties\n\n    def _box_func(self, x: np.datetime64) -> Timestamp | NaTType:\n        # GH#42228\n        value = x.view(\"i8\")\n        ts = Timestamp._from_value_and_reso(value, reso=self._creso, tz=self.tz)\n        return ts\n\n    @property\n    # error: Return type \"Union[dtype, DatetimeTZDtype]\" of \"dtype\"\n    # incompatible with return type \"ExtensionDtype\" in supertype\n    # \"ExtensionArray\"\n    def dtype(self) -> np.dtype[np.datetime64] | DatetimeTZDtype:  # type: ignore[override]\n        \"\"\"\n        The dtype for the DatetimeArray.\n\n        .. warning::\n\n           A future version of pandas will change dtype to never be a\n           ``numpy.dtype``. Instead, :attr:`DatetimeArray.dtype` will\n           always be an instance of an ``ExtensionDtype`` subclass.\n\n        Returns\n        -------\n        numpy.dtype or DatetimeTZDtype\n            If the values are tz-naive, then ``np.dtype('datetime64[ns]')``\n            is returned.\n\n            If the values are tz-aware, then the ``DatetimeTZDtype``\n            is returned.\n        \"\"\"\n        return self._dtype\n\n    @property\n    def tz(self) -> tzinfo | None:\n        \"\"\"\n        Return the timezone.\n\n        Returns\n        -------\n        zoneinfo.ZoneInfo,, datetime.tzinfo, pytz.tzinfo.BaseTZInfo, dateutil.tz.tz.tzfile, or None\n            Returns None when the array is tz-naive.\n\n        See Also\n        --------\n        DatetimeIndex.tz_localize : Localize tz-naive DatetimeIndex to a\n            given time zone, or remove timezone from a tz-aware DatetimeIndex.\n        DatetimeIndex.tz_convert : Convert tz-aware DatetimeIndex from\n            one time zone to another.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series([\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"])\n        >>> s = pd.to_datetime(s)\n        >>> s\n        0   2020-01-01 10:00:00+00:00\n        1   2020-02-01 11:00:00+00:00\n        dtype: datetime64[s, UTC]\n        >>> s.dt.tz\n        datetime.timezone.utc\n\n        For DatetimeIndex:\n\n        >>> idx = pd.DatetimeIndex(\n        ...     [\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"]\n        ... )\n        >>> idx.tz\n        datetime.timezone.utc\n        \"\"\"  # noqa: E501\n        # GH 18595\n        return getattr(self.dtype, \"tz\", None)\n\n    @tz.setter\n    def tz(self, value):\n        # GH 3746: Prevent localizing or converting the index by setting tz\n        raise AttributeError(\n            \"Cannot directly set timezone. Use tz_localize() \"\n            \"or tz_convert() as appropriate\"\n        )\n\n    @property\n    def tzinfo(self) -> tzinfo | None:\n        \"\"\"\n        Alias for tz attribute\n        \"\"\"\n        return self.tz\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def is_normalized(self) -> bool:\n        \"\"\"\n        Returns True if all of the dates are at midnight (\"no time\")\n        \"\"\"\n        return is_date_array_normalized(self.asi8, self.tz, reso=self._creso)\n\n    @property  # NB: override with cache_readonly in immutable subclasses\n    def _resolution_obj(self) -> Resolution:\n        return get_resolution(self.asi8, self.tz, reso=self._creso)\n\n    # ----------------------------------------------------------------\n    # Array-Like / EA-Interface Methods\n\n    def __array__(self, dtype=None, copy=None) -> np.ndarray:\n        if dtype is None and self.tz:\n            # The default for tz-aware is object, to preserve tz info\n            dtype = object\n\n        return super().__array__(dtype=dtype, copy=copy)\n\n    def __iter__(self) -> Iterator:\n        \"\"\"\n        Return an iterator over the boxed values\n\n        Yields\n        ------\n        tstamp : Timestamp\n        \"\"\"\n        if self.ndim > 1:\n            for i in range(len(self)):\n                yield self[i]\n        else:\n            # convert in chunks of 10k for efficiency\n            data = self.asi8\n            length = len(self)\n            chunksize = _ITER_CHUNKSIZE\n            chunks = (length // chunksize) + 1\n\n            for i in range(chunks):\n                start_i = i * chunksize\n                end_i = min((i + 1) * chunksize, length)\n                converted = ints_to_pydatetime(\n                    data[start_i:end_i],\n                    tz=self.tz,\n                    box=\"timestamp\",\n                    reso=self._creso,\n                )\n                yield from converted\n\n    def astype(self, dtype, copy: bool = True):\n        # We handle\n        #   --> datetime\n        #   --> period\n        # DatetimeLikeArrayMixin Super handles the rest.\n        dtype = pandas_dtype(dtype)\n\n        if dtype == self.dtype:\n            if copy:\n                return self.copy()\n            return self\n\n        elif isinstance(dtype, ExtensionDtype):\n            if not isinstance(dtype, DatetimeTZDtype):\n                # e.g. Sparse[datetime64[ns]]\n                return super().astype(dtype, copy=copy)\n            elif self.tz is None:\n                # pre-2.0 this did self.tz_localize(dtype.tz), which did not match\n                #  the Series behavior which did\n                #  values.tz_localize(\"UTC\").tz_convert(dtype.tz)\n                raise TypeError(\n                    \"Cannot use .astype to convert from timezone-naive dtype to \"\n                    \"timezone-aware dtype. Use obj.tz_localize instead or \"\n                    \"series.dt.tz_localize instead\"\n                )\n            else:\n                # tzaware unit conversion e.g. datetime64[s, UTC]\n                np_dtype = np.dtype(dtype.str)\n                res_values = astype_overflowsafe(self._ndarray, np_dtype, copy=copy)\n                return type(self)._simple_new(res_values, dtype=dtype, freq=self.freq)\n\n        elif (\n            self.tz is None\n            and lib.is_np_dtype(dtype, \"M\")\n            and not is_unitless(dtype)\n            and is_supported_dtype(dtype)\n        ):\n            # unit conversion e.g. datetime64[s]\n            res_values = astype_overflowsafe(self._ndarray, dtype, copy=True)\n            return type(self)._simple_new(res_values, dtype=res_values.dtype)\n            # TODO: preserve freq?\n\n        elif self.tz is not None and lib.is_np_dtype(dtype, \"M\"):\n            # pre-2.0 behavior for DTA/DTI was\n            #  values.tz_convert(\"UTC\").tz_localize(None), which did not match\n            #  the Series behavior\n            raise TypeError(\n                \"Cannot use .astype to convert from timezone-aware dtype to \"\n                \"timezone-naive dtype. Use obj.tz_localize(None) or \"\n                \"obj.tz_convert('UTC').tz_localize(None) instead.\"\n            )\n\n        elif (\n            self.tz is None\n            and lib.is_np_dtype(dtype, \"M\")\n            and dtype != self.dtype\n            and is_unitless(dtype)\n        ):\n            raise TypeError(\n                \"Casting to unit-less dtype 'datetime64' is not supported. \"\n                \"Pass e.g. 'datetime64[ns]' instead.\"\n            )\n\n        elif isinstance(dtype, PeriodDtype):\n            return self.to_period(freq=dtype.freq)\n        return dtl.DatetimeLikeArrayMixin.astype(self, dtype, copy)\n\n    # -----------------------------------------------------------------\n    # Rendering Methods\n\n    def _format_native_types(\n        self, *, na_rep: str | float = \"NaT\", date_format=None, **kwargs\n    ) -> npt.NDArray[np.object_]:\n        if date_format is None and self._is_dates_only:\n            # Only dates and no timezone: provide a default format\n            date_format = \"%Y-%m-%d\"\n\n        return tslib.format_array_from_datetime(\n            self.asi8, tz=self.tz, format=date_format, na_rep=na_rep, reso=self._creso\n        )\n\n    # -----------------------------------------------------------------\n    # Comparison Methods\n\n    def _assert_tzawareness_compat(self, other) -> None:\n        # adapted from _Timestamp._assert_tzawareness_compat\n        other_tz = getattr(other, \"tzinfo\", None)\n        other_dtype = getattr(other, \"dtype\", None)\n\n        if isinstance(other_dtype, DatetimeTZDtype):\n            # Get tzinfo from Series dtype\n            other_tz = other.dtype.tz\n        if other is NaT:\n            # pd.NaT quacks both aware and naive\n            pass\n        elif self.tz is None:\n            if other_tz is not None:\n                raise TypeError(\n                    \"Cannot compare tz-naive and tz-aware datetime-like objects.\"\n                )\n        elif other_tz is None:\n            raise TypeError(\n                \"Cannot compare tz-naive and tz-aware datetime-like objects\"\n            )\n\n    # -----------------------------------------------------------------\n    # Arithmetic Methods\n\n    def _add_offset(self, offset: BaseOffset) -> Self:\n        assert not isinstance(offset, Tick)\n\n        if self.tz is not None:\n            values = self.tz_localize(None)\n        else:\n            values = self\n\n        try:\n            res_values = offset._apply_array(values._ndarray)\n            if res_values.dtype.kind == \"i\":\n                # error: Argument 1 to \"view\" of \"ndarray\" has incompatible type\n                # \"dtype[datetime64] | DatetimeTZDtype\"; expected\n                # \"dtype[Any] | type[Any] | _SupportsDType[dtype[Any]]\"\n                res_values = res_values.view(values.dtype)  # type: ignore[arg-type]\n        except NotImplementedError:\n            if get_option(\"performance_warnings\"):\n                warnings.warn(\n                    \"Non-vectorized DateOffset being applied to Series or \"\n                    \"DatetimeIndex.\",\n                    PerformanceWarning,\n                    stacklevel=find_stack_level(),\n                )\n            res_values = self.astype(\"O\") + offset\n            # TODO(GH#55564): as_unit will be unnecessary\n            result = type(self)._from_sequence(res_values).as_unit(self.unit)\n            if not len(self):\n                # GH#30336 _from_sequence won't be able to infer self.tz\n                return result.tz_localize(self.tz)\n\n        else:\n            result = type(self)._simple_new(res_values, dtype=res_values.dtype)\n            if offset.normalize:\n                result = result.normalize()\n                result._freq = None\n\n            if self.tz is not None:\n                result = result.tz_localize(self.tz)\n\n        return result\n\n    # -----------------------------------------------------------------\n    # Timezone Conversion and Localization Methods\n\n    def _local_timestamps(self) -> npt.NDArray[np.int64]:\n        \"\"\"\n        Convert to an i8 (unix-like nanosecond timestamp) representation\n        while keeping the local timezone and not using UTC.\n        This is used to calculate time-of-day information as if the timestamps\n        were timezone-naive.\n        \"\"\"\n        if self.tz is None or timezones.is_utc(self.tz):\n            # Avoid the copy that would be made in tzconversion\n            return self.asi8\n        return tz_convert_from_utc(self.asi8, self.tz, reso=self._creso)\n\n    def tz_convert(self, tz) -> Self:\n        \"\"\"\n        Convert tz-aware Datetime Array/Index from one time zone to another.\n\n        Parameters\n        ----------\n        tz : str, zoneinfo.ZoneInfo, pytz.timezone, dateutil.tz.tzfile, datetime.tzinfo or None\n            Time zone for time. Corresponding timestamps would be converted\n            to this time zone of the Datetime Array/Index. A `tz` of None will\n            convert to UTC and remove the timezone information.\n\n        Returns\n        -------\n        Array or Index\n            Datetme Array/Index with target `tz`.\n\n        Raises\n        ------\n        TypeError\n            If Datetime Array/Index is tz-naive.\n\n        See Also\n        --------\n        DatetimeIndex.tz : A timezone that has a variable offset from UTC.\n        DatetimeIndex.tz_localize : Localize tz-naive DatetimeIndex to a\n            given time zone, or remove timezone from a tz-aware DatetimeIndex.\n\n        Examples\n        --------\n        With the `tz` parameter, we can change the DatetimeIndex\n        to other time zones:\n\n        >>> dti = pd.date_range(\n        ...     start=\"2014-08-01 09:00\", freq=\"h\", periods=3, tz=\"Europe/Berlin\"\n        ... )\n\n        >>> dti\n        DatetimeIndex(['2014-08-01 09:00:00+02:00',\n                       '2014-08-01 10:00:00+02:00',\n                       '2014-08-01 11:00:00+02:00'],\n                      dtype='datetime64[ns, Europe/Berlin]', freq='h')\n\n        >>> dti.tz_convert(\"US/Central\")\n        DatetimeIndex(['2014-08-01 02:00:00-05:00',\n                       '2014-08-01 03:00:00-05:00',\n                       '2014-08-01 04:00:00-05:00'],\n                      dtype='datetime64[ns, US/Central]', freq='h')\n\n        With the ``tz=None``, we can remove the timezone (after converting\n        to UTC if necessary):\n\n        >>> dti = pd.date_range(\n        ...     start=\"2014-08-01 09:00\", freq=\"h\", periods=3, tz=\"Europe/Berlin\"\n        ... )\n\n        >>> dti\n        DatetimeIndex(['2014-08-01 09:00:00+02:00',\n                       '2014-08-01 10:00:00+02:00',\n                       '2014-08-01 11:00:00+02:00'],\n                        dtype='datetime64[ns, Europe/Berlin]', freq='h')\n\n        >>> dti.tz_convert(None)\n        DatetimeIndex(['2014-08-01 07:00:00',\n                       '2014-08-01 08:00:00',\n                       '2014-08-01 09:00:00'],\n                        dtype='datetime64[ns]', freq='h')\n        \"\"\"  # noqa: E501\n        tz = timezones.maybe_get_tz(tz)\n\n        if self.tz is None:\n            # tz naive, use tz_localize\n            raise TypeError(\n                \"Cannot convert tz-naive timestamps, use tz_localize to localize\"\n            )\n\n        # No conversion since timestamps are all UTC to begin with\n        dtype = tz_to_dtype(tz, unit=self.unit)\n        return self._simple_new(self._ndarray, dtype=dtype, freq=self.freq)\n\n    @dtl.ravel_compat\n    def tz_localize(\n        self,\n        tz,\n        ambiguous: TimeAmbiguous = \"raise\",\n        nonexistent: TimeNonexistent = \"raise\",\n    ) -> Self:\n        \"\"\"\n        Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.\n\n        This method takes a time zone (tz) naive Datetime Array/Index object\n        and makes this time zone aware. It does not move the time to another\n        time zone.\n\n        This method can also be used to do the inverse -- to create a time\n        zone unaware object from an aware object. To that end, pass `tz=None`.\n\n        Parameters\n        ----------\n        tz : str, zoneinfo.ZoneInfo,, pytz.timezone, dateutil.tz.tzfile, datetime.tzinfo or None\n            Time zone to convert timestamps to. Passing ``None`` will\n            remove the time zone information preserving local time.\n        ambiguous : 'infer', 'NaT', bool array, default 'raise'\n            When clocks moved backward due to DST, ambiguous times may arise.\n            For example in Central European Time (UTC+01), when going from\n            03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at\n            00:30:00 UTC and at 01:30:00 UTC. In such a situation, the\n            `ambiguous` parameter dictates how ambiguous times should be\n            handled.\n\n            - 'infer' will attempt to infer fall dst-transition hours based on\n              order\n            - bool-ndarray where True signifies a DST time, False signifies a\n              non-DST time (note that this flag is only applicable for\n              ambiguous times)\n            - 'NaT' will return NaT where there are ambiguous times\n            - 'raise' will raise a ValueError if there are ambiguous\n              times.\n\n        nonexistent : 'shift_forward', 'shift_backward, 'NaT', timedelta, \\\ndefault 'raise'\n            A nonexistent time does not exist in a particular timezone\n            where clocks moved forward due to DST.\n\n            - 'shift_forward' will shift the nonexistent time forward to the\n              closest existing time\n            - 'shift_backward' will shift the nonexistent time backward to the\n              closest existing time\n            - 'NaT' will return NaT where there are nonexistent times\n            - timedelta objects will shift nonexistent times by the timedelta\n            - 'raise' will raise a ValueError if there are\n              nonexistent times.\n\n        Returns\n        -------\n        Same type as self\n            Array/Index converted to the specified time zone.\n\n        Raises\n        ------\n        TypeError\n            If the Datetime Array/Index is tz-aware and tz is not None.\n\n        See Also\n        --------\n        DatetimeIndex.tz_convert : Convert tz-aware DatetimeIndex from\n            one time zone to another.\n\n        Examples\n        --------\n        >>> tz_naive = pd.date_range('2018-03-01 09:00', periods=3)\n        >>> tz_naive\n        DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                       '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq='D')\n\n        Localize DatetimeIndex in US/Eastern time zone:\n\n        >>> tz_aware = tz_naive.tz_localize(tz='US/Eastern')\n        >>> tz_aware\n        DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                       '2018-03-02 09:00:00-05:00',\n                       '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq=None)\n\n        With the ``tz=None``, we can remove the time zone information\n        while keeping the local time (not converted to UTC):\n\n        >>> tz_aware.tz_localize(None)\n        DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                       '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n\n        Be careful with DST changes. When there is sequential data, pandas can\n        infer the DST time:\n\n        >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:30:00',\n        ...                               '2018-10-28 02:00:00',\n        ...                               '2018-10-28 02:30:00',\n        ...                               '2018-10-28 02:00:00',\n        ...                               '2018-10-28 02:30:00',\n        ...                               '2018-10-28 03:00:00',\n        ...                               '2018-10-28 03:30:00']))\n        >>> s.dt.tz_localize('CET', ambiguous='infer')\n        0   2018-10-28 01:30:00+02:00\n        1   2018-10-28 02:00:00+02:00\n        2   2018-10-28 02:30:00+02:00\n        3   2018-10-28 02:00:00+01:00\n        4   2018-10-28 02:30:00+01:00\n        5   2018-10-28 03:00:00+01:00\n        6   2018-10-28 03:30:00+01:00\n        dtype: datetime64[s, CET]\n\n        In some cases, inferring the DST is impossible. In such cases, you can\n        pass an ndarray to the ambiguous parameter to set the DST explicitly\n\n        >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:20:00',\n        ...                               '2018-10-28 02:36:00',\n        ...                               '2018-10-28 03:46:00']))\n        >>> s.dt.tz_localize('CET', ambiguous=np.array([True, True, False]))\n        0   2018-10-28 01:20:00+02:00\n        1   2018-10-28 02:36:00+02:00\n        2   2018-10-28 03:46:00+01:00\n        dtype: datetime64[s, CET]\n\n        If the DST transition causes nonexistent times, you can shift these\n        dates forward or backwards with a timedelta object or `'shift_forward'`\n        or `'shift_backwards'`.\n\n        >>> s = pd.to_datetime(pd.Series(['2015-03-29 02:30:00',\n        ...                               '2015-03-29 03:30:00'], dtype=\"M8[ns]\"))\n        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n        0   2015-03-29 03:00:00+02:00\n        1   2015-03-29 03:30:00+02:00\n        dtype: datetime64[ns, Europe/Warsaw]\n\n        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n        0   2015-03-29 01:59:59.999999999+01:00\n        1   2015-03-29 03:30:00+02:00\n        dtype: datetime64[ns, Europe/Warsaw]\n\n        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1h'))\n        0   2015-03-29 03:30:00+02:00\n        1   2015-03-29 03:30:00+02:00\n        dtype: datetime64[ns, Europe/Warsaw]\n        \"\"\"  # noqa: E501\n        nonexistent_options = (\"raise\", \"NaT\", \"shift_forward\", \"shift_backward\")\n        if nonexistent not in nonexistent_options and not isinstance(\n            nonexistent, timedelta\n        ):\n            raise ValueError(\n                \"The nonexistent argument must be one of 'raise', \"\n                \"'NaT', 'shift_forward', 'shift_backward' or \"\n                \"a timedelta object\"\n            )\n\n        if self.tz is not None:\n            if tz is None:\n                new_dates = tz_convert_from_utc(self.asi8, self.tz, reso=self._creso)\n            else:\n                raise TypeError(\"Already tz-aware, use tz_convert to convert.\")\n        else:\n            tz = timezones.maybe_get_tz(tz)\n            # Convert to UTC\n\n            new_dates = tzconversion.tz_localize_to_utc(\n                self.asi8,\n                tz,\n                ambiguous=ambiguous,\n                nonexistent=nonexistent,\n                creso=self._creso,\n            )\n        new_dates_dt64 = new_dates.view(f\"M8[{self.unit}]\")\n        dtype = tz_to_dtype(tz, unit=self.unit)\n\n        freq = None\n        if timezones.is_utc(tz) or (len(self) == 1 and not isna(new_dates_dt64[0])):\n            # we can preserve freq\n            # TODO: Also for fixed-offsets\n            freq = self.freq\n        elif tz is None and self.tz is None:\n            # no-op\n            freq = self.freq\n        return self._simple_new(new_dates_dt64, dtype=dtype, freq=freq)\n\n    # ----------------------------------------------------------------\n    # Conversion Methods - Vectorized analogues of Timestamp methods\n\n    def to_pydatetime(self) -> npt.NDArray[np.object_]:\n        \"\"\"\n        Return an ndarray of ``datetime.datetime`` objects.\n\n        Returns\n        -------\n        numpy.ndarray\n            An ndarray of ``datetime.datetime`` objects.\n\n        See Also\n        --------\n        DatetimeIndex.to_julian_date : Converts Datetime Array to float64 ndarray\n            of Julian Dates.\n\n        Examples\n        --------\n        >>> idx = pd.date_range(\"2018-02-27\", periods=3)\n        >>> idx.to_pydatetime()\n        array([datetime.datetime(2018, 2, 27, 0, 0),\n               datetime.datetime(2018, 2, 28, 0, 0),\n               datetime.datetime(2018, 3, 1, 0, 0)], dtype=object)\n        \"\"\"\n        return ints_to_pydatetime(self.asi8, tz=self.tz, reso=self._creso)\n\n    def normalize(self) -> Self:\n        \"\"\"\n        Convert times to midnight.\n\n        The time component of the date-time is converted to midnight i.e.\n        00:00:00. This is useful in cases, when the time does not matter.\n        Length is unaltered. The timezones are unaffected.\n\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on Datetime Array/Index.\n\n        Returns\n        -------\n        DatetimeArray, DatetimeIndex or Series\n            The same type as the original data. Series will have the same\n            name and index. DatetimeIndex will have the same name.\n\n        See Also\n        --------\n        floor : Floor the datetimes to the specified freq.\n        ceil : Ceil the datetimes to the specified freq.\n        round : Round the datetimes to the specified freq.\n\n        Examples\n        --------\n        >>> idx = pd.date_range(\n        ...     start=\"2014-08-01 10:00\", freq=\"h\", periods=3, tz=\"Asia/Calcutta\"\n        ... )\n        >>> idx\n        DatetimeIndex(['2014-08-01 10:00:00+05:30',\n                       '2014-08-01 11:00:00+05:30',\n                       '2014-08-01 12:00:00+05:30'],\n                        dtype='datetime64[ns, Asia/Calcutta]', freq='h')\n        >>> idx.normalize()\n        DatetimeIndex(['2014-08-01 00:00:00+05:30',\n                       '2014-08-01 00:00:00+05:30',\n                       '2014-08-01 00:00:00+05:30'],\n                       dtype='datetime64[ns, Asia/Calcutta]', freq=None)\n        \"\"\"\n        new_values = normalize_i8_timestamps(self.asi8, self.tz, reso=self._creso)\n        dt64_values = new_values.view(self._ndarray.dtype)\n\n        dta = type(self)._simple_new(dt64_values, dtype=dt64_values.dtype)\n        dta = dta._with_freq(\"infer\")\n        if self.tz is not None:\n            dta = dta.tz_localize(self.tz)\n        return dta\n\n    def to_period(self, freq=None) -> PeriodArray:\n        \"\"\"\n        Cast to PeriodArray/PeriodIndex at a particular frequency.\n\n        Converts DatetimeArray/Index to PeriodArray/PeriodIndex.\n\n        Parameters\n        ----------\n        freq : str or Period, optional\n            One of pandas' :ref:`period aliases <timeseries.period_aliases>`\n            or an Period object. Will be inferred by default.\n\n        Returns\n        -------\n        PeriodArray/PeriodIndex\n            Immutable ndarray holding ordinal values at a particular frequency.\n\n        Raises\n        ------\n        ValueError\n            When converting a DatetimeArray/Index with non-regular values,\n            so that a frequency cannot be inferred.\n\n        See Also\n        --------\n        PeriodIndex: Immutable ndarray holding ordinal values.\n        DatetimeIndex.to_pydatetime: Return DatetimeIndex as object.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(\n        ...     {\"y\": [1, 2, 3]},\n        ...     index=pd.to_datetime(\n        ...         [\n        ...             \"2000-03-31 00:00:00\",\n        ...             \"2000-05-31 00:00:00\",\n        ...             \"2000-08-31 00:00:00\",\n        ...         ]\n        ...     ),\n        ... )\n        >>> df.index.to_period(\"M\")\n        PeriodIndex(['2000-03', '2000-05', '2000-08'],\n                    dtype='period[M]')\n\n        Infer the daily frequency\n\n        >>> idx = pd.date_range(\"2017-01-01\", periods=2)\n        >>> idx.to_period()\n        PeriodIndex(['2017-01-01', '2017-01-02'],\n                    dtype='period[D]')\n        \"\"\"\n        from pandas.core.arrays import PeriodArray\n\n        if self.tz is not None:\n            warnings.warn(\n                \"Converting to PeriodArray/Index representation \"\n                \"will drop timezone information.\",\n                UserWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        if freq is None:\n            freq = self.freqstr or self.inferred_freq\n            if isinstance(self.freq, BaseOffset) and hasattr(\n                self.freq, \"_period_dtype_code\"\n            ):\n                freq = PeriodDtype(self.freq)._freqstr\n\n            if freq is None:\n                raise ValueError(\n                    \"You must pass a freq argument as current index has none.\"\n                )\n\n            res = get_period_alias(freq)\n\n            #  https://github.com/pandas-dev/pandas/issues/33358\n            if res is None:\n                res = freq\n\n            freq = res\n        return PeriodArray._from_datetime64(self._ndarray, freq, tz=self.tz)\n\n    # -----------------------------------------------------------------\n    # Properties - Vectorized Timestamp Properties/Methods\n\n    def month_name(self, locale=None) -> npt.NDArray[np.object_]:\n        \"\"\"\n        Return the month names with specified locale.\n\n        Parameters\n        ----------\n        locale : str, optional\n            Locale determining the language in which to return the month name.\n            Default is English locale (``'en_US.utf8'``). Use the command\n            ``locale -a`` on your terminal on Unix systems to find your locale\n            language code.\n\n        Returns\n        -------\n        Series or Index\n            Series or Index of month names.\n\n        See Also\n        --------\n        DatetimeIndex.day_name : Return the day names with specified locale.\n\n        Examples\n        --------\n        >>> s = pd.Series(pd.date_range(start=\"2018-01\", freq=\"ME\", periods=3))\n        >>> s\n        0   2018-01-31\n        1   2018-02-28\n        2   2018-03-31\n        dtype: datetime64[ns]\n        >>> s.dt.month_name()\n        0     January\n        1    February\n        2       March\n        dtype: object\n\n        >>> idx = pd.date_range(start=\"2018-01\", freq=\"ME\", periods=3)\n        >>> idx\n        DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],\n                      dtype='datetime64[ns]', freq='ME')\n        >>> idx.month_name()\n        Index(['January', 'February', 'March'], dtype='object')\n\n        Using the ``locale`` parameter you can set a different locale language,\n        for example: ``idx.month_name(locale='pt_BR.utf8')`` will return month\n        names in Brazilian Portuguese language.\n\n        >>> idx = pd.date_range(start=\"2018-01\", freq=\"ME\", periods=3)\n        >>> idx\n        DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],\n                      dtype='datetime64[ns]', freq='ME')\n        >>> idx.month_name(locale=\"pt_BR.utf8\")  # doctest: +SKIP\n        Index(['Janeiro', 'Fevereiro', 'Maro'], dtype='object')\n        \"\"\"\n        values = self._local_timestamps()\n\n        result = fields.get_date_name_field(\n            values, \"month_name\", locale=locale, reso=self._creso\n        )\n        result = self._maybe_mask_results(result, fill_value=None)\n        return result\n\n    def day_name(self, locale=None) -> npt.NDArray[np.object_]:\n        \"\"\"\n        Return the day names with specified locale.\n\n        Parameters\n        ----------\n        locale : str, optional\n            Locale determining the language in which to return the day name.\n            Default is English locale (``'en_US.utf8'``). Use the command\n            ``locale -a`` on your terminal on Unix systems to find your locale\n            language code.\n\n        Returns\n        -------\n        Series or Index\n            Series or Index of day names.\n\n        See Also\n        --------\n        DatetimeIndex.month_name : Return the month names with specified locale.\n\n        Examples\n        --------\n        >>> s = pd.Series(pd.date_range(start=\"2018-01-01\", freq=\"D\", periods=3))\n        >>> s\n        0   2018-01-01\n        1   2018-01-02\n        2   2018-01-03\n        dtype: datetime64[ns]\n        >>> s.dt.day_name()\n        0       Monday\n        1      Tuesday\n        2    Wednesday\n        dtype: object\n\n        >>> idx = pd.date_range(start=\"2018-01-01\", freq=\"D\", periods=3)\n        >>> idx\n        DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],\n                      dtype='datetime64[ns]', freq='D')\n        >>> idx.day_name()\n        Index(['Monday', 'Tuesday', 'Wednesday'], dtype='object')\n\n        Using the ``locale`` parameter you can set a different locale language,\n        for example: ``idx.day_name(locale='pt_BR.utf8')`` will return day\n        names in Brazilian Portuguese language.\n\n        >>> idx = pd.date_range(start=\"2018-01-01\", freq=\"D\", periods=3)\n        >>> idx\n        DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],\n                      dtype='datetime64[ns]', freq='D')\n        >>> idx.day_name(locale=\"pt_BR.utf8\")  # doctest: +SKIP\n        Index(['Segunda', 'Tera', 'Quarta'], dtype='object')\n        \"\"\"\n        values = self._local_timestamps()\n\n        result = fields.get_date_name_field(\n            values, \"day_name\", locale=locale, reso=self._creso\n        )\n        result = self._maybe_mask_results(result, fill_value=None)\n        return result\n\n    @property\n    def time(self) -> npt.NDArray[np.object_]:\n        \"\"\"\n        Returns numpy array of :class:`datetime.time` objects.\n\n        The time part of the Timestamps.\n\n        See Also\n        --------\n        DatetimeIndex.timetz : Returns numpy array of :class:`datetime.time`\n            objects with timezones. The time part of the Timestamps.\n        DatetimeIndex.date : Returns numpy array of python :class:`datetime.date`\n            objects. Namely, the date part of Timestamps without time and timezone\n            information.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series([\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"])\n        >>> s = pd.to_datetime(s)\n        >>> s\n        0   2020-01-01 10:00:00+00:00\n        1   2020-02-01 11:00:00+00:00\n        dtype: datetime64[s, UTC]\n        >>> s.dt.time\n        0    10:00:00\n        1    11:00:00\n        dtype: object\n\n        For DatetimeIndex:\n\n        >>> idx = pd.DatetimeIndex(\n        ...     [\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"]\n        ... )\n        >>> idx.time\n        array([datetime.time(10, 0), datetime.time(11, 0)], dtype=object)\n        \"\"\"\n        # If the Timestamps have a timezone that is not UTC,\n        # convert them into their i8 representation while\n        # keeping their timezone and not using UTC\n        timestamps = self._local_timestamps()\n\n        return ints_to_pydatetime(timestamps, box=\"time\", reso=self._creso)\n\n    @property\n    def timetz(self) -> npt.NDArray[np.object_]:\n        \"\"\"\n        Returns numpy array of :class:`datetime.time` objects with timezones.\n\n        The time part of the Timestamps.\n\n        See Also\n        --------\n        DatetimeIndex.time : Returns numpy array of :class:`datetime.time` objects.\n            The time part of the Timestamps.\n        DatetimeIndex.tz : Return the timezone.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series([\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"])\n        >>> s = pd.to_datetime(s)\n        >>> s\n        0   2020-01-01 10:00:00+00:00\n        1   2020-02-01 11:00:00+00:00\n        dtype: datetime64[s, UTC]\n        >>> s.dt.timetz\n        0    10:00:00+00:00\n        1    11:00:00+00:00\n        dtype: object\n\n        For DatetimeIndex:\n\n        >>> idx = pd.DatetimeIndex(\n        ...     [\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"]\n        ... )\n        >>> idx.timetz\n        array([datetime.time(10, 0, tzinfo=datetime.timezone.utc),\n        datetime.time(11, 0, tzinfo=datetime.timezone.utc)], dtype=object)\n        \"\"\"\n        return ints_to_pydatetime(self.asi8, self.tz, box=\"time\", reso=self._creso)\n\n    @property\n    def date(self) -> npt.NDArray[np.object_]:\n        \"\"\"\n        Returns numpy array of python :class:`datetime.date` objects.\n\n        Namely, the date part of Timestamps without time and\n        timezone information.\n\n        See Also\n        --------\n        DatetimeIndex.time : Returns numpy array of :class:`datetime.time` objects.\n            The time part of the Timestamps.\n        DatetimeIndex.year : The year of the datetime.\n        DatetimeIndex.month : The month as January=1, December=12.\n        DatetimeIndex.day : The day of the datetime.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series([\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"])\n        >>> s = pd.to_datetime(s)\n        >>> s\n        0   2020-01-01 10:00:00+00:00\n        1   2020-02-01 11:00:00+00:00\n        dtype: datetime64[s, UTC]\n        >>> s.dt.date\n        0    2020-01-01\n        1    2020-02-01\n        dtype: object\n\n        For DatetimeIndex:\n\n        >>> idx = pd.DatetimeIndex(\n        ...     [\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"]\n        ... )\n        >>> idx.date\n        array([datetime.date(2020, 1, 1), datetime.date(2020, 2, 1)], dtype=object)\n        \"\"\"\n        # If the Timestamps have a timezone that is not UTC,\n        # convert them into their i8 representation while\n        # keeping their timezone and not using UTC\n        timestamps = self._local_timestamps()\n\n        return ints_to_pydatetime(timestamps, box=\"date\", reso=self._creso)\n\n    def isocalendar(self) -> DataFrame:\n        \"\"\"\n        Calculate year, week, and day according to the ISO 8601 standard.\n\n        Returns\n        -------\n        DataFrame\n            With columns year, week and day.\n\n        See Also\n        --------\n        Timestamp.isocalendar : Function return a 3-tuple containing ISO year,\n            week number, and weekday for the given Timestamp object.\n        datetime.date.isocalendar : Return a named tuple object with\n            three components: year, week and weekday.\n\n        Examples\n        --------\n        >>> idx = pd.date_range(start=\"2019-12-29\", freq=\"D\", periods=4)\n        >>> idx.isocalendar()\n                    year  week  day\n        2019-12-29  2019    52    7\n        2019-12-30  2020     1    1\n        2019-12-31  2020     1    2\n        2020-01-01  2020     1    3\n        >>> idx.isocalendar().week\n        2019-12-29    52\n        2019-12-30     1\n        2019-12-31     1\n        2020-01-01     1\n        Freq: D, Name: week, dtype: UInt32\n        \"\"\"\n        from pandas import DataFrame\n\n        values = self._local_timestamps()\n        sarray = fields.build_isocalendar_sarray(values, reso=self._creso)\n        iso_calendar_df = DataFrame(\n            sarray, columns=[\"year\", \"week\", \"day\"], dtype=\"UInt32\"\n        )\n        if self._hasna:\n            iso_calendar_df.iloc[self._isnan] = None\n        return iso_calendar_df\n\n    year = _field_accessor(\n        \"year\",\n        \"Y\",\n        \"\"\"\n        The year of the datetime.\n\n        See Also\n        --------\n        DatetimeIndex.month: The month as January=1, December=12.\n        DatetimeIndex.day: The day of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"YE\")\n        ... )\n        >>> datetime_series\n        0   2000-12-31\n        1   2001-12-31\n        2   2002-12-31\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.year\n        0    2000\n        1    2001\n        2    2002\n        dtype: int32\n        \"\"\",\n    )\n    month = _field_accessor(\n        \"month\",\n        \"M\",\n        \"\"\"\n        The month as January=1, December=12.\n\n        See Also\n        --------\n        DatetimeIndex.year: The year of the datetime.\n        DatetimeIndex.day: The day of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"ME\")\n        ... )\n        >>> datetime_series\n        0   2000-01-31\n        1   2000-02-29\n        2   2000-03-31\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.month\n        0    1\n        1    2\n        2    3\n        dtype: int32\n        \"\"\",\n    )\n    day = _field_accessor(\n        \"day\",\n        \"D\",\n        \"\"\"\n        The day of the datetime.\n\n        See Also\n        --------\n        DatetimeIndex.year: The year of the datetime.\n        DatetimeIndex.month: The month as January=1, December=12.\n        DatetimeIndex.hour: The hours of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01\n        1   2000-01-02\n        2   2000-01-03\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.day\n        0    1\n        1    2\n        2    3\n        dtype: int32\n        \"\"\",\n    )\n    hour = _field_accessor(\n        \"hour\",\n        \"h\",\n        \"\"\"\n        The hours of the datetime.\n\n        See Also\n        --------\n        DatetimeIndex.day: The day of the datetime.\n        DatetimeIndex.minute: The minutes of the datetime.\n        DatetimeIndex.second: The seconds of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"h\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00\n        1   2000-01-01 01:00:00\n        2   2000-01-01 02:00:00\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.hour\n        0    0\n        1    1\n        2    2\n        dtype: int32\n        \"\"\",\n    )\n    minute = _field_accessor(\n        \"minute\",\n        \"m\",\n        \"\"\"\n        The minutes of the datetime.\n\n        See Also\n        --------\n        DatetimeIndex.hour: The hours of the datetime.\n        DatetimeIndex.second: The seconds of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"min\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00\n        1   2000-01-01 00:01:00\n        2   2000-01-01 00:02:00\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.minute\n        0    0\n        1    1\n        2    2\n        dtype: int32\n        \"\"\",\n    )\n    second = _field_accessor(\n        \"second\",\n        \"s\",\n        \"\"\"\n        The seconds of the datetime.\n\n        See Also\n        --------\n        DatetimeIndex.minute: The minutes of the datetime.\n        DatetimeIndex.microsecond: The microseconds of the datetime.\n        DatetimeIndex.nanosecond: The nanoseconds of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"s\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00\n        1   2000-01-01 00:00:01\n        2   2000-01-01 00:00:02\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.second\n        0    0\n        1    1\n        2    2\n        dtype: int32\n        \"\"\",\n    )\n    microsecond = _field_accessor(\n        \"microsecond\",\n        \"us\",\n        \"\"\"\n        The microseconds of the datetime.\n\n        See Also\n        --------\n        DatetimeIndex.second: The seconds of the datetime.\n        DatetimeIndex.nanosecond: The nanoseconds of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"us\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00.000000\n        1   2000-01-01 00:00:00.000001\n        2   2000-01-01 00:00:00.000002\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.microsecond\n        0       0\n        1       1\n        2       2\n        dtype: int32\n        \"\"\",\n    )\n    nanosecond = _field_accessor(\n        \"nanosecond\",\n        \"ns\",\n        \"\"\"\n        The nanoseconds of the datetime.\n\n        See Also\n        --------\n        DatetimeIndex.second: The seconds of the datetime.\n        DatetimeIndex.microsecond: The microseconds of the datetime.\n\n        Examples\n        --------\n        >>> datetime_series = pd.Series(\n        ...     pd.date_range(\"2000-01-01\", periods=3, freq=\"ns\")\n        ... )\n        >>> datetime_series\n        0   2000-01-01 00:00:00.000000000\n        1   2000-01-01 00:00:00.000000001\n        2   2000-01-01 00:00:00.000000002\n        dtype: datetime64[ns]\n        >>> datetime_series.dt.nanosecond\n        0       0\n        1       1\n        2       2\n        dtype: int32\n        \"\"\",\n    )\n    _dayofweek_doc = \"\"\"\n    The day of the week with Monday=0, Sunday=6.\n\n    Return the day of the week. It is assumed the week starts on\n    Monday, which is denoted by 0 and ends on Sunday which is denoted\n    by 6. This method is available on both Series with datetime\n    values (using the `dt` accessor) or DatetimeIndex.\n\n    Returns\n    -------\n    Series or Index\n        Containing integers indicating the day number.\n\n    See Also\n    --------\n    Series.dt.dayofweek : Alias.\n    Series.dt.weekday : Alias.\n    Series.dt.day_name : Returns the name of the day of the week.\n\n    Examples\n    --------\n    >>> s = pd.date_range('2016-12-31', '2017-01-08', freq='D').to_series()\n    >>> s.dt.dayofweek\n    2016-12-31    5\n    2017-01-01    6\n    2017-01-02    0\n    2017-01-03    1\n    2017-01-04    2\n    2017-01-05    3\n    2017-01-06    4\n    2017-01-07    5\n    2017-01-08    6\n    Freq: D, dtype: int32\n    \"\"\"\n    day_of_week = _field_accessor(\"day_of_week\", \"dow\", _dayofweek_doc)\n    dayofweek = day_of_week\n    weekday = day_of_week\n\n    day_of_year = _field_accessor(\n        \"dayofyear\",\n        \"doy\",\n        \"\"\"\n        The ordinal day of the year.\n\n        See Also\n        --------\n        DatetimeIndex.dayofweek : The day of the week with Monday=0, Sunday=6.\n        DatetimeIndex.day : The day of the datetime.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series([\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"])\n        >>> s = pd.to_datetime(s)\n        >>> s\n        0   2020-01-01 10:00:00+00:00\n        1   2020-02-01 11:00:00+00:00\n        dtype: datetime64[s, UTC]\n        >>> s.dt.dayofyear\n        0    1\n        1   32\n        dtype: int32\n\n        For DatetimeIndex:\n\n        >>> idx = pd.DatetimeIndex([\"1/1/2020 10:00:00+00:00\",\n        ...                         \"2/1/2020 11:00:00+00:00\"])\n        >>> idx.dayofyear\n        Index([1, 32], dtype='int32')\n        \"\"\",\n    )\n    dayofyear = day_of_year\n    quarter = _field_accessor(\n        \"quarter\",\n        \"q\",\n        \"\"\"\n        The quarter of the date.\n\n        See Also\n        --------\n        DatetimeIndex.snap : Snap time stamps to nearest occurring frequency.\n        DatetimeIndex.time : Returns numpy array of datetime.time objects.\n            The time part of the Timestamps.\n\n        Examples\n        --------\n        For Series:\n\n        >>> s = pd.Series([\"1/1/2020 10:00:00+00:00\", \"4/1/2020 11:00:00+00:00\"])\n        >>> s = pd.to_datetime(s)\n        >>> s\n        0   2020-01-01 10:00:00+00:00\n        1   2020-04-01 11:00:00+00:00\n        dtype: datetime64[s, UTC]\n        >>> s.dt.quarter\n        0    1\n        1    2\n        dtype: int32\n\n        For DatetimeIndex:\n\n        >>> idx = pd.DatetimeIndex([\"1/1/2020 10:00:00+00:00\",\n        ...                         \"2/1/2020 11:00:00+00:00\"])\n        >>> idx.quarter\n        Index([1, 1], dtype='int32')\n        \"\"\",\n    )\n    days_in_month = _field_accessor(\n        \"days_in_month\",\n        \"dim\",\n        \"\"\"\n        The number of days in the month.\n\n        See Also\n        --------\n        Series.dt.day : Return the day of the month.\n        Series.dt.is_month_end : Return a boolean indicating if the\n            date is the last day of the month.\n        Series.dt.is_month_start : Return a boolean indicating if the\n            date is the first day of the month.\n        Series.dt.month : Return the month as January=1 through December=12.\n\n        Examples\n        --------\n        >>> s = pd.Series([\"1/1/2020 10:00:00+00:00\", \"2/1/2020 11:00:00+00:00\"])\n        >>> s = pd.to_datetime(s)\n        >>> s\n        0   2020-01-01 10:00:00+00:00\n        1   2020-02-01 11:00:00+00:00\n        dtype: datetime64[s, UTC]\n        >>> s.dt.daysinmonth\n        0    31\n        1    29\n        dtype: int32\n        \"\"\",\n    )\n    daysinmonth = days_in_month\n    _is_month_doc = \"\"\"\n        Indicates whether the date is the {first_or_last} day of the month.\n\n        Returns\n        -------\n        Series or array\n            For Series, returns a Series with boolean values.\n            For DatetimeIndex, returns a boolean array.\n\n        See Also\n        --------\n        is_month_start : Return a boolean indicating whether the date\n            is the first day of the month.\n        is_month_end : Return a boolean indicating whether the date\n            is the last day of the month.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> s = pd.Series(pd.date_range(\"2018-02-27\", periods=3))\n        >>> s\n        0   2018-02-27\n        1   2018-02-28\n        2   2018-03-01\n        dtype: datetime64[ns]\n        >>> s.dt.is_month_start\n        0    False\n        1    False\n        2    True\n        dtype: bool\n        >>> s.dt.is_month_end\n        0    False\n        1    True\n        2    False\n        dtype: bool\n\n        >>> idx = pd.date_range(\"2018-02-27\", periods=3)\n        >>> idx.is_month_start\n        array([False, False, True])\n        >>> idx.is_month_end\n        array([False, True, False])\n    \"\"\"\n    is_month_start = _field_accessor(\n        \"is_month_start\", \"is_month_start\", _is_month_doc.format(first_or_last=\"first\")\n    )\n\n    is_month_end = _field_accessor(\n        \"is_month_end\", \"is_month_end\", _is_month_doc.format(first_or_last=\"last\")\n    )\n\n    is_quarter_start = _field_accessor(\n        \"is_quarter_start\",\n        \"is_quarter_start\",\n        \"\"\"\n        Indicator for whether the date is the first day of a quarter.\n\n        Returns\n        -------\n        is_quarter_start : Series or DatetimeIndex\n            The same type as the original data with boolean values. Series will\n            have the same name and index. DatetimeIndex will have the same\n            name.\n\n        See Also\n        --------\n        quarter : Return the quarter of the date.\n        is_quarter_end : Similar property for indicating the quarter end.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> df = pd.DataFrame({'dates': pd.date_range(\"2017-03-30\",\n        ...                   periods=4)})\n        >>> df.assign(quarter=df.dates.dt.quarter,\n        ...           is_quarter_start=df.dates.dt.is_quarter_start)\n               dates  quarter  is_quarter_start\n        0 2017-03-30        1             False\n        1 2017-03-31        1             False\n        2 2017-04-01        2              True\n        3 2017-04-02        2             False\n\n        >>> idx = pd.date_range('2017-03-30', periods=4)\n        >>> idx\n        DatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],\n                      dtype='datetime64[ns]', freq='D')\n\n        >>> idx.is_quarter_start\n        array([False, False,  True, False])\n        \"\"\",\n    )\n    is_quarter_end = _field_accessor(\n        \"is_quarter_end\",\n        \"is_quarter_end\",\n        \"\"\"\n        Indicator for whether the date is the last day of a quarter.\n\n        Returns\n        -------\n        is_quarter_end : Series or DatetimeIndex\n            The same type as the original data with boolean values. Series will\n            have the same name and index. DatetimeIndex will have the same\n            name.\n\n        See Also\n        --------\n        quarter : Return the quarter of the date.\n        is_quarter_start : Similar property indicating the quarter start.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> df = pd.DataFrame({'dates': pd.date_range(\"2017-03-30\",\n        ...                    periods=4)})\n        >>> df.assign(quarter=df.dates.dt.quarter,\n        ...           is_quarter_end=df.dates.dt.is_quarter_end)\n               dates  quarter    is_quarter_end\n        0 2017-03-30        1             False\n        1 2017-03-31        1              True\n        2 2017-04-01        2             False\n        3 2017-04-02        2             False\n\n        >>> idx = pd.date_range('2017-03-30', periods=4)\n        >>> idx\n        DatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],\n                      dtype='datetime64[ns]', freq='D')\n\n        >>> idx.is_quarter_end\n        array([False,  True, False, False])\n        \"\"\",\n    )\n    is_year_start = _field_accessor(\n        \"is_year_start\",\n        \"is_year_start\",\n        \"\"\"\n        Indicate whether the date is the first day of a year.\n\n        Returns\n        -------\n        Series or DatetimeIndex\n            The same type as the original data with boolean values. Series will\n            have the same name and index. DatetimeIndex will have the same\n            name.\n\n        See Also\n        --------\n        is_year_end : Similar property indicating the last day of the year.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> dates = pd.Series(pd.date_range(\"2017-12-30\", periods=3))\n        >>> dates\n        0   2017-12-30\n        1   2017-12-31\n        2   2018-01-01\n        dtype: datetime64[ns]\n\n        >>> dates.dt.is_year_start\n        0    False\n        1    False\n        2    True\n        dtype: bool\n\n        >>> idx = pd.date_range(\"2017-12-30\", periods=3)\n        >>> idx\n        DatetimeIndex(['2017-12-30', '2017-12-31', '2018-01-01'],\n                      dtype='datetime64[ns]', freq='D')\n\n        >>> idx.is_year_start\n        array([False, False,  True])\n\n        This method, when applied to Series with datetime values under\n        the ``.dt`` accessor, will lose information about Business offsets.\n\n        >>> dates = pd.Series(pd.date_range(\"2020-10-30\", periods=4, freq=\"BYS\"))\n        >>> dates\n        0   2021-01-01\n        1   2022-01-03\n        2   2023-01-02\n        3   2024-01-01\n        dtype: datetime64[ns]\n\n        >>> dates.dt.is_year_start\n        0    True\n        1    False\n        2    False\n        3    True\n        dtype: bool\n\n        >>> idx = pd.date_range(\"2020-10-30\", periods=4, freq=\"BYS\")\n        >>> idx\n        DatetimeIndex(['2021-01-01', '2022-01-03', '2023-01-02', '2024-01-01'],\n                      dtype='datetime64[ns]', freq='BYS-JAN')\n\n        >>> idx.is_year_start\n        array([ True,  True,  True,  True])\n        \"\"\",\n    )\n    is_year_end = _field_accessor(\n        \"is_year_end\",\n        \"is_year_end\",\n        \"\"\"\n        Indicate whether the date is the last day of the year.\n\n        Returns\n        -------\n        Series or DatetimeIndex\n            The same type as the original data with boolean values. Series will\n            have the same name and index. DatetimeIndex will have the same\n            name.\n\n        See Also\n        --------\n        is_year_start : Similar property indicating the start of the year.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> dates = pd.Series(pd.date_range(\"2017-12-30\", periods=3))\n        >>> dates\n        0   2017-12-30\n        1   2017-12-31\n        2   2018-01-01\n        dtype: datetime64[ns]\n\n        >>> dates.dt.is_year_end\n        0    False\n        1     True\n        2    False\n        dtype: bool\n\n        >>> idx = pd.date_range(\"2017-12-30\", periods=3)\n        >>> idx\n        DatetimeIndex(['2017-12-30', '2017-12-31', '2018-01-01'],\n                      dtype='datetime64[ns]', freq='D')\n\n        >>> idx.is_year_end\n        array([False,  True, False])\n        \"\"\",\n    )\n    is_leap_year = _field_accessor(\n        \"is_leap_year\",\n        \"is_leap_year\",\n        \"\"\"\n        Boolean indicator if the date belongs to a leap year.\n\n        A leap year is a year, which has 366 days (instead of 365) including\n        29th of February as an intercalary day.\n        Leap years are years which are multiples of four with the exception\n        of years divisible by 100 but not by 400.\n\n        Returns\n        -------\n        Series or ndarray\n             Booleans indicating if dates belong to a leap year.\n\n        See Also\n        --------\n        DatetimeIndex.is_year_end : Indicate whether the date is the\n            last day of the year.\n        DatetimeIndex.is_year_start : Indicate whether the date is the first\n            day of a year.\n\n        Examples\n        --------\n        This method is available on Series with datetime values under\n        the ``.dt`` accessor, and directly on DatetimeIndex.\n\n        >>> idx = pd.date_range(\"2012-01-01\", \"2015-01-01\", freq=\"YE\")\n        >>> idx\n        DatetimeIndex(['2012-12-31', '2013-12-31', '2014-12-31'],\n                      dtype='datetime64[ns]', freq='YE-DEC')\n        >>> idx.is_leap_year\n        array([ True, False, False])\n\n        >>> dates_series = pd.Series(idx)\n        >>> dates_series\n        0   2012-12-31\n        1   2013-12-31\n        2   2014-12-31\n        dtype: datetime64[ns]\n        >>> dates_series.dt.is_leap_year\n        0     True\n        1    False\n        2    False\n        dtype: bool\n        \"\"\",\n    )\n\n    def to_julian_date(self) -> npt.NDArray[np.float64]:\n        \"\"\"\n        Convert Datetime Array to float64 ndarray of Julian Dates.\n        0 Julian date is noon January 1, 4713 BC.\n        https://en.wikipedia.org/wiki/Julian_day\n        \"\"\"\n\n        # http://mysite.verizon.net/aesir_research/date/jdalg2.htm\n        year = np.asarray(self.year)\n        month = np.asarray(self.month)\n        day = np.asarray(self.day)\n        testarr = month < 3\n        year[testarr] -= 1\n        month[testarr] += 12\n        return (\n            day\n            + np.fix((153 * month - 457) / 5)\n            + 365 * year\n            + np.floor(year / 4)\n            - np.floor(year / 100)\n            + np.floor(year / 400)\n            + 1_721_118.5\n            + (\n                self.hour\n                + self.minute / 60\n                + self.second / 3600\n                + self.microsecond / 3600 / 10**6\n                + self.nanosecond / 3600 / 10**9\n            )\n            / 24\n        )\n\n    # -----------------------------------------------------------------\n    # Reductions\n\n    def _reduce(\n        self, name: str, *, skipna: bool = True, keepdims: bool = False, **kwargs\n    ):\n        result = super()._reduce(name, skipna=skipna, keepdims=keepdims, **kwargs)\n        if keepdims and isinstance(result, np.ndarray):\n            if name == \"std\":\n                from pandas.core.arrays import TimedeltaArray\n\n                return TimedeltaArray._from_sequence(result)\n            else:\n                return self._from_sequence(result, dtype=self.dtype)\n        return result\n\n    def std(\n        self,\n        axis=None,\n        dtype=None,\n        out=None,\n        ddof: int = 1,\n        keepdims: bool = False,\n        skipna: bool = True,\n    ) -> Timedelta:\n        \"\"\"\n        Return sample standard deviation over requested axis.\n\n        Normalized by `N-1` by default. This can be changed using ``ddof``.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis for the function to be applied on. For :class:`pandas.Series`\n            this parameter is unused and defaults to ``None``.\n        dtype : dtype, optional, default None\n            Type to use in computing the standard deviation. For arrays of\n            integer type the default is float64, for arrays of float types\n            it is the same as the array type.\n        out : ndarray, optional, default None\n            Alternative output array in which to place the result. It must have\n            the same shape as the expected output but the type (of the\n            calculated values) will be cast if necessary.\n        ddof : int, default 1\n            Degrees of Freedom. The divisor used in calculations is `N - ddof`,\n            where `N` represents the number of elements.\n        keepdims : bool, optional\n            If this is set to True, the axes which are reduced are left in the\n            result as dimensions with size one. With this option, the result\n            will broadcast correctly against the input array. If the default\n            value is passed, then keepdims will not be passed through to the\n            std method of sub-classes of ndarray, however any non-default value\n            will be. If the sub-class method does not implement keepdims any\n            exceptions will be raised.\n        skipna : bool, default True\n            Exclude NA/null values. If an entire row/column is ``NA``, the result\n            will be ``NA``.\n\n        Returns\n        -------\n        Timedelta\n            Standard deviation over requested axis.\n\n        See Also\n        --------\n        numpy.ndarray.std : Returns the standard deviation of the array elements\n            along given axis.\n        Series.std : Return sample standard deviation over requested axis.\n\n        Examples\n        --------\n        For :class:`pandas.DatetimeIndex`:\n\n        >>> idx = pd.date_range(\"2001-01-01 00:00\", periods=3)\n        >>> idx\n        DatetimeIndex(['2001-01-01', '2001-01-02', '2001-01-03'],\n                      dtype='datetime64[ns]', freq='D')\n        >>> idx.std()\n        Timedelta('1 days 00:00:00')\n        \"\"\"\n        # Because std is translation-invariant, we can get self.std\n        #  by calculating (self - Timestamp(0)).std, and we can do it\n        #  without creating a copy by using a view on self._ndarray\n        from pandas.core.arrays import TimedeltaArray\n\n        # Find the td64 dtype with the same resolution as our dt64 dtype\n        dtype_str = self._ndarray.dtype.name.replace(\"datetime64\", \"timedelta64\")\n        dtype = np.dtype(dtype_str)\n\n        tda = TimedeltaArray._simple_new(self._ndarray.view(dtype), dtype=dtype)\n\n        return tda.std(axis=axis, out=out, ddof=ddof, keepdims=keepdims, skipna=skipna)\n\n\n# -------------------------------------------------------------------\n# Constructor Helpers\n\n\ndef _sequence_to_dt64(\n    data: ArrayLike,\n    *,\n    copy: bool = False,\n    tz: tzinfo | None = None,\n    dayfirst: bool = False,\n    yearfirst: bool = False,\n    ambiguous: TimeAmbiguous = \"raise\",\n    out_unit: str | None = None,\n) -> tuple[np.ndarray, tzinfo | None]:\n    \"\"\"\n    Parameters\n    ----------\n    data : np.ndarray or ExtensionArray\n        dtl.ensure_arraylike_for_datetimelike has already been called.\n    copy : bool, default False\n    tz : tzinfo or None, default None\n    dayfirst : bool, default False\n    yearfirst : bool, default False\n    ambiguous : str, bool, or arraylike, default 'raise'\n        See pandas._libs.tslibs.tzconversion.tz_localize_to_utc.\n    out_unit : str or None, default None\n        Desired output resolution.\n\n    Returns\n    -------\n    result : numpy.ndarray\n        The sequence converted to a numpy array with dtype ``datetime64[unit]``.\n        Where `unit` is \"ns\" unless specified otherwise by `out_unit`.\n    tz : tzinfo or None\n        Either the user-provided tzinfo or one inferred from the data.\n\n    Raises\n    ------\n    TypeError : PeriodDType data is passed\n    \"\"\"\n\n    # By this point we are assured to have either a numpy array or Index\n    data, copy = maybe_convert_dtype(data, copy, tz=tz)\n    data_dtype = getattr(data, \"dtype\", None)\n\n    out_dtype = DT64NS_DTYPE\n    if out_unit is not None:\n        out_dtype = np.dtype(f\"M8[{out_unit}]\")\n\n    if data_dtype == object or is_string_dtype(data_dtype):\n        # TODO: We do not have tests specific to string-dtypes,\n        #  also complex or categorical or other extension\n        data = cast(np.ndarray, data)\n        copy = False\n        if lib.infer_dtype(data, skipna=False) == \"integer\":\n            # Much more performant than going through array_to_datetime\n            data = data.astype(np.int64)\n        elif tz is not None and ambiguous == \"raise\":\n            obj_data = np.asarray(data, dtype=object)\n            result = tslib.array_to_datetime_with_tz(\n                obj_data,\n                tz=tz,\n                dayfirst=dayfirst,\n                yearfirst=yearfirst,\n                creso=abbrev_to_npy_unit(out_unit),\n            )\n            return result, tz\n        else:\n            converted, inferred_tz = objects_to_datetime64(\n                data,\n                dayfirst=dayfirst,\n                yearfirst=yearfirst,\n                allow_object=False,\n                out_unit=out_unit,\n            )\n            copy = False\n            if tz and inferred_tz:\n                #  two timezones: convert to intended from base UTC repr\n                # GH#42505 by convention, these are _already_ UTC\n                result = converted\n\n            elif inferred_tz:\n                tz = inferred_tz\n                result = converted\n\n            else:\n                result, _ = _construct_from_dt64_naive(\n                    converted, tz=tz, copy=copy, ambiguous=ambiguous\n                )\n            return result, tz\n\n        data_dtype = data.dtype\n\n    # `data` may have originally been a Categorical[datetime64[ns, tz]],\n    # so we need to handle these types.\n    if isinstance(data_dtype, DatetimeTZDtype):\n        # DatetimeArray -> ndarray\n        data = cast(DatetimeArray, data)\n        tz = _maybe_infer_tz(tz, data.tz)\n        result = data._ndarray\n\n    elif lib.is_np_dtype(data_dtype, \"M\"):\n        # tz-naive DatetimeArray or ndarray[datetime64]\n        if isinstance(data, DatetimeArray):\n            data = data._ndarray\n\n        data = cast(np.ndarray, data)\n        result, copy = _construct_from_dt64_naive(\n            data, tz=tz, copy=copy, ambiguous=ambiguous\n        )\n\n    else:\n        # must be integer dtype otherwise\n        # assume this data are epoch timestamps\n        if data.dtype != INT64_DTYPE:\n            data = data.astype(np.int64, copy=False)\n            copy = False\n        data = cast(np.ndarray, data)\n        result = data.view(out_dtype)\n\n    if copy:\n        result = result.copy()\n\n    assert isinstance(result, np.ndarray), type(result)\n    assert result.dtype.kind == \"M\"\n    assert result.dtype != \"M8\"\n    assert is_supported_dtype(result.dtype)\n    return result, tz\n\n\ndef _construct_from_dt64_naive(\n    data: np.ndarray, *, tz: tzinfo | None, copy: bool, ambiguous: TimeAmbiguous\n) -> tuple[np.ndarray, bool]:\n    \"\"\"\n    Convert datetime64 data to a supported dtype, localizing if necessary.\n    \"\"\"\n    # Caller is responsible for ensuring\n    #  lib.is_np_dtype(data.dtype)\n\n    new_dtype = data.dtype\n    if not is_supported_dtype(new_dtype):\n        # Cast to the nearest supported unit, generally \"s\"\n        new_dtype = get_supported_dtype(new_dtype)\n        data = astype_overflowsafe(data, dtype=new_dtype, copy=False)\n        copy = False\n\n    if data.dtype.byteorder == \">\":\n        # TODO: better way to handle this?  non-copying alternative?\n        #  without this, test_constructor_datetime64_bigendian fails\n        data = data.astype(data.dtype.newbyteorder(\"<\"))\n        new_dtype = data.dtype\n        copy = False\n\n    if tz is not None:\n        # Convert tz-naive to UTC\n        # TODO: if tz is UTC, are there situations where we *don't* want a\n        #  copy?  tz_localize_to_utc always makes one.\n        shape = data.shape\n        if data.ndim > 1:\n            data = data.ravel()\n\n        data_unit = get_unit_from_dtype(new_dtype)\n        data = tzconversion.tz_localize_to_utc(\n            data.view(\"i8\"), tz, ambiguous=ambiguous, creso=data_unit\n        )\n        data = data.view(new_dtype)\n        data = data.reshape(shape)\n\n    assert data.dtype == new_dtype, data.dtype\n    result = data\n\n    return result, copy\n\n\ndef objects_to_datetime64(\n    data: np.ndarray,\n    dayfirst,\n    yearfirst,\n    utc: bool = False,\n    errors: DateTimeErrorChoices = \"raise\",\n    allow_object: bool = False,\n    out_unit: str | None = None,\n) -> tuple[np.ndarray, tzinfo | None]:\n    \"\"\"\n    Convert data to array of timestamps.\n\n    Parameters\n    ----------\n    data : np.ndarray[object]\n    dayfirst : bool\n    yearfirst : bool\n    utc : bool, default False\n        Whether to convert/localize timestamps to UTC.\n    errors : {'raise', 'coerce'}\n    allow_object : bool\n        Whether to return an object-dtype ndarray instead of raising if the\n        data contains more than one timezone.\n    out_unit : str or None, default None\n        None indicates we should do resolution inference.\n\n    Returns\n    -------\n    result : ndarray\n        np.datetime64[out_unit] if returned values represent wall times or UTC\n        timestamps.\n        object if mixed timezones\n    inferred_tz : tzinfo or None\n        If not None, then the datetime64 values in `result` denote UTC timestamps.\n\n    Raises\n    ------\n    ValueError : if data cannot be converted to datetimes\n    TypeError  : When a type cannot be converted to datetime\n    \"\"\"\n    assert errors in [\"raise\", \"coerce\"]\n\n    # if str-dtype, convert\n    data = np.asarray(data, dtype=np.object_)\n\n    result, tz_parsed = tslib.array_to_datetime(\n        data,\n        errors=errors,\n        utc=utc,\n        dayfirst=dayfirst,\n        yearfirst=yearfirst,\n        creso=abbrev_to_npy_unit(out_unit),\n    )\n\n    if tz_parsed is not None:\n        # We can take a shortcut since the datetime64 numpy array\n        #  is in UTC\n        return result, tz_parsed\n    elif result.dtype.kind == \"M\":\n        return result, tz_parsed\n    elif result.dtype == object:\n        # GH#23675 when called via `pd.to_datetime`, returning an object-dtype\n        #  array is allowed.  When called via `pd.DatetimeIndex`, we can\n        #  only accept datetime64 dtype, so raise TypeError if object-dtype\n        #  is returned, as that indicates the values can be recognized as\n        #  datetimes but they have conflicting timezones/awareness\n        if allow_object:\n            return result, tz_parsed\n        raise TypeError(\"DatetimeIndex has mixed timezones\")\n    else:  # pragma: no cover\n        # GH#23675 this TypeError should never be hit, whereas the TypeError\n        #  in the object-dtype branch above is reachable.\n        raise TypeError(result)\n\n\ndef maybe_convert_dtype(data, copy: bool, tz: tzinfo | None = None):\n    \"\"\"\n    Convert data based on dtype conventions, issuing\n    errors where appropriate.\n\n    Parameters\n    ----------\n    data : np.ndarray or pd.Index\n    copy : bool\n    tz : tzinfo or None, default None\n\n    Returns\n    -------\n    data : np.ndarray or pd.Index\n    copy : bool\n\n    Raises\n    ------\n    TypeError : PeriodDType data is passed\n    \"\"\"\n    if not hasattr(data, \"dtype\"):\n        # e.g. collections.deque\n        return data, copy\n\n    if is_float_dtype(data.dtype):\n        # pre-2.0 we treated these as wall-times, inconsistent with ints\n        # GH#23675, GH#45573 deprecated to treat symmetrically with integer dtypes.\n        # Note: data.astype(np.int64) fails ARM tests, see\n        # https://github.com/pandas-dev/pandas/issues/49468.\n        data = data.astype(DT64NS_DTYPE).view(\"i8\")\n        copy = False\n\n    elif lib.is_np_dtype(data.dtype, \"m\") or is_bool_dtype(data.dtype):\n        # GH#29794 enforcing deprecation introduced in GH#23539\n        raise TypeError(f\"dtype {data.dtype} cannot be converted to datetime64[ns]\")\n    elif isinstance(data.dtype, PeriodDtype):\n        # Note: without explicitly raising here, PeriodIndex\n        #  test_setops.test_join_does_not_recur fails\n        raise TypeError(\n            \"Passing PeriodDtype data is invalid. Use `data.to_timestamp()` instead\"\n        )\n\n    elif isinstance(data.dtype, ExtensionDtype) and not isinstance(\n        data.dtype, DatetimeTZDtype\n    ):\n        # TODO: We have no tests for these\n        data = np.array(data, dtype=np.object_)\n        copy = False\n\n    return data, copy\n\n\n# -------------------------------------------------------------------\n# Validation and Inference\n\n\ndef _maybe_infer_tz(tz: tzinfo | None, inferred_tz: tzinfo | None) -> tzinfo | None:\n    \"\"\"\n    If a timezone is inferred from data, check that it is compatible with\n    the user-provided timezone, if any.\n\n    Parameters\n    ----------\n    tz : tzinfo or None\n    inferred_tz : tzinfo or None\n\n    Returns\n    -------\n    tz : tzinfo or None\n\n    Raises\n    ------\n    TypeError : if both timezones are present but do not match\n    \"\"\"\n    if tz is None:\n        tz = inferred_tz\n    elif inferred_tz is None:\n        pass\n    elif not timezones.tz_compare(tz, inferred_tz):\n        raise TypeError(\n            f\"data is already tz-aware {inferred_tz}, unable to \"\n            f\"set specified tz: {tz}\"\n        )\n    return tz\n\n\ndef _validate_dt64_dtype(dtype):\n    \"\"\"\n    Check that a dtype, if passed, represents either a numpy datetime64[ns]\n    dtype or a pandas DatetimeTZDtype.\n\n    Parameters\n    ----------\n    dtype : object\n\n    Returns\n    -------\n    dtype : None, numpy.dtype, or DatetimeTZDtype\n\n    Raises\n    ------\n    ValueError : invalid dtype\n\n    Notes\n    -----\n    Unlike _validate_tz_from_dtype, this does _not_ allow non-existent\n    tz errors to go through\n    \"\"\"\n    if dtype is not None:\n        dtype = pandas_dtype(dtype)\n        if dtype == np.dtype(\"M8\"):\n            # no precision, disallowed GH#24806\n            msg = (\n                \"Passing in 'datetime64' dtype with no precision is not allowed. \"\n                \"Please pass in 'datetime64[ns]' instead.\"\n            )\n            raise ValueError(msg)\n\n        if (\n            isinstance(dtype, np.dtype)\n            and (dtype.kind != \"M\" or not is_supported_dtype(dtype))\n        ) or not isinstance(dtype, (np.dtype, DatetimeTZDtype)):\n            raise ValueError(\n                f\"Unexpected value for 'dtype': '{dtype}'. \"\n                \"Must be 'datetime64[s]', 'datetime64[ms]', 'datetime64[us]', \"\n                \"'datetime64[ns]' or DatetimeTZDtype'.\"\n            )\n\n        if getattr(dtype, \"tz\", None):\n            # https://github.com/pandas-dev/pandas/issues/18595\n            # Ensure that we have a standard timezone for pytz objects.\n            # Without this, things like adding an array of timedeltas and\n            # a  tz-aware Timestamp (with a tz specific to its datetime) will\n            # be incorrect(ish?) for the array as a whole\n            dtype = cast(DatetimeTZDtype, dtype)\n            dtype = DatetimeTZDtype(\n                unit=dtype.unit, tz=timezones.tz_standardize(dtype.tz)\n            )\n\n    return dtype\n\n\ndef _validate_tz_from_dtype(\n    dtype, tz: tzinfo | None, explicit_tz_none: bool = False\n) -> tzinfo | None:\n    \"\"\"\n    If the given dtype is a DatetimeTZDtype, extract the implied\n    tzinfo object from it and check that it does not conflict with the given\n    tz.\n\n    Parameters\n    ----------\n    dtype : dtype, str\n    tz : None, tzinfo\n    explicit_tz_none : bool, default False\n        Whether tz=None was passed explicitly, as opposed to lib.no_default.\n\n    Returns\n    -------\n    tz : consensus tzinfo\n\n    Raises\n    ------\n    ValueError : on tzinfo mismatch\n    \"\"\"\n    if dtype is not None:\n        if isinstance(dtype, str):\n            try:\n                dtype = DatetimeTZDtype.construct_from_string(dtype)\n            except TypeError:\n                # Things like `datetime64[ns]`, which is OK for the\n                # constructors, but also nonsense, which should be validated\n                # but not by us. We *do* allow non-existent tz errors to\n                # go through\n                pass\n        dtz = getattr(dtype, \"tz\", None)\n        if dtz is not None:\n            if tz is not None and not timezones.tz_compare(tz, dtz):\n                raise ValueError(\"cannot supply both a tz and a dtype with a tz\")\n            if explicit_tz_none:\n                raise ValueError(\"Cannot pass both a timezone-aware dtype and tz=None\")\n            tz = dtz\n\n        if tz is not None and lib.is_np_dtype(dtype, \"M\"):\n            # We also need to check for the case where the user passed a\n            #  tz-naive dtype (i.e. datetime64[ns])\n            if tz is not None and not timezones.tz_compare(tz, dtz):\n                raise ValueError(\n                    \"cannot supply both a tz and a \"\n                    \"timezone-naive dtype (i.e. datetime64[ns])\"\n                )\n\n    return tz\n\n\ndef _infer_tz_from_endpoints(\n    start: Timestamp, end: Timestamp, tz: tzinfo | None\n) -> tzinfo | None:\n    \"\"\"\n    If a timezone is not explicitly given via `tz`, see if one can\n    be inferred from the `start` and `end` endpoints.  If more than one\n    of these inputs provides a timezone, require that they all agree.\n\n    Parameters\n    ----------\n    start : Timestamp\n    end : Timestamp\n    tz : tzinfo or None\n\n    Returns\n    -------\n    tz : tzinfo or None\n\n    Raises\n    ------\n    TypeError : if start and end timezones do not agree\n    \"\"\"\n    try:\n        inferred_tz = timezones.infer_tzinfo(start, end)\n    except AssertionError as err:\n        # infer_tzinfo raises AssertionError if passed mismatched timezones\n        raise TypeError(\n            \"Start and end cannot both be tz-aware with different timezones\"\n        ) from err\n\n    inferred_tz = timezones.maybe_get_tz(inferred_tz)\n    tz = timezones.maybe_get_tz(tz)\n\n    if tz is not None and inferred_tz is not None:\n        if not timezones.tz_compare(inferred_tz, tz):\n            raise AssertionError(\"Inferred time zone not equal to passed time zone\")\n\n    elif inferred_tz is not None:\n        tz = inferred_tz\n\n    return tz\n\n\ndef _maybe_normalize_endpoints(\n    start: _TimestampNoneT1, end: _TimestampNoneT2, normalize: bool\n) -> tuple[_TimestampNoneT1, _TimestampNoneT2]:\n    if normalize:\n        if start is not None:\n            start = start.normalize()\n\n        if end is not None:\n            end = end.normalize()\n\n    return start, end\n\n\ndef _maybe_localize_point(\n    ts: Timestamp | None, freq, tz, ambiguous, nonexistent\n) -> Timestamp | None:\n    \"\"\"\n    Localize a start or end Timestamp to the timezone of the corresponding\n    start or end Timestamp\n\n    Parameters\n    ----------\n    ts : start or end Timestamp to potentially localize\n    freq : Tick, DateOffset, or None\n    tz : str, timezone object or None\n    ambiguous: str, localization behavior for ambiguous times\n    nonexistent: str, localization behavior for nonexistent times\n\n    Returns\n    -------\n    ts : Timestamp\n    \"\"\"\n    # Make sure start and end are timezone localized if:\n    # 1) freq = a Timedelta-like frequency (Tick)\n    # 2) freq = None i.e. generating a linspaced range\n    if ts is not None and ts.tzinfo is None:\n        # Note: We can't ambiguous='infer' a singular ambiguous time; however,\n        # we have historically defaulted ambiguous=False\n        ambiguous = ambiguous if ambiguous != \"infer\" else False\n        localize_args = {\"ambiguous\": ambiguous, \"nonexistent\": nonexistent, \"tz\": None}\n        if isinstance(freq, Tick) or freq is None:\n            localize_args[\"tz\"] = tz\n        ts = ts.tz_localize(**localize_args)\n    return ts\n\n\ndef _generate_range(\n    start: Timestamp | None,\n    end: Timestamp | None,\n    periods: int | None,\n    offset: BaseOffset,\n    *,\n    unit: str,\n) -> Generator[Timestamp, None, None]:\n    \"\"\"\n    Generates a sequence of dates corresponding to the specified time\n    offset. Similar to dateutil.rrule except uses pandas DateOffset\n    objects to represent time increments.\n\n    Parameters\n    ----------\n    start : Timestamp or None\n    end : Timestamp or None\n    periods : int or None\n    offset : DateOffset\n    unit : str\n\n    Notes\n    -----\n    * This method is faster for generating weekdays than dateutil.rrule\n    * At least two of (start, end, periods) must be specified.\n    * If both start and end are specified, the returned dates will\n    satisfy start <= date <= end.\n\n    Returns\n    -------\n    dates : generator object\n    \"\"\"\n    offset = to_offset(offset)\n\n    # Argument 1 to \"Timestamp\" has incompatible type \"Optional[Timestamp]\";\n    # expected \"Union[integer[Any], float, str, date, datetime64]\"\n    start = Timestamp(start)  # type: ignore[arg-type]\n    if start is not NaT:\n        start = start.as_unit(unit)\n    else:\n        start = None\n\n    # Argument 1 to \"Timestamp\" has incompatible type \"Optional[Timestamp]\";\n    # expected \"Union[integer[Any], float, str, date, datetime64]\"\n    end = Timestamp(end)  # type: ignore[arg-type]\n    if end is not NaT:\n        end = end.as_unit(unit)\n    else:\n        end = None\n\n    if start and not offset.is_on_offset(start):\n        # Incompatible types in assignment (expression has type \"datetime\",\n        # variable has type \"Optional[Timestamp]\")\n\n        # GH #56147 account for negative direction and range bounds\n        if offset.n >= 0:\n            start = offset.rollforward(start)  # type: ignore[assignment]\n        else:\n            start = offset.rollback(start)  # type: ignore[assignment]\n\n    # Unsupported operand types for < (\"Timestamp\" and \"None\")\n    if periods is None and end < start and offset.n >= 0:  # type: ignore[operator]\n        end = None\n        periods = 0\n\n    if end is None:\n        # error: No overload variant of \"__radd__\" of \"BaseOffset\" matches\n        # argument type \"None\"\n        end = start + (periods - 1) * offset  # type: ignore[operator]\n\n    if start is None:\n        # error: No overload variant of \"__radd__\" of \"BaseOffset\" matches\n        # argument type \"None\"\n        start = end - (periods - 1) * offset  # type: ignore[operator]\n\n    start = cast(Timestamp, start)\n    end = cast(Timestamp, end)\n\n    cur = start\n    if offset.n >= 0:\n        while cur <= end:\n            yield cur\n\n            if cur == end:\n                # GH#24252 avoid overflows by not performing the addition\n                # in offset.apply unless we have to\n                break\n\n            # faster than cur + offset\n            next_date = offset._apply(cur)\n            next_date = next_date.as_unit(unit)\n            if next_date <= cur:\n                raise ValueError(f\"Offset {offset} did not increment date\")\n            cur = next_date\n    else:\n        while cur >= end:\n            yield cur\n\n            if cur == end:\n                # GH#24252 avoid overflows by not performing the addition\n                # in offset.apply unless we have to\n                break\n\n            # faster than cur + offset\n            next_date = offset._apply(cur)\n            next_date = next_date.as_unit(unit)\n            if next_date >= cur:\n                raise ValueError(f\"Offset {offset} did not decrement date\")\n            cur = next_date\n"
    },
    {
      "filename": "pandas/core/dtypes/dtypes.py",
      "content": "\"\"\"\nDefine extension dtypes.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import (\n    date,\n    datetime,\n    time,\n    timedelta,\n)\nfrom decimal import Decimal\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    cast,\n)\nimport warnings\nimport zoneinfo\n\nimport numpy as np\n\nfrom pandas._config.config import get_option\n\nfrom pandas._libs import (\n    lib,\n    missing as libmissing,\n)\nfrom pandas._libs.interval import Interval\nfrom pandas._libs.properties import cache_readonly\nfrom pandas._libs.tslibs import (\n    BaseOffset,\n    NaT,\n    NaTType,\n    Period,\n    Timedelta,\n    Timestamp,\n    timezones,\n    to_offset,\n    tz_compare,\n)\nfrom pandas._libs.tslibs.dtypes import (\n    PeriodDtypeBase,\n    abbrev_to_npy_unit,\n)\nfrom pandas._libs.tslibs.offsets import BDay\nfrom pandas.compat import pa_version_under10p1\nfrom pandas.errors import PerformanceWarning\nfrom pandas.util._exceptions import find_stack_level\n\nfrom pandas.core.dtypes.base import (\n    ExtensionDtype,\n    StorageExtensionDtype,\n    register_extension_dtype,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCCategoricalIndex,\n    ABCIndex,\n    ABCRangeIndex,\n)\nfrom pandas.core.dtypes.inference import (\n    is_bool,\n    is_list_like,\n)\n\nif not pa_version_under10p1:\n    import pyarrow as pa\n\nif TYPE_CHECKING:\n    from collections.abc import MutableMapping\n    from datetime import tzinfo\n\n    import pyarrow as pa  # noqa: TCH004\n\n    from pandas._typing import (\n        Dtype,\n        DtypeObj,\n        IntervalClosedType,\n        Ordered,\n        Scalar,\n        Self,\n        npt,\n        type_t,\n    )\n\n    from pandas import (\n        Categorical,\n        CategoricalIndex,\n        DatetimeIndex,\n        Index,\n        IntervalIndex,\n        PeriodIndex,\n    )\n    from pandas.core.arrays import (\n        BaseMaskedArray,\n        DatetimeArray,\n        IntervalArray,\n        NumpyExtensionArray,\n        PeriodArray,\n        SparseArray,\n    )\n    from pandas.core.arrays.arrow import ArrowExtensionArray\n\nstr_type = str\n\n\nclass PandasExtensionDtype(ExtensionDtype):\n    \"\"\"\n    A np.dtype duck-typed class, suitable for holding a custom dtype.\n\n    THIS IS NOT A REAL NUMPY DTYPE\n    \"\"\"\n\n    type: Any\n    kind: Any\n    # The Any type annotations above are here only because mypy seems to have a\n    # problem dealing with multiple inheritance from PandasExtensionDtype\n    # and ExtensionDtype's @properties in the subclasses below. The kind and\n    # type variables in those subclasses are explicitly typed below.\n    subdtype = None\n    str: str_type\n    num = 100\n    shape: tuple[int, ...] = ()\n    itemsize = 8\n    base: DtypeObj | None = None\n    isbuiltin = 0\n    isnative = 0\n    _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}\n\n    def __repr__(self) -> str_type:\n        \"\"\"\n        Return a string representation for a particular object.\n        \"\"\"\n        return str(self)\n\n    def __hash__(self) -> int:\n        raise NotImplementedError(\"sub-classes should implement an __hash__ method\")\n\n    def __getstate__(self) -> dict[str_type, Any]:\n        # pickle support; we don't want to pickle the cache\n        return {k: getattr(self, k, None) for k in self._metadata}\n\n    @classmethod\n    def reset_cache(cls) -> None:\n        \"\"\"clear the cache\"\"\"\n        cls._cache_dtypes = {}\n\n\nclass CategoricalDtypeType(type):\n    \"\"\"\n    the type of CategoricalDtype, this metaclass determines subclass ability\n    \"\"\"\n\n\n@register_extension_dtype\nclass CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n    \"\"\"\n    Type for categorical data with the categories and orderedness.\n\n    Parameters\n    ----------\n    categories : sequence, optional\n        Must be unique, and must not contain any nulls.\n        The categories are stored in an Index,\n        and if an index is provided the dtype of that index will be used.\n    ordered : bool or None, default False\n        Whether or not this categorical is treated as a ordered categorical.\n        None can be used to maintain the ordered value of existing categoricals when\n        used in operations that combine categoricals, e.g. astype, and will resolve to\n        False if there is no existing ordered to maintain.\n\n    Attributes\n    ----------\n    categories\n    ordered\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    Categorical : Represent a categorical variable in classic R / S-plus fashion.\n\n    Notes\n    -----\n    This class is useful for specifying the type of a ``Categorical``\n    independent of the values. See :ref:`categorical.categoricaldtype`\n    for more.\n\n    Examples\n    --------\n    >>> t = pd.CategoricalDtype(categories=[\"b\", \"a\"], ordered=True)\n    >>> pd.Series([\"a\", \"b\", \"a\", \"c\"], dtype=t)\n    0      a\n    1      b\n    2      a\n    3    NaN\n    dtype: category\n    Categories (2, object): ['b' < 'a']\n\n    An empty CategoricalDtype with a specific dtype can be created\n    by providing an empty index. As follows,\n\n    >>> pd.CategoricalDtype(pd.DatetimeIndex([])).categories.dtype\n    dtype('<M8[s]')\n    \"\"\"\n\n    # TODO: Document public vs. private API\n    name = \"category\"\n    type: type[CategoricalDtypeType] = CategoricalDtypeType\n    kind: str_type = \"O\"\n    str = \"|O08\"\n    base = np.dtype(\"O\")\n    _metadata = (\"categories\", \"ordered\")\n    _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}\n    _supports_2d = False\n    _can_fast_transpose = False\n\n    def __init__(self, categories=None, ordered: Ordered = False) -> None:\n        self._finalize(categories, ordered, fastpath=False)\n\n    @classmethod\n    def _from_fastpath(\n        cls, categories=None, ordered: bool | None = None\n    ) -> CategoricalDtype:\n        self = cls.__new__(cls)\n        self._finalize(categories, ordered, fastpath=True)\n        return self\n\n    @classmethod\n    def _from_categorical_dtype(\n        cls, dtype: CategoricalDtype, categories=None, ordered: Ordered | None = None\n    ) -> CategoricalDtype:\n        if categories is ordered is None:\n            return dtype\n        if categories is None:\n            categories = dtype.categories\n        if ordered is None:\n            ordered = dtype.ordered\n        return cls(categories, ordered)\n\n    @classmethod\n    def _from_values_or_dtype(\n        cls,\n        values=None,\n        categories=None,\n        ordered: bool | None = None,\n        dtype: Dtype | None = None,\n    ) -> CategoricalDtype:\n        \"\"\"\n        Construct dtype from the input parameters used in :class:`Categorical`.\n\n        This constructor method specifically does not do the factorization\n        step, if that is needed to find the categories. This constructor may\n        therefore return ``CategoricalDtype(categories=None, ordered=None)``,\n        which may not be useful. Additional steps may therefore have to be\n        taken to create the final dtype.\n\n        The return dtype is specified from the inputs in this prioritized\n        order:\n        1. if dtype is a CategoricalDtype, return dtype\n        2. if dtype is the string 'category', create a CategoricalDtype from\n           the supplied categories and ordered parameters, and return that.\n        3. if values is a categorical, use value.dtype, but override it with\n           categories and ordered if either/both of those are not None.\n        4. if dtype is None and values is not a categorical, construct the\n           dtype from categories and ordered, even if either of those is None.\n\n        Parameters\n        ----------\n        values : list-like, optional\n            The list-like must be 1-dimensional.\n        categories : list-like, optional\n            Categories for the CategoricalDtype.\n        ordered : bool, optional\n            Designating if the categories are ordered.\n        dtype : CategoricalDtype or the string \"category\", optional\n            If ``CategoricalDtype``, cannot be used together with\n            `categories` or `ordered`.\n\n        Returns\n        -------\n        CategoricalDtype\n\n        Examples\n        --------\n        >>> pd.CategoricalDtype._from_values_or_dtype()\n        CategoricalDtype(categories=None, ordered=None, categories_dtype=None)\n        >>> pd.CategoricalDtype._from_values_or_dtype(\n        ...     categories=[\"a\", \"b\"], ordered=True\n        ... )\n        CategoricalDtype(categories=['a', 'b'], ordered=True, categories_dtype=object)\n        >>> dtype1 = pd.CategoricalDtype([\"a\", \"b\"], ordered=True)\n        >>> dtype2 = pd.CategoricalDtype([\"x\", \"y\"], ordered=False)\n        >>> c = pd.Categorical([0, 1], dtype=dtype1)\n        >>> pd.CategoricalDtype._from_values_or_dtype(\n        ...     c, [\"x\", \"y\"], ordered=True, dtype=dtype2\n        ... )\n        Traceback (most recent call last):\n            ...\n        ValueError: Cannot specify `categories` or `ordered` together with\n        `dtype`.\n\n        The supplied dtype takes precedence over values' dtype:\n\n        >>> pd.CategoricalDtype._from_values_or_dtype(c, dtype=dtype2)\n        CategoricalDtype(categories=['x', 'y'], ordered=False, categories_dtype=object)\n        \"\"\"\n\n        if dtype is not None:\n            # The dtype argument takes precedence over values.dtype (if any)\n            if isinstance(dtype, str):\n                if dtype == \"category\":\n                    if ordered is None and cls.is_dtype(values):\n                        # GH#49309 preserve orderedness\n                        ordered = values.dtype.ordered\n\n                    dtype = CategoricalDtype(categories, ordered)\n                else:\n                    raise ValueError(f\"Unknown dtype {dtype!r}\")\n            elif categories is not None or ordered is not None:\n                raise ValueError(\n                    \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                )\n            elif not isinstance(dtype, CategoricalDtype):\n                raise ValueError(f\"Cannot not construct CategoricalDtype from {dtype}\")\n        elif cls.is_dtype(values):\n            # If no \"dtype\" was passed, use the one from \"values\", but honor\n            # the \"ordered\" and \"categories\" arguments\n            dtype = values.dtype._from_categorical_dtype(\n                values.dtype, categories, ordered\n            )\n        else:\n            # If dtype=None and values is not categorical, create a new dtype.\n            # Note: This could potentially have categories=None and\n            # ordered=None.\n            dtype = CategoricalDtype(categories, ordered)\n\n        return cast(CategoricalDtype, dtype)\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> CategoricalDtype:\n        \"\"\"\n        Construct a CategoricalDtype from a string.\n\n        Parameters\n        ----------\n        string : str\n            Must be the string \"category\" in order to be successfully constructed.\n\n        Returns\n        -------\n        CategoricalDtype\n            Instance of the dtype.\n\n        Raises\n        ------\n        TypeError\n            If a CategoricalDtype cannot be constructed from the input.\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n        if string != cls.name:\n            raise TypeError(f\"Cannot construct a 'CategoricalDtype' from '{string}'\")\n\n        # need ordered=None to ensure that operations specifying dtype=\"category\" don't\n        # override the ordered value for existing categoricals\n        return cls(ordered=None)\n\n    def _finalize(self, categories, ordered: Ordered, fastpath: bool = False) -> None:\n        if ordered is not None:\n            self.validate_ordered(ordered)\n\n        if categories is not None:\n            categories = self.validate_categories(categories, fastpath=fastpath)\n\n        self._categories = categories\n        self._ordered = ordered\n\n    def __setstate__(self, state: MutableMapping[str_type, Any]) -> None:\n        # for pickle compat. __get_state__ is defined in the\n        # PandasExtensionDtype superclass and uses the public properties to\n        # pickle -> need to set the settable private ones here (see GH26067)\n        self._categories = state.pop(\"categories\", None)\n        self._ordered = state.pop(\"ordered\", False)\n\n    def __hash__(self) -> int:\n        # _hash_categories returns a uint64, so use the negative\n        # space for when we have unknown categories to avoid a conflict\n        if self.categories is None:\n            if self.ordered:\n                return -1\n            else:\n                return -2\n        # We *do* want to include the real self.ordered here\n        return int(self._hash_categories)\n\n    def __eq__(self, other: object) -> bool:\n        \"\"\"\n        Rules for CDT equality:\n        1) Any CDT is equal to the string 'category'\n        2) Any CDT is equal to itself\n        3) Any CDT is equal to a CDT with categories=None regardless of ordered\n        4) A CDT with ordered=True is only equal to another CDT with\n           ordered=True and identical categories in the same order\n        5) A CDT with ordered={False, None} is only equal to another CDT with\n           ordered={False, None} and identical categories, but same order is\n           not required. There is no distinction between False/None.\n        6) Any other comparison returns False\n        \"\"\"\n        if isinstance(other, str):\n            return other == self.name\n        elif other is self:\n            return True\n        elif not (hasattr(other, \"ordered\") and hasattr(other, \"categories\")):\n            return False\n        elif self.categories is None or other.categories is None:\n            # For non-fully-initialized dtypes, these are only equal to\n            #  - the string \"category\" (handled above)\n            #  - other CategoricalDtype with categories=None\n            return self.categories is other.categories\n        elif self.ordered or other.ordered:\n            # At least one has ordered=True; equal if both have ordered=True\n            # and the same values for categories in the same order.\n            return (self.ordered == other.ordered) and self.categories.equals(\n                other.categories\n            )\n        else:\n            # Neither has ordered=True; equal if both have the same categories,\n            # but same order is not necessary.  There is no distinction between\n            # ordered=False and ordered=None: CDT(., False) and CDT(., None)\n            # will be equal if they have the same categories.\n            left = self.categories\n            right = other.categories\n\n            # GH#36280 the ordering of checks here is for performance\n            if not left.dtype == right.dtype:\n                return False\n\n            if len(left) != len(right):\n                return False\n\n            if self.categories.equals(other.categories):\n                # Check and see if they happen to be identical categories\n                return True\n\n            if left.dtype != object:\n                # Faster than calculating hash\n                indexer = left.get_indexer(right)\n                # Because left and right have the same length and are unique,\n                #  `indexer` not having any -1s implies that there is a\n                #  bijection between `left` and `right`.\n                return (indexer != -1).all()\n\n            # With object-dtype we need a comparison that identifies\n            #  e.g. int(2) as distinct from float(2)\n            return set(left) == set(right)\n\n    def __repr__(self) -> str_type:\n        if self.categories is None:\n            data = \"None\"\n            dtype = \"None\"\n        else:\n            data = self.categories._format_data(name=type(self).__name__)\n            if isinstance(self.categories, ABCRangeIndex):\n                data = str(self.categories._range)\n            data = data.rstrip(\", \")\n            dtype = self.categories.dtype\n\n        return (\n            f\"CategoricalDtype(categories={data}, ordered={self.ordered}, \"\n            f\"categories_dtype={dtype})\"\n        )\n\n    @cache_readonly\n    def _hash_categories(self) -> int:\n        from pandas.core.util.hashing import (\n            combine_hash_arrays,\n            hash_array,\n            hash_tuples,\n        )\n\n        categories = self.categories\n        ordered = self.ordered\n\n        if len(categories) and isinstance(categories[0], tuple):\n            # assumes if any individual category is a tuple, then all our. ATM\n            # I don't really want to support just some of the categories being\n            # tuples.\n            cat_list = list(categories)  # breaks if a np.array of categories\n            cat_array = hash_tuples(cat_list)\n        else:\n            if categories.dtype == \"O\" and len({type(x) for x in categories}) != 1:\n                # TODO: hash_array doesn't handle mixed types. It casts\n                # everything to a str first, which means we treat\n                # {'1', '2'} the same as {'1', 2}\n                # find a better solution\n                hashed = hash((tuple(categories), ordered))\n                return hashed\n\n            if DatetimeTZDtype.is_dtype(categories.dtype):\n                # Avoid future warning.\n                categories = categories.view(\"datetime64[ns]\")\n\n            cat_array = hash_array(np.asarray(categories), categorize=False)\n        if ordered:\n            cat_array = np.vstack(\n                [cat_array, np.arange(len(cat_array), dtype=cat_array.dtype)]\n            )\n        else:\n            cat_array = np.array([cat_array])\n        combined_hashed = combine_hash_arrays(iter(cat_array), num_items=len(cat_array))\n        return np.bitwise_xor.reduce(combined_hashed)\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[Categorical]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas import Categorical\n\n        return Categorical\n\n    @staticmethod\n    def validate_ordered(ordered: Ordered) -> None:\n        \"\"\"\n        Validates that we have a valid ordered parameter. If\n        it is not a boolean, a TypeError will be raised.\n\n        Parameters\n        ----------\n        ordered : object\n            The parameter to be verified.\n\n        Raises\n        ------\n        TypeError\n            If 'ordered' is not a boolean.\n        \"\"\"\n        if not is_bool(ordered):\n            raise TypeError(\"'ordered' must either be 'True' or 'False'\")\n\n    @staticmethod\n    def validate_categories(categories, fastpath: bool = False) -> Index:\n        \"\"\"\n        Validates that we have good categories\n\n        Parameters\n        ----------\n        categories : array-like\n        fastpath : bool\n            Whether to skip nan and uniqueness checks\n\n        Returns\n        -------\n        categories : Index\n        \"\"\"\n        from pandas.core.indexes.base import Index\n\n        if not fastpath and not is_list_like(categories):\n            raise TypeError(\n                f\"Parameter 'categories' must be list-like, was {categories!r}\"\n            )\n        if not isinstance(categories, ABCIndex):\n            categories = Index._with_infer(categories, tupleize_cols=False)\n\n        if not fastpath:\n            if categories.hasnans:\n                raise ValueError(\"Categorical categories cannot be null\")\n\n            if not categories.is_unique:\n                raise ValueError(\"Categorical categories must be unique\")\n\n        if isinstance(categories, ABCCategoricalIndex):\n            categories = categories.categories\n\n        return categories\n\n    def update_dtype(self, dtype: str_type | CategoricalDtype) -> CategoricalDtype:\n        \"\"\"\n        Returns a CategoricalDtype with categories and ordered taken from dtype\n        if specified, otherwise falling back to self if unspecified\n\n        Parameters\n        ----------\n        dtype : CategoricalDtype\n\n        Returns\n        -------\n        new_dtype : CategoricalDtype\n        \"\"\"\n        if isinstance(dtype, str) and dtype == \"category\":\n            # dtype='category' should not change anything\n            return self\n        elif not self.is_dtype(dtype):\n            raise ValueError(\n                f\"a CategoricalDtype must be passed to perform an update, \"\n                f\"got {dtype!r}\"\n            )\n        else:\n            # from here on, dtype is a CategoricalDtype\n            dtype = cast(CategoricalDtype, dtype)\n\n        # update categories/ordered unless they've been explicitly passed as None\n        new_categories = (\n            dtype.categories if dtype.categories is not None else self.categories\n        )\n        new_ordered = dtype.ordered if dtype.ordered is not None else self.ordered\n\n        return CategoricalDtype(new_categories, new_ordered)\n\n    @property\n    def categories(self) -> Index:\n        \"\"\"\n        An ``Index`` containing the unique categories allowed.\n\n        See Also\n        --------\n        ordered : Whether the categories have an ordered relationship.\n\n        Examples\n        --------\n        >>> cat_type = pd.CategoricalDtype(categories=[\"a\", \"b\"], ordered=True)\n        >>> cat_type.categories\n        Index(['a', 'b'], dtype='object')\n        \"\"\"\n        return self._categories\n\n    @property\n    def ordered(self) -> Ordered:\n        \"\"\"\n        Whether the categories have an ordered relationship.\n\n        See Also\n        --------\n        categories : An Index containing the unique categories allowed.\n\n        Examples\n        --------\n        >>> cat_type = pd.CategoricalDtype(categories=[\"a\", \"b\"], ordered=True)\n        >>> cat_type.ordered\n        True\n\n        >>> cat_type = pd.CategoricalDtype(categories=[\"a\", \"b\"], ordered=False)\n        >>> cat_type.ordered\n        False\n        \"\"\"\n        return self._ordered\n\n    @property\n    def _is_boolean(self) -> bool:\n        from pandas.core.dtypes.common import is_bool_dtype\n\n        return is_bool_dtype(self.categories)\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        # check if we have all categorical dtype with identical categories\n        if all(isinstance(x, CategoricalDtype) for x in dtypes):\n            first = dtypes[0]\n            if all(first == other for other in dtypes[1:]):\n                return first\n\n        # special case non-initialized categorical\n        # TODO we should figure out the expected return value in general\n        non_init_cats = [\n            isinstance(x, CategoricalDtype) and x.categories is None for x in dtypes\n        ]\n        if all(non_init_cats):\n            return self\n        elif any(non_init_cats):\n            return None\n\n        # categorical is aware of Sparse -> extract sparse subdtypes\n        subtypes = (x.subtype if isinstance(x, SparseDtype) else x for x in dtypes)\n        # extract the categories' dtype\n        non_cat_dtypes = [\n            x.categories.dtype if isinstance(x, CategoricalDtype) else x\n            for x in subtypes\n        ]\n        # TODO should categorical always give an answer?\n        from pandas.core.dtypes.cast import find_common_type\n\n        return find_common_type(non_cat_dtypes)\n\n    @cache_readonly\n    def index_class(self) -> type_t[CategoricalIndex]:\n        from pandas import CategoricalIndex\n\n        return CategoricalIndex\n\n\n@register_extension_dtype\nclass DatetimeTZDtype(PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for timezone-aware datetime data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    unit : str, default \"ns\"\n        The precision of the datetime data. Valid options are\n        ``\"s\"``, ``\"ms\"``, ``\"us\"``, ``\"ns\"``.\n    tz : str, int, or datetime.tzinfo\n        The timezone.\n\n    Attributes\n    ----------\n    unit\n    tz\n\n    Methods\n    -------\n    None\n\n    Raises\n    ------\n    ZoneInfoNotFoundError\n        When the requested timezone cannot be found.\n\n    See Also\n    --------\n    numpy.datetime64 : Numpy data type for datetime.\n    datetime.datetime : Python datetime object.\n\n    Examples\n    --------\n    >>> from zoneinfo import ZoneInfo\n    >>> pd.DatetimeTZDtype(tz=ZoneInfo(\"UTC\"))\n    datetime64[ns, UTC]\n\n    >>> pd.DatetimeTZDtype(tz=ZoneInfo(\"Europe/Paris\"))\n    datetime64[ns, Europe/Paris]\n    \"\"\"\n\n    type: type[Timestamp] = Timestamp\n    kind: str_type = \"M\"\n    num = 101\n    _metadata = (\"unit\", \"tz\")\n    _match = re.compile(r\"(datetime64|M8)\\[(?P<unit>.+), (?P<tz>.+)\\]\")\n    _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}\n    _supports_2d = True\n    _can_fast_transpose = True\n\n    @property\n    def na_value(self) -> NaTType:\n        return NaT\n\n    @cache_readonly\n    def base(self) -> DtypeObj:  # type: ignore[override]\n        return np.dtype(f\"M8[{self.unit}]\")\n\n    # error: Signature of \"str\" incompatible with supertype \"PandasExtensionDtype\"\n    @cache_readonly\n    def str(self) -> str:  # type: ignore[override]\n        return f\"|M8[{self.unit}]\"\n\n    def __init__(self, unit: str_type | DatetimeTZDtype = \"ns\", tz=None) -> None:\n        if isinstance(unit, DatetimeTZDtype):\n            # error: \"str\" has no attribute \"tz\"\n            unit, tz = unit.unit, unit.tz  # type: ignore[attr-defined]\n\n        if unit != \"ns\":\n            if isinstance(unit, str) and tz is None:\n                # maybe a string like datetime64[ns, tz], which we support for\n                # now.\n                result = type(self).construct_from_string(unit)\n                unit = result.unit\n                tz = result.tz\n                msg = (\n                    f\"Passing a dtype alias like 'datetime64[ns, {tz}]' \"\n                    \"to DatetimeTZDtype is no longer supported. Use \"\n                    \"'DatetimeTZDtype.construct_from_string()' instead.\"\n                )\n                raise ValueError(msg)\n            if unit not in [\"s\", \"ms\", \"us\", \"ns\"]:\n                raise ValueError(\"DatetimeTZDtype only supports s, ms, us, ns units\")\n\n        if tz:\n            tz = timezones.maybe_get_tz(tz)\n            tz = timezones.tz_standardize(tz)\n        elif tz is not None:\n            raise zoneinfo.ZoneInfoNotFoundError(tz)\n        if tz is None:\n            raise TypeError(\"A 'tz' is required.\")\n\n        self._unit = unit\n        self._tz = tz\n\n    @cache_readonly\n    def _creso(self) -> int:\n        \"\"\"\n        The NPY_DATETIMEUNIT corresponding to this dtype's resolution.\n        \"\"\"\n        return abbrev_to_npy_unit(self.unit)\n\n    @property\n    def unit(self) -> str_type:\n        \"\"\"\n        The precision of the datetime data.\n\n        See Also\n        --------\n        DatetimeTZDtype.tz : Retrieves the timezone.\n\n        Examples\n        --------\n        >>> from zoneinfo import ZoneInfo\n        >>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo(\"America/Los_Angeles\"))\n        >>> dtype.unit\n        'ns'\n        \"\"\"\n        return self._unit\n\n    @property\n    def tz(self) -> tzinfo:\n        \"\"\"\n        The timezone.\n\n        See Also\n        --------\n        DatetimeTZDtype.unit : Retrieves precision of the datetime data.\n\n        Examples\n        --------\n        >>> from zoneinfo import ZoneInfo\n        >>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo(\"America/Los_Angeles\"))\n        >>> dtype.tz\n        zoneinfo.ZoneInfo(key='America/Los_Angeles')\n        \"\"\"\n        return self._tz\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[DatetimeArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import DatetimeArray\n\n        return DatetimeArray\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> DatetimeTZDtype:\n        \"\"\"\n        Construct a DatetimeTZDtype from a string.\n\n        Parameters\n        ----------\n        string : str\n            The string alias for this DatetimeTZDtype.\n            Should be formatted like ``datetime64[ns, <tz>]``,\n            where ``<tz>`` is the timezone name.\n\n        Examples\n        --------\n        >>> DatetimeTZDtype.construct_from_string(\"datetime64[ns, UTC]\")\n        datetime64[ns, UTC]\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n\n        msg = f\"Cannot construct a 'DatetimeTZDtype' from '{string}'\"\n        match = cls._match.match(string)\n        if match:\n            d = match.groupdict()\n            try:\n                return cls(unit=d[\"unit\"], tz=d[\"tz\"])\n            except (KeyError, TypeError, ValueError) as err:\n                # KeyError if maybe_get_tz tries and fails to get a\n                #  zoneinfo timezone (actually zoneinfo.ZoneInfoNotFoundError).\n                # TypeError if we pass a nonsense tz;\n                # ValueError if we pass a unit other than \"ns\"\n                raise TypeError(msg) from err\n        raise TypeError(msg)\n\n    def __str__(self) -> str_type:\n        return f\"datetime64[{self.unit}, {self.tz}]\"\n\n    @property\n    def name(self) -> str_type:\n        \"\"\"A string representation of the dtype.\"\"\"\n        return str(self)\n\n    def __hash__(self) -> int:\n        # make myself hashable\n        # TODO: update this.\n        return hash(str(self))\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, str):\n            if other.startswith(\"M8[\"):\n                other = f\"datetime64[{other[3:]}\"\n            return other == self.name\n\n        return (\n            isinstance(other, DatetimeTZDtype)\n            and self.unit == other.unit\n            and tz_compare(self.tz, other.tz)\n        )\n\n    def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> DatetimeArray:\n        \"\"\"\n        Construct DatetimeArray from pyarrow Array/ChunkedArray.\n\n        Note: If the units in the pyarrow Array are the same as this\n        DatetimeDtype, then values corresponding to the integer representation\n        of ``NaT`` (e.g. one nanosecond before :attr:`pandas.Timestamp.min`)\n        are converted to ``NaT``, regardless of the null indicator in the\n        pyarrow array.\n\n        Parameters\n        ----------\n        array : pyarrow.Array or pyarrow.ChunkedArray\n            The Arrow array to convert to DatetimeArray.\n\n        Returns\n        -------\n        extension array : DatetimeArray\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays import DatetimeArray\n\n        array = array.cast(pyarrow.timestamp(unit=self._unit), safe=True)\n\n        if isinstance(array, pyarrow.Array):\n            np_arr = array.to_numpy(zero_copy_only=False)\n        else:\n            np_arr = array.to_numpy()\n\n        return DatetimeArray._simple_new(np_arr, dtype=self)\n\n    def __setstate__(self, state) -> None:\n        # for pickle compat. __get_state__ is defined in the\n        # PandasExtensionDtype superclass and uses the public properties to\n        # pickle -> need to set the settable private ones here (see GH26067)\n        self._tz = state[\"tz\"]\n        self._unit = state[\"unit\"]\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        if all(isinstance(t, DatetimeTZDtype) and t.tz == self.tz for t in dtypes):\n            np_dtype = np.max([cast(DatetimeTZDtype, t).base for t in [self, *dtypes]])\n            unit = np.datetime_data(np_dtype)[0]\n            return type(self)(unit=unit, tz=self.tz)\n        return super()._get_common_dtype(dtypes)\n\n    @cache_readonly\n    def index_class(self) -> type_t[DatetimeIndex]:\n        from pandas import DatetimeIndex\n\n        return DatetimeIndex\n\n\n@register_extension_dtype\nclass PeriodDtype(PeriodDtypeBase, PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for Period data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    freq : str or DateOffset\n        The frequency of this PeriodDtype.\n\n    Attributes\n    ----------\n    freq\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> pd.PeriodDtype(freq=\"D\")\n    period[D]\n\n    >>> pd.PeriodDtype(freq=pd.offsets.MonthEnd())\n    period[M]\n    \"\"\"\n\n    type: type[Period] = Period\n    kind: str_type = \"O\"\n    str = \"|O08\"\n    base = np.dtype(\"O\")\n    num = 102\n    _metadata = (\"freq\",)\n    _match = re.compile(r\"(P|p)eriod\\[(?P<freq>.+)\\]\")\n    # error: Incompatible types in assignment (expression has type\n    # \"Dict[int, PandasExtensionDtype]\", base class \"PandasExtensionDtype\"\n    # defined the type as \"Dict[str, PandasExtensionDtype]\")  [assignment]\n    _cache_dtypes: dict[BaseOffset, int] = {}  # type: ignore[assignment]\n    __hash__ = PeriodDtypeBase.__hash__\n    _freq: BaseOffset\n    _supports_2d = True\n    _can_fast_transpose = True\n\n    def __new__(cls, freq) -> PeriodDtype:  # noqa: PYI034\n        \"\"\"\n        Parameters\n        ----------\n        freq : PeriodDtype, BaseOffset, or string\n        \"\"\"\n        if isinstance(freq, PeriodDtype):\n            return freq\n\n        if not isinstance(freq, BaseOffset):\n            freq = cls._parse_dtype_strict(freq)\n\n        if isinstance(freq, BDay):\n            # GH#53446\n            # TODO(3.0): enforcing this will close GH#10575\n            warnings.warn(\n                \"PeriodDtype[B] is deprecated and will be removed in a future \"\n                \"version. Use a DatetimeIndex with freq='B' instead\",\n                FutureWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        try:\n            dtype_code = cls._cache_dtypes[freq]\n        except KeyError:\n            dtype_code = freq._period_dtype_code\n            cls._cache_dtypes[freq] = dtype_code\n        u = PeriodDtypeBase.__new__(cls, dtype_code, freq.n)\n        u._freq = freq\n        return u\n\n    def __reduce__(self) -> tuple[type_t[Self], tuple[str_type]]:\n        return type(self), (self.name,)\n\n    @property\n    def freq(self) -> BaseOffset:\n        \"\"\"\n        The frequency object of this PeriodDtype.\n\n        Examples\n        --------\n        >>> dtype = pd.PeriodDtype(freq=\"D\")\n        >>> dtype.freq\n        <Day>\n        \"\"\"\n        return self._freq\n\n    @classmethod\n    def _parse_dtype_strict(cls, freq: str_type) -> BaseOffset:\n        if isinstance(freq, str):  # note: freq is already of type str!\n            if freq.startswith((\"Period[\", \"period[\")):\n                m = cls._match.search(freq)\n                if m is not None:\n                    freq = m.group(\"freq\")\n\n            freq_offset = to_offset(freq, is_period=True)\n            if freq_offset is not None:\n                return freq_offset\n\n        raise TypeError(\n            \"PeriodDtype argument should be string or BaseOffset, \"\n            f\"got {type(freq).__name__}\"\n        )\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> PeriodDtype:\n        \"\"\"\n        Strict construction from a string, raise a TypeError if not\n        possible\n        \"\"\"\n        if (\n            isinstance(string, str)\n            and (string.startswith((\"period[\", \"Period[\")))\n            or isinstance(string, BaseOffset)\n        ):\n            # do not parse string like U as period[U]\n            # avoid tuple to be regarded as freq\n            try:\n                return cls(freq=string)\n            except ValueError:\n                pass\n        if isinstance(string, str):\n            msg = f\"Cannot construct a 'PeriodDtype' from '{string}'\"\n        else:\n            msg = f\"'construct_from_string' expects a string, got {type(string)}\"\n        raise TypeError(msg)\n\n    def __str__(self) -> str_type:\n        return self.name\n\n    @property\n    def name(self) -> str_type:\n        return f\"period[{self._freqstr}]\"\n\n    @property\n    def na_value(self) -> NaTType:\n        return NaT\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, str):\n            return other[:1].lower() + other[1:] == self.name\n\n        return super().__eq__(other)\n\n    def __ne__(self, other: object) -> bool:\n        return not self.__eq__(other)\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        \"\"\"\n        Return a boolean if we if the passed type is an actual dtype that we\n        can match (via string or type)\n        \"\"\"\n        if isinstance(dtype, str):\n            # PeriodDtype can be instantiated from freq string like \"U\",\n            # but doesn't regard freq str like \"U\" as dtype.\n            if dtype.startswith((\"period[\", \"Period[\")):\n                try:\n                    return cls._parse_dtype_strict(dtype) is not None\n                except ValueError:\n                    return False\n            else:\n                return False\n        return super().is_dtype(dtype)\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[PeriodArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import PeriodArray\n\n        return PeriodArray\n\n    def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> PeriodArray:\n        \"\"\"\n        Construct PeriodArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays import PeriodArray\n        from pandas.core.arrays.arrow._arrow_utils import (\n            pyarrow_array_to_numpy_and_mask,\n        )\n\n        if isinstance(array, pyarrow.Array):\n            chunks = [array]\n        else:\n            chunks = array.chunks\n\n        results = []\n        for arr in chunks:\n            data, mask = pyarrow_array_to_numpy_and_mask(arr, dtype=np.dtype(np.int64))\n            parr = PeriodArray(data.copy(), dtype=self, copy=False)\n            # error: Invalid index type \"ndarray[Any, dtype[bool_]]\" for \"PeriodArray\";\n            # expected type \"Union[int, Sequence[int], Sequence[bool], slice]\"\n            parr[~mask] = NaT  # type: ignore[index]\n            results.append(parr)\n\n        if not results:\n            return PeriodArray(np.array([], dtype=\"int64\"), dtype=self, copy=False)\n        return PeriodArray._concat_same_type(results)\n\n    @cache_readonly\n    def index_class(self) -> type_t[PeriodIndex]:\n        from pandas import PeriodIndex\n\n        return PeriodIndex\n\n\n@register_extension_dtype\nclass IntervalDtype(PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for Interval data.\n\n    **This is not an actual numpy dtype**, but a duck type.\n\n    Parameters\n    ----------\n    subtype : str, np.dtype\n        The dtype of the Interval bounds.\n    closed : {'right', 'left', 'both', 'neither'}, default 'right'\n        Whether the interval is closed on the left-side, right-side, both or\n        neither. See the Notes for more detailed explanation.\n\n    Attributes\n    ----------\n    subtype\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    PeriodDtype : An ExtensionDtype for Period data.\n\n    Examples\n    --------\n    >>> pd.IntervalDtype(subtype=\"int64\", closed=\"both\")\n    interval[int64, both]\n    \"\"\"\n\n    name = \"interval\"\n    kind: str_type = \"O\"\n    str = \"|O08\"\n    base = np.dtype(\"O\")\n    num = 103\n    _metadata = (\n        \"subtype\",\n        \"closed\",\n    )\n\n    _match = re.compile(\n        r\"(I|i)nterval\\[(?P<subtype>[^,]+(\\[.+\\])?)\"\n        r\"(, (?P<closed>(right|left|both|neither)))?\\]\"\n    )\n\n    _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}\n    _subtype: None | np.dtype\n    _closed: IntervalClosedType | None\n\n    def __init__(self, subtype=None, closed: IntervalClosedType | None = None) -> None:\n        from pandas.core.dtypes.common import (\n            is_string_dtype,\n            pandas_dtype,\n        )\n\n        if closed is not None and closed not in {\"right\", \"left\", \"both\", \"neither\"}:\n            raise ValueError(\"closed must be one of 'right', 'left', 'both', 'neither'\")\n\n        if isinstance(subtype, IntervalDtype):\n            if closed is not None and closed != subtype.closed:\n                raise ValueError(\n                    \"dtype.closed and 'closed' do not match. \"\n                    \"Try IntervalDtype(dtype.subtype, closed) instead.\"\n                )\n            self._subtype = subtype._subtype\n            self._closed = subtype._closed\n        elif subtype is None:\n            # we are called as an empty constructor\n            # generally for pickle compat\n            self._subtype = None\n            self._closed = closed\n        elif isinstance(subtype, str) and subtype.lower() == \"interval\":\n            self._subtype = None\n            self._closed = closed\n        else:\n            if isinstance(subtype, str):\n                m = IntervalDtype._match.search(subtype)\n                if m is not None:\n                    gd = m.groupdict()\n                    subtype = gd[\"subtype\"]\n                    if gd.get(\"closed\", None) is not None:\n                        if closed is not None:\n                            if closed != gd[\"closed\"]:\n                                raise ValueError(\n                                    \"'closed' keyword does not match value \"\n                                    \"specified in dtype string\"\n                                )\n                        closed = gd[\"closed\"]  # type: ignore[assignment]\n\n            try:\n                subtype = pandas_dtype(subtype)\n            except TypeError as err:\n                raise TypeError(\"could not construct IntervalDtype\") from err\n            if CategoricalDtype.is_dtype(subtype) or is_string_dtype(subtype):\n                # GH 19016\n                msg = (\n                    \"category, object, and string subtypes are not supported \"\n                    \"for IntervalDtype\"\n                )\n                raise TypeError(msg)\n            self._subtype = subtype\n            self._closed = closed\n\n    @cache_readonly\n    def _can_hold_na(self) -> bool:\n        subtype = self._subtype\n        if subtype is None:\n            # partially-initialized\n            raise NotImplementedError(\n                \"_can_hold_na is not defined for partially-initialized IntervalDtype\"\n            )\n        if subtype.kind in \"iu\":\n            return False\n        return True\n\n    @property\n    def closed(self) -> IntervalClosedType:\n        return self._closed  # type: ignore[return-value]\n\n    @property\n    def subtype(self):\n        \"\"\"\n        The dtype of the Interval bounds.\n\n        See Also\n        --------\n        IntervalDtype: An ExtensionDtype for Interval data.\n\n        Examples\n        --------\n        >>> dtype = pd.IntervalDtype(subtype=\"int64\", closed=\"both\")\n        >>> dtype.subtype\n        dtype('int64')\n        \"\"\"\n        return self._subtype\n\n    @classmethod\n    def construct_array_type(cls) -> type[IntervalArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import IntervalArray\n\n        return IntervalArray\n\n    @classmethod\n    def construct_from_string(cls, string: str_type) -> IntervalDtype:\n        \"\"\"\n        attempt to construct this type from a string, raise a TypeError\n        if its not possible\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n\n        if string.lower() == \"interval\" or cls._match.search(string) is not None:\n            return cls(string)\n\n        msg = (\n            f\"Cannot construct a 'IntervalDtype' from '{string}'.\\n\\n\"\n            \"Incorrectly formatted string passed to constructor. \"\n            \"Valid formats include Interval or Interval[dtype] \"\n            \"where dtype is numeric, datetime, or timedelta\"\n        )\n        raise TypeError(msg)\n\n    @property\n    def type(self) -> type[Interval]:\n        return Interval\n\n    def __str__(self) -> str_type:\n        if self.subtype is None:\n            return \"interval\"\n        if self.closed is None:\n            # Only partially initialized GH#38394\n            return f\"interval[{self.subtype}]\"\n        return f\"interval[{self.subtype}, {self.closed}]\"\n\n    def __hash__(self) -> int:\n        # make myself hashable\n        return hash(str(self))\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, str):\n            return other.lower() in (self.name.lower(), str(self).lower())\n        elif not isinstance(other, IntervalDtype):\n            return False\n        elif self.subtype is None or other.subtype is None:\n            # None should match any subtype\n            return True\n        elif self.closed != other.closed:\n            return False\n        else:\n            return self.subtype == other.subtype\n\n    def __setstate__(self, state) -> None:\n        # for pickle compat. __get_state__ is defined in the\n        # PandasExtensionDtype superclass and uses the public properties to\n        # pickle -> need to set the settable private ones here (see GH26067)\n        self._subtype = state[\"subtype\"]\n\n        # backward-compat older pickles won't have \"closed\" key\n        self._closed = state.pop(\"closed\", None)\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        \"\"\"\n        Return a boolean if we if the passed type is an actual dtype that we\n        can match (via string or type)\n        \"\"\"\n        if isinstance(dtype, str):\n            if dtype.lower().startswith(\"interval\"):\n                try:\n                    return cls.construct_from_string(dtype) is not None\n                except (ValueError, TypeError):\n                    return False\n            else:\n                return False\n        return super().is_dtype(dtype)\n\n    def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> IntervalArray:\n        \"\"\"\n        Construct IntervalArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        import pyarrow\n\n        from pandas.core.arrays import IntervalArray\n\n        if isinstance(array, pyarrow.Array):\n            chunks = [array]\n        else:\n            chunks = array.chunks\n\n        results = []\n        for arr in chunks:\n            if isinstance(arr, pyarrow.ExtensionArray):\n                arr = arr.storage\n            left = np.asarray(arr.field(\"left\"), dtype=self.subtype)\n            right = np.asarray(arr.field(\"right\"), dtype=self.subtype)\n            iarr = IntervalArray.from_arrays(left, right, closed=self.closed)\n            results.append(iarr)\n\n        if not results:\n            return IntervalArray.from_arrays(\n                np.array([], dtype=self.subtype),\n                np.array([], dtype=self.subtype),\n                closed=self.closed,\n            )\n        return IntervalArray._concat_same_type(results)\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        if not all(isinstance(x, IntervalDtype) for x in dtypes):\n            return None\n\n        closed = cast(\"IntervalDtype\", dtypes[0]).closed\n        if not all(cast(\"IntervalDtype\", x).closed == closed for x in dtypes):\n            return np.dtype(object)\n\n        from pandas.core.dtypes.cast import find_common_type\n\n        common = find_common_type([cast(\"IntervalDtype\", x).subtype for x in dtypes])\n        if common == object:\n            return np.dtype(object)\n        return IntervalDtype(common, closed=closed)\n\n    @cache_readonly\n    def index_class(self) -> type_t[IntervalIndex]:\n        from pandas import IntervalIndex\n\n        return IntervalIndex\n\n\nclass NumpyEADtype(ExtensionDtype):\n    \"\"\"\n    A Pandas ExtensionDtype for NumPy dtypes.\n\n    This is mostly for internal compatibility, and is not especially\n    useful on its own.\n\n    Parameters\n    ----------\n    dtype : object\n        Object to be converted to a NumPy data type object.\n\n    See Also\n    --------\n    numpy.dtype\n    \"\"\"\n\n    _metadata = (\"_dtype\",)\n    _supports_2d = False\n    _can_fast_transpose = False\n\n    def __init__(self, dtype: npt.DTypeLike | NumpyEADtype | None) -> None:\n        if isinstance(dtype, NumpyEADtype):\n            # make constructor idempotent\n            dtype = dtype.numpy_dtype\n        self._dtype = np.dtype(dtype)\n\n    def __repr__(self) -> str:\n        return f\"NumpyEADtype({self.name!r})\"\n\n    @property\n    def numpy_dtype(self) -> np.dtype:\n        \"\"\"\n        The NumPy dtype this NumpyEADtype wraps.\n        \"\"\"\n        return self._dtype\n\n    @property\n    def name(self) -> str:\n        \"\"\"\n        A bit-width name for this data-type.\n        \"\"\"\n        return self._dtype.name\n\n    @property\n    def type(self) -> type[np.generic]:\n        \"\"\"\n        The type object used to instantiate a scalar of this NumPy data-type.\n        \"\"\"\n        return self._dtype.type\n\n    @property\n    def _is_numeric(self) -> bool:\n        # exclude object, str, unicode, void.\n        return self.kind in set(\"biufc\")\n\n    @property\n    def _is_boolean(self) -> bool:\n        return self.kind == \"b\"\n\n    @classmethod\n    def construct_from_string(cls, string: str) -> NumpyEADtype:\n        try:\n            dtype = np.dtype(string)\n        except TypeError as err:\n            if not isinstance(string, str):\n                msg = f\"'construct_from_string' expects a string, got {type(string)}\"\n            else:\n                msg = f\"Cannot construct a 'NumpyEADtype' from '{string}'\"\n            raise TypeError(msg) from err\n        return cls(dtype)\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[NumpyExtensionArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays import NumpyExtensionArray\n\n        return NumpyExtensionArray\n\n    @property\n    def kind(self) -> str:\n        \"\"\"\n        A character code (one of 'biufcmMOSUV') identifying the general kind of data.\n        \"\"\"\n        return self._dtype.kind\n\n    @property\n    def itemsize(self) -> int:\n        \"\"\"\n        The element size of this data-type object.\n        \"\"\"\n        return self._dtype.itemsize\n\n\nclass BaseMaskedDtype(ExtensionDtype):\n    \"\"\"\n    Base class for dtypes for BaseMaskedArray subclasses.\n    \"\"\"\n\n    base = None\n    type: type\n    _internal_fill_value: Scalar\n\n    @property\n    def _truthy_value(self):\n        # Fill values used for 'any'\n        if self.kind == \"f\":\n            return 1.0\n        if self.kind in \"iu\":\n            return 1\n        return True\n\n    @property\n    def _falsey_value(self):\n        # Fill values used for 'all'\n        if self.kind == \"f\":\n            return 0.0\n        if self.kind in \"iu\":\n            return 0\n        return False\n\n    @property\n    def na_value(self) -> libmissing.NAType:\n        return libmissing.NA\n\n    @cache_readonly\n    def numpy_dtype(self) -> np.dtype:\n        \"\"\"Return an instance of our numpy dtype\"\"\"\n        return np.dtype(self.type)\n\n    @cache_readonly\n    def kind(self) -> str:\n        return self.numpy_dtype.kind\n\n    @cache_readonly\n    def itemsize(self) -> int:\n        \"\"\"Return the number of bytes in this dtype\"\"\"\n        return self.numpy_dtype.itemsize\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[BaseMaskedArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def from_numpy_dtype(cls, dtype: np.dtype) -> BaseMaskedDtype:\n        \"\"\"\n        Construct the MaskedDtype corresponding to the given numpy dtype.\n        \"\"\"\n        if dtype.kind == \"b\":\n            from pandas.core.arrays.boolean import BooleanDtype\n\n            return BooleanDtype()\n        elif dtype.kind in \"iu\":\n            from pandas.core.arrays.integer import NUMPY_INT_TO_DTYPE\n\n            return NUMPY_INT_TO_DTYPE[dtype]\n        elif dtype.kind == \"f\":\n            from pandas.core.arrays.floating import NUMPY_FLOAT_TO_DTYPE\n\n            return NUMPY_FLOAT_TO_DTYPE[dtype]\n        else:\n            raise NotImplementedError(dtype)\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        # We unwrap any masked dtypes, find the common dtype we would use\n        #  for that, then re-mask the result.\n        from pandas.core.dtypes.cast import find_common_type\n\n        new_dtype = find_common_type(\n            [\n                dtype.numpy_dtype if isinstance(dtype, BaseMaskedDtype) else dtype\n                for dtype in dtypes\n            ]\n        )\n        if not isinstance(new_dtype, np.dtype):\n            # If we ever support e.g. Masked[DatetimeArray] then this will change\n            return None\n        try:\n            return type(self).from_numpy_dtype(new_dtype)\n        except (KeyError, NotImplementedError):\n            return None\n\n\n@register_extension_dtype\nclass SparseDtype(ExtensionDtype):\n    \"\"\"\n    Dtype for data stored in :class:`SparseArray`.\n\n    ``SparseDtype`` is used as the data type for :class:`SparseArray`, enabling\n    more efficient storage of data that contains a significant number of\n    repetitive values typically represented by a fill value. It supports any\n    scalar dtype as the underlying data type of the non-fill values.\n\n    Parameters\n    ----------\n    dtype : str, ExtensionDtype, numpy.dtype, type, default numpy.float64\n        The dtype of the underlying array storing the non-fill value values.\n    fill_value : scalar, optional\n        The scalar value not stored in the SparseArray. By default, this\n        depends on ``dtype``.\n\n        =========== ==========\n        dtype       na_value\n        =========== ==========\n        float       ``np.nan``\n        complex     ``np.nan``\n        int         ``0``\n        bool        ``False``\n        datetime64  ``pd.NaT``\n        timedelta64 ``pd.NaT``\n        =========== ==========\n\n        The default value may be overridden by specifying a ``fill_value``.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    arrays.SparseArray : The array structure that uses SparseDtype\n        for data representation.\n\n    Examples\n    --------\n    >>> ser = pd.Series([1, 0, 0], dtype=pd.SparseDtype(dtype=int, fill_value=0))\n    >>> ser\n    0    1\n    1    0\n    2    0\n    dtype: Sparse[int64, 0]\n    >>> ser.sparse.density\n    0.3333333333333333\n    \"\"\"\n\n    _is_immutable = True\n\n    # We include `_is_na_fill_value` in the metadata to avoid hash collisions\n    # between SparseDtype(float, 0.0) and SparseDtype(float, nan).\n    # Without is_na_fill_value in the comparison, those would be equal since\n    # hash(nan) is (sometimes?) 0.\n    _metadata = (\"_dtype\", \"_fill_value\", \"_is_na_fill_value\")\n\n    def __init__(self, dtype: Dtype = np.float64, fill_value: Any = None) -> None:\n        if isinstance(dtype, type(self)):\n            if fill_value is None:\n                fill_value = dtype.fill_value\n            dtype = dtype.subtype\n\n        from pandas.core.dtypes.common import (\n            is_string_dtype,\n            pandas_dtype,\n        )\n        from pandas.core.dtypes.missing import na_value_for_dtype\n\n        dtype = pandas_dtype(dtype)\n        if is_string_dtype(dtype):\n            dtype = np.dtype(\"object\")\n        if not isinstance(dtype, np.dtype):\n            # GH#53160\n            raise TypeError(\"SparseDtype subtype must be a numpy dtype\")\n\n        if fill_value is None:\n            fill_value = na_value_for_dtype(dtype)\n\n        self._dtype = dtype\n        self._fill_value = fill_value\n        self._check_fill_value()\n\n    def __hash__(self) -> int:\n        # Python3 doesn't inherit __hash__ when a base class overrides\n        # __eq__, so we explicitly do it here.\n        return super().__hash__()\n\n    def __eq__(self, other: object) -> bool:\n        # We have to override __eq__ to handle NA values in _metadata.\n        # The base class does simple == checks, which fail for NA.\n        if isinstance(other, str):\n            try:\n                other = self.construct_from_string(other)\n            except TypeError:\n                return False\n\n        if isinstance(other, type(self)):\n            subtype = self.subtype == other.subtype\n            if self._is_na_fill_value or other._is_na_fill_value:\n                # this case is complicated by two things:\n                # SparseDtype(float, float(nan)) == SparseDtype(float, np.nan)\n                # SparseDtype(float, np.nan)     != SparseDtype(float, pd.NaT)\n                # i.e. we want to treat any floating-point NaN as equal, but\n                # not a floating-point NaN and a datetime NaT.\n                fill_value = isinstance(\n                    self.fill_value, type(other.fill_value)\n                ) or isinstance(other.fill_value, type(self.fill_value))\n            else:\n                with warnings.catch_warnings():\n                    # Ignore spurious numpy warning\n                    warnings.filterwarnings(\n                        \"ignore\",\n                        \"elementwise comparison failed\",\n                        category=DeprecationWarning,\n                    )\n\n                    fill_value = self.fill_value == other.fill_value\n\n            return subtype and fill_value\n        return False\n\n    @property\n    def fill_value(self):\n        \"\"\"\n        The fill value of the array.\n\n        Converting the SparseArray to a dense ndarray will fill the\n        array with this value.\n\n        .. warning::\n\n           It's possible to end up with a SparseArray that has ``fill_value``\n           values in ``sp_values``. This can occur, for example, when setting\n           ``SparseArray.fill_value`` directly.\n        \"\"\"\n        return self._fill_value\n\n    def _check_fill_value(self) -> None:\n        if not lib.is_scalar(self._fill_value):\n            raise ValueError(\n                f\"fill_value must be a scalar. Got {self._fill_value} instead\"\n            )\n\n        from pandas.core.dtypes.cast import can_hold_element\n        from pandas.core.dtypes.missing import (\n            is_valid_na_for_dtype,\n            isna,\n        )\n\n        from pandas.core.construction import ensure_wrapped_if_datetimelike\n\n        # GH#23124 require fill_value and subtype to match\n        val = self._fill_value\n        if isna(val):\n            if not is_valid_na_for_dtype(val, self.subtype):\n                raise ValueError(\n                    # GH#53043\n                    \"fill_value must be a valid value for the SparseDtype.subtype\"\n                )\n        else:\n            dummy = np.empty(0, dtype=self.subtype)\n            dummy = ensure_wrapped_if_datetimelike(dummy)\n\n            if not can_hold_element(dummy, val):\n                raise ValueError(\n                    # GH#53043\n                    \"fill_value must be a valid value for the SparseDtype.subtype\"\n                )\n\n    @property\n    def _is_na_fill_value(self) -> bool:\n        from pandas import isna\n\n        return isna(self.fill_value)\n\n    @property\n    def _is_numeric(self) -> bool:\n        return not self.subtype == object\n\n    @property\n    def _is_boolean(self) -> bool:\n        return self.subtype.kind == \"b\"\n\n    @property\n    def kind(self) -> str:\n        \"\"\"\n        The sparse kind. Either 'integer', or 'block'.\n        \"\"\"\n        return self.subtype.kind\n\n    @property\n    def type(self):\n        return self.subtype.type\n\n    @property\n    def subtype(self):\n        return self._dtype\n\n    @property\n    def name(self) -> str:\n        return f\"Sparse[{self.subtype.name}, {self.fill_value!r}]\"\n\n    def __repr__(self) -> str:\n        return self.name\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[SparseArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays.sparse.array import SparseArray\n\n        return SparseArray\n\n    @classmethod\n    def construct_from_string(cls, string: str) -> SparseDtype:\n        \"\"\"\n        Construct a SparseDtype from a string form.\n\n        Parameters\n        ----------\n        string : str\n            Can take the following forms.\n\n            string           dtype\n            ================ ============================\n            'int'            SparseDtype[np.int64, 0]\n            'Sparse'         SparseDtype[np.float64, nan]\n            'Sparse[int]'    SparseDtype[np.int64, 0]\n            'Sparse[int, 0]' SparseDtype[np.int64, 0]\n            ================ ============================\n\n            It is not possible to specify non-default fill values\n            with a string. An argument like ``'Sparse[int, 1]'``\n            will raise a ``TypeError`` because the default fill value\n            for integers is 0.\n\n        Returns\n        -------\n        SparseDtype\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n        msg = f\"Cannot construct a 'SparseDtype' from '{string}'\"\n        if string.startswith(\"Sparse\"):\n            try:\n                sub_type, has_fill_value = cls._parse_subtype(string)\n            except ValueError as err:\n                raise TypeError(msg) from err\n            else:\n                result = SparseDtype(sub_type)\n                msg = (\n                    f\"Cannot construct a 'SparseDtype' from '{string}'.\\n\\nIt \"\n                    \"looks like the fill_value in the string is not \"\n                    \"the default for the dtype. Non-default fill_values \"\n                    \"are not supported. Use the 'SparseDtype()' \"\n                    \"constructor instead.\"\n                )\n                if has_fill_value and str(result) != string:\n                    raise TypeError(msg)\n                return result\n        else:\n            raise TypeError(msg)\n\n    @staticmethod\n    def _parse_subtype(dtype: str) -> tuple[str, bool]:\n        \"\"\"\n        Parse a string to get the subtype\n\n        Parameters\n        ----------\n        dtype : str\n            A string like\n\n            * Sparse[subtype]\n            * Sparse[subtype, fill_value]\n\n        Returns\n        -------\n        subtype : str\n\n        Raises\n        ------\n        ValueError\n            When the subtype cannot be extracted.\n        \"\"\"\n        xpr = re.compile(r\"Sparse\\[(?P<subtype>[^,]*)(, )?(?P<fill_value>.*?)?\\]$\")\n        m = xpr.match(dtype)\n        has_fill_value = False\n        if m:\n            subtype = m.groupdict()[\"subtype\"]\n            has_fill_value = bool(m.groupdict()[\"fill_value\"])\n        elif dtype == \"Sparse\":\n            subtype = \"float64\"\n        else:\n            raise ValueError(f\"Cannot parse {dtype}\")\n        return subtype, has_fill_value\n\n    @classmethod\n    def is_dtype(cls, dtype: object) -> bool:\n        dtype = getattr(dtype, \"dtype\", dtype)\n        if isinstance(dtype, str) and dtype.startswith(\"Sparse\"):\n            sub_type, _ = cls._parse_subtype(dtype)\n            dtype = np.dtype(sub_type)\n        elif isinstance(dtype, cls):\n            return True\n        return isinstance(dtype, np.dtype) or dtype == \"Sparse\"\n\n    def update_dtype(self, dtype) -> SparseDtype:\n        \"\"\"\n        Convert the SparseDtype to a new dtype.\n\n        This takes care of converting the ``fill_value``.\n\n        Parameters\n        ----------\n        dtype : Union[str, numpy.dtype, SparseDtype]\n            The new dtype to use.\n\n            * For a SparseDtype, it is simply returned\n            * For a NumPy dtype (or str), the current fill value\n              is converted to the new dtype, and a SparseDtype\n              with `dtype` and the new fill value is returned.\n\n        Returns\n        -------\n        SparseDtype\n            A new SparseDtype with the correct `dtype` and fill value\n            for that `dtype`.\n\n        Raises\n        ------\n        ValueError\n            When the current fill value cannot be converted to the\n            new `dtype` (e.g. trying to convert ``np.nan`` to an\n            integer dtype).\n\n\n        Examples\n        --------\n        >>> SparseDtype(int, 0).update_dtype(float)\n        Sparse[float64, 0.0]\n\n        >>> SparseDtype(int, 1).update_dtype(SparseDtype(float, np.nan))\n        Sparse[float64, nan]\n        \"\"\"\n        from pandas.core.dtypes.astype import astype_array\n        from pandas.core.dtypes.common import pandas_dtype\n\n        cls = type(self)\n        dtype = pandas_dtype(dtype)\n\n        if not isinstance(dtype, cls):\n            if not isinstance(dtype, np.dtype):\n                raise TypeError(\"sparse arrays of extension dtypes not supported\")\n\n            fv_asarray = np.atleast_1d(np.array(self.fill_value))\n            fvarr = astype_array(fv_asarray, dtype)\n            # NB: not fv_0d.item(), as that casts dt64->int\n            fill_value = fvarr[0]\n            dtype = cls(dtype, fill_value=fill_value)\n\n        return dtype\n\n    @property\n    def _subtype_with_str(self):\n        \"\"\"\n        Whether the SparseDtype's subtype should be considered ``str``.\n\n        Typically, pandas will store string data in an object-dtype array.\n        When converting values to a dtype, e.g. in ``.astype``, we need to\n        be more specific, we need the actual underlying type.\n\n        Returns\n        -------\n        >>> SparseDtype(int, 1)._subtype_with_str\n        dtype('int64')\n\n        >>> SparseDtype(object, 1)._subtype_with_str\n        dtype('O')\n\n        >>> dtype = SparseDtype(str, \"\")\n        >>> dtype.subtype\n        dtype('O')\n\n        >>> dtype._subtype_with_str\n        <class 'str'>\n        \"\"\"\n        if isinstance(self.fill_value, str):\n            return type(self.fill_value)\n        return self.subtype\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        # TODO for now only handle SparseDtypes and numpy dtypes => extend\n        # with other compatible extension dtypes\n        from pandas.core.dtypes.cast import np_find_common_type\n\n        if any(\n            isinstance(x, ExtensionDtype) and not isinstance(x, SparseDtype)\n            for x in dtypes\n        ):\n            return None\n\n        fill_values = [x.fill_value for x in dtypes if isinstance(x, SparseDtype)]\n        fill_value = fill_values[0]\n\n        from pandas import isna\n\n        # np.nan isn't a singleton, so we may end up with multiple\n        # NaNs here, so we ignore the all NA case too.\n        if get_option(\"performance_warnings\") and (\n            not (len(set(fill_values)) == 1 or isna(fill_values).all())\n        ):\n            warnings.warn(\n                \"Concatenating sparse arrays with multiple fill \"\n                f\"values: '{fill_values}'. Picking the first and \"\n                \"converting the rest.\",\n                PerformanceWarning,\n                stacklevel=find_stack_level(),\n            )\n\n        np_dtypes = (x.subtype if isinstance(x, SparseDtype) else x for x in dtypes)\n        return SparseDtype(np_find_common_type(*np_dtypes), fill_value=fill_value)\n\n\n@register_extension_dtype\nclass ArrowDtype(StorageExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for PyArrow data types.\n\n    .. warning::\n\n       ArrowDtype is considered experimental. The implementation and\n       parts of the API may change without warning.\n\n    While most ``dtype`` arguments can accept the \"string\"\n    constructor, e.g. ``\"int64[pyarrow]\"``, ArrowDtype is useful\n    if the data type contains parameters like ``pyarrow.timestamp``.\n\n    Parameters\n    ----------\n    pyarrow_dtype : pa.DataType\n        An instance of a `pyarrow.DataType <https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions>`__.\n\n    Attributes\n    ----------\n    pyarrow_dtype\n\n    Methods\n    -------\n    None\n\n    Returns\n    -------\n    ArrowDtype\n\n    See Also\n    --------\n    DataFrame.convert_dtypes : Convert columns to the best possible dtypes.\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> pd.ArrowDtype(pa.int64())\n    int64[pyarrow]\n\n    Types with parameters must be constructed with ArrowDtype.\n\n    >>> pd.ArrowDtype(pa.timestamp(\"s\", tz=\"America/New_York\"))\n    timestamp[s, tz=America/New_York][pyarrow]\n    >>> pd.ArrowDtype(pa.list_(pa.int64()))\n    list<item: int64>[pyarrow]\n    \"\"\"\n\n    _metadata = (\"storage\", \"pyarrow_dtype\")  # type: ignore[assignment]\n\n    def __init__(self, pyarrow_dtype: pa.DataType) -> None:\n        super().__init__(\"pyarrow\")\n        if pa_version_under10p1:\n            raise ImportError(\"pyarrow>=10.0.1 is required for ArrowDtype\")\n        if not isinstance(pyarrow_dtype, pa.DataType):\n            raise ValueError(\n                f\"pyarrow_dtype ({pyarrow_dtype}) must be an instance \"\n                f\"of a pyarrow.DataType. Got {type(pyarrow_dtype)} instead.\"\n            )\n        self.pyarrow_dtype = pyarrow_dtype\n\n    def __repr__(self) -> str:\n        return self.name\n\n    def __hash__(self) -> int:\n        # make myself hashable\n        return hash(str(self))\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, type(self)):\n            return super().__eq__(other)\n        return self.pyarrow_dtype == other.pyarrow_dtype\n\n    @property\n    def type(self):\n        \"\"\"\n        Returns associated scalar type.\n        \"\"\"\n        pa_type = self.pyarrow_dtype\n        if pa.types.is_integer(pa_type):\n            return int\n        elif pa.types.is_floating(pa_type):\n            return float\n        elif pa.types.is_string(pa_type) or pa.types.is_large_string(pa_type):\n            return str\n        elif (\n            pa.types.is_binary(pa_type)\n            or pa.types.is_fixed_size_binary(pa_type)\n            or pa.types.is_large_binary(pa_type)\n        ):\n            return bytes\n        elif pa.types.is_boolean(pa_type):\n            return bool\n        elif pa.types.is_duration(pa_type):\n            if pa_type.unit == \"ns\":\n                return Timedelta\n            else:\n                return timedelta\n        elif pa.types.is_timestamp(pa_type):\n            if pa_type.unit == \"ns\":\n                return Timestamp\n            else:\n                return datetime\n        elif pa.types.is_date(pa_type):\n            return date\n        elif pa.types.is_time(pa_type):\n            return time\n        elif pa.types.is_decimal(pa_type):\n            return Decimal\n        elif pa.types.is_dictionary(pa_type):\n            # TODO: Potentially change this & CategoricalDtype.type to\n            #  something more representative of the scalar\n            return CategoricalDtypeType\n        elif pa.types.is_list(pa_type) or pa.types.is_large_list(pa_type):\n            return list\n        elif pa.types.is_fixed_size_list(pa_type):\n            return list\n        elif pa.types.is_map(pa_type):\n            return list\n        elif pa.types.is_struct(pa_type):\n            return dict\n        elif pa.types.is_null(pa_type):\n            # TODO: None? pd.NA? pa.null?\n            return type(pa_type)\n        elif isinstance(pa_type, pa.ExtensionType):\n            return type(self)(pa_type.storage_type).type\n        raise NotImplementedError(pa_type)\n\n    @property\n    def name(self) -> str:  # type: ignore[override]\n        \"\"\"\n        A string identifying the data type.\n        \"\"\"\n        return f\"{self.pyarrow_dtype!s}[{self.storage}]\"\n\n    @cache_readonly\n    def numpy_dtype(self) -> np.dtype:\n        \"\"\"Return an instance of the related numpy dtype\"\"\"\n        if pa.types.is_timestamp(self.pyarrow_dtype):\n            # pa.timestamp(unit).to_pandas_dtype() returns ns units\n            # regardless of the pyarrow timestamp units.\n            # This can be removed if/when pyarrow addresses it:\n            # https://github.com/apache/arrow/issues/34462\n            return np.dtype(f\"datetime64[{self.pyarrow_dtype.unit}]\")\n        if pa.types.is_duration(self.pyarrow_dtype):\n            # pa.duration(unit).to_pandas_dtype() returns ns units\n            # regardless of the pyarrow duration units\n            # This can be removed if/when pyarrow addresses it:\n            # https://github.com/apache/arrow/issues/34462\n            return np.dtype(f\"timedelta64[{self.pyarrow_dtype.unit}]\")\n        if pa.types.is_string(self.pyarrow_dtype) or pa.types.is_large_string(\n            self.pyarrow_dtype\n        ):\n            # pa.string().to_pandas_dtype() = object which we don't want\n            return np.dtype(str)\n        try:\n            return np.dtype(self.pyarrow_dtype.to_pandas_dtype())\n        except (NotImplementedError, TypeError):\n            return np.dtype(object)\n\n    @cache_readonly\n    def kind(self) -> str:\n        if pa.types.is_timestamp(self.pyarrow_dtype):\n            # To mirror DatetimeTZDtype\n            return \"M\"\n        return self.numpy_dtype.kind\n\n    @cache_readonly\n    def itemsize(self) -> int:\n        \"\"\"Return the number of bytes in this dtype\"\"\"\n        return self.numpy_dtype.itemsize\n\n    @classmethod\n    def construct_array_type(cls) -> type_t[ArrowExtensionArray]:\n        \"\"\"\n        Return the array type associated with this dtype.\n\n        Returns\n        -------\n        type\n        \"\"\"\n        from pandas.core.arrays.arrow import ArrowExtensionArray\n\n        return ArrowExtensionArray\n\n    @classmethod\n    def construct_from_string(cls, string: str) -> ArrowDtype:\n        \"\"\"\n        Construct this type from a string.\n\n        Parameters\n        ----------\n        string : str\n            string should follow the format f\"{pyarrow_type}[pyarrow]\"\n            e.g. int64[pyarrow]\n        \"\"\"\n        if not isinstance(string, str):\n            raise TypeError(\n                f\"'construct_from_string' expects a string, got {type(string)}\"\n            )\n        if not string.endswith(\"[pyarrow]\"):\n            raise TypeError(f\"'{string}' must end with '[pyarrow]'\")\n        if string == \"string[pyarrow]\":\n            # Ensure Registry.find skips ArrowDtype to use StringDtype instead\n            raise TypeError(\"string[pyarrow] should be constructed by StringDtype\")\n\n        base_type = string[:-9]  # get rid of \"[pyarrow]\"\n        try:\n            pa_dtype = pa.type_for_alias(base_type)\n        except ValueError as err:\n            has_parameters = re.search(r\"[\\[\\(].*[\\]\\)]\", base_type)\n            if has_parameters:\n                # Fallback to try common temporal types\n                try:\n                    return cls._parse_temporal_dtype_string(base_type)\n                except (NotImplementedError, ValueError):\n                    # Fall through to raise with nice exception message below\n                    pass\n\n                raise NotImplementedError(\n                    \"Passing pyarrow type specific parameters \"\n                    f\"({has_parameters.group()}) in the string is not supported. \"\n                    \"Please construct an ArrowDtype object with a pyarrow_dtype \"\n                    \"instance with specific parameters.\"\n                ) from err\n            raise TypeError(f\"'{base_type}' is not a valid pyarrow data type.\") from err\n        return cls(pa_dtype)\n\n    # TODO(arrow#33642): This can be removed once supported by pyarrow\n    @classmethod\n    def _parse_temporal_dtype_string(cls, string: str) -> ArrowDtype:\n        \"\"\"\n        Construct a temporal ArrowDtype from string.\n        \"\"\"\n        # we assume\n        #  1) \"[pyarrow]\" has already been stripped from the end of our string.\n        #  2) we know \"[\" is present\n        head, tail = string.split(\"[\", 1)\n\n        if not tail.endswith(\"]\"):\n            raise ValueError\n        tail = tail[:-1]\n\n        if head == \"timestamp\":\n            assert \",\" in tail  # otherwise type_for_alias should work\n            unit, tz = tail.split(\",\", 1)\n            unit = unit.strip()\n            tz = tz.strip()\n            if tz.startswith(\"tz=\"):\n                tz = tz[3:]\n\n            pa_type = pa.timestamp(unit, tz=tz)\n            dtype = cls(pa_type)\n            return dtype\n\n        raise NotImplementedError(string)\n\n    @property\n    def _is_numeric(self) -> bool:\n        \"\"\"\n        Whether columns with this dtype should be considered numeric.\n        \"\"\"\n        # TODO: pa.types.is_boolean?\n        return (\n            pa.types.is_integer(self.pyarrow_dtype)\n            or pa.types.is_floating(self.pyarrow_dtype)\n            or pa.types.is_decimal(self.pyarrow_dtype)\n        )\n\n    @property\n    def _is_boolean(self) -> bool:\n        \"\"\"\n        Whether this dtype should be considered boolean.\n        \"\"\"\n        return pa.types.is_boolean(self.pyarrow_dtype)\n\n    def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:\n        # We unwrap any masked dtypes, find the common dtype we would use\n        #  for that, then re-mask the result.\n        # Mirrors BaseMaskedDtype\n        from pandas.core.dtypes.cast import find_common_type\n\n        null_dtype = type(self)(pa.null())\n\n        new_dtype = find_common_type(\n            [\n                dtype.numpy_dtype if isinstance(dtype, ArrowDtype) else dtype\n                for dtype in dtypes\n                if dtype != null_dtype\n            ]\n        )\n        if not isinstance(new_dtype, np.dtype):\n            return None\n        try:\n            pa_dtype = pa.from_numpy_dtype(new_dtype)\n            return type(self)(pa_dtype)\n        except NotImplementedError:\n            return None\n\n    def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> ArrowExtensionArray:\n        \"\"\"\n        Construct IntegerArray/FloatingArray from pyarrow Array/ChunkedArray.\n        \"\"\"\n        array_class = self.construct_array_type()\n        arr = array.cast(self.pyarrow_dtype, safe=True)\n        return array_class(arr)\n"
    }
  ]
}