{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "27781",
  "issue_description": "# Filtering dataframe with sparse column leads to NAs in sparse column\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf1 = pd.DataFrame({\"A\": pd.SparseArray([0, 0, 0]), 'B': [1,2,3]})\r\n# df1_filtered will have NAs in column A\r\ndf1_filtered = df1.loc[df1['B'] != 2]\r\n\r\ndf2 = pd.DataFrame({\"A\": pd.SparseArray([0, 1, 0]), 'B': [1,2,3]})\r\n# df2_filtered has no NAs in column A\r\ndf2_filtered = df2.loc[df2['B'] != 2]\r\n```\r\nwhere `df1_filtered` will look like\r\n```\r\n\tA\tB\r\n0\tNaN\t1\r\n2\tNaN\t3\r\n```\r\nand `df2_filtered` like\r\n```\r\n\tA\tB\r\n0\t0\t1\r\n2\t0\t3\r\n```\r\n#### Problem description\r\n\r\nFiltering a dataframe with an all-zero sparse column can lead to NAs in the sparse column.\r\n\r\n#### Expected Output\r\n\r\nBoth data frames should be the same, as filtering a dataframe with non-missing data should not lead to missing data.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\n\r\npandas           : 0.25.0\r\nnumpy            : 1.16.2\r\npytz             : 2019.1\r\ndateutil         : 2.8.0\r\npip              : 19.2.1\r\nsetuptools       : 39.1.0\r\nCython           : None\r\npytest           : 4.3.1\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : 2.8.3 (dt dec pq3 ext lo64)\r\njinja2           : 2.10.1\r\nIPython          : 7.6.1\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : 0.2.1\r\nlxml.etree       : None\r\nmatplotlib       : 3.1.1\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 0.13.0\r\npytables         : None\r\ns3fs             : 0.2.0\r\nscipy            : 1.2.1\r\nsqlalchemy       : 1.3.5\r\ntables           : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\n</details>\r\n",
  "issue_comments": [
    {
      "id": 518837184,
      "user": "jorisvandenbossche",
      "body": "@stelsemeyer Thanks for the report! \r\nThis seems to be a regression compared to 0.23 (compared to SparseSeries).\r\n\r\nThis is a bug in the `SparseArray.take` implementation, when `allow_fill=True` it uses a wrong fill value (nan instead of 0):\r\n\r\n```\r\nIn [3]: df1.A.array.take([0, 2]) \r\nOut[3]: \r\n[0, 0]\r\nFill: 0\r\nBlockIndex\r\nBlock locations: array([], dtype=int32)\r\nBlock lengths: array([], dtype=int32)\r\n\r\nIn [4]: df1.A.array.take([0, 2], allow_fill=True) \r\nOut[4]: \r\n[nan, nan]\r\nFill: 0\r\nIntIndex\r\nIndices: array([0, 1], dtype=int32)\r\n```\r\n\r\n(both above should give the same result)\r\n\r\nAlways welcome to take a look to see how it could be fixed!"
    },
    {
      "id": 519133539,
      "user": "stelsemeyer",
      "body": "@jorisvandenbossche: Thanks for investigating!\r\nI checked the SparseArray, a naive solution would be to use `self.fill_value` if `fill_value` is `None` in `_take_with_fill`, here: https://github.com/pandas-dev/pandas/blob/d1accd032b648c9affd6dce1f81feb9c99422483/pandas/core/arrays/sparse.py#L1173-L1174"
    },
    {
      "id": 522671612,
      "user": "TomAugspurger",
      "body": "Removing from the 0.25.1 milestone, but if anyone is working on this LMK and we can probably get it in.\r\n\r\n@stelsemeyer your proposal looks reasonable."
    },
    {
      "id": 532022720,
      "user": "scottgigante",
      "body": "I think this is a related issue:\r\n```\r\n>>> import pandas as pd\r\n>>> X = pd.DataFrame([[0,1,0], [1,0,0], [1,1,0]]).astype(pd.SparseDtype(float, fill_value=0.0))\r\n>>> X\r\n     0    1    2\r\n0  0.0  1.0  0.0\r\n1  1.0  0.0  0.0\r\n2  1.0  1.0  0.0\r\n>>> X.loc[0]\r\n0    0.0\r\n1    1.0\r\n2    0.0\r\nName: 0, dtype: Sparse[float64, 0.0]\r\n>>> X.loc[[0,1]]\r\n     0    1   2\r\n0  0.0  1.0 NaN\r\n1  1.0  0.0 NaN\r\n>>> X.iloc[[0,1]]\r\n     0    1   2\r\n0  0.0  1.0 NaN\r\n1  1.0  0.0 NaN\r\n```"
    },
    {
      "id": 533821915,
      "user": "scottgigante",
      "body": "I edited the proposed line, but to no avail. The error in @jorisvandenbossche's answer is resolved:\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> df1 = pd.DataFrame({\"A\": pd.SparseArray([0, 0, 0]), 'B': [1,2,3]})\r\n>>> df1.A.array.take([0, 2])\r\n[0, 0]\r\nFill: 0\r\nBlockIndex\r\nBlock locations: array([], dtype=int32)\r\nBlock lengths: array([], dtype=int32)\r\n>>> df1.A.array.take([0, 2], allow_fill=True)\r\n[0, 0]\r\nFill: 0\r\nIntIndex\r\nIndices: array([], dtype=int32)\r\n```\r\nbut my and @stelsemeyer's issues remain.\r\n```\r\n>>> df1.loc[df1['B'] != 2]\r\n    A  B\r\n0 NaN  1\r\n2 NaN  3\r\n>>> \r\n>>> X = pd.DataFrame([[0,1,0], [1,0,0], [1,1,0]]).astype(pd.SparseDtype(float, fill_value=0.0))\r\n>>> X.loc[[0,1]]\r\n     0    1   2\r\n0  0.0  1.0 NaN\r\n1  1.0  0.0 NaN\r\n```\r\n\r\nSeems to me that there is another problem here: \r\n\r\nhttps://github.com/pandas-dev/pandas/blob/a45760fd45b434caf9107bb19f1536636cc3fbd8/pandas/core/internals/managers.py#L1262\r\n\r\n```\r\n>>> blk = X._data.blocks[2]\r\n>>> blk.take_nd(indexer=np.array([0,1]), axis=1).values\r\n[0.0, 0.0]\r\nFill: 0.0\r\nIntIndex\r\nIndices: array([], dtype=int32)\r\n>>> blk.take_nd(indexer=np.array([0,1]), axis=1, fill_tuple=(blk.fill_value,)).values\r\n[nan, nan]\r\nFill: 0.0\r\nIntIndex\r\nIndices: array([0, 1], dtype=int32)\r\n```\r\n\r\nwhich is because of a discrepancy between `blk.fill_value` and `blk.dtype.fill_value`\r\n\r\n```\r\n>>> blk.fill_value\r\nnan\r\n>>> blk.dtype.fill_value\r\n0.0\r\n```\r\n\r\nI don't know if we should a) reference `blk.dtype.fill_value` or b) make `blk.dtype.fill_value` consistent with `blk.fill_value`."
    },
    {
      "id": 536139620,
      "user": "scottgigante",
      "body": "@TomAugspurger any thoughts on this? I'm happy to write the PR, just need some guidance."
    },
    {
      "id": 537715243,
      "user": "TomAugspurger",
      "body": "Mmm I'm not sure I understand the issue. But note that doing a `.take` which introduces missing values via a `-1` in the indices should result in a NaN in the output, regardless of the fill value. Not sure if that helps or not."
    },
    {
      "id": 538463297,
      "user": "scottgigante",
      "body": "Can you give an example of how/why that would happen? I don't understand quite how we should get a NaN when the fill_value is not nan."
    },
    {
      "id": 538486398,
      "user": "scottgigante",
      "body": "Here's my proposed solution:\r\n\r\nReplace\r\n```\r\n    @property\r\n    def fill_value(self):\r\n        # Used in reindex_indexer\r\n        return self.values.dtype.na_value\r\n```\r\nfrom https://github.com/pandas-dev/pandas/blob/master/pandas/core/internals/blocks.py#L1730 with\r\n```\r\n    @property\r\n    def fill_value(self):\r\n        # Used in reindex_indexer\r\n        try:\r\n            return self.values.dtype.fill_value\r\n        except AttributeError:\r\n            return self.values.dtype.na_value\r\n```\r\nThoughts?"
    },
    {
      "id": 565257697,
      "user": "akdor1154",
      "body": "Pretty sure my dataset is showing this. Seems to apply to some columns but not all.\r\n[strange_test.pickle.gz](https://github.com/pandas-dev/pandas/files/3958518/strange_test.pickle.gz)\r\n\r\nIf anyone wants a large real-world set to test this on:\r\n```python\r\nimport pandas as pd\r\ntest = pd.read_pickle('strange_test.pickle.gz')\r\nany(test.DistinctSKUs_SEPB.isna())\r\n> False\r\nany(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\n> True # !?!\r\n```"
    },
    {
      "id": 565319232,
      "user": "scottgigante",
      "body": "@akdor1154 can you try this monkey patch and see if it solves your issue?\r\n\r\n```\r\ndef fill_value(self):\r\n    # Used in reindex_indexer\r\n    try:\r\n        return self.values.dtype.fill_value\r\n    except AttributeError:\r\n        return self.values.dtype.na_value\r\n\r\nfrom pandas.core.internals.blocks import ExtensionBlock\r\n\r\nsetattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n```"
    },
    {
      "id": 616253132,
      "user": "mroeschke",
      "body": "As mentioned https://github.com/pandas-dev/pandas/issues/29321#issuecomment-551251530, it may be an issue as well when the sparse series matches the fill value"
    },
    {
      "id": 620863882,
      "user": "scottgigante",
      "body": "Pretty sure my monkey patch works. I can write a PR if I can get approval from @TomAugspurger or @jorisvandenbossche \r\n\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> df1 = pd.DataFrame({\"A\": pd.arrays.SparseArray([0, 0, 0]), 'B': [1,2,3]})\r\n>>> df1.loc[df1['B'] != 2]\r\n    A  B\r\n0 NaN  1\r\n2 NaN  3\r\n>>> def fill_value(self):\r\n...     # Used in reindex_indexer\r\n...     try:\r\n...         return self.values.dtype.fill_value\r\n...     except AttributeError:\r\n...         return self.values.dtype.na_value\r\n...\r\n>>> from pandas.core.internals.blocks import ExtensionBlock\r\n>>>\r\n>>> setattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n>>> df1.loc[df1['B'] != 2]\r\n   A  B\r\n0  0  1\r\n2  0  3\r\n```\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> X = pd.DataFrame([[0,1,0], [1,0,0], [1,1,0]]).astype(\r\n...     pd.SparseDtype(float, fill_value=0.0))\r\n>>> X.loc[[0,1]]\r\n     0    1   2\r\n0  0.0  1.0 NaN\r\n1  1.0  0.0 NaN\r\n>>> def fill_value(self):\r\n...     # Used in reindex_indexer\r\n...     try:\r\n...         return self.values.dtype.fill_value\r\n...     except AttributeError:\r\n...         return self.values.dtype.na_value\r\n...\r\n>>> from pandas.core.internals.blocks import ExtensionBlock\r\n>>>\r\n>>> setattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n>>> X.loc[[0,1]]\r\n     0    1    2\r\n0  0.0  1.0  0.0\r\n1  1.0  0.0  0.0\r\n```\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> test = pd.read_pickle('strange_test.pickle.gz')\r\n>>> any(test.DistinctSKUs_SEPB.isna())\r\nFalse\r\n>>> any(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\nTrue\r\n>>> def fill_value(self):\r\n...     # Used in reindex_indexer\r\n...     try:\r\n...         return self.values.dtype.fill_value\r\n...     except AttributeError:\r\n...         return self.values.dtype.na_value\r\n...\r\n>>> from pandas.core.internals.blocks import ExtensionBlock\r\n>>>\r\n>>> setattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n>>> any(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\nFalse\r\n```"
    },
    {
      "id": 620989041,
      "user": "akdor1154",
      "body": "@scottgigante sorry, from memory I tested it at the time and it worked, thanks."
    },
    {
      "id": 627791292,
      "user": "connesy",
      "body": "@scottgigante Just tested your monkey patch, it works for me in `pandas 1.0.2`."
    },
    {
      "id": 658793493,
      "user": "TomAugspurger",
      "body": "The fix here is being revereted in https://github.com/pandas-dev/pandas/pull/35287. Some discussion on a potential fix at https://github.com/pandas-dev/pandas/issues/35286#issuecomment-658788801.\r\n\r\ncc @scottgigante if you want to take another shot :)"
    },
    {
      "id": 877691655,
      "user": "mroeschke",
      "body": "This looks fixed on master. Could use a test\r\n\r\n```\r\nIn [4]: df1_filtered\r\nOut[4]:\r\n   A  B\r\n0  0  1\r\n2  0  3\r\n\r\nIn [5]: df2_filtered\r\nOut[5]:\r\n   A  B\r\n0  0  1\r\n2  0  3\r\n```"
    },
    {
      "id": 1447884273,
      "user": "EnerH",
      "body": "What about this:\r\n\r\n`df1_filtered['A'] = df1_filtered['A'].fillna(0)`\r\n\r\nSimilarly, to change the NaN values in column 'A' of df2_filtered to 0, you can use the same method:\r\n\r\n`df2_filtered['A'] = df2_filtered['A'].fillna(0)` \r\n\r\n"
    },
    {
      "id": 1489098221,
      "user": "ConnorMcKinley",
      "body": "Take"
    }
  ],
  "text_context": "# Filtering dataframe with sparse column leads to NAs in sparse column\n\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf1 = pd.DataFrame({\"A\": pd.SparseArray([0, 0, 0]), 'B': [1,2,3]})\r\n# df1_filtered will have NAs in column A\r\ndf1_filtered = df1.loc[df1['B'] != 2]\r\n\r\ndf2 = pd.DataFrame({\"A\": pd.SparseArray([0, 1, 0]), 'B': [1,2,3]})\r\n# df2_filtered has no NAs in column A\r\ndf2_filtered = df2.loc[df2['B'] != 2]\r\n```\r\nwhere `df1_filtered` will look like\r\n```\r\n\tA\tB\r\n0\tNaN\t1\r\n2\tNaN\t3\r\n```\r\nand `df2_filtered` like\r\n```\r\n\tA\tB\r\n0\t0\t1\r\n2\t0\t3\r\n```\r\n#### Problem description\r\n\r\nFiltering a dataframe with an all-zero sparse column can lead to NAs in the sparse column.\r\n\r\n#### Expected Output\r\n\r\nBoth data frames should be the same, as filtering a dataframe with non-missing data should not lead to missing data.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``pd.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : None\r\n\r\npandas           : 0.25.0\r\nnumpy            : 1.16.2\r\npytz             : 2019.1\r\ndateutil         : 2.8.0\r\npip              : 19.2.1\r\nsetuptools       : 39.1.0\r\nCython           : None\r\npytest           : 4.3.1\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : 2.8.3 (dt dec pq3 ext lo64)\r\njinja2           : 2.10.1\r\nIPython          : 7.6.1\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nfastparquet      : None\r\ngcsfs            : 0.2.1\r\nlxml.etree       : None\r\nmatplotlib       : 3.1.1\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 0.13.0\r\npytables         : None\r\ns3fs             : 0.2.0\r\nscipy            : 1.2.1\r\nsqlalchemy       : 1.3.5\r\ntables           : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nxlsxwriter       : None\r\n</details>\r\n\n\n@stelsemeyer Thanks for the report! \r\nThis seems to be a regression compared to 0.23 (compared to SparseSeries).\r\n\r\nThis is a bug in the `SparseArray.take` implementation, when `allow_fill=True` it uses a wrong fill value (nan instead of 0):\r\n\r\n```\r\nIn [3]: df1.A.array.take([0, 2]) \r\nOut[3]: \r\n[0, 0]\r\nFill: 0\r\nBlockIndex\r\nBlock locations: array([], dtype=int32)\r\nBlock lengths: array([], dtype=int32)\r\n\r\nIn [4]: df1.A.array.take([0, 2], allow_fill=True) \r\nOut[4]: \r\n[nan, nan]\r\nFill: 0\r\nIntIndex\r\nIndices: array([0, 1], dtype=int32)\r\n```\r\n\r\n(both above should give the same result)\r\n\r\nAlways welcome to take a look to see how it could be fixed!\n\n@jorisvandenbossche: Thanks for investigating!\r\nI checked the SparseArray, a naive solution would be to use `self.fill_value` if `fill_value` is `None` in `_take_with_fill`, here: https://github.com/pandas-dev/pandas/blob/d1accd032b648c9affd6dce1f81feb9c99422483/pandas/core/arrays/sparse.py#L1173-L1174\n\nRemoving from the 0.25.1 milestone, but if anyone is working on this LMK and we can probably get it in.\r\n\r\n@stelsemeyer your proposal looks reasonable.\n\nI think this is a related issue:\r\n```\r\n>>> import pandas as pd\r\n>>> X = pd.DataFrame([[0,1,0], [1,0,0], [1,1,0]]).astype(pd.SparseDtype(float, fill_value=0.0))\r\n>>> X\r\n     0    1    2\r\n0  0.0  1.0  0.0\r\n1  1.0  0.0  0.0\r\n2  1.0  1.0  0.0\r\n>>> X.loc[0]\r\n0    0.0\r\n1    1.0\r\n2    0.0\r\nName: 0, dtype: Sparse[float64, 0.0]\r\n>>> X.loc[[0,1]]\r\n     0    1   2\r\n0  0.0  1.0 NaN\r\n1  1.0  0.0 NaN\r\n>>> X.iloc[[0,1]]\r\n     0    1   2\r\n0  0.0  1.0 NaN\r\n1  1.0  0.0 NaN\r\n```\n\nI edited the proposed line, but to no avail. The error in @jorisvandenbossche's answer is resolved:\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> df1 = pd.DataFrame({\"A\": pd.SparseArray([0, 0, 0]), 'B': [1,2,3]})\r\n>>> df1.A.array.take([0, 2])\r\n[0, 0]\r\nFill: 0\r\nBlockIndex\r\nBlock locations: array([], dtype=int32)\r\nBlock lengths: array([], dtype=int32)\r\n>>> df1.A.array.take([0, 2], allow_fill=True)\r\n[0, 0]\r\nFill: 0\r\nIntIndex\r\nIndices: array([], dtype=int32)\r\n```\r\nbut my and @stelsemeyer's issues remain.\r\n```\r\n>>> df1.loc[df1['B'] != 2]\r\n    A  B\r\n0 NaN  1\r\n2 NaN  3\r\n>>> \r\n>>> X = pd.DataFrame([[0,1,0], [1,0,0], [1,1,0]]).astype(pd.SparseDtype(float, fill_value=0.0))\r\n>>> X.loc[[0,1]]\r\n     0    1   2\r\n0  0.0  1.0 NaN\r\n1  1.0  0.0 NaN\r\n```\r\n\r\nSeems to me that there is another problem here: \r\n\r\nhttps://github.com/pandas-dev/pandas/blob/a45760fd45b434caf9107bb19f1536636cc3fbd8/pandas/core/internals/managers.py#L1262\r\n\r\n```\r\n>>> blk = X._data.blocks[2]\r\n>>> blk.take_nd(indexer=np.array([0,1]), axis=1).values\r\n[0.0, 0.0]\r\nFill: 0.0\r\nIntIndex\r\nIndices: array([], dtype=int32)\r\n>>> blk.take_nd(indexer=np.array([0,1]), axis=1, fill_tuple=(blk.fill_value,)).values\r\n[nan, nan]\r\nFill: 0.0\r\nIntIndex\r\nIndices: array([0, 1], dtype=int32)\r\n```\r\n\r\nwhich is because of a discrepancy between `blk.fill_value` and `blk.dtype.fill_value`\r\n\r\n```\r\n>>> blk.fill_value\r\nnan\r\n>>> blk.dtype.fill_value\r\n0.0\r\n```\r\n\r\nI don't know if we should a) reference `blk.dtype.fill_value` or b) make `blk.dtype.fill_value` consistent with `blk.fill_value`.\n\n@TomAugspurger any thoughts on this? I'm happy to write the PR, just need some guidance.\n\nMmm I'm not sure I understand the issue. But note that doing a `.take` which introduces missing values via a `-1` in the indices should result in a NaN in the output, regardless of the fill value. Not sure if that helps or not.\n\nCan you give an example of how/why that would happen? I don't understand quite how we should get a NaN when the fill_value is not nan.\n\nHere's my proposed solution:\r\n\r\nReplace\r\n```\r\n    @property\r\n    def fill_value(self):\r\n        # Used in reindex_indexer\r\n        return self.values.dtype.na_value\r\n```\r\nfrom https://github.com/pandas-dev/pandas/blob/master/pandas/core/internals/blocks.py#L1730 with\r\n```\r\n    @property\r\n    def fill_value(self):\r\n        # Used in reindex_indexer\r\n        try:\r\n            return self.values.dtype.fill_value\r\n        except AttributeError:\r\n            return self.values.dtype.na_value\r\n```\r\nThoughts?\n\nPretty sure my dataset is showing this. Seems to apply to some columns but not all.\r\n[strange_test.pickle.gz](https://github.com/pandas-dev/pandas/files/3958518/strange_test.pickle.gz)\r\n\r\nIf anyone wants a large real-world set to test this on:\r\n```python\r\nimport pandas as pd\r\ntest = pd.read_pickle('strange_test.pickle.gz')\r\nany(test.DistinctSKUs_SEPB.isna())\r\n> False\r\nany(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\n> True # !?!\r\n```\n\n@akdor1154 can you try this monkey patch and see if it solves your issue?\r\n\r\n```\r\ndef fill_value(self):\r\n    # Used in reindex_indexer\r\n    try:\r\n        return self.values.dtype.fill_value\r\n    except AttributeError:\r\n        return self.values.dtype.na_value\r\n\r\nfrom pandas.core.internals.blocks import ExtensionBlock\r\n\r\nsetattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n```\n\nAs mentioned https://github.com/pandas-dev/pandas/issues/29321#issuecomment-551251530, it may be an issue as well when the sparse series matches the fill value\n\nPretty sure my monkey patch works. I can write a PR if I can get approval from @TomAugspurger or @jorisvandenbossche \r\n\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> df1 = pd.DataFrame({\"A\": pd.arrays.SparseArray([0, 0, 0]), 'B': [1,2,3]})\r\n>>> df1.loc[df1['B'] != 2]\r\n    A  B\r\n0 NaN  1\r\n2 NaN  3\r\n>>> def fill_value(self):\r\n...     # Used in reindex_indexer\r\n...     try:\r\n...         return self.values.dtype.fill_value\r\n...     except AttributeError:\r\n...         return self.values.dtype.na_value\r\n...\r\n>>> from pandas.core.internals.blocks import ExtensionBlock\r\n>>>\r\n>>> setattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n>>> df1.loc[df1['B'] != 2]\r\n   A  B\r\n0  0  1\r\n2  0  3\r\n```\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> X = pd.DataFrame([[0,1,0], [1,0,0], [1,1,0]]).astype(\r\n...     pd.SparseDtype(float, fill_value=0.0))\r\n>>> X.loc[[0,1]]\r\n     0    1   2\r\n0  0.0  1.0 NaN\r\n1  1.0  0.0 NaN\r\n>>> def fill_value(self):\r\n...     # Used in reindex_indexer\r\n...     try:\r\n...         return self.values.dtype.fill_value\r\n...     except AttributeError:\r\n...         return self.values.dtype.na_value\r\n...\r\n>>> from pandas.core.internals.blocks import ExtensionBlock\r\n>>>\r\n>>> setattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n>>> X.loc[[0,1]]\r\n     0    1    2\r\n0  0.0  1.0  0.0\r\n1  1.0  0.0  0.0\r\n```\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> test = pd.read_pickle('strange_test.pickle.gz')\r\n>>> any(test.DistinctSKUs_SEPB.isna())\r\nFalse\r\n>>> any(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\nTrue\r\n>>> def fill_value(self):\r\n...     # Used in reindex_indexer\r\n...     try:\r\n...         return self.values.dtype.fill_value\r\n...     except AttributeError:\r\n...         return self.values.dtype.na_value\r\n...\r\n>>> from pandas.core.internals.blocks import ExtensionBlock\r\n>>>\r\n>>> setattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n>>> any(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\nFalse\r\n```\n\n@scottgigante sorry, from memory I tested it at the time and it worked, thanks.\n\n@scottgigante Just tested your monkey patch, it works for me in `pandas 1.0.2`.\n\nThe fix here is being revereted in https://github.com/pandas-dev/pandas/pull/35287. Some discussion on a potential fix at https://github.com/pandas-dev/pandas/issues/35286#issuecomment-658788801.\r\n\r\ncc @scottgigante if you want to take another shot :)\n\nThis looks fixed on master. Could use a test\r\n\r\n```\r\nIn [4]: df1_filtered\r\nOut[4]:\r\n   A  B\r\n0  0  1\r\n2  0  3\r\n\r\nIn [5]: df2_filtered\r\nOut[5]:\r\n   A  B\r\n0  0  1\r\n2  0  3\r\n```\n\nWhat about this:\r\n\r\n`df1_filtered['A'] = df1_filtered['A'].fillna(0)`\r\n\r\nSimilarly, to change the NaN values in column 'A' of df2_filtered to 0, you can use the same method:\r\n\r\n`df2_filtered['A'] = df2_filtered['A'].fillna(0)` \r\n\r\n\n\nTake",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/35287",
  "code_context": [
    {
      "filename": "pandas/core/arrays/sparse/array.py",
      "content": "\"\"\"\nSparseArray data structure\n\"\"\"\nfrom collections import abc\nimport numbers\nimport operator\nfrom typing import Any, Callable, Union\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import lib\nimport pandas._libs.sparse as splib\nfrom pandas._libs.sparse import BlockIndex, IntIndex, SparseIndex\nfrom pandas._libs.tslibs import NaT\nfrom pandas._typing import Scalar\nimport pandas.compat as compat\nfrom pandas.compat.numpy import function as nv\nfrom pandas.errors import PerformanceWarning\n\nfrom pandas.core.dtypes.cast import (\n    astype_nansafe,\n    construct_1d_arraylike_from_scalar,\n    find_common_type,\n    infer_dtype_from_scalar,\n)\nfrom pandas.core.dtypes.common import (\n    is_array_like,\n    is_bool_dtype,\n    is_datetime64_any_dtype,\n    is_datetime64tz_dtype,\n    is_dtype_equal,\n    is_integer,\n    is_object_dtype,\n    is_scalar,\n    is_string_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.generic import ABCIndexClass, ABCSeries\nfrom pandas.core.dtypes.missing import isna, na_value_for_dtype, notna\n\nimport pandas.core.algorithms as algos\nfrom pandas.core.arrays import ExtensionArray, ExtensionOpsMixin\nfrom pandas.core.arrays.sparse.dtype import SparseDtype\nfrom pandas.core.base import PandasObject\nimport pandas.core.common as com\nfrom pandas.core.construction import extract_array, sanitize_array\nfrom pandas.core.indexers import check_array_indexer\nfrom pandas.core.missing import interpolate_2d\nfrom pandas.core.nanops import check_below_min_count\nimport pandas.core.ops as ops\nfrom pandas.core.ops.common import unpack_zerodim_and_defer\n\nimport pandas.io.formats.printing as printing\n\n# ----------------------------------------------------------------------------\n# Array\n\n\n_sparray_doc_kwargs = dict(klass=\"SparseArray\")\n\n\ndef _get_fill(arr: \"SparseArray\") -> np.ndarray:\n    \"\"\"\n    Create a 0-dim ndarray containing the fill value\n\n    Parameters\n    ----------\n    arr : SparseArray\n\n    Returns\n    -------\n    fill_value : ndarray\n        0-dim ndarray with just the fill value.\n\n    Notes\n    -----\n    coerce fill_value to arr dtype if possible\n    int64 SparseArray can have NaN as fill_value if there is no missing\n    \"\"\"\n    try:\n        return np.asarray(arr.fill_value, dtype=arr.dtype.subtype)\n    except ValueError:\n        return np.asarray(arr.fill_value)\n\n\ndef _sparse_array_op(\n    left: \"SparseArray\", right: \"SparseArray\", op: Callable, name: str\n) -> Any:\n    \"\"\"\n    Perform a binary operation between two arrays.\n\n    Parameters\n    ----------\n    left : Union[SparseArray, ndarray]\n    right : Union[SparseArray, ndarray]\n    op : Callable\n        The binary operation to perform\n    name str\n        Name of the callable.\n\n    Returns\n    -------\n    SparseArray\n    \"\"\"\n    if name.startswith(\"__\"):\n        # For lookups in _libs.sparse we need non-dunder op name\n        name = name[2:-2]\n\n    # dtype used to find corresponding sparse method\n    ltype = left.dtype.subtype\n    rtype = right.dtype.subtype\n\n    if not is_dtype_equal(ltype, rtype):\n        subtype = find_common_type([ltype, rtype])\n        ltype = SparseDtype(subtype, left.fill_value)\n        rtype = SparseDtype(subtype, right.fill_value)\n\n        # TODO(GH-23092): pass copy=False. Need to fix astype_nansafe\n        left = left.astype(ltype)\n        right = right.astype(rtype)\n        dtype = ltype.subtype\n    else:\n        dtype = ltype\n\n    # dtype the result must have\n    result_dtype = None\n\n    if left.sp_index.ngaps == 0 or right.sp_index.ngaps == 0:\n        with np.errstate(all=\"ignore\"):\n            result = op(left.to_dense(), right.to_dense())\n            fill = op(_get_fill(left), _get_fill(right))\n\n        if left.sp_index.ngaps == 0:\n            index = left.sp_index\n        else:\n            index = right.sp_index\n    elif left.sp_index.equals(right.sp_index):\n        with np.errstate(all=\"ignore\"):\n            result = op(left.sp_values, right.sp_values)\n            fill = op(_get_fill(left), _get_fill(right))\n        index = left.sp_index\n    else:\n        if name[0] == \"r\":\n            left, right = right, left\n            name = name[1:]\n\n        if name in (\"and\", \"or\", \"xor\") and dtype == \"bool\":\n            opname = f\"sparse_{name}_uint8\"\n            # to make template simple, cast here\n            left_sp_values = left.sp_values.view(np.uint8)\n            right_sp_values = right.sp_values.view(np.uint8)\n            result_dtype = bool\n        else:\n            opname = f\"sparse_{name}_{dtype}\"\n            left_sp_values = left.sp_values\n            right_sp_values = right.sp_values\n\n        sparse_op = getattr(splib, opname)\n\n        with np.errstate(all=\"ignore\"):\n            result, index, fill = sparse_op(\n                left_sp_values,\n                left.sp_index,\n                left.fill_value,\n                right_sp_values,\n                right.sp_index,\n                right.fill_value,\n            )\n\n    if result_dtype is None:\n        result_dtype = result.dtype\n\n    return _wrap_result(name, result, index, fill, dtype=result_dtype)\n\n\ndef _wrap_result(name, data, sparse_index, fill_value, dtype=None):\n    \"\"\"\n    wrap op result to have correct dtype\n    \"\"\"\n    if name.startswith(\"__\"):\n        # e.g. __eq__ --> eq\n        name = name[2:-2]\n\n    if name in (\"eq\", \"ne\", \"lt\", \"gt\", \"le\", \"ge\"):\n        dtype = bool\n\n    fill_value = lib.item_from_zerodim(fill_value)\n\n    if is_bool_dtype(dtype):\n        # fill_value may be np.bool_\n        fill_value = bool(fill_value)\n    return SparseArray(\n        data, sparse_index=sparse_index, fill_value=fill_value, dtype=dtype\n    )\n\n\nclass SparseArray(PandasObject, ExtensionArray, ExtensionOpsMixin):\n    \"\"\"\n    An ExtensionArray for storing sparse data.\n\n    .. versionchanged:: 0.24.0\n\n       Implements the ExtensionArray interface.\n\n    Parameters\n    ----------\n    data : array-like\n        A dense array of values to store in the SparseArray. This may contain\n        `fill_value`.\n    sparse_index : SparseIndex, optional\n    index : Index\n    fill_value : scalar, optional\n        Elements in `data` that are `fill_value` are not stored in the\n        SparseArray. For memory savings, this should be the most common value\n        in `data`. By default, `fill_value` depends on the dtype of `data`:\n\n        =========== ==========\n        data.dtype  na_value\n        =========== ==========\n        float       ``np.nan``\n        int         ``0``\n        bool        False\n        datetime64  ``pd.NaT``\n        timedelta64 ``pd.NaT``\n        =========== ==========\n\n        The fill value is potentially specified in three ways. In order of\n        precedence, these are\n\n        1. The `fill_value` argument\n        2. ``dtype.fill_value`` if `fill_value` is None and `dtype` is\n           a ``SparseDtype``\n        3. ``data.dtype.fill_value`` if `fill_value` is None and `dtype`\n           is not a ``SparseDtype`` and `data` is a ``SparseArray``.\n\n    kind : {'int', 'block'}, default 'int'\n        The type of storage for sparse locations.\n\n        * 'block': Stores a `block` and `block_length` for each\n          contiguous *span* of sparse values. This is best when\n          sparse data tends to be clumped together, with large\n          regions of ``fill-value`` values between sparse values.\n        * 'integer': uses an integer to store the location of\n          each sparse value.\n\n    dtype : np.dtype or SparseDtype, optional\n        The dtype to use for the SparseArray. For numpy dtypes, this\n        determines the dtype of ``self.sp_values``. For SparseDtype,\n        this determines ``self.sp_values`` and ``self.fill_value``.\n    copy : bool, default False\n        Whether to explicitly copy the incoming `data` array.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Examples\n    --------\n    >>> from pandas.arrays import SparseArray\n    >>> arr = SparseArray([0, 0, 1, 2])\n    >>> arr\n    [0, 0, 1, 2]\n    Fill: 0\n    IntIndex\n    Indices: array([2, 3], dtype=int32)\n    \"\"\"\n\n    _subtyp = \"sparse_array\"  # register ABCSparseArray\n    _deprecations = PandasObject._deprecations | frozenset([\"get_values\"])\n    _sparse_index: SparseIndex\n\n    def __init__(\n        self,\n        data,\n        sparse_index=None,\n        index=None,\n        fill_value=None,\n        kind=\"integer\",\n        dtype=None,\n        copy=False,\n    ):\n\n        if fill_value is None and isinstance(dtype, SparseDtype):\n            fill_value = dtype.fill_value\n\n        if isinstance(data, type(self)):\n            # disable normal inference on dtype, sparse_index, & fill_value\n            if sparse_index is None:\n                sparse_index = data.sp_index\n            if fill_value is None:\n                fill_value = data.fill_value\n            if dtype is None:\n                dtype = data.dtype\n            # TODO: make kind=None, and use data.kind?\n            data = data.sp_values\n\n        # Handle use-provided dtype\n        if isinstance(dtype, str):\n            # Two options: dtype='int', regular numpy dtype\n            # or dtype='Sparse[int]', a sparse dtype\n            try:\n                dtype = SparseDtype.construct_from_string(dtype)\n            except TypeError:\n                dtype = pandas_dtype(dtype)\n\n        if isinstance(dtype, SparseDtype):\n            if fill_value is None:\n                fill_value = dtype.fill_value\n            dtype = dtype.subtype\n\n        if index is not None and not is_scalar(data):\n            raise Exception(\"must only pass scalars with an index\")\n\n        if is_scalar(data):\n            if index is not None:\n                if data is None:\n                    data = np.nan\n\n            if index is not None:\n                npoints = len(index)\n            elif sparse_index is None:\n                npoints = 1\n            else:\n                npoints = sparse_index.length\n\n            dtype = infer_dtype_from_scalar(data)[0]\n            data = construct_1d_arraylike_from_scalar(data, npoints, dtype)\n\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        # TODO: disentangle the fill_value dtype inference from\n        # dtype inference\n        if data is None:\n            # TODO: What should the empty dtype be? Object or float?\n            data = np.array([], dtype=dtype)\n\n        if not is_array_like(data):\n            try:\n                # probably shared code in sanitize_series\n\n                data = sanitize_array(data, index=None)\n            except ValueError:\n                # NumPy may raise a ValueError on data like [1, []]\n                # we retry with object dtype here.\n                if dtype is None:\n                    dtype = object\n                    data = np.atleast_1d(np.asarray(data, dtype=dtype))\n                else:\n                    raise\n\n        if copy:\n            # TODO: avoid double copy when dtype forces cast.\n            data = data.copy()\n\n        if fill_value is None:\n            fill_value_dtype = data.dtype if dtype is None else dtype\n            if fill_value_dtype is None:\n                fill_value = np.nan\n            else:\n                fill_value = na_value_for_dtype(fill_value_dtype)\n\n        if isinstance(data, type(self)) and sparse_index is None:\n            sparse_index = data._sparse_index\n            sparse_values = np.asarray(data.sp_values, dtype=dtype)\n        elif sparse_index is None:\n            data = extract_array(data, extract_numpy=True)\n            if not isinstance(data, np.ndarray):\n                # EA\n                if is_datetime64tz_dtype(data.dtype):\n                    warnings.warn(\n                        f\"Creating SparseArray from {data.dtype} data \"\n                        \"loses timezone information.  Cast to object before \"\n                        \"sparse to retain timezone information.\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n                    data = np.asarray(data, dtype=\"datetime64[ns]\")\n                data = np.asarray(data)\n            sparse_values, sparse_index, fill_value = make_sparse(\n                data, kind=kind, fill_value=fill_value, dtype=dtype\n            )\n        else:\n            sparse_values = np.asarray(data, dtype=dtype)\n            if len(sparse_values) != sparse_index.npoints:\n                raise AssertionError(\n                    f\"Non array-like type {type(sparse_values)} must \"\n                    \"have the same length as the index\"\n                )\n        self._sparse_index = sparse_index\n        self._sparse_values = sparse_values\n        self._dtype = SparseDtype(sparse_values.dtype, fill_value)\n\n    @classmethod\n    def _simple_new(\n        cls, sparse_array: np.ndarray, sparse_index: SparseIndex, dtype: SparseDtype\n    ) -> \"SparseArray\":\n        new = object.__new__(cls)\n        new._sparse_index = sparse_index\n        new._sparse_values = sparse_array\n        new._dtype = dtype\n        return new\n\n    @classmethod\n    def from_spmatrix(cls, data):\n        \"\"\"\n        Create a SparseArray from a scipy.sparse matrix.\n\n        .. versionadded:: 0.25.0\n\n        Parameters\n        ----------\n        data : scipy.sparse.sp_matrix\n            This should be a SciPy sparse matrix where the size\n            of the second dimension is 1. In other words, a\n            sparse matrix with a single column.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> import scipy.sparse\n        >>> mat = scipy.sparse.coo_matrix((4, 1))\n        >>> pd.arrays.SparseArray.from_spmatrix(mat)\n        [0.0, 0.0, 0.0, 0.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([], dtype=int32)\n        \"\"\"\n        length, ncol = data.shape\n\n        if ncol != 1:\n            raise ValueError(f\"'data' must have a single column, not '{ncol}'\")\n\n        # our sparse index classes require that the positions be strictly\n        # increasing. So we need to sort loc, and arr accordingly.\n        data = data.tocsc()\n        data.sort_indices()\n        arr = data.data\n        idx = data.indices\n\n        zero = np.array(0, dtype=arr.dtype).item()\n        dtype = SparseDtype(arr.dtype, zero)\n        index = IntIndex(length, idx)\n\n        return cls._simple_new(arr, index, dtype)\n\n    def __array__(self, dtype=None, copy=True) -> np.ndarray:\n        fill_value = self.fill_value\n\n        if self.sp_index.ngaps == 0:\n            # Compat for na dtype and int values.\n            return self.sp_values\n        if dtype is None:\n            # Can NumPy represent this type?\n            # If not, `np.result_type` will raise. We catch that\n            # and return object.\n            if is_datetime64_any_dtype(self.sp_values.dtype):\n                # However, we *do* special-case the common case of\n                # a datetime64 with pandas NaT.\n                if fill_value is NaT:\n                    # Can't put pd.NaT in a datetime64[ns]\n                    fill_value = np.datetime64(\"NaT\")\n            try:\n                dtype = np.result_type(self.sp_values.dtype, type(fill_value))\n            except TypeError:\n                dtype = object\n\n        out = np.full(self.shape, fill_value, dtype=dtype)\n        out[self.sp_index.to_int_index().indices] = self.sp_values\n        return out\n\n    def __setitem__(self, key, value):\n        # I suppose we could allow setting of non-fill_value elements.\n        # TODO(SparseArray.__setitem__): remove special cases in\n        # ExtensionBlock.where\n        msg = \"SparseArray does not support item assignment via setitem\"\n        raise TypeError(msg)\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        return cls(scalars, dtype=dtype)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return cls(values, dtype=original.dtype)\n\n    # ------------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------------\n    @property\n    def sp_index(self):\n        \"\"\"\n        The SparseIndex containing the location of non- ``fill_value`` points.\n        \"\"\"\n        return self._sparse_index\n\n    @property\n    def sp_values(self) -> np.ndarray:\n        \"\"\"\n        An ndarray containing the non- ``fill_value`` values.\n\n        Examples\n        --------\n        >>> s = SparseArray([0, 0, 1, 0, 2], fill_value=0)\n        >>> s.sp_values\n        array([1, 2])\n        \"\"\"\n        return self._sparse_values\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def fill_value(self):\n        \"\"\"\n        Elements in `data` that are `fill_value` are not stored.\n\n        For memory savings, this should be the most common value in the array.\n        \"\"\"\n        return self.dtype.fill_value\n\n    @fill_value.setter\n    def fill_value(self, value):\n        self._dtype = SparseDtype(self.dtype.subtype, value)\n\n    @property\n    def kind(self) -> str:\n        \"\"\"\n        The kind of sparse index for this array. One of {'integer', 'block'}.\n        \"\"\"\n        if isinstance(self.sp_index, IntIndex):\n            return \"integer\"\n        else:\n            return \"block\"\n\n    @property\n    def _valid_sp_values(self):\n        sp_vals = self.sp_values\n        mask = notna(sp_vals)\n        return sp_vals[mask]\n\n    def __len__(self) -> int:\n        return self.sp_index.length\n\n    @property\n    def _null_fill_value(self):\n        return self._dtype._is_na_fill_value\n\n    def _fill_value_matches(self, fill_value):\n        if self._null_fill_value:\n            return isna(fill_value)\n        else:\n            return self.fill_value == fill_value\n\n    @property\n    def nbytes(self) -> int:\n        return self.sp_values.nbytes + self.sp_index.nbytes\n\n    @property\n    def density(self):\n        \"\"\"\n        The percent of non- ``fill_value`` points, as decimal.\n\n        Examples\n        --------\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\n        >>> s.density\n        0.6\n        \"\"\"\n        r = float(self.sp_index.npoints) / float(self.sp_index.length)\n        return r\n\n    @property\n    def npoints(self) -> int:\n        \"\"\"\n        The number of non- ``fill_value`` points.\n\n        Examples\n        --------\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\n        >>> s.npoints\n        3\n        \"\"\"\n        return self.sp_index.npoints\n\n    def isna(self):\n        # If null fill value, we want SparseDtype[bool, true]\n        # to preserve the same memory usage.\n        dtype = SparseDtype(bool, self._null_fill_value)\n        return type(self)._simple_new(isna(self.sp_values), self.sp_index, dtype)\n\n    def fillna(self, value=None, method=None, limit=None):\n        \"\"\"\n        Fill missing values with `value`.\n\n        Parameters\n        ----------\n        value : scalar, optional\n        method : str, optional\n\n            .. warning::\n\n               Using 'method' will result in high memory use,\n               as all `fill_value` methods will be converted to\n               an in-memory ndarray\n\n        limit : int, optional\n\n        Returns\n        -------\n        SparseArray\n\n        Notes\n        -----\n        When `value` is specified, the result's ``fill_value`` depends on\n        ``self.fill_value``. The goal is to maintain low-memory use.\n\n        If ``self.fill_value`` is NA, the result dtype will be\n        ``SparseDtype(self.dtype, fill_value=value)``. This will preserve\n        amount of memory used before and after filling.\n\n        When ``self.fill_value`` is not NA, the result dtype will be\n        ``self.dtype``. Again, this preserves the amount of memory used.\n        \"\"\"\n        if (method is None and value is None) or (\n            method is not None and value is not None\n        ):\n            raise ValueError(\"Must specify one of 'method' or 'value'.\")\n\n        elif method is not None:\n            msg = \"fillna with 'method' requires high memory usage.\"\n            warnings.warn(msg, PerformanceWarning)\n            filled = interpolate_2d(np.asarray(self), method=method, limit=limit)\n            return type(self)(filled, fill_value=self.fill_value)\n\n        else:\n            new_values = np.where(isna(self.sp_values), value, self.sp_values)\n\n            if self._null_fill_value:\n                # This is essentially just updating the dtype.\n                new_dtype = SparseDtype(self.dtype.subtype, fill_value=value)\n            else:\n                new_dtype = self.dtype\n\n        return self._simple_new(new_values, self._sparse_index, new_dtype)\n\n    def shift(self, periods=1, fill_value=None):\n\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n\n        subtype = np.result_type(fill_value, self.dtype.subtype)\n\n        if subtype != self.dtype.subtype:\n            # just coerce up front\n            arr = self.astype(SparseDtype(subtype, self.fill_value))\n        else:\n            arr = self\n\n        empty = self._from_sequence(\n            [fill_value] * min(abs(periods), len(self)), dtype=arr.dtype\n        )\n\n        if periods > 0:\n            a = empty\n            b = arr[:-periods]\n        else:\n            a = arr[abs(periods) :]\n            b = empty\n        return arr._concat_same_type([a, b])\n\n    def _first_fill_value_loc(self):\n        \"\"\"\n        Get the location of the first missing value.\n\n        Returns\n        -------\n        int\n        \"\"\"\n        if len(self) == 0 or self.sp_index.npoints == len(self):\n            return -1\n\n        indices = self.sp_index.to_int_index().indices\n        if not len(indices) or indices[0] > 0:\n            return 0\n\n        diff = indices[1:] - indices[:-1]\n        return np.searchsorted(diff, 2) + 1\n\n    def unique(self):\n        uniques = list(algos.unique(self.sp_values))\n        fill_loc = self._first_fill_value_loc()\n        if fill_loc >= 0:\n            uniques.insert(fill_loc, self.fill_value)\n        return type(self)._from_sequence(uniques, dtype=self.dtype)\n\n    def _values_for_factorize(self):\n        # Still override this for hash_pandas_object\n        return np.asarray(self), self.fill_value\n\n    def factorize(self, na_sentinel=-1):\n        # Currently, ExtensionArray.factorize -> Tuple[ndarray, EA]\n        # The sparsity on this is backwards from what Sparse would want. Want\n        # ExtensionArray.factorize -> Tuple[EA, EA]\n        # Given that we have to return a dense array of codes, why bother\n        # implementing an efficient factorize?\n        codes, uniques = algos.factorize(np.asarray(self), na_sentinel=na_sentinel)\n        uniques = SparseArray(uniques, dtype=self.dtype)\n        return codes, uniques\n\n    def value_counts(self, dropna=True):\n        \"\"\"\n        Returns a Series containing counts of unique values.\n\n        Parameters\n        ----------\n        dropna : boolean, default True\n            Don't include counts of NaN, even if NaN is in sp_values.\n\n        Returns\n        -------\n        counts : Series\n        \"\"\"\n        from pandas import Index, Series\n\n        keys, counts = algos._value_counts_arraylike(self.sp_values, dropna=dropna)\n        fcounts = self.sp_index.ngaps\n        if fcounts > 0:\n            if self._null_fill_value and dropna:\n                pass\n            else:\n                if self._null_fill_value:\n                    mask = isna(keys)\n                else:\n                    mask = keys == self.fill_value\n\n                if mask.any():\n                    counts[mask] += fcounts\n                else:\n                    keys = np.insert(keys, 0, self.fill_value)\n                    counts = np.insert(counts, 0, fcounts)\n\n        if not isinstance(keys, ABCIndexClass):\n            keys = Index(keys)\n        result = Series(counts, index=keys)\n        return result\n\n    # --------\n    # Indexing\n    # --------\n\n    def __getitem__(self, key):\n        # avoid mypy issues when importing at the top-level\n        from pandas.core.indexing import check_bool_indexer\n\n        if isinstance(key, tuple):\n            if len(key) > 1:\n                raise IndexError(\"too many indices for array.\")\n            key = key[0]\n\n        if is_integer(key):\n            return self._get_val_at(key)\n        elif isinstance(key, tuple):\n            data_slice = self.to_dense()[key]\n        elif isinstance(key, slice):\n            # special case to preserve dtypes\n            if key == slice(None):\n                return self.copy()\n            # TODO: this logic is surely elsewhere\n            # TODO: this could be more efficient\n            indices = np.arange(len(self), dtype=np.int32)[key]\n            return self.take(indices)\n        else:\n            # TODO: I think we can avoid densifying when masking a\n            # boolean SparseArray with another. Need to look at the\n            # key's fill_value for True / False, and then do an intersection\n            # on the indices of the sp_values.\n            if isinstance(key, SparseArray):\n                if is_bool_dtype(key):\n                    key = key.to_dense()\n                else:\n                    key = np.asarray(key)\n\n            key = check_array_indexer(self, key)\n\n            if com.is_bool_indexer(key):\n                key = check_bool_indexer(self, key)\n\n                return self.take(np.arange(len(key), dtype=np.int32)[key])\n            elif hasattr(key, \"__len__\"):\n                return self.take(key)\n            else:\n                raise ValueError(f\"Cannot slice with '{key}'\")\n\n        return type(self)(data_slice, kind=self.kind)\n\n    def _get_val_at(self, loc):\n        n = len(self)\n        if loc < 0:\n            loc += n\n\n        if loc >= n or loc < 0:\n            raise IndexError(\"Out of bounds access\")\n\n        sp_loc = self.sp_index.lookup(loc)\n        if sp_loc == -1:\n            return self.fill_value\n        else:\n            val = self.sp_values[sp_loc]\n            val = com.maybe_box_datetimelike(val, self.sp_values.dtype)\n            return val\n\n    def take(self, indices, allow_fill=False, fill_value=None) -> \"SparseArray\":\n        if is_scalar(indices):\n            raise ValueError(f\"'indices' must be an array, not a scalar '{indices}'.\")\n        indices = np.asarray(indices, dtype=np.int32)\n\n        if indices.size == 0:\n            result = np.array([], dtype=\"object\")\n            kwargs = {\"dtype\": self.dtype}\n        elif allow_fill:\n            result = self._take_with_fill(indices, fill_value=fill_value)\n            kwargs = {}\n        else:\n            result = self._take_without_fill(indices)\n            kwargs = {\"dtype\": self.dtype}\n\n        return type(self)(result, fill_value=self.fill_value, kind=self.kind, **kwargs)\n\n    def _take_with_fill(self, indices, fill_value=None) -> np.ndarray:\n        if fill_value is None:\n            fill_value = self.dtype.na_value\n\n        if indices.min() < -1:\n            raise ValueError(\n                \"Invalid value in 'indices'. Must be between -1 \"\n                \"and the length of the array.\"\n            )\n\n        if indices.max() >= len(self):\n            raise IndexError(\"out of bounds value in 'indices'.\")\n\n        if len(self) == 0:\n            # Empty... Allow taking only if all empty\n            if (indices == -1).all():\n                dtype = np.result_type(self.sp_values, type(fill_value))\n                taken = np.empty_like(indices, dtype=dtype)\n                taken.fill(fill_value)\n                return taken\n            else:\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\n\n        # sp_indexer may be -1 for two reasons\n        # 1.) we took for an index of -1 (new)\n        # 2.) we took a value that was self.fill_value (old)\n        sp_indexer = self.sp_index.lookup_array(indices)\n        new_fill_indices = indices == -1\n        old_fill_indices = (sp_indexer == -1) & ~new_fill_indices\n\n        if self.sp_index.npoints == 0 and old_fill_indices.all():\n            # We've looked up all valid points on an all-sparse array.\n            taken = np.full(\n                sp_indexer.shape, fill_value=self.fill_value, dtype=self.dtype.subtype\n            )\n\n        elif self.sp_index.npoints == 0:\n            # Avoid taking from the empty self.sp_values\n            _dtype = np.result_type(self.dtype.subtype, type(fill_value))\n            taken = np.full(sp_indexer.shape, fill_value=fill_value, dtype=_dtype)\n        else:\n            taken = self.sp_values.take(sp_indexer)\n\n            # Fill in two steps.\n            # Old fill values\n            # New fill values\n            # potentially coercing to a new dtype at each stage.\n\n            m0 = sp_indexer[old_fill_indices] < 0\n            m1 = sp_indexer[new_fill_indices] < 0\n\n            result_type = taken.dtype\n\n            if m0.any():\n                result_type = np.result_type(result_type, type(self.fill_value))\n                taken = taken.astype(result_type)\n                taken[old_fill_indices] = self.fill_value\n\n            if m1.any():\n                result_type = np.result_type(result_type, type(fill_value))\n                taken = taken.astype(result_type)\n                taken[new_fill_indices] = fill_value\n\n        return taken\n\n    def _take_without_fill(self, indices) -> Union[np.ndarray, \"SparseArray\"]:\n        to_shift = indices < 0\n        indices = indices.copy()\n\n        n = len(self)\n\n        if (indices.max() >= n) or (indices.min() < -n):\n            if n == 0:\n                raise IndexError(\"cannot do a non-empty take from an empty axes.\")\n            else:\n                raise IndexError(\"out of bounds value in 'indices'.\")\n\n        if to_shift.any():\n            indices[to_shift] += n\n\n        if self.sp_index.npoints == 0:\n            # edge case in take...\n            # I think just return\n            out = np.full(\n                indices.shape,\n                self.fill_value,\n                dtype=np.result_type(type(self.fill_value)),\n            )\n            arr, sp_index, fill_value = make_sparse(out, fill_value=self.fill_value)\n            return type(self)(arr, sparse_index=sp_index, fill_value=fill_value)\n\n        sp_indexer = self.sp_index.lookup_array(indices)\n        taken = self.sp_values.take(sp_indexer)\n        fillable = sp_indexer < 0\n\n        if fillable.any():\n            # TODO: may need to coerce array to fill value\n            result_type = np.result_type(taken, type(self.fill_value))\n            taken = taken.astype(result_type)\n            taken[fillable] = self.fill_value\n\n        return taken\n\n    def searchsorted(self, v, side=\"left\", sorter=None):\n        msg = \"searchsorted requires high memory usage.\"\n        warnings.warn(msg, PerformanceWarning, stacklevel=2)\n        if not is_scalar(v):\n            v = np.asarray(v)\n        v = np.asarray(v)\n        return np.asarray(self, dtype=self.dtype.subtype).searchsorted(v, side, sorter)\n\n    def copy(self):\n        values = self.sp_values.copy()\n        return self._simple_new(values, self.sp_index, self.dtype)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        fill_value = to_concat[0].fill_value\n\n        values = []\n        length = 0\n\n        if to_concat:\n            sp_kind = to_concat[0].kind\n        else:\n            sp_kind = \"integer\"\n\n        if sp_kind == \"integer\":\n            indices = []\n\n            for arr in to_concat:\n                idx = arr.sp_index.to_int_index().indices.copy()\n                idx += length  # TODO: wraparound\n                length += arr.sp_index.length\n\n                values.append(arr.sp_values)\n                indices.append(idx)\n\n            data = np.concatenate(values)\n            indices = np.concatenate(indices)\n            sp_index = IntIndex(length, indices)\n\n        else:\n            # when concatenating block indices, we don't claim that you'll\n            # get an identical index as concating the values and then\n            # creating a new index. We don't want to spend the time trying\n            # to merge blocks across arrays in `to_concat`, so the resulting\n            # BlockIndex may have more blocs.\n            blengths = []\n            blocs = []\n\n            for arr in to_concat:\n                idx = arr.sp_index.to_block_index()\n\n                values.append(arr.sp_values)\n                blocs.append(idx.blocs.copy() + length)\n                blengths.append(idx.blengths)\n                length += arr.sp_index.length\n\n            data = np.concatenate(values)\n            blocs = np.concatenate(blocs)\n            blengths = np.concatenate(blengths)\n\n            sp_index = BlockIndex(length, blocs, blengths)\n\n        return cls(data, sparse_index=sp_index, fill_value=fill_value)\n\n    def astype(self, dtype=None, copy=True):\n        \"\"\"\n        Change the dtype of a SparseArray.\n\n        The output will always be a SparseArray. To convert to a dense\n        ndarray with a certain dtype, use :meth:`numpy.asarray`.\n\n        Parameters\n        ----------\n        dtype : np.dtype or ExtensionDtype\n            For SparseDtype, this changes the dtype of\n            ``self.sp_values`` and the ``self.fill_value``.\n\n            For other dtypes, this only changes the dtype of\n            ``self.sp_values``.\n\n        copy : bool, default True\n            Whether to ensure a copy is made, even if not necessary.\n\n        Returns\n        -------\n        SparseArray\n\n        Examples\n        --------\n        >>> arr = pd.arrays.SparseArray([0, 0, 1, 2])\n        >>> arr\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        >>> arr.astype(np.dtype('int32'))\n        [0, 0, 1, 2]\n        Fill: 0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Using a NumPy dtype with a different kind (e.g. float) will coerce\n        just ``self.sp_values``.\n\n        >>> arr.astype(np.dtype('float64'))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [0.0, 0.0, 1.0, 2.0]\n        Fill: 0.0\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n\n        Use a SparseDtype if you wish to be change the fill value as well.\n\n        >>> arr.astype(SparseDtype(\"float64\", fill_value=np.nan))\n        ... # doctest: +NORMALIZE_WHITESPACE\n        [nan, nan, 1.0, 2.0]\n        Fill: nan\n        IntIndex\n        Indices: array([2, 3], dtype=int32)\n        \"\"\"\n        dtype = self.dtype.update_dtype(dtype)\n        subtype = dtype._subtype_with_str\n        # TODO copy=False is broken for astype_nansafe with int -> float, so cannot\n        # passthrough copy keyword: https://github.com/pandas-dev/pandas/issues/34456\n        sp_values = astype_nansafe(self.sp_values, subtype, copy=True)\n        if sp_values is self.sp_values and copy:\n            sp_values = sp_values.copy()\n\n        return self._simple_new(sp_values, self.sp_index, dtype)\n\n    def map(self, mapper):\n        \"\"\"\n        Map categories using input correspondence (dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : dict, Series, callable\n            The correspondence from old values to new.\n\n        Returns\n        -------\n        SparseArray\n            The output array will have the same density as the input.\n            The output fill value will be the result of applying the\n            mapping to ``self.fill_value``\n\n        Examples\n        --------\n        >>> arr = pd.arrays.SparseArray([0, 1, 2])\n        >>> arr.map(lambda x: x + 10)\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n\n        >>> arr.map({0: 10, 1: 11, 2: 12})\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n\n        >>> arr.map(pd.Series([10, 11, 12], index=[0, 1, 2]))\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n        \"\"\"\n        # this is used in apply.\n        # We get hit since we're an \"is_extension_type\" but regular extension\n        # types are not hit. This may be worth adding to the interface.\n        if isinstance(mapper, ABCSeries):\n            mapper = mapper.to_dict()\n\n        if isinstance(mapper, abc.Mapping):\n            fill_value = mapper.get(self.fill_value, self.fill_value)\n            sp_values = [mapper.get(x, None) for x in self.sp_values]\n        else:\n            fill_value = mapper(self.fill_value)\n            sp_values = [mapper(x) for x in self.sp_values]\n\n        return type(self)(sp_values, sparse_index=self.sp_index, fill_value=fill_value)\n\n    def to_dense(self):\n        \"\"\"\n        Convert SparseArray to a NumPy array.\n\n        Returns\n        -------\n        arr : NumPy array\n        \"\"\"\n        return np.asarray(self, dtype=self.sp_values.dtype)\n\n    _internal_get_values = to_dense\n\n    # ------------------------------------------------------------------------\n    # IO\n    # ------------------------------------------------------------------------\n    def __setstate__(self, state):\n        \"\"\"Necessary for making this object picklable\"\"\"\n        if isinstance(state, tuple):\n            # Compat for pandas < 0.24.0\n            nd_state, (fill_value, sp_index) = state\n            sparse_values = np.array([])\n            sparse_values.__setstate__(nd_state)\n\n            self._sparse_values = sparse_values\n            self._sparse_index = sp_index\n            self._dtype = SparseDtype(sparse_values.dtype, fill_value)\n        else:\n            self.__dict__.update(state)\n\n    def nonzero(self):\n        if self.fill_value == 0:\n            return (self.sp_index.to_int_index().indices,)\n        else:\n            return (self.sp_index.to_int_index().indices[self.sp_values != 0],)\n\n    # ------------------------------------------------------------------------\n    # Reductions\n    # ------------------------------------------------------------------------\n\n    def _reduce(self, name, skipna=True, **kwargs):\n        method = getattr(self, name, None)\n\n        if method is None:\n            raise TypeError(f\"cannot perform {name} with type {self.dtype}\")\n\n        if skipna:\n            arr = self\n        else:\n            arr = self.dropna()\n\n        # we don't support these kwargs.\n        # They should only be present when called via pandas, so do it here.\n        # instead of in `any` / `all` (which will raise if they're present,\n        # thanks to nv.validate\n        kwargs.pop(\"filter_type\", None)\n        kwargs.pop(\"numeric_only\", None)\n        kwargs.pop(\"op\", None)\n        return getattr(arr, name)(**kwargs)\n\n    def all(self, axis=None, *args, **kwargs):\n        \"\"\"\n        Tests whether all elements evaluate True\n\n        Returns\n        -------\n        all : bool\n\n        See Also\n        --------\n        numpy.all\n        \"\"\"\n        nv.validate_all(args, kwargs)\n\n        values = self.sp_values\n\n        if len(values) != len(self) and not np.all(self.fill_value):\n            return False\n\n        return values.all()\n\n    def any(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Tests whether at least one of elements evaluate True\n\n        Returns\n        -------\n        any : bool\n\n        See Also\n        --------\n        numpy.any\n        \"\"\"\n        nv.validate_any(args, kwargs)\n\n        values = self.sp_values\n\n        if len(values) != len(self) and np.any(self.fill_value):\n            return True\n\n        return values.any().item()\n\n    def sum(self, axis: int = 0, min_count: int = 0, *args, **kwargs) -> Scalar:\n        \"\"\"\n        Sum of non-NA/null values\n\n        Parameters\n        ----------\n        axis : int, default 0\n            Not Used. NumPy compatibility.\n        min_count : int, default 0\n            The required number of valid values to perform the summation. If fewer\n            than ``min_count`` valid values are present, the result will be the missing\n            value indicator for subarray type.\n        *args, **kwargs\n            Not Used. NumPy compatibility.\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        nv.validate_sum(args, kwargs)\n        valid_vals = self._valid_sp_values\n        sp_sum = valid_vals.sum()\n        if self._null_fill_value:\n            if check_below_min_count(valid_vals.shape, None, min_count):\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\n            return sp_sum\n        else:\n            nsparse = self.sp_index.ngaps\n            if check_below_min_count(valid_vals.shape, None, min_count - nsparse):\n                return na_value_for_dtype(self.dtype.subtype, compat=False)\n            return sp_sum + self.fill_value * nsparse\n\n    def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Cumulative sum of non-NA/null values.\n\n        When performing the cumulative summation, any non-NA/null values will\n        be skipped. The resulting SparseArray will preserve the locations of\n        NaN values, but the fill value will be `np.nan` regardless.\n\n        Parameters\n        ----------\n        axis : int or None\n            Axis over which to perform the cumulative summation. If None,\n            perform cumulative summation over flattened array.\n\n        Returns\n        -------\n        cumsum : SparseArray\n        \"\"\"\n        nv.validate_cumsum(args, kwargs)\n\n        if axis is not None and axis >= self.ndim:  # Mimic ndarray behaviour.\n            raise ValueError(f\"axis(={axis}) out of bounds\")\n\n        if not self._null_fill_value:\n            return SparseArray(self.to_dense()).cumsum()\n\n        return SparseArray(\n            self.sp_values.cumsum(),\n            sparse_index=self.sp_index,\n            fill_value=self.fill_value,\n        )\n\n    def mean(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Mean of non-NA/null values\n\n        Returns\n        -------\n        mean : float\n        \"\"\"\n        nv.validate_mean(args, kwargs)\n        valid_vals = self._valid_sp_values\n        sp_sum = valid_vals.sum()\n        ct = len(valid_vals)\n\n        if self._null_fill_value:\n            return sp_sum / ct\n        else:\n            nsparse = self.sp_index.ngaps\n            return (sp_sum + self.fill_value * nsparse) / (ct + nsparse)\n\n    def transpose(self, *axes) -> \"SparseArray\":\n        \"\"\"\n        Returns the SparseArray.\n        \"\"\"\n        return self\n\n    @property\n    def T(self) -> \"SparseArray\":\n        \"\"\"\n        Returns the SparseArray.\n        \"\"\"\n        return self\n\n    # ------------------------------------------------------------------------\n    # Ufuncs\n    # ------------------------------------------------------------------------\n\n    _HANDLED_TYPES = (np.ndarray, numbers.Number)\n\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n        out = kwargs.get(\"out\", ())\n\n        for x in inputs + out:\n            if not isinstance(x, self._HANDLED_TYPES + (SparseArray,)):\n                return NotImplemented\n\n        # for binary ops, use our custom dunder methods\n        result = ops.maybe_dispatch_ufunc_to_dunder_op(\n            self, ufunc, method, *inputs, **kwargs\n        )\n        if result is not NotImplemented:\n            return result\n\n        if len(inputs) == 1:\n            # No alignment necessary.\n            sp_values = getattr(ufunc, method)(self.sp_values, **kwargs)\n            fill_value = getattr(ufunc, method)(self.fill_value, **kwargs)\n\n            if isinstance(sp_values, tuple):\n                # multiple outputs. e.g. modf\n                arrays = tuple(\n                    self._simple_new(\n                        sp_value, self.sp_index, SparseDtype(sp_value.dtype, fv)\n                    )\n                    for sp_value, fv in zip(sp_values, fill_value)\n                )\n                return arrays\n            elif is_scalar(sp_values):\n                # e.g. reductions\n                return sp_values\n\n            return self._simple_new(\n                sp_values, self.sp_index, SparseDtype(sp_values.dtype, fill_value)\n            )\n\n        result = getattr(ufunc, method)(*[np.asarray(x) for x in inputs], **kwargs)\n        if out:\n            if len(out) == 1:\n                out = out[0]\n            return out\n\n        if type(result) is tuple:\n            return tuple(type(self)(x) for x in result)\n        elif method == \"at\":\n            # no return value\n            return None\n        else:\n            return type(self)(result)\n\n    def __abs__(self):\n        return np.abs(self)\n\n    # ------------------------------------------------------------------------\n    # Ops\n    # ------------------------------------------------------------------------\n\n    @classmethod\n    def _create_unary_method(cls, op) -> Callable[[\"SparseArray\"], \"SparseArray\"]:\n        def sparse_unary_method(self) -> \"SparseArray\":\n            fill_value = op(np.array(self.fill_value)).item()\n            values = op(self.sp_values)\n            dtype = SparseDtype(values.dtype, fill_value)\n            return cls._simple_new(values, self.sp_index, dtype)\n\n        name = f\"__{op.__name__}__\"\n        return compat.set_function_name(sparse_unary_method, name, cls)\n\n    @classmethod\n    def _create_arithmetic_method(cls, op):\n        op_name = op.__name__\n\n        @unpack_zerodim_and_defer(op_name)\n        def sparse_arithmetic_method(self, other):\n\n            if isinstance(other, SparseArray):\n                return _sparse_array_op(self, other, op, op_name)\n\n            elif is_scalar(other):\n                with np.errstate(all=\"ignore\"):\n                    fill = op(_get_fill(self), np.asarray(other))\n                    result = op(self.sp_values, other)\n\n                if op_name == \"divmod\":\n                    left, right = result\n                    lfill, rfill = fill\n                    return (\n                        _wrap_result(op_name, left, self.sp_index, lfill),\n                        _wrap_result(op_name, right, self.sp_index, rfill),\n                    )\n\n                return _wrap_result(op_name, result, self.sp_index, fill)\n\n            else:\n                other = np.asarray(other)\n                with np.errstate(all=\"ignore\"):\n                    # TODO: look into _wrap_result\n                    if len(self) != len(other):\n                        raise AssertionError(\n                            (f\"length mismatch: {len(self)} vs. {len(other)}\")\n                        )\n                    if not isinstance(other, SparseArray):\n                        dtype = getattr(other, \"dtype\", None)\n                        other = SparseArray(\n                            other, fill_value=self.fill_value, dtype=dtype\n                        )\n                    return _sparse_array_op(self, other, op, op_name)\n\n        name = f\"__{op.__name__}__\"\n        return compat.set_function_name(sparse_arithmetic_method, name, cls)\n\n    @classmethod\n    def _create_comparison_method(cls, op):\n        op_name = op.__name__\n        if op_name in {\"and_\", \"or_\"}:\n            op_name = op_name[:-1]\n\n        @unpack_zerodim_and_defer(op_name)\n        def cmp_method(self, other):\n\n            if not is_scalar(other) and not isinstance(other, type(self)):\n                # convert list-like to ndarray\n                other = np.asarray(other)\n\n            if isinstance(other, np.ndarray):\n                # TODO: make this more flexible than just ndarray...\n                if len(self) != len(other):\n                    raise AssertionError(\n                        f\"length mismatch: {len(self)} vs. {len(other)}\"\n                    )\n                other = SparseArray(other, fill_value=self.fill_value)\n\n            if isinstance(other, SparseArray):\n                return _sparse_array_op(self, other, op, op_name)\n            else:\n                with np.errstate(all=\"ignore\"):\n                    fill_value = op(self.fill_value, other)\n                    result = op(self.sp_values, other)\n\n                return type(self)(\n                    result,\n                    sparse_index=self.sp_index,\n                    fill_value=fill_value,\n                    dtype=np.bool_,\n                )\n\n        name = f\"__{op.__name__}__\"\n        return compat.set_function_name(cmp_method, name, cls)\n\n    @classmethod\n    def _add_unary_ops(cls):\n        cls.__pos__ = cls._create_unary_method(operator.pos)\n        cls.__neg__ = cls._create_unary_method(operator.neg)\n        cls.__invert__ = cls._create_unary_method(operator.invert)\n\n    @classmethod\n    def _add_comparison_ops(cls):\n        cls.__and__ = cls._create_comparison_method(operator.and_)\n        cls.__or__ = cls._create_comparison_method(operator.or_)\n        cls.__xor__ = cls._create_arithmetic_method(operator.xor)\n        super()._add_comparison_ops()\n\n    # ----------\n    # Formatting\n    # -----------\n    def __repr__(self) -> str:\n        pp_str = printing.pprint_thing(self)\n        pp_fill = printing.pprint_thing(self.fill_value)\n        pp_index = printing.pprint_thing(self.sp_index)\n        return f\"{pp_str}\\nFill: {pp_fill}\\n{pp_index}\"\n\n    def _formatter(self, boxed=False):\n        # Defer to the formatter from the GenericArrayFormatter calling us.\n        # This will infer the correct formatter from the dtype of the values.\n        return None\n\n\nSparseArray._add_arithmetic_ops()\nSparseArray._add_comparison_ops()\nSparseArray._add_unary_ops()\n\n\ndef make_sparse(arr: np.ndarray, kind=\"block\", fill_value=None, dtype=None, copy=False):\n    \"\"\"\n    Convert ndarray to sparse format\n\n    Parameters\n    ----------\n    arr : ndarray\n    kind : {'block', 'integer'}\n    fill_value : NaN or another value\n    dtype : np.dtype, optional\n    copy : bool, default False\n\n    Returns\n    -------\n    (sparse_values, index, fill_value) : (ndarray, SparseIndex, Scalar)\n    \"\"\"\n    assert isinstance(arr, np.ndarray)\n\n    if arr.ndim > 1:\n        raise TypeError(\"expected dimension <= 1 data\")\n\n    if fill_value is None:\n        fill_value = na_value_for_dtype(arr.dtype)\n\n    if isna(fill_value):\n        mask = notna(arr)\n    else:\n        # cast to object comparison to be safe\n        if is_string_dtype(arr.dtype):\n            arr = arr.astype(object)\n\n        if is_object_dtype(arr.dtype):\n            # element-wise equality check method in numpy doesn't treat\n            # each element type, eg. 0, 0.0, and False are treated as\n            # same. So we have to check the both of its type and value.\n            mask = splib.make_mask_object_ndarray(arr, fill_value)\n        else:\n            mask = arr != fill_value\n\n    length = len(arr)\n    if length != len(mask):\n        # the arr is a SparseArray\n        indices = mask.sp_index.indices\n    else:\n        indices = mask.nonzero()[0].astype(np.int32)\n\n    index = _make_index(length, indices, kind)\n    sparsified_values = arr[mask]\n    if dtype is not None:\n        sparsified_values = astype_nansafe(sparsified_values, dtype=dtype)\n    # TODO: copy\n    return sparsified_values, index, fill_value\n\n\ndef _make_index(length, indices, kind):\n\n    if kind == \"block\" or isinstance(kind, BlockIndex):\n        locs, lens = splib.get_blocks(indices)\n        index = BlockIndex(length, locs, lens)\n    elif kind == \"integer\" or isinstance(kind, IntIndex):\n        index = IntIndex(length, indices)\n    else:  # pragma: no cover\n        raise ValueError(\"must be block or integer type\")\n    return index\n"
    },
    {
      "filename": "pandas/core/internals/blocks.py",
      "content": "from datetime import datetime, timedelta\nimport inspect\nimport re\nfrom typing import TYPE_CHECKING, Any, List, Optional\nimport warnings\n\nimport numpy as np\n\nfrom pandas._libs import NaT, algos as libalgos, lib, writers\nimport pandas._libs.internals as libinternals\nfrom pandas._libs.internals import BlockPlacement\nfrom pandas._libs.tslibs import conversion\nfrom pandas._libs.tslibs.timezones import tz_compare\nfrom pandas._typing import ArrayLike\nfrom pandas.util._validators import validate_bool_kwarg\n\nfrom pandas.core.dtypes.cast import (\n    astype_nansafe,\n    convert_scalar_for_putitemlike,\n    find_common_type,\n    infer_dtype_from,\n    infer_dtype_from_scalar,\n    maybe_downcast_numeric,\n    maybe_downcast_to_dtype,\n    maybe_infer_dtype_type,\n    maybe_promote,\n    maybe_upcast,\n    soft_convert_objects,\n)\nfrom pandas.core.dtypes.common import (\n    DT64NS_DTYPE,\n    TD64NS_DTYPE,\n    is_bool_dtype,\n    is_categorical_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_dtype_equal,\n    is_extension_array_dtype,\n    is_float_dtype,\n    is_integer,\n    is_integer_dtype,\n    is_interval_dtype,\n    is_list_like,\n    is_object_dtype,\n    is_period_dtype,\n    is_re,\n    is_re_compilable,\n    is_sparse,\n    is_timedelta64_dtype,\n    pandas_dtype,\n)\nfrom pandas.core.dtypes.dtypes import ExtensionDtype\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCIndexClass,\n    ABCPandasArray,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import _isna_compat, is_valid_nat_for_dtype, isna\n\nimport pandas.core.algorithms as algos\nfrom pandas.core.array_algos.transforms import shift\nfrom pandas.core.arrays import (\n    Categorical,\n    DatetimeArray,\n    ExtensionArray,\n    PandasArray,\n    PandasDtype,\n    TimedeltaArray,\n)\nfrom pandas.core.base import PandasObject\nimport pandas.core.common as com\nfrom pandas.core.construction import extract_array\nfrom pandas.core.indexers import (\n    check_setitem_lengths,\n    is_empty_indexer,\n    is_scalar_indexer,\n)\nimport pandas.core.missing as missing\nfrom pandas.core.nanops import nanpercentile\n\nif TYPE_CHECKING:\n    from pandas import Index\n\n\nclass Block(PandasObject):\n    \"\"\"\n    Canonical n-dimensional unit of homogeneous dtype contained in a pandas\n    data structure\n\n    Index-ignorant; let the container take care of that\n    \"\"\"\n\n    __slots__ = [\"_mgr_locs\", \"values\", \"ndim\"]\n    is_numeric = False\n    is_float = False\n    is_integer = False\n    is_complex = False\n    is_datetime = False\n    is_datetimetz = False\n    is_timedelta = False\n    is_bool = False\n    is_object = False\n    is_categorical = False\n    is_extension = False\n    _can_hold_na = False\n    _can_consolidate = True\n    _verify_integrity = True\n    _validate_ndim = True\n\n    @classmethod\n    def _simple_new(\n        cls, values: ArrayLike, placement: BlockPlacement, ndim: int\n    ) -> \"Block\":\n        \"\"\"\n        Fastpath constructor, does *no* validation\n        \"\"\"\n        obj = object.__new__(cls)\n        obj.ndim = ndim\n        obj.values = values\n        obj._mgr_locs = placement\n        return obj\n\n    def __init__(self, values, placement, ndim=None):\n        self.ndim = self._check_ndim(values, ndim)\n        self.mgr_locs = placement\n        self.values = values\n\n        if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):\n            raise ValueError(\n                f\"Wrong number of items passed {len(self.values)}, \"\n                f\"placement implies {len(self.mgr_locs)}\"\n            )\n\n    def _check_ndim(self, values, ndim):\n        \"\"\"\n        ndim inference and validation.\n\n        Infers ndim from 'values' if not provided to __init__.\n        Validates that values.ndim and ndim are consistent if and only if\n        the class variable '_validate_ndim' is True.\n\n        Parameters\n        ----------\n        values : array-like\n        ndim : int or None\n\n        Returns\n        -------\n        ndim : int\n\n        Raises\n        ------\n        ValueError : the number of dimensions do not match\n        \"\"\"\n        if ndim is None:\n            ndim = values.ndim\n\n        if self._validate_ndim and values.ndim != ndim:\n            raise ValueError(\n                \"Wrong number of dimensions. \"\n                f\"values.ndim != ndim [{values.ndim} != {ndim}]\"\n            )\n        return ndim\n\n    @property\n    def _holder(self):\n        \"\"\"\n        The array-like that can hold the underlying values.\n\n        None for 'Block', overridden by subclasses that don't\n        use an ndarray.\n        \"\"\"\n        return None\n\n    @property\n    def _consolidate_key(self):\n        return (self._can_consolidate, self.dtype.name)\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\" return a boolean if I am possibly a view \"\"\"\n        return self.values.base is not None\n\n    @property\n    def is_datelike(self) -> bool:\n        \"\"\" return True if I am a non-datelike \"\"\"\n        return self.is_datetime or self.is_timedelta\n\n    def external_values(self):\n        \"\"\"\n        The array that Series.values returns (public attribute).\n\n        This has some historical constraints, and is overridden in block\n        subclasses to return the correct array (e.g. period returns\n        object ndarray and datetimetz a datetime64[ns] ndarray instead of\n        proper extension array).\n        \"\"\"\n        return self.values\n\n    def internal_values(self):\n        \"\"\"\n        The array that Series._values returns (internal values).\n        \"\"\"\n        return self.values\n\n    def array_values(self) -> ExtensionArray:\n        \"\"\"\n        The array that Series.array returns. Always an ExtensionArray.\n        \"\"\"\n        return PandasArray(self.values)\n\n    def get_values(self, dtype=None):\n        \"\"\"\n        return an internal format, currently just the ndarray\n        this is often overridden to handle to_dense like operations\n        \"\"\"\n        if is_object_dtype(dtype):\n            return self.values.astype(object)\n        return self.values\n\n    def get_block_values_for_json(self) -> np.ndarray:\n        \"\"\"\n        This is used in the JSON C code.\n        \"\"\"\n        # TODO(EA2D): reshape will be unnecessary with 2D EAs\n        return np.asarray(self.values).reshape(self.shape)\n\n    @property\n    def fill_value(self):\n        return np.nan\n\n    @property\n    def mgr_locs(self):\n        return self._mgr_locs\n\n    @mgr_locs.setter\n    def mgr_locs(self, new_mgr_locs):\n        if not isinstance(new_mgr_locs, libinternals.BlockPlacement):\n            new_mgr_locs = libinternals.BlockPlacement(new_mgr_locs)\n\n        self._mgr_locs = new_mgr_locs\n\n    def make_block(self, values, placement=None) -> \"Block\":\n        \"\"\"\n        Create a new block, with type inference propagate any values that are\n        not specified\n        \"\"\"\n        if placement is None:\n            placement = self.mgr_locs\n        if self.is_extension:\n            values = _block_shape(values, ndim=self.ndim)\n\n        return make_block(values, placement=placement, ndim=self.ndim)\n\n    def make_block_same_class(self, values, placement=None, ndim=None):\n        \"\"\" Wrap given values in a block of same type as self. \"\"\"\n        if placement is None:\n            placement = self.mgr_locs\n        if ndim is None:\n            ndim = self.ndim\n        return type(self)(values, placement=placement, ndim=ndim)\n\n    def __repr__(self) -> str:\n        # don't want to print out all of the items here\n        name = type(self).__name__\n        if self.ndim == 1:\n            result = f\"{name}: {len(self)} dtype: {self.dtype}\"\n        else:\n\n            shape = \" x \".join(str(s) for s in self.shape)\n            result = f\"{name}: {self.mgr_locs.indexer}, {shape}, dtype: {self.dtype}\"\n\n        return result\n\n    def __len__(self) -> int:\n        return len(self.values)\n\n    def __getstate__(self):\n        return self.mgr_locs.indexer, self.values\n\n    def __setstate__(self, state):\n        self.mgr_locs = libinternals.BlockPlacement(state[0])\n        self.values = state[1]\n        self.ndim = self.values.ndim\n\n    def _slice(self, slicer):\n        \"\"\" return a slice of my values \"\"\"\n\n        return self.values[slicer]\n\n    def getitem_block(self, slicer, new_mgr_locs=None):\n        \"\"\"\n        Perform __getitem__-like, return result as block.\n\n        As of now, only supports slices that preserve dimensionality.\n        \"\"\"\n        if new_mgr_locs is None:\n            axis0_slicer = slicer[0] if isinstance(slicer, tuple) else slicer\n            new_mgr_locs = self.mgr_locs[axis0_slicer]\n        elif not isinstance(new_mgr_locs, BlockPlacement):\n            new_mgr_locs = BlockPlacement(new_mgr_locs)\n\n        new_values = self._slice(slicer)\n\n        if self._validate_ndim and new_values.ndim != self.ndim:\n            raise ValueError(\"Only same dim slicing is allowed\")\n\n        return type(self)._simple_new(new_values, new_mgr_locs, self.ndim)\n\n    @property\n    def shape(self):\n        return self.values.shape\n\n    @property\n    def dtype(self):\n        return self.values.dtype\n\n    def iget(self, i):\n        return self.values[i]\n\n    def set(self, locs, values):\n        \"\"\"\n        Modify block values in-place with new item value.\n\n        Notes\n        -----\n        `set` never creates a new array or new Block, whereas `setitem` _may_\n        create a new array and always creates a new Block.\n        \"\"\"\n        self.values[locs] = values\n\n    def delete(self, loc) -> None:\n        \"\"\"\n        Delete given loc(-s) from block in-place.\n        \"\"\"\n        self.values = np.delete(self.values, loc, 0)\n        self.mgr_locs = self.mgr_locs.delete(loc)\n\n    def apply(self, func, **kwargs) -> List[\"Block\"]:\n        \"\"\"\n        apply the function to my values; return a block if we are not\n        one\n        \"\"\"\n        with np.errstate(all=\"ignore\"):\n            result = func(self.values, **kwargs)\n\n        return self._split_op_result(result)\n\n    def _split_op_result(self, result) -> List[\"Block\"]:\n        # See also: split_and_operate\n        if is_extension_array_dtype(result) and result.ndim > 1:\n            # TODO(EA2D): unnecessary with 2D EAs\n            # if we get a 2D ExtensionArray, we need to split it into 1D pieces\n            nbs = []\n            for i, loc in enumerate(self.mgr_locs):\n                vals = result[i]\n                block = self.make_block(values=vals, placement=[loc])\n                nbs.append(block)\n            return nbs\n\n        if not isinstance(result, Block):\n            result = self.make_block(result)\n\n        return [result]\n\n    def fillna(\n        self, value, limit=None, inplace: bool = False, downcast=None\n    ) -> List[\"Block\"]:\n        \"\"\"\n        fillna on the block with the value. If we fail, then convert to\n        ObjectBlock and try again\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        mask = isna(self.values)\n        if limit is not None:\n            limit = libalgos._validate_limit(None, limit=limit)\n            mask[mask.cumsum(self.ndim - 1) > limit] = False\n\n        if not self._can_hold_na:\n            if inplace:\n                return [self]\n            else:\n                return [self.copy()]\n\n        if self._can_hold_element(value):\n            # equivalent: _try_coerce_args(value) would not raise\n            blocks = self.putmask(mask, value, inplace=inplace)\n            return self._maybe_downcast(blocks, downcast)\n\n        # we can't process the value, but nothing to do\n        if not mask.any():\n            return [self] if inplace else [self.copy()]\n\n        # operate column-by-column\n        def f(mask, val, idx):\n            block = self.coerce_to_target_dtype(value)\n\n            # slice out our block\n            if idx is not None:\n                # i.e. self.ndim == 2\n                block = block.getitem_block(slice(idx, idx + 1))\n            return block.fillna(value, limit=limit, inplace=inplace, downcast=None)\n\n        return self.split_and_operate(None, f, inplace)\n\n    def split_and_operate(self, mask, f, inplace: bool) -> List[\"Block\"]:\n        \"\"\"\n        split the block per-column, and apply the callable f\n        per-column, return a new block for each. Handle\n        masking which will not change a block unless needed.\n\n        Parameters\n        ----------\n        mask : 2-d boolean mask\n        f : callable accepting (1d-mask, 1d values, indexer)\n        inplace : boolean\n\n        Returns\n        -------\n        list of blocks\n        \"\"\"\n        if mask is None:\n            mask = np.broadcast_to(True, shape=self.shape)\n\n        new_values = self.values\n\n        def make_a_block(nv, ref_loc):\n            if isinstance(nv, list):\n                assert len(nv) == 1, nv\n                assert isinstance(nv[0], Block)\n                block = nv[0]\n            else:\n                # Put back the dimension that was taken from it and make\n                # a block out of the result.\n                nv = _block_shape(nv, ndim=self.ndim)\n                block = self.make_block(values=nv, placement=ref_loc)\n            return block\n\n        # ndim == 1\n        if self.ndim == 1:\n            if mask.any():\n                nv = f(mask, new_values, None)\n            else:\n                nv = new_values if inplace else new_values.copy()\n            block = make_a_block(nv, self.mgr_locs)\n            return [block]\n\n        # ndim > 1\n        new_blocks = []\n        for i, ref_loc in enumerate(self.mgr_locs):\n            m = mask[i]\n            v = new_values[i]\n\n            # need a new block\n            if m.any():\n                nv = f(m, v, i)\n            else:\n                nv = v if inplace else v.copy()\n\n            block = make_a_block(nv, [ref_loc])\n            new_blocks.append(block)\n\n        return new_blocks\n\n    def _maybe_downcast(self, blocks: List[\"Block\"], downcast=None) -> List[\"Block\"]:\n\n        # no need to downcast our float\n        # unless indicated\n        if downcast is None and (\n            self.is_float or self.is_timedelta or self.is_datetime\n        ):\n            return blocks\n\n        return _extend_blocks([b.downcast(downcast) for b in blocks])\n\n    def downcast(self, dtypes=None):\n        \"\"\" try to downcast each item to the dict of dtypes if present \"\"\"\n        # turn it off completely\n        if dtypes is False:\n            return self\n\n        values = self.values\n\n        if self.ndim == 1:\n\n            # try to cast all non-floats here\n            if dtypes is None:\n                dtypes = \"infer\"\n\n            nv = maybe_downcast_to_dtype(values, dtypes)\n            return self.make_block(nv)\n\n        # ndim > 1\n        if dtypes is None:\n            return self\n\n        if not (dtypes == \"infer\" or isinstance(dtypes, dict)):\n            raise ValueError(\n                \"downcast must have a dictionary or 'infer' as its argument\"\n            )\n        elif dtypes != \"infer\":\n            raise AssertionError(\"dtypes as dict is not supported yet\")\n\n        # operate column-by-column\n        # this is expensive as it splits the blocks items-by-item\n        def f(mask, val, idx):\n            val = maybe_downcast_to_dtype(val, dtype=\"infer\")\n            return val\n\n        return self.split_and_operate(None, f, False)\n\n    def astype(self, dtype, copy: bool = False, errors: str = \"raise\"):\n        \"\"\"\n        Coerce to the new dtype.\n\n        Parameters\n        ----------\n        dtype : str, dtype convertible\n        copy : bool, default False\n            copy if indicated\n        errors : str, {'raise', 'ignore'}, default 'ignore'\n            - ``raise`` : allow exceptions to be raised\n            - ``ignore`` : suppress exceptions. On error return original object\n\n        Returns\n        -------\n        Block\n        \"\"\"\n        errors_legal_values = (\"raise\", \"ignore\")\n\n        if errors not in errors_legal_values:\n            invalid_arg = (\n                \"Expected value of kwarg 'errors' to be one of \"\n                f\"{list(errors_legal_values)}. Supplied value is '{errors}'\"\n            )\n            raise ValueError(invalid_arg)\n\n        if inspect.isclass(dtype) and issubclass(dtype, ExtensionDtype):\n            msg = (\n                f\"Expected an instance of {dtype.__name__}, \"\n                \"but got the class instead. Try instantiating 'dtype'.\"\n            )\n            raise TypeError(msg)\n\n        if dtype is not None:\n            dtype = pandas_dtype(dtype)\n\n        # may need to convert to categorical\n        if is_categorical_dtype(dtype):\n\n            if is_categorical_dtype(self.values.dtype):\n                # GH 10696/18593: update an existing categorical efficiently\n                return self.make_block(self.values.astype(dtype, copy=copy))\n\n            return self.make_block(Categorical(self.values, dtype=dtype))\n\n        dtype = pandas_dtype(dtype)\n\n        # astype processing\n        if is_dtype_equal(self.dtype, dtype):\n            if copy:\n                return self.copy()\n            return self\n\n        # force the copy here\n        if self.is_extension:\n            # TODO: Should we try/except this astype?\n            values = self.values.astype(dtype)\n        else:\n            if issubclass(dtype.type, str):\n\n                # use native type formatting for datetime/tz/timedelta\n                if self.is_datelike:\n                    values = self.to_native_types()\n\n                # astype formatting\n                else:\n                    # Because we have neither is_extension nor is_datelike,\n                    #  self.values already has the correct shape\n                    values = self.values\n\n            else:\n                values = self.get_values(dtype=dtype)\n\n            # _astype_nansafe works fine with 1-d only\n            vals1d = values.ravel()\n            try:\n                values = astype_nansafe(vals1d, dtype, copy=True)\n            except (ValueError, TypeError):\n                # e.g. astype_nansafe can fail on object-dtype of strings\n                #  trying to convert to float\n                if errors == \"raise\":\n                    raise\n                newb = self.copy() if copy else self\n                return newb\n\n        # TODO(EA2D): special case not needed with 2D EAs\n        if isinstance(values, np.ndarray):\n            values = values.reshape(self.shape)\n\n        newb = make_block(values, placement=self.mgr_locs, ndim=self.ndim)\n\n        if newb.is_numeric and self.is_numeric:\n            if newb.shape != self.shape:\n                raise TypeError(\n                    f\"cannot set astype for copy = [{copy}] for dtype \"\n                    f\"({self.dtype.name} [{self.shape}]) to different shape \"\n                    f\"({newb.dtype.name} [{newb.shape}])\"\n                )\n        return newb\n\n    def convert(\n        self,\n        copy: bool = True,\n        datetime: bool = True,\n        numeric: bool = True,\n        timedelta: bool = True,\n        coerce: bool = False,\n    ):\n        \"\"\"\n        attempt to coerce any object types to better types return a copy\n        of the block (if copy = True) by definition we are not an ObjectBlock\n        here!\n        \"\"\"\n        return self.copy() if copy else self\n\n    def _can_hold_element(self, element: Any) -> bool:\n        \"\"\" require the same dtype as ourselves \"\"\"\n        dtype = self.values.dtype.type\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return issubclass(tipo.type, dtype)\n        return isinstance(element, dtype)\n\n    def should_store(self, value: ArrayLike) -> bool:\n        \"\"\"\n        Should we set self.values[indexer] = value inplace or do we need to cast?\n\n        Parameters\n        ----------\n        value : np.ndarray or ExtensionArray\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return is_dtype_equal(value.dtype, self.dtype)\n\n    def to_native_types(self, na_rep=\"nan\", quoting=None, **kwargs):\n        \"\"\" convert to our native types format \"\"\"\n        values = self.values\n\n        mask = isna(values)\n        itemsize = writers.word_len(na_rep)\n\n        if not self.is_object and not quoting and itemsize:\n            values = values.astype(str)\n            if values.dtype.itemsize / np.dtype(\"U1\").itemsize < itemsize:\n                # enlarge for the na_rep\n                values = values.astype(f\"<U{itemsize}\")\n        else:\n            values = np.array(values, dtype=\"object\")\n\n        values[mask] = na_rep\n        return values\n\n    # block actions #\n    def copy(self, deep: bool = True):\n        \"\"\" copy constructor \"\"\"\n        values = self.values\n        if deep:\n            values = values.copy()\n        return self.make_block_same_class(values, ndim=self.ndim)\n\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        regex: bool = False,\n        convert: bool = True,\n    ):\n        \"\"\"\n        replace the to_replace value with value, possible to create new\n        blocks here this is just a call to putmask. regex is not used here.\n        It is used in ObjectBlocks.  It is here for API compatibility.\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        original_to_replace = to_replace\n\n        # If we cannot replace with own dtype, convert to ObjectBlock and\n        # retry\n        if not self._can_hold_element(to_replace):\n            if not isinstance(to_replace, list):\n                if inplace:\n                    return [self]\n                return [self.copy()]\n\n            to_replace = [x for x in to_replace if self._can_hold_element(x)]\n            if not len(to_replace):\n                # GH#28084 avoid costly checks since we can infer\n                #  that there is nothing to replace in this block\n                if inplace:\n                    return [self]\n                return [self.copy()]\n\n            if len(to_replace) == 1:\n                # _can_hold_element checks have reduced this back to the\n                #  scalar case and we can avoid a costly object cast\n                return self.replace(\n                    to_replace[0], value, inplace=inplace, regex=regex, convert=convert,\n                )\n\n            # GH 22083, TypeError or ValueError occurred within error handling\n            # causes infinite loop. Cast and retry only if not objectblock.\n            if is_object_dtype(self):\n                raise AssertionError\n\n            # try again with a compatible block\n            block = self.astype(object)\n            return block.replace(\n                to_replace=to_replace,\n                value=value,\n                inplace=inplace,\n                regex=regex,\n                convert=convert,\n            )\n\n        values = self.values\n        if lib.is_scalar(to_replace) and isinstance(values, np.ndarray):\n            # The only non-DatetimeLike class that also has a non-trivial\n            #  try_coerce_args is ObjectBlock, but that overrides replace,\n            #  so does not get here.\n            to_replace = convert_scalar_for_putitemlike(to_replace, values.dtype)\n\n        mask = missing.mask_missing(values, to_replace)\n\n        try:\n            blocks = self.putmask(mask, value, inplace=inplace)\n            # Note: it is _not_ the case that self._can_hold_element(value)\n            #  is always true at this point.  In particular, that can fail\n            #  for:\n            #   \"2u\" with bool-dtype, float-dtype\n            #   0.5 with int64-dtype\n            #   np.nan with int64-dtype\n        except (TypeError, ValueError):\n            # GH 22083, TypeError or ValueError occurred within error handling\n            # causes infinite loop. Cast and retry only if not objectblock.\n            if is_object_dtype(self):\n                raise\n\n            if not self.is_extension:\n                # TODO: https://github.com/pandas-dev/pandas/issues/32586\n                # Need an ExtensionArray._can_hold_element to indicate whether\n                # a scalar value can be placed in the array.\n                assert not self._can_hold_element(value), value\n\n            # try again with a compatible block\n            block = self.astype(object)\n            return block.replace(\n                to_replace=original_to_replace,\n                value=value,\n                inplace=inplace,\n                regex=regex,\n                convert=convert,\n            )\n        if convert:\n            blocks = [b.convert(numeric=False, copy=not inplace) for b in blocks]\n        return blocks\n\n    def _replace_single(self, *args, **kwargs):\n        \"\"\" no-op on a non-ObjectBlock \"\"\"\n        return self if kwargs[\"inplace\"] else self.copy()\n\n    def setitem(self, indexer, value):\n        \"\"\"\n        Attempt self.values[indexer] = value, possibly creating a new array.\n\n        Parameters\n        ----------\n        indexer : tuple, list-like, array-like, slice\n            The subset of self.values to set\n        value : object\n            The value being set\n\n        Returns\n        -------\n        Block\n\n        Notes\n        -----\n        `indexer` is a direct slice/positional indexer. `value` must\n        be a compatible shape.\n        \"\"\"\n        transpose = self.ndim == 2\n\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n\n        # coerce None values, if appropriate\n        if value is None:\n            if self.is_numeric:\n                value = np.nan\n\n        # coerce if block dtype can store value\n        values = self.values\n        if self._can_hold_element(value):\n            # We only get here for non-Extension Blocks, so _try_coerce_args\n            #  is only relevant for DatetimeBlock and TimedeltaBlock\n            if lib.is_scalar(value):\n                value = convert_scalar_for_putitemlike(value, values.dtype)\n\n        else:\n            # current dtype cannot store value, coerce to common dtype\n\n            if hasattr(value, \"dtype\"):\n                dtype = value.dtype\n\n            elif lib.is_scalar(value) and not isna(value):\n                dtype, _ = infer_dtype_from_scalar(value, pandas_dtype=True)\n\n            else:\n                # e.g. we are bool dtype and value is nan\n                # TODO: watch out for case with listlike value and scalar/empty indexer\n                dtype, _ = maybe_promote(np.array(value).dtype)\n                return self.astype(dtype).setitem(indexer, value)\n\n            dtype = find_common_type([values.dtype, dtype])\n            assert not is_dtype_equal(self.dtype, dtype)\n            # otherwise should have _can_hold_element\n\n            return self.astype(dtype).setitem(indexer, value)\n\n        # value must be storable at this moment\n        if is_extension_array_dtype(getattr(value, \"dtype\", None)):\n            # We need to be careful not to allow through strings that\n            #  can be parsed to EADtypes\n            is_ea_value = True\n            arr_value = value\n        else:\n            is_ea_value = False\n            arr_value = np.array(value)\n\n        if transpose:\n            values = values.T\n\n        # length checking\n        check_setitem_lengths(indexer, value, values)\n        exact_match = (\n            len(arr_value.shape)\n            and arr_value.shape[0] == values.shape[0]\n            and arr_value.size == values.size\n        )\n        if is_empty_indexer(indexer, arr_value):\n            # GH#8669 empty indexers\n            pass\n\n        elif is_scalar_indexer(indexer, self.ndim):\n            # setting a single element for each dim and with a rhs that could\n            #  be e.g. a list; see GH#6043\n            values[indexer] = value\n\n        elif exact_match and is_categorical_dtype(arr_value.dtype):\n            # GH25495 - If the current dtype is not categorical,\n            # we need to create a new categorical block\n            values[indexer] = value\n            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))\n\n        elif exact_match and is_ea_value:\n            # GH#32395 if we're going to replace the values entirely, just\n            #  substitute in the new array\n            return self.make_block(arr_value)\n\n        # if we are an exact match (ex-broadcasting),\n        # then use the resultant dtype\n        elif exact_match:\n            # We are setting _all_ of the array's values, so can cast to new dtype\n            values[indexer] = value\n\n            values = values.astype(arr_value.dtype, copy=False)\n\n        # set\n        else:\n            values[indexer] = value\n\n        if transpose:\n            values = values.T\n        block = self.make_block(values)\n        return block\n\n    def putmask(\n        self, mask, new, inplace: bool = False, axis: int = 0, transpose: bool = False,\n    ) -> List[\"Block\"]:\n        \"\"\"\n        putmask the data to the block; it is possible that we may create a\n        new dtype of block\n\n        Return the resulting block(s).\n\n        Parameters\n        ----------\n        mask : np.ndarray[bool], SparseArray[bool], or BooleanArray\n        new : a ndarray/object\n        inplace : bool, default False\n            Perform inplace modification.\n        axis : int\n        transpose : bool, default False\n            Set to True if self is stored with axes reversed.\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        mask = _extract_bool_array(mask)\n        assert not isinstance(new, (ABCIndexClass, ABCSeries, ABCDataFrame))\n\n        new_values = self.values  # delay copy if possible.\n        # if we are passed a scalar None, convert it here\n        if not is_list_like(new) and isna(new) and not self.is_object:\n            # FIXME: make sure we have compatible NA\n            new = self.fill_value\n\n        if self._can_hold_element(new):\n            # We only get here for non-Extension Blocks, so _try_coerce_args\n            #  is only relevant for DatetimeBlock and TimedeltaBlock\n            if lib.is_scalar(new):\n                new = convert_scalar_for_putitemlike(new, self.values.dtype)\n\n            if transpose:\n                new_values = new_values.T\n\n            # If the default repeat behavior in np.putmask would go in the\n            # wrong direction, then explicitly repeat and reshape new instead\n            if getattr(new, \"ndim\", 0) >= 1:\n                if self.ndim - 1 == new.ndim and axis == 1:\n                    new = np.repeat(new, new_values.shape[-1]).reshape(self.shape)\n                new = new.astype(new_values.dtype)\n\n            if new_values is self.values and not inplace:\n                new_values = new_values.copy()\n            # we require exact matches between the len of the\n            # values we are setting (or is compat). np.putmask\n            # doesn't check this and will simply truncate / pad\n            # the output, but we want sane error messages\n            #\n            # TODO: this prob needs some better checking\n            # for 2D cases\n            if (\n                is_list_like(new)\n                and np.any(mask[mask])\n                and getattr(new, \"ndim\", 1) == 1\n            ):\n                if mask[mask].shape[-1] == len(new):\n                    # GH 30567\n                    # If length of ``new`` is less than the length of ``new_values``,\n                    # `np.putmask` would first repeat the ``new`` array and then\n                    # assign the masked values hence produces incorrect result.\n                    # `np.place` on the other hand uses the ``new`` values at it is\n                    # to place in the masked locations of ``new_values``\n                    np.place(new_values, mask, new)\n                elif mask.shape[-1] == len(new) or len(new) == 1:\n                    np.putmask(new_values, mask, new)\n                else:\n                    raise ValueError(\"cannot assign mismatch length to masked array\")\n            else:\n                np.putmask(new_values, mask, new)\n\n        # maybe upcast me\n        elif mask.any():\n            if transpose:\n                mask = mask.T\n                if isinstance(new, np.ndarray):\n                    new = new.T\n                axis = new_values.ndim - axis - 1\n\n            # Pseudo-broadcast\n            if getattr(new, \"ndim\", 0) >= 1:\n                if self.ndim - 1 == new.ndim:\n                    new_shape = list(new.shape)\n                    new_shape.insert(axis, 1)\n                    new = new.reshape(tuple(new_shape))\n\n            # operate column-by-column\n            def f(mask, val, idx):\n\n                if idx is None:\n                    # ndim==1 case.\n                    n = new\n                else:\n\n                    if isinstance(new, np.ndarray):\n                        n = np.squeeze(new[idx % new.shape[0]])\n                    else:\n                        n = np.array(new)\n\n                    # type of the new block\n                    dtype, _ = maybe_promote(n.dtype)\n\n                    # we need to explicitly astype here to make a copy\n                    n = n.astype(dtype)\n\n                nv = _putmask_smart(val, mask, n)\n                return nv\n\n            new_blocks = self.split_and_operate(mask, f, inplace)\n            return new_blocks\n\n        if inplace:\n            return [self]\n\n        if transpose:\n            if new_values is None:\n                new_values = self.values if inplace else self.values.copy()\n            new_values = new_values.T\n\n        return [self.make_block(new_values)]\n\n    def coerce_to_target_dtype(self, other):\n        \"\"\"\n        coerce the current block to a dtype compat for other\n        we will return a block, possibly object, and not raise\n\n        we can also safely try to coerce to the same dtype\n        and will receive the same block\n        \"\"\"\n        # if we cannot then coerce to object\n        dtype, _ = infer_dtype_from(other, pandas_dtype=True)\n\n        if is_dtype_equal(self.dtype, dtype):\n            return self\n\n        if self.is_bool or is_object_dtype(dtype) or is_bool_dtype(dtype):\n            # we don't upcast to bool\n            return self.astype(object)\n\n        elif (self.is_float or self.is_complex) and (\n            is_integer_dtype(dtype) or is_float_dtype(dtype)\n        ):\n            # don't coerce float/complex to int\n            return self\n\n        elif (\n            self.is_datetime\n            or is_datetime64_dtype(dtype)\n            or is_datetime64tz_dtype(dtype)\n        ):\n\n            # not a datetime\n            if not (\n                (is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype))\n                and self.is_datetime\n            ):\n                return self.astype(object)\n\n            # don't upcast timezone with different timezone or no timezone\n            mytz = getattr(self.dtype, \"tz\", None)\n            othertz = getattr(dtype, \"tz\", None)\n\n            if not tz_compare(mytz, othertz):\n                return self.astype(object)\n\n            raise AssertionError(\n                f\"possible recursion in coerce_to_target_dtype: {self} {other}\"\n            )\n\n        elif self.is_timedelta or is_timedelta64_dtype(dtype):\n\n            # not a timedelta\n            if not (is_timedelta64_dtype(dtype) and self.is_timedelta):\n                return self.astype(object)\n\n            raise AssertionError(\n                f\"possible recursion in coerce_to_target_dtype: {self} {other}\"\n            )\n\n        try:\n            return self.astype(dtype)\n        except (ValueError, TypeError, OverflowError):\n            return self.astype(object)\n\n    def interpolate(\n        self,\n        method: str = \"pad\",\n        axis: int = 0,\n        index: Optional[\"Index\"] = None,\n        inplace: bool = False,\n        limit: Optional[int] = None,\n        limit_direction: str = \"forward\",\n        limit_area: Optional[str] = None,\n        fill_value: Optional[Any] = None,\n        coerce: bool = False,\n        downcast: Optional[str] = None,\n        **kwargs,\n    ):\n\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        # Only FloatBlocks will contain NaNs. timedelta subclasses IntBlock\n        if (self.is_bool or self.is_integer) and not self.is_timedelta:\n            return self if inplace else self.copy()\n\n        # a fill na type method\n        try:\n            m = missing.clean_fill_method(method)\n        except ValueError:\n            m = None\n\n        if m is not None:\n            return self._interpolate_with_fill(\n                method=m,\n                axis=axis,\n                inplace=inplace,\n                limit=limit,\n                fill_value=fill_value,\n                coerce=coerce,\n                downcast=downcast,\n            )\n        # validate the interp method\n        m = missing.clean_interp_method(method, **kwargs)\n\n        assert index is not None  # for mypy\n\n        return self._interpolate(\n            method=m,\n            index=index,\n            axis=axis,\n            limit=limit,\n            limit_direction=limit_direction,\n            limit_area=limit_area,\n            fill_value=fill_value,\n            inplace=inplace,\n            downcast=downcast,\n            **kwargs,\n        )\n\n    def _interpolate_with_fill(\n        self,\n        method: str = \"pad\",\n        axis: int = 0,\n        inplace: bool = False,\n        limit: Optional[int] = None,\n        fill_value: Optional[Any] = None,\n        coerce: bool = False,\n        downcast: Optional[str] = None,\n    ) -> List[\"Block\"]:\n        \"\"\" fillna but using the interpolate machinery \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        # if we are coercing, then don't force the conversion\n        # if the block can't hold the type\n        if coerce:\n            if not self._can_hold_na:\n                if inplace:\n                    return [self]\n                else:\n                    return [self.copy()]\n\n        values = self.values if inplace else self.values.copy()\n\n        # We only get here for non-ExtensionBlock\n        fill_value = convert_scalar_for_putitemlike(fill_value, self.values.dtype)\n\n        values = missing.interpolate_2d(\n            values,\n            method=method,\n            axis=axis,\n            limit=limit,\n            fill_value=fill_value,\n            dtype=self.dtype,\n        )\n\n        blocks = [self.make_block_same_class(values, ndim=self.ndim)]\n        return self._maybe_downcast(blocks, downcast)\n\n    def _interpolate(\n        self,\n        method: str,\n        index: \"Index\",\n        fill_value: Optional[Any] = None,\n        axis: int = 0,\n        limit: Optional[int] = None,\n        limit_direction: str = \"forward\",\n        limit_area: Optional[str] = None,\n        inplace: bool = False,\n        downcast: Optional[str] = None,\n        **kwargs,\n    ) -> List[\"Block\"]:\n        \"\"\" interpolate using scipy wrappers \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        data = self.values if inplace else self.values.copy()\n\n        # only deal with floats\n        if not self.is_float:\n            if not self.is_integer:\n                return [self]\n            data = data.astype(np.float64)\n\n        if fill_value is None:\n            fill_value = self.fill_value\n\n        if method in (\"krogh\", \"piecewise_polynomial\", \"pchip\"):\n            if not index.is_monotonic:\n                raise ValueError(\n                    f\"{method} interpolation requires that the index be monotonic.\"\n                )\n        # process 1-d slices in the axis direction\n\n        def func(yvalues: np.ndarray) -> np.ndarray:\n\n            # process a 1-d slice, returning it\n            # should the axis argument be handled below in apply_along_axis?\n            # i.e. not an arg to missing.interpolate_1d\n            return missing.interpolate_1d(\n                xvalues=index,\n                yvalues=yvalues,\n                method=method,\n                limit=limit,\n                limit_direction=limit_direction,\n                limit_area=limit_area,\n                fill_value=fill_value,\n                bounds_error=False,\n                **kwargs,\n            )\n\n        # interp each column independently\n        interp_values = np.apply_along_axis(func, axis, data)\n\n        blocks = [self.make_block_same_class(interp_values)]\n        return self._maybe_downcast(blocks, downcast)\n\n    def take_nd(self, indexer, axis: int, new_mgr_locs=None, fill_value=lib.no_default):\n        \"\"\"\n        Take values according to indexer and return them as a block.bb\n\n        \"\"\"\n        # algos.take_nd dispatches for DatetimeTZBlock, CategoricalBlock\n        # so need to preserve types\n        # sparse is treated like an ndarray, but needs .get_values() shaping\n\n        values = self.values\n\n        if fill_value is lib.no_default:\n            fill_value = self.fill_value\n            allow_fill = False\n        else:\n            allow_fill = True\n\n        new_values = algos.take_nd(\n            values, indexer, axis=axis, allow_fill=allow_fill, fill_value=fill_value\n        )\n\n        # Called from three places in managers, all of which satisfy\n        #  this assertion\n        assert not (axis == 0 and new_mgr_locs is None)\n        if new_mgr_locs is None:\n            new_mgr_locs = self.mgr_locs\n\n        if not is_dtype_equal(new_values.dtype, self.dtype):\n            return self.make_block(new_values, new_mgr_locs)\n        else:\n            return self.make_block_same_class(new_values, new_mgr_locs)\n\n    def diff(self, n: int, axis: int = 1) -> List[\"Block\"]:\n        \"\"\" return block for the diff of the values \"\"\"\n        new_values = algos.diff(self.values, n, axis=axis, stacklevel=7)\n        return [self.make_block(values=new_values)]\n\n    def shift(self, periods: int, axis: int = 0, fill_value=None):\n        \"\"\" shift the block by periods, possibly upcast \"\"\"\n        # convert integer to float if necessary. need to do a lot more than\n        # that, handle boolean etc also\n        new_values, fill_value = maybe_upcast(self.values, fill_value)\n\n        new_values = shift(new_values, periods, axis, fill_value)\n\n        return [self.make_block(new_values)]\n\n    def where(\n        self, other, cond, errors=\"raise\", try_cast: bool = False, axis: int = 0,\n    ) -> List[\"Block\"]:\n        \"\"\"\n        evaluate the block; return result block(s) from the result\n\n        Parameters\n        ----------\n        other : a ndarray/object\n        cond : np.ndarray[bool], SparseArray[bool], or BooleanArray\n        errors : str, {'raise', 'ignore'}, default 'raise'\n            - ``raise`` : allow exceptions to be raised\n            - ``ignore`` : suppress exceptions. On error return original object\n        axis : int, default 0\n\n        Returns\n        -------\n        List[Block]\n        \"\"\"\n        import pandas.core.computation.expressions as expressions\n\n        cond = _extract_bool_array(cond)\n        assert not isinstance(other, (ABCIndexClass, ABCSeries, ABCDataFrame))\n\n        assert errors in [\"raise\", \"ignore\"]\n        transpose = self.ndim == 2\n\n        values = self.values\n        orig_other = other\n        if transpose:\n            values = values.T\n\n        # If the default broadcasting would go in the wrong direction, then\n        # explicitly reshape other instead\n        if getattr(other, \"ndim\", 0) >= 1:\n            if values.ndim - 1 == other.ndim and axis == 1:\n                other = other.reshape(tuple(other.shape + (1,)))\n            elif transpose and values.ndim == self.ndim - 1:\n                cond = cond.T\n\n        if not hasattr(cond, \"shape\"):\n            raise ValueError(\"where must have a condition that is ndarray like\")\n\n        def where_func(cond, values, other):\n\n            if not (\n                (self.is_integer or self.is_bool)\n                and lib.is_float(other)\n                and np.isnan(other)\n            ):\n                # np.where will cast integer array to floats in this case\n                if not self._can_hold_element(other):\n                    raise TypeError\n                if lib.is_scalar(other) and isinstance(values, np.ndarray):\n                    # convert datetime to datetime64, timedelta to timedelta64\n                    other = convert_scalar_for_putitemlike(other, values.dtype)\n\n            # By the time we get here, we should have all Series/Index\n            #  args extracted to  ndarray\n            fastres = expressions.where(cond, values, other)\n            return fastres\n\n        if cond.ravel(\"K\").all():\n            result = values\n        else:\n            # see if we can operate on the entire block, or need item-by-item\n            # or if we are a single block (ndim == 1)\n            try:\n                result = where_func(cond, values, other)\n            except TypeError:\n\n                # we cannot coerce, return a compat dtype\n                # we are explicitly ignoring errors\n                block = self.coerce_to_target_dtype(other)\n                blocks = block.where(\n                    orig_other, cond, errors=errors, try_cast=try_cast, axis=axis,\n                )\n                return self._maybe_downcast(blocks, \"infer\")\n\n        if self._can_hold_na or self.ndim == 1:\n\n            if transpose:\n                result = result.T\n\n            return [self.make_block(result)]\n\n        # might need to separate out blocks\n        axis = cond.ndim - 1\n        cond = cond.swapaxes(axis, 0)\n        mask = np.array([cond[i].all() for i in range(cond.shape[0])], dtype=bool)\n\n        result_blocks = []\n        for m in [mask, ~mask]:\n            if m.any():\n                taken = result.take(m.nonzero()[0], axis=axis)\n                r = maybe_downcast_numeric(taken, self.dtype)\n                nb = self.make_block(r.T, placement=self.mgr_locs[m])\n                result_blocks.append(nb)\n\n        return result_blocks\n\n    def _unstack(self, unstacker, fill_value, new_placement):\n        \"\"\"\n        Return a list of unstacked blocks of self\n\n        Parameters\n        ----------\n        unstacker : reshape._Unstacker\n        fill_value : int\n            Only used in ExtensionBlock._unstack\n\n        Returns\n        -------\n        blocks : list of Block\n            New blocks of unstacked values.\n        mask : array_like of bool\n            The mask of columns of `blocks` we should keep.\n        \"\"\"\n        new_values, mask = unstacker.get_new_values(\n            self.values.T, fill_value=fill_value\n        )\n\n        mask = mask.any(0)\n        # TODO: in all tests we have mask.all(); can we rely on that?\n\n        new_values = new_values.T[mask]\n        new_placement = new_placement[mask]\n\n        blocks = [self.make_block_same_class(new_values, placement=new_placement)]\n        return blocks, mask\n\n    def quantile(self, qs, interpolation=\"linear\", axis: int = 0):\n        \"\"\"\n        compute the quantiles of the\n\n        Parameters\n        ----------\n        qs: a scalar or list of the quantiles to be computed\n        interpolation: type of interpolation, default 'linear'\n        axis: axis to compute, default 0\n\n        Returns\n        -------\n        Block\n        \"\"\"\n        # We should always have ndim == 2 because Series dispatches to DataFrame\n        assert self.ndim == 2\n\n        values = self.get_values()\n\n        is_empty = values.shape[axis] == 0\n        orig_scalar = not is_list_like(qs)\n        if orig_scalar:\n            # make list-like, unpack later\n            qs = [qs]\n\n        if is_empty:\n            # create the array of na_values\n            # 2d len(values) * len(qs)\n            result = np.repeat(\n                np.array([self.fill_value] * len(qs)), len(values)\n            ).reshape(len(values), len(qs))\n        else:\n            # asarray needed for Sparse, see GH#24600\n            mask = np.asarray(isna(values))\n            result = nanpercentile(\n                values,\n                np.array(qs) * 100,\n                axis=axis,\n                na_value=self.fill_value,\n                mask=mask,\n                ndim=values.ndim,\n                interpolation=interpolation,\n            )\n\n            result = np.array(result, copy=False)\n            result = result.T\n\n        if orig_scalar and not lib.is_scalar(result):\n            # result could be scalar in case with is_empty and self.ndim == 1\n            assert result.shape[-1] == 1, result.shape\n            result = result[..., 0]\n            result = lib.item_from_zerodim(result)\n\n        ndim = np.ndim(result)\n        return make_block(result, placement=np.arange(len(result)), ndim=ndim)\n\n    def _replace_coerce(\n        self,\n        to_replace,\n        value,\n        inplace: bool = True,\n        regex: bool = False,\n        convert: bool = False,\n        mask=None,\n    ):\n        \"\"\"\n        Replace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default True\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        A new block if there is anything to replace or the original block.\n        \"\"\"\n        if mask.any():\n            if not regex:\n                self = self.coerce_to_target_dtype(value)\n                return self.putmask(mask, value, inplace=inplace)\n            else:\n                return self._replace_single(\n                    to_replace,\n                    value,\n                    inplace=inplace,\n                    regex=regex,\n                    convert=convert,\n                    mask=mask,\n                )\n        return self\n\n\nclass ExtensionBlock(Block):\n    \"\"\"\n    Block for holding extension types.\n\n    Notes\n    -----\n    This holds all 3rd-party extension array types. It's also the immediate\n    parent class for our internal extension types' blocks, CategoricalBlock.\n\n    ExtensionArrays are limited to 1-D.\n    \"\"\"\n\n    _can_consolidate = False\n    _verify_integrity = False\n    _validate_ndim = False\n    is_extension = True\n\n    def __init__(self, values, placement, ndim=None):\n        \"\"\"\n        Initialize a non-consolidatable block.\n\n        'ndim' may be inferred from 'placement'.\n\n        This will call continue to call __init__ for the other base\n        classes mixed in with this Mixin.\n        \"\"\"\n        values = self._maybe_coerce_values(values)\n\n        # Placement must be converted to BlockPlacement so that we can check\n        # its length\n        if not isinstance(placement, libinternals.BlockPlacement):\n            placement = libinternals.BlockPlacement(placement)\n\n        # Maybe infer ndim from placement\n        if ndim is None:\n            if len(placement) != 1:\n                ndim = 1\n            else:\n                ndim = 2\n        super().__init__(values, placement, ndim=ndim)\n\n        if self.ndim == 2 and len(self.mgr_locs) != 1:\n            # TODO(EA2D): check unnecessary with 2D EAs\n            raise AssertionError(\"block.size != values.size\")\n\n    @property\n    def shape(self):\n        # TODO(EA2D): override unnecessary with 2D EAs\n        if self.ndim == 1:\n            return ((len(self.values)),)\n        return (len(self.mgr_locs), len(self.values))\n\n    def iget(self, col):\n\n        if self.ndim == 2 and isinstance(col, tuple):\n            # TODO(EA2D): unnecessary with 2D EAs\n            col, loc = col\n            if not com.is_null_slice(col) and col != 0:\n                raise IndexError(f\"{self} only contains one item\")\n            elif isinstance(col, slice):\n                if col != slice(None):\n                    raise NotImplementedError(col)\n                return self.values[[loc]]\n            return self.values[loc]\n        else:\n            if col != 0:\n                raise IndexError(f\"{self} only contains one item\")\n            return self.values\n\n    def should_store(self, value: ArrayLike) -> bool:\n        \"\"\"\n        Can we set the given array-like value inplace?\n        \"\"\"\n        return isinstance(value, self._holder)\n\n    def set(self, locs, values):\n        assert locs.tolist() == [0]\n        self.values[:] = values\n\n    def putmask(\n        self, mask, new, inplace: bool = False, axis: int = 0, transpose: bool = False,\n    ) -> List[\"Block\"]:\n        \"\"\"\n        See Block.putmask.__doc__\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        mask = _extract_bool_array(mask)\n\n        new_values = self.values if inplace else self.values.copy()\n\n        if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):\n            new = new[mask]\n\n        mask = _safe_reshape(mask, new_values.shape)\n\n        new_values[mask] = new\n        return [self.make_block(values=new_values)]\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"\n        Unbox to an extension array.\n\n        This will unbox an ExtensionArray stored in an Index or Series.\n        ExtensionArrays pass through. No dtype coercion is done.\n\n        Parameters\n        ----------\n        values : Index, Series, ExtensionArray\n\n        Returns\n        -------\n        ExtensionArray\n        \"\"\"\n        return extract_array(values)\n\n    @property\n    def _holder(self):\n        # For extension blocks, the holder is values-dependent.\n        return type(self.values)\n\n    @property\n    def fill_value(self):\n        # Used in reindex_indexer\n        return self.values.dtype.na_value\n\n    @property\n    def _can_hold_na(self):\n        # The default ExtensionArray._can_hold_na is True\n        return self._holder._can_hold_na\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\"Extension arrays are never treated as views.\"\"\"\n        return False\n\n    @property\n    def is_numeric(self):\n        return self.values.dtype._is_numeric\n\n    def setitem(self, indexer, value):\n        \"\"\"\n        Attempt self.values[indexer] = value, possibly creating a new array.\n\n        This differs from Block.setitem by not allowing setitem to change\n        the dtype of the Block.\n\n        Parameters\n        ----------\n        indexer : tuple, list-like, array-like, slice\n            The subset of self.values to set\n        value : object\n            The value being set\n\n        Returns\n        -------\n        Block\n\n        Notes\n        -----\n        `indexer` is a direct slice/positional indexer. `value` must\n        be a compatible shape.\n        \"\"\"\n        if isinstance(indexer, tuple):\n            # TODO(EA2D): not needed with 2D EAs\n            # we are always 1-D\n            indexer = indexer[0]\n\n        check_setitem_lengths(indexer, value, self.values)\n        self.values[indexer] = value\n        return self\n\n    def get_values(self, dtype=None):\n        # ExtensionArrays must be iterable, so this works.\n        # TODO(EA2D): reshape not needed with 2D EAs\n        return np.asarray(self.values).reshape(self.shape)\n\n    def array_values(self) -> ExtensionArray:\n        return self.values\n\n    def to_native_types(self, na_rep=\"nan\", quoting=None, **kwargs):\n        \"\"\"override to use ExtensionArray astype for the conversion\"\"\"\n        values = self.values\n        mask = isna(values)\n\n        values = np.asarray(values.astype(object))\n        values[mask] = na_rep\n\n        # TODO(EA2D): reshape not needed with 2D EAs\n        # we are expected to return a 2-d ndarray\n        return values.reshape(1, len(values))\n\n    def take_nd(\n        self, indexer, axis: int = 0, new_mgr_locs=None, fill_value=lib.no_default\n    ):\n        \"\"\"\n        Take values according to indexer and return them as a block.\n        \"\"\"\n        if fill_value is lib.no_default:\n            fill_value = None\n\n        # TODO(EA2D): special case not needed with 2D EAs\n        # axis doesn't matter; we are really a single-dim object\n        # but are passed the axis depending on the calling routing\n        # if its REALLY axis 0, then this will be a reindex and not a take\n        new_values = self.values.take(indexer, fill_value=fill_value, allow_fill=True)\n\n        # Called from three places in managers, all of which satisfy\n        #  this assertion\n        assert not (self.ndim == 1 and new_mgr_locs is None)\n        if new_mgr_locs is None:\n            new_mgr_locs = self.mgr_locs\n\n        return self.make_block_same_class(new_values, new_mgr_locs)\n\n    def _can_hold_element(self, element: Any) -> bool:\n        # TODO: We may need to think about pushing this onto the array.\n        # We're doing the same as CategoricalBlock here.\n        return True\n\n    def _slice(self, slicer):\n        \"\"\"\n        Return a slice of my values.\n\n        Parameters\n        ----------\n        slicer : slice, ndarray[int], or a tuple of these\n            Valid (non-reducing) indexer for self.values.\n\n        Returns\n        -------\n        np.ndarray or ExtensionArray\n        \"\"\"\n        # return same dims as we currently have\n        if not isinstance(slicer, tuple) and self.ndim == 2:\n            # reached via getitem_block via _slice_take_blocks_ax0\n            # TODO(EA2D): wont be necessary with 2D EAs\n            slicer = (slicer, slice(None))\n\n        if isinstance(slicer, tuple) and len(slicer) == 2:\n            first = slicer[0]\n            if not isinstance(first, slice):\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", first\n                )\n            # GH#32959 only full-slicers along fake-dim0 are valid\n            # TODO(EA2D): wont be necessary with 2D EAs\n            new_locs = self.mgr_locs[first]\n            if len(new_locs):\n                # effectively slice(None)\n                slicer = slicer[1]\n            else:\n                raise AssertionError(\n                    \"invalid slicing for a 1-ndim ExtensionArray\", slicer\n                )\n\n        return self.values[slicer]\n\n    def fillna(self, value, limit=None, inplace=False, downcast=None):\n        values = self.values if inplace else self.values.copy()\n        values = values.fillna(value=value, limit=limit)\n        return [\n            self.make_block_same_class(\n                values=values, placement=self.mgr_locs, ndim=self.ndim\n            )\n        ]\n\n    def interpolate(\n        self, method=\"pad\", axis=0, inplace=False, limit=None, fill_value=None, **kwargs\n    ):\n\n        values = self.values if inplace else self.values.copy()\n        return self.make_block_same_class(\n            values=values.fillna(value=fill_value, method=method, limit=limit),\n            placement=self.mgr_locs,\n        )\n\n    def diff(self, n: int, axis: int = 1) -> List[\"Block\"]:\n        if axis == 0 and n != 0:\n            # n==0 case will be a no-op so let is fall through\n            # Since we only have one column, the result will be all-NA.\n            #  Create this result by shifting along axis=0 past the length of\n            #  our values.\n            return super().diff(len(self.values), axis=0)\n        if axis == 1:\n            # TODO(EA2D): unnecessary with 2D EAs\n            # we are by definition 1D.\n            axis = 0\n        return super().diff(n, axis)\n\n    def shift(\n        self, periods: int, axis: int = 0, fill_value: Any = None,\n    ) -> List[\"ExtensionBlock\"]:\n        \"\"\"\n        Shift the block by `periods`.\n\n        Dispatches to underlying ExtensionArray and re-boxes in an\n        ExtensionBlock.\n        \"\"\"\n        return [\n            self.make_block_same_class(\n                self.values.shift(periods=periods, fill_value=fill_value),\n                placement=self.mgr_locs,\n                ndim=self.ndim,\n            )\n        ]\n\n    def where(\n        self, other, cond, errors=\"raise\", try_cast: bool = False, axis: int = 0,\n    ) -> List[\"Block\"]:\n\n        cond = _extract_bool_array(cond)\n        assert not isinstance(other, (ABCIndexClass, ABCSeries, ABCDataFrame))\n\n        if isinstance(other, np.ndarray) and other.ndim == 2:\n            # TODO(EA2D): unnecessary with 2D EAs\n            assert other.shape[1] == 1\n            other = other[:, 0]\n\n        if isinstance(cond, np.ndarray) and cond.ndim == 2:\n            # TODO(EA2D): unnecessary with 2D EAs\n            assert cond.shape[1] == 1\n            cond = cond[:, 0]\n\n        if lib.is_scalar(other) and isna(other):\n            # The default `other` for Series / Frame is np.nan\n            # we want to replace that with the correct NA value\n            # for the type\n            other = self.dtype.na_value\n\n        if is_sparse(self.values):\n            # TODO(SparseArray.__setitem__): remove this if condition\n            # We need to re-infer the type of the data after doing the\n            # where, for cases where the subtypes don't match\n            dtype = None\n        else:\n            dtype = self.dtype\n\n        result = self.values.copy()\n        icond = ~cond\n        if lib.is_scalar(other):\n            set_other = other\n        else:\n            set_other = other[icond]\n        try:\n            result[icond] = set_other\n        except (NotImplementedError, TypeError):\n            # NotImplementedError for class not implementing `__setitem__`\n            # TypeError for SparseArray, which implements just to raise\n            # a TypeError\n            result = self._holder._from_sequence(\n                np.where(cond, self.values, other), dtype=dtype\n            )\n\n        return [self.make_block_same_class(result, placement=self.mgr_locs)]\n\n    def _unstack(self, unstacker, fill_value, new_placement):\n        # ExtensionArray-safe unstack.\n        # We override ObjectBlock._unstack, which unstacks directly on the\n        # values of the array. For EA-backed blocks, this would require\n        # converting to a 2-D ndarray of objects.\n        # Instead, we unstack an ndarray of integer positions, followed by\n        # a `take` on the actual values.\n        n_rows = self.shape[-1]\n        dummy_arr = np.arange(n_rows)\n\n        new_values, mask = unstacker.get_new_values(dummy_arr, fill_value=-1)\n        mask = mask.any(0)\n        # TODO: in all tests we have mask.all(); can we rely on that?\n\n        blocks = [\n            self.make_block_same_class(\n                self.values.take(indices, allow_fill=True, fill_value=fill_value),\n                [place],\n            )\n            for indices, place in zip(new_values.T, new_placement)\n        ]\n        return blocks, mask\n\n\nclass ObjectValuesExtensionBlock(ExtensionBlock):\n    \"\"\"\n    Block providing backwards-compatibility for `.values`.\n\n    Used by PeriodArray and IntervalArray to ensure that\n    Series[T].values is an ndarray of objects.\n    \"\"\"\n\n    def external_values(self):\n        return self.values.astype(object)\n\n\nclass NumericBlock(Block):\n    __slots__ = ()\n    is_numeric = True\n    _can_hold_na = True\n\n\nclass FloatOrComplexBlock(NumericBlock):\n    __slots__ = ()\n\n\nclass FloatBlock(FloatOrComplexBlock):\n    __slots__ = ()\n    is_float = True\n\n    def _can_hold_element(self, element: Any) -> bool:\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return issubclass(tipo.type, (np.floating, np.integer)) and not issubclass(\n                tipo.type, (np.datetime64, np.timedelta64)\n            )\n        return isinstance(\n            element, (float, int, np.floating, np.int_)\n        ) and not isinstance(\n            element,\n            (bool, np.bool_, datetime, timedelta, np.datetime64, np.timedelta64),\n        )\n\n    def to_native_types(\n        self, na_rep=\"\", float_format=None, decimal=\".\", quoting=None, **kwargs,\n    ):\n        \"\"\" convert to our native types format \"\"\"\n        values = self.values\n\n        # see gh-13418: no special formatting is desired at the\n        # output (important for appropriate 'quoting' behaviour),\n        # so do not pass it through the FloatArrayFormatter\n        if float_format is None and decimal == \".\":\n            mask = isna(values)\n\n            if not quoting:\n                values = values.astype(str)\n            else:\n                values = np.array(values, dtype=\"object\")\n\n            values[mask] = na_rep\n            return values\n\n        from pandas.io.formats.format import FloatArrayFormatter\n\n        formatter = FloatArrayFormatter(\n            values,\n            na_rep=na_rep,\n            float_format=float_format,\n            decimal=decimal,\n            quoting=quoting,\n            fixed_width=False,\n        )\n        return formatter.get_result_as_array()\n\n\nclass ComplexBlock(FloatOrComplexBlock):\n    __slots__ = ()\n    is_complex = True\n\n    def _can_hold_element(self, element: Any) -> bool:\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return issubclass(tipo.type, (np.floating, np.integer, np.complexfloating))\n        return isinstance(\n            element, (float, int, complex, np.float_, np.int_)\n        ) and not isinstance(element, (bool, np.bool_))\n\n    def should_store(self, value: ArrayLike) -> bool:\n        return issubclass(value.dtype.type, np.complexfloating)\n\n\nclass IntBlock(NumericBlock):\n    __slots__ = ()\n    is_integer = True\n    _can_hold_na = False\n\n    def _can_hold_element(self, element: Any) -> bool:\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return (\n                issubclass(tipo.type, np.integer)\n                and not issubclass(tipo.type, (np.datetime64, np.timedelta64))\n                and self.dtype.itemsize >= tipo.itemsize\n            )\n        return is_integer(element)\n\n\nclass DatetimeLikeBlockMixin:\n    \"\"\"Mixin class for DatetimeBlock, DatetimeTZBlock, and TimedeltaBlock.\"\"\"\n\n    @property\n    def _holder(self):\n        return DatetimeArray\n\n    @property\n    def fill_value(self):\n        return np.datetime64(\"NaT\", \"ns\")\n\n    def get_values(self, dtype=None):\n        \"\"\"\n        return object dtype as boxed values, such as Timestamps/Timedelta\n        \"\"\"\n        if is_object_dtype(dtype):\n            # DTA/TDA constructor and astype can handle 2D\n            return self._holder(self.values).astype(object)\n        return self.values\n\n    def internal_values(self):\n        # Override to return DatetimeArray and TimedeltaArray\n        return self.array_values()\n\n    def array_values(self):\n        return self._holder._simple_new(self.values)\n\n    def iget(self, key):\n        # GH#31649 we need to wrap scalars in Timestamp/Timedelta\n        # TODO(EA2D): this can be removed if we ever have 2D EA\n        return self.array_values().reshape(self.shape)[key]\n\n    def shift(self, periods, axis=0, fill_value=None):\n        # TODO(EA2D) this is unnecessary if these blocks are backed by 2D EAs\n        values = self.array_values()\n        new_values = values.shift(periods, fill_value=fill_value, axis=axis)\n        return self.make_block_same_class(new_values)\n\n\nclass DatetimeBlock(DatetimeLikeBlockMixin, Block):\n    __slots__ = ()\n    is_datetime = True\n\n    def __init__(self, values, placement, ndim=None):\n        values = self._maybe_coerce_values(values)\n        super().__init__(values, placement=placement, ndim=ndim)\n\n    @property\n    def _can_hold_na(self):\n        return True\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"\n        Input validation for values passed to __init__. Ensure that\n        we have datetime64ns, coercing if necessary.\n\n        Parameters\n        ----------\n        values : array-like\n            Must be convertible to datetime64\n\n        Returns\n        -------\n        values : ndarray[datetime64ns]\n\n        Overridden by DatetimeTZBlock.\n        \"\"\"\n        if values.dtype != DT64NS_DTYPE:\n            values = conversion.ensure_datetime64ns(values)\n\n        if isinstance(values, DatetimeArray):\n            values = values._data\n\n        assert isinstance(values, np.ndarray), type(values)\n        return values\n\n    def astype(self, dtype, copy: bool = False, errors: str = \"raise\"):\n        \"\"\"\n        these automatically copy, so copy=True has no effect\n        raise on an except if raise == True\n        \"\"\"\n        dtype = pandas_dtype(dtype)\n\n        # if we are passed a datetime64[ns, tz]\n        if is_datetime64tz_dtype(dtype):\n            values = self.values\n            if copy:\n                # this should be the only copy\n                values = values.copy()\n            if getattr(values, \"tz\", None) is None:\n                values = DatetimeArray(values).tz_localize(\"UTC\")\n            values = values.tz_convert(dtype.tz)\n            return self.make_block(values)\n\n        # delegate\n        return super().astype(dtype=dtype, copy=copy, errors=errors)\n\n    def _can_hold_element(self, element: Any) -> bool:\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            if self.is_datetimetz:\n                # require exact match, since non-nano does not exist\n                return is_dtype_equal(tipo, self.dtype) or is_valid_nat_for_dtype(\n                    element, self.dtype\n                )\n\n            # GH#27419 if we get a non-nano datetime64 object\n            return is_datetime64_dtype(tipo)\n        elif element is NaT:\n            return True\n        elif isinstance(element, datetime):\n            if self.is_datetimetz:\n                return tz_compare(element.tzinfo, self.dtype.tz)\n            return element.tzinfo is None\n\n        return is_valid_nat_for_dtype(element, self.dtype)\n\n    def to_native_types(self, na_rep=\"NaT\", date_format=None, **kwargs):\n        \"\"\" convert to our native types format \"\"\"\n        dta = self.array_values()\n\n        result = dta._format_native_types(\n            na_rep=na_rep, date_format=date_format, **kwargs\n        )\n        return np.atleast_2d(result)\n\n    def set(self, locs, values):\n        \"\"\"\n        See Block.set.__doc__\n        \"\"\"\n        values = conversion.ensure_datetime64ns(values, copy=False)\n\n        self.values[locs] = values\n\n\nclass DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n    \"\"\" implement a datetime64 block with a tz attribute \"\"\"\n\n    __slots__ = ()\n    is_datetimetz = True\n    is_extension = True\n\n    internal_values = Block.internal_values\n    _can_hold_element = DatetimeBlock._can_hold_element\n    to_native_types = DatetimeBlock.to_native_types\n    fill_value = np.datetime64(\"NaT\", \"ns\")\n    should_store = Block.should_store\n    array_values = ExtensionBlock.array_values\n\n    @property\n    def _holder(self):\n        return DatetimeArray\n\n    def _maybe_coerce_values(self, values):\n        \"\"\"\n        Input validation for values passed to __init__. Ensure that\n        we have datetime64TZ, coercing if necessary.\n\n        Parameters\n        ----------\n        values : array-like\n            Must be convertible to datetime64\n\n        Returns\n        -------\n        values : DatetimeArray\n        \"\"\"\n        if not isinstance(values, self._holder):\n            values = self._holder(values)\n\n        if values.tz is None:\n            raise ValueError(\"cannot create a DatetimeTZBlock without a tz\")\n\n        return values\n\n    @property\n    def is_view(self) -> bool:\n        \"\"\" return a boolean if I am possibly a view \"\"\"\n        # check the ndarray values of the DatetimeIndex values\n        return self.values._data.base is not None\n\n    def get_values(self, dtype=None):\n        \"\"\"\n        Returns an ndarray of values.\n\n        Parameters\n        ----------\n        dtype : np.dtype\n            Only `object`-like dtypes are respected here (not sure\n            why).\n\n        Returns\n        -------\n        values : ndarray\n            When ``dtype=object``, then and object-dtype ndarray of\n            boxed values is returned. Otherwise, an M8[ns] ndarray\n            is returned.\n\n            DatetimeArray is always 1-d. ``get_values`` will reshape\n            the return value to be the same dimensionality as the\n            block.\n        \"\"\"\n        values = self.values\n        if is_object_dtype(dtype):\n            values = values.astype(object)\n\n        # TODO(EA2D): reshape unnecessary with 2D EAs\n        # Ensure that our shape is correct for DataFrame.\n        # ExtensionArrays are always 1-D, even in a DataFrame when\n        # the analogous NumPy-backed column would be a 2-D ndarray.\n        return np.asarray(values).reshape(self.shape)\n\n    def external_values(self):\n        # NB: this is different from np.asarray(self.values), since that\n        #  return an object-dtype ndarray of Timestamps.\n        return np.asarray(self.values.astype(\"datetime64[ns]\", copy=False))\n\n    def diff(self, n: int, axis: int = 0) -> List[\"Block\"]:\n        \"\"\"\n        1st discrete difference.\n\n        Parameters\n        ----------\n        n : int\n            Number of periods to diff.\n        axis : int, default 0\n            Axis to diff upon.\n\n        Returns\n        -------\n        A list with a new TimeDeltaBlock.\n\n        Notes\n        -----\n        The arguments here are mimicking shift so they are called correctly\n        by apply.\n        \"\"\"\n        if axis == 0:\n            # TODO(EA2D): special case not needed with 2D EAs\n            # Cannot currently calculate diff across multiple blocks since this\n            # function is invoked via apply\n            raise NotImplementedError\n\n        if n == 0:\n            # Fastpath avoids making a copy in `shift`\n            new_values = np.zeros(self.values.shape, dtype=np.int64)\n        else:\n            new_values = (self.values - self.shift(n, axis=axis)[0].values).asi8\n\n        # Reshape the new_values like how algos.diff does for timedelta data\n        new_values = new_values.reshape(1, len(new_values))\n        new_values = new_values.astype(\"timedelta64[ns]\")\n        return [TimeDeltaBlock(new_values, placement=self.mgr_locs.indexer)]\n\n    def fillna(self, value, limit=None, inplace=False, downcast=None):\n        # We support filling a DatetimeTZ with a `value` whose timezone\n        # is different by coercing to object.\n        if self._can_hold_element(value):\n            return super().fillna(value, limit, inplace, downcast)\n\n        # different timezones, or a non-tz\n        return self.astype(object).fillna(\n            value, limit=limit, inplace=inplace, downcast=downcast\n        )\n\n    def setitem(self, indexer, value):\n        # https://github.com/pandas-dev/pandas/issues/24020\n        # Need a dedicated setitem until #24020 (type promotion in setitem\n        # for extension arrays) is designed and implemented.\n        if self._can_hold_element(value) or (\n            isinstance(indexer, np.ndarray) and indexer.size == 0\n        ):\n            return super().setitem(indexer, value)\n\n        obj_vals = self.values.astype(object)\n        newb = make_block(\n            obj_vals, placement=self.mgr_locs, klass=ObjectBlock, ndim=self.ndim\n        )\n        return newb.setitem(indexer, value)\n\n    def quantile(self, qs, interpolation=\"linear\", axis=0):\n        naive = self.values.view(\"M8[ns]\")\n\n        # TODO(EA2D): kludge for 2D block with 1D values\n        naive = naive.reshape(self.shape)\n\n        blk = self.make_block(naive)\n        res_blk = blk.quantile(qs, interpolation=interpolation, axis=axis)\n\n        # TODO(EA2D): ravel is kludge for 2D block with 1D values, assumes column-like\n        aware = self._holder(res_blk.values.ravel(), dtype=self.dtype)\n        return self.make_block_same_class(aware, ndim=res_blk.ndim)\n\n\nclass TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n    __slots__ = ()\n    is_timedelta = True\n    _can_hold_na = True\n    is_numeric = False\n    fill_value = np.timedelta64(\"NaT\", \"ns\")\n\n    def __init__(self, values, placement, ndim=None):\n        if values.dtype != TD64NS_DTYPE:\n            # e.g. non-nano or int64\n            values = TimedeltaArray._from_sequence(values)._data\n        if isinstance(values, TimedeltaArray):\n            values = values._data\n        assert isinstance(values, np.ndarray), type(values)\n        super().__init__(values, placement=placement, ndim=ndim)\n\n    @property\n    def _holder(self):\n        return TimedeltaArray\n\n    def _can_hold_element(self, element: Any) -> bool:\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return issubclass(tipo.type, np.timedelta64)\n        elif element is NaT:\n            return True\n        elif isinstance(element, (timedelta, np.timedelta64)):\n            return True\n        return is_valid_nat_for_dtype(element, self.dtype)\n\n    def fillna(self, value, **kwargs):\n\n        # allow filling with integers to be\n        # interpreted as nanoseconds\n        if is_integer(value):\n            # Deprecation GH#24694, GH#19233\n            raise TypeError(\n                \"Passing integers to fillna for timedelta64[ns] dtype is no \"\n                \"longer supported.  To obtain the old behavior, pass \"\n                \"`pd.Timedelta(seconds=n)` instead.\"\n            )\n        return super().fillna(value, **kwargs)\n\n    def to_native_types(self, na_rep=\"NaT\", **kwargs):\n        \"\"\" convert to our native types format \"\"\"\n        tda = self.array_values()\n        return tda._format_native_types(na_rep, **kwargs)\n\n\nclass BoolBlock(NumericBlock):\n    __slots__ = ()\n    is_bool = True\n    _can_hold_na = False\n\n    def _can_hold_element(self, element: Any) -> bool:\n        tipo = maybe_infer_dtype_type(element)\n        if tipo is not None:\n            return issubclass(tipo.type, np.bool_)\n        return isinstance(element, (bool, np.bool_))\n\n    def replace(self, to_replace, value, inplace=False, regex=False, convert=True):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        to_replace_values = np.atleast_1d(to_replace)\n        if not np.can_cast(to_replace_values, bool):\n            return self\n        return super().replace(\n            to_replace, value, inplace=inplace, regex=regex, convert=convert,\n        )\n\n\nclass ObjectBlock(Block):\n    __slots__ = ()\n    is_object = True\n    _can_hold_na = True\n\n    def __init__(self, values, placement=None, ndim=2):\n        if issubclass(values.dtype.type, str):\n            values = np.array(values, dtype=object)\n\n        super().__init__(values, ndim=ndim, placement=placement)\n\n    @property\n    def is_bool(self):\n        \"\"\"\n        we can be a bool if we have only bool values but are of type\n        object\n        \"\"\"\n        return lib.is_bool_array(self.values.ravel(\"K\"))\n\n    def convert(\n        self,\n        copy: bool = True,\n        datetime: bool = True,\n        numeric: bool = True,\n        timedelta: bool = True,\n        coerce: bool = False,\n    ):\n        \"\"\"\n        attempt to coerce any object types to better types return a copy of\n        the block (if copy = True) by definition we ARE an ObjectBlock!!!!!\n\n        can return multiple blocks!\n        \"\"\"\n        # operate column-by-column\n        def f(mask, val, idx):\n            shape = val.shape\n            values = soft_convert_objects(\n                val.ravel(),\n                datetime=datetime,\n                numeric=numeric,\n                timedelta=timedelta,\n                coerce=coerce,\n                copy=copy,\n            )\n            if isinstance(values, np.ndarray):\n                # TODO(EA2D): allow EA once reshape is supported\n                values = values.reshape(shape)\n\n            return values\n\n        if self.ndim == 2:\n            blocks = self.split_and_operate(None, f, False)\n        else:\n            values = f(None, self.values.ravel(), None)\n            blocks = [make_block(values, ndim=self.ndim, placement=self.mgr_locs)]\n\n        return blocks\n\n    def _maybe_downcast(self, blocks: List[\"Block\"], downcast=None) -> List[\"Block\"]:\n\n        if downcast is not None:\n            return blocks\n\n        # split and convert the blocks\n        return _extend_blocks([b.convert(datetime=True, numeric=False) for b in blocks])\n\n    def _can_hold_element(self, element: Any) -> bool:\n        return True\n\n    def replace(self, to_replace, value, inplace=False, regex=False, convert=True):\n        to_rep_is_list = is_list_like(to_replace)\n        value_is_list = is_list_like(value)\n        both_lists = to_rep_is_list and value_is_list\n        either_list = to_rep_is_list or value_is_list\n\n        result_blocks = []\n        blocks = [self]\n\n        if not either_list and is_re(to_replace):\n            return self._replace_single(\n                to_replace, value, inplace=inplace, regex=True, convert=convert,\n            )\n        elif not (either_list or regex):\n            return super().replace(\n                to_replace, value, inplace=inplace, regex=regex, convert=convert,\n            )\n        elif both_lists:\n            for to_rep, v in zip(to_replace, value):\n                result_blocks = []\n                for b in blocks:\n                    result = b._replace_single(\n                        to_rep, v, inplace=inplace, regex=regex, convert=convert,\n                    )\n                    result_blocks = _extend_blocks(result, result_blocks)\n                blocks = result_blocks\n            return result_blocks\n\n        elif to_rep_is_list and regex:\n            for to_rep in to_replace:\n                result_blocks = []\n                for b in blocks:\n                    result = b._replace_single(\n                        to_rep, value, inplace=inplace, regex=regex, convert=convert,\n                    )\n                    result_blocks = _extend_blocks(result, result_blocks)\n                blocks = result_blocks\n            return result_blocks\n\n        return self._replace_single(\n            to_replace, value, inplace=inplace, convert=convert, regex=regex,\n        )\n\n    def _replace_single(\n        self, to_replace, value, inplace=False, regex=False, convert=True, mask=None,\n    ):\n        \"\"\"\n        Replace elements by the given value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        a new block, the result after replacing\n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n\n        # to_replace is regex compilable\n        to_rep_re = regex and is_re_compilable(to_replace)\n\n        # regex is regex compilable\n        regex_re = is_re_compilable(regex)\n\n        # only one will survive\n        if to_rep_re and regex_re:\n            raise AssertionError(\n                \"only one of to_replace and regex can be regex compilable\"\n            )\n\n        # if regex was passed as something that can be a regex (rather than a\n        # boolean)\n        if regex_re:\n            to_replace = regex\n\n        regex = regex_re or to_rep_re\n\n        # try to get the pattern attribute (compiled re) or it's a string\n        if is_re(to_replace):\n            pattern = to_replace.pattern\n        else:\n            pattern = to_replace\n\n        # if the pattern is not empty and to_replace is either a string or a\n        # regex\n        if regex and pattern:\n            rx = re.compile(to_replace)\n        else:\n            # if the thing to replace is not a string or compiled regex call\n            # the superclass method -> to_replace is some kind of object\n            return super().replace(to_replace, value, inplace=inplace, regex=regex)\n\n        new_values = self.values if inplace else self.values.copy()\n\n        # deal with replacing values with objects (strings) that match but\n        # whose replacement is not a string (numeric, nan, object)\n        if isna(value) or not isinstance(value, str):\n\n            def re_replacer(s):\n                if is_re(rx) and isinstance(s, str):\n                    return value if rx.search(s) is not None else s\n                else:\n                    return s\n\n        else:\n            # value is guaranteed to be a string here, s can be either a string\n            # or null if it's null it gets returned\n            def re_replacer(s):\n                if is_re(rx) and isinstance(s, str):\n                    return rx.sub(value, s)\n                else:\n                    return s\n\n        f = np.vectorize(re_replacer, otypes=[self.dtype])\n\n        if mask is None:\n            new_values[:] = f(new_values)\n        else:\n            new_values[mask] = f(new_values[mask])\n\n        # convert\n        block = self.make_block(new_values)\n        if convert:\n            block = block.convert(numeric=False)\n        return block\n\n    def _replace_coerce(\n        self, to_replace, value, inplace=True, regex=False, convert=False, mask=None\n    ):\n        \"\"\"\n        Replace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        A new block if there is anything to replace or the original block.\n        \"\"\"\n        if mask.any():\n            block = super()._replace_coerce(\n                to_replace=to_replace,\n                value=value,\n                inplace=inplace,\n                regex=regex,\n                convert=convert,\n                mask=mask,\n            )\n            if convert:\n                block = [b.convert(numeric=False, copy=True) for b in block]\n            return block\n        if convert:\n            return [self.convert(numeric=False, copy=True)]\n        return self\n\n\nclass CategoricalBlock(ExtensionBlock):\n    __slots__ = ()\n    is_categorical = True\n    _verify_integrity = True\n    _can_hold_na = True\n\n    should_store = Block.should_store\n\n    def __init__(self, values, placement, ndim=None):\n        # coerce to categorical if we can\n        values = extract_array(values)\n        assert isinstance(values, Categorical), type(values)\n        super().__init__(values, placement=placement, ndim=ndim)\n\n    @property\n    def _holder(self):\n        return Categorical\n\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        regex: bool = False,\n        convert: bool = True,\n    ):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = self if inplace else self.copy()\n\n        result.values.replace(to_replace, value, inplace=True)\n        return result\n\n\n# -----------------------------------------------------------------\n# Constructor Helpers\n\n\ndef get_block_type(values, dtype=None):\n    \"\"\"\n    Find the appropriate Block subclass to use for the given values and dtype.\n\n    Parameters\n    ----------\n    values : ndarray-like\n    dtype : numpy or pandas dtype\n\n    Returns\n    -------\n    cls : class, subclass of Block\n    \"\"\"\n    dtype = dtype or values.dtype\n    vtype = dtype.type\n\n    if is_sparse(dtype):\n        # Need this first(ish) so that Sparse[datetime] is sparse\n        cls = ExtensionBlock\n    elif is_categorical_dtype(values.dtype):\n        cls = CategoricalBlock\n    elif issubclass(vtype, np.datetime64):\n        assert not is_datetime64tz_dtype(values.dtype)\n        cls = DatetimeBlock\n    elif is_datetime64tz_dtype(values.dtype):\n        cls = DatetimeTZBlock\n    elif is_interval_dtype(dtype) or is_period_dtype(dtype):\n        cls = ObjectValuesExtensionBlock\n    elif is_extension_array_dtype(values.dtype):\n        cls = ExtensionBlock\n    elif issubclass(vtype, np.floating):\n        cls = FloatBlock\n    elif issubclass(vtype, np.timedelta64):\n        assert issubclass(vtype, np.integer)\n        cls = TimeDeltaBlock\n    elif issubclass(vtype, np.complexfloating):\n        cls = ComplexBlock\n    elif issubclass(vtype, np.integer):\n        cls = IntBlock\n    elif dtype == np.bool_:\n        cls = BoolBlock\n    else:\n        cls = ObjectBlock\n    return cls\n\n\ndef make_block(values, placement, klass=None, ndim=None, dtype=None):\n    # Ensure that we don't allow PandasArray / PandasDtype in internals.\n    # For now, blocks should be backed by ndarrays when possible.\n    if isinstance(values, ABCPandasArray):\n        values = values.to_numpy()\n        if ndim and ndim > 1:\n            # TODO(EA2D): special case not needed with 2D EAs\n            values = np.atleast_2d(values)\n\n    if isinstance(dtype, PandasDtype):\n        dtype = dtype.numpy_dtype\n\n    if klass is None:\n        dtype = dtype or values.dtype\n        klass = get_block_type(values, dtype)\n\n    elif klass is DatetimeTZBlock and not is_datetime64tz_dtype(values.dtype):\n        # TODO: This is no longer hit internally; does it need to be retained\n        #  for e.g. pyarrow?\n        values = DatetimeArray._simple_new(values, dtype=dtype)\n\n    return klass(values, ndim=ndim, placement=placement)\n\n\n# -----------------------------------------------------------------\n\n\ndef _extend_blocks(result, blocks=None):\n    \"\"\" return a new extended blocks, given the result \"\"\"\n    if blocks is None:\n        blocks = []\n    if isinstance(result, list):\n        for r in result:\n            if isinstance(r, list):\n                blocks.extend(r)\n            else:\n                blocks.append(r)\n    else:\n        assert isinstance(result, Block), type(result)\n        blocks.append(result)\n    return blocks\n\n\ndef _block_shape(values: ArrayLike, ndim: int = 1) -> ArrayLike:\n    \"\"\" guarantee the shape of the values to be at least 1 d \"\"\"\n    if values.ndim < ndim:\n        shape = values.shape\n        if not is_extension_array_dtype(values.dtype):\n            # TODO(EA2D): https://github.com/pandas-dev/pandas/issues/23023\n            # block.shape is incorrect for \"2D\" ExtensionArrays\n            # We can't, and don't need to, reshape.\n            values = values.reshape(tuple((1,) + shape))  # type: ignore\n    return values\n\n\ndef _safe_reshape(arr, new_shape):\n    \"\"\"\n    If possible, reshape `arr` to have shape `new_shape`,\n    with a couple of exceptions (see gh-13012):\n\n    1) If `arr` is a ExtensionArray or Index, `arr` will be\n       returned as is.\n    2) If `arr` is a Series, the `_values` attribute will\n       be reshaped and returned.\n\n    Parameters\n    ----------\n    arr : array-like, object to be reshaped\n    new_shape : int or tuple of ints, the new shape\n    \"\"\"\n    if isinstance(arr, ABCSeries):\n        arr = arr._values\n    if not is_extension_array_dtype(arr.dtype):\n        # Note: this will include TimedeltaArray and tz-naive DatetimeArray\n        # TODO(EA2D): special case will be unnecessary with 2D EAs\n        arr = np.asarray(arr).reshape(new_shape)\n    return arr\n\n\ndef _putmask_smart(v: np.ndarray, mask: np.ndarray, n) -> np.ndarray:\n    \"\"\"\n    Return a new ndarray, try to preserve dtype if possible.\n\n    Parameters\n    ----------\n    v : np.ndarray\n        `values`, updated in-place.\n    mask : np.ndarray[bool]\n        Applies to both sides (array like).\n    n : `new values` either scalar or an array like aligned with `values`\n\n    Returns\n    -------\n    values : ndarray with updated values\n        this *may* be a copy of the original\n\n    See Also\n    --------\n    ndarray.putmask\n    \"\"\"\n    # we cannot use np.asarray() here as we cannot have conversions\n    # that numpy does when numeric are mixed with strings\n\n    # n should be the length of the mask or a scalar here\n    if not is_list_like(n):\n        n = np.repeat(n, len(mask))\n\n    # see if we are only masking values that if putted\n    # will work in the current dtype\n    try:\n        nn = n[mask]\n    except TypeError:\n        # TypeError: only integer scalar arrays can be converted to a scalar index\n        pass\n    else:\n        # make sure that we have a nullable type\n        # if we have nulls\n        if not _isna_compat(v, nn[0]):\n            pass\n        elif not (is_float_dtype(nn.dtype) or is_integer_dtype(nn.dtype)):\n            # only compare integers/floats\n            pass\n        elif not (is_float_dtype(v.dtype) or is_integer_dtype(v.dtype)):\n            # only compare integers/floats\n            pass\n        else:\n\n            # we ignore ComplexWarning here\n            with warnings.catch_warnings(record=True):\n                warnings.simplefilter(\"ignore\", np.ComplexWarning)\n                nn_at = nn.astype(v.dtype)\n\n            comp = nn == nn_at\n            if is_list_like(comp) and comp.all():\n                nv = v.copy()\n                nv[mask] = nn_at\n                return nv\n\n    n = np.asarray(n)\n\n    def _putmask_preserve(nv, n):\n        try:\n            nv[mask] = n[mask]\n        except (IndexError, ValueError):\n            nv[mask] = n\n        return nv\n\n    # preserves dtype if possible\n    if v.dtype.kind == n.dtype.kind:\n        return _putmask_preserve(v, n)\n\n    # change the dtype if needed\n    dtype, _ = maybe_promote(n.dtype)\n\n    v = v.astype(dtype)\n\n    return _putmask_preserve(v, n)\n\n\ndef _extract_bool_array(mask: ArrayLike) -> np.ndarray:\n    \"\"\"\n    If we have a SparseArray or BooleanArray, convert it to ndarray[bool].\n    \"\"\"\n    if isinstance(mask, ExtensionArray):\n        # We could have BooleanArray, Sparse[bool], ...\n        mask = np.asarray(mask, dtype=np.bool_)\n\n    assert isinstance(mask, np.ndarray), type(mask)\n    assert mask.dtype == bool, mask.dtype\n    return mask\n"
    },
    {
      "filename": "pandas/tests/arrays/sparse/test_array.py",
      "content": "import operator\nimport re\nimport warnings\n\nimport numpy as np\nimport pytest\n\nfrom pandas._libs.sparse import IntIndex\nimport pandas.util._test_decorators as td\n\nimport pandas as pd\nfrom pandas import isna\nimport pandas._testing as tm\nfrom pandas.core.arrays.sparse import SparseArray, SparseDtype\n\n\n@pytest.fixture(params=[\"integer\", \"block\"])\ndef kind(request):\n    return request.param\n\n\nclass TestSparseArray:\n    def setup_method(self, method):\n        self.arr_data = np.array([np.nan, np.nan, 1, 2, 3, np.nan, 4, 5, np.nan, 6])\n        self.arr = SparseArray(self.arr_data)\n        self.zarr = SparseArray([0, 0, 1, 2, 3, 0, 4, 5, 0, 6], fill_value=0)\n\n    def test_constructor_dtype(self):\n        arr = SparseArray([np.nan, 1, 2, np.nan])\n        assert arr.dtype == SparseDtype(np.float64, np.nan)\n        assert arr.dtype.subtype == np.float64\n        assert np.isnan(arr.fill_value)\n\n        arr = SparseArray([np.nan, 1, 2, np.nan], fill_value=0)\n        assert arr.dtype == SparseDtype(np.float64, 0)\n        assert arr.fill_value == 0\n\n        arr = SparseArray([0, 1, 2, 4], dtype=np.float64)\n        assert arr.dtype == SparseDtype(np.float64, np.nan)\n        assert np.isnan(arr.fill_value)\n\n        arr = SparseArray([0, 1, 2, 4], dtype=np.int64)\n        assert arr.dtype == SparseDtype(np.int64, 0)\n        assert arr.fill_value == 0\n\n        arr = SparseArray([0, 1, 2, 4], fill_value=0, dtype=np.int64)\n        assert arr.dtype == SparseDtype(np.int64, 0)\n        assert arr.fill_value == 0\n\n        arr = SparseArray([0, 1, 2, 4], dtype=None)\n        assert arr.dtype == SparseDtype(np.int64, 0)\n        assert arr.fill_value == 0\n\n        arr = SparseArray([0, 1, 2, 4], fill_value=0, dtype=None)\n        assert arr.dtype == SparseDtype(np.int64, 0)\n        assert arr.fill_value == 0\n\n    def test_constructor_dtype_str(self):\n        result = SparseArray([1, 2, 3], dtype=\"int\")\n        expected = SparseArray([1, 2, 3], dtype=int)\n        tm.assert_sp_array_equal(result, expected)\n\n    def test_constructor_sparse_dtype(self):\n        result = SparseArray([1, 0, 0, 1], dtype=SparseDtype(\"int64\", -1))\n        expected = SparseArray([1, 0, 0, 1], fill_value=-1, dtype=np.int64)\n        tm.assert_sp_array_equal(result, expected)\n        assert result.sp_values.dtype == np.dtype(\"int64\")\n\n    def test_constructor_sparse_dtype_str(self):\n        result = SparseArray([1, 0, 0, 1], dtype=\"Sparse[int32]\")\n        expected = SparseArray([1, 0, 0, 1], dtype=np.int32)\n        tm.assert_sp_array_equal(result, expected)\n        assert result.sp_values.dtype == np.dtype(\"int32\")\n\n    def test_constructor_object_dtype(self):\n        # GH 11856\n        arr = SparseArray([\"A\", \"A\", np.nan, \"B\"], dtype=object)\n        assert arr.dtype == SparseDtype(object)\n        assert np.isnan(arr.fill_value)\n\n        arr = SparseArray([\"A\", \"A\", np.nan, \"B\"], dtype=object, fill_value=\"A\")\n        assert arr.dtype == SparseDtype(object, \"A\")\n        assert arr.fill_value == \"A\"\n\n        # GH 17574\n        data = [False, 0, 100.0, 0.0]\n        arr = SparseArray(data, dtype=object, fill_value=False)\n        assert arr.dtype == SparseDtype(object, False)\n        assert arr.fill_value is False\n        arr_expected = np.array(data, dtype=object)\n        it = (type(x) == type(y) and x == y for x, y in zip(arr, arr_expected))\n        assert np.fromiter(it, dtype=np.bool_).all()\n\n    @pytest.mark.parametrize(\"dtype\", [SparseDtype(int, 0), int])\n    def test_constructor_na_dtype(self, dtype):\n        with pytest.raises(ValueError, match=\"Cannot convert\"):\n            SparseArray([0, 1, np.nan], dtype=dtype)\n\n    def test_constructor_warns_when_losing_timezone(self):\n        # GH#32501 warn when losing timezone inforamtion\n        dti = pd.date_range(\"2016-01-01\", periods=3, tz=\"US/Pacific\")\n\n        expected = SparseArray(np.asarray(dti, dtype=\"datetime64[ns]\"))\n\n        with tm.assert_produces_warning(UserWarning):\n            result = SparseArray(dti)\n\n        tm.assert_sp_array_equal(result, expected)\n\n        with tm.assert_produces_warning(UserWarning):\n            result = SparseArray(pd.Series(dti))\n\n        tm.assert_sp_array_equal(result, expected)\n\n    def test_constructor_spindex_dtype(self):\n        arr = SparseArray(data=[1, 2], sparse_index=IntIndex(4, [1, 2]))\n        # XXX: Behavior change: specifying SparseIndex no longer changes the\n        # fill_value\n        expected = SparseArray([0, 1, 2, 0], kind=\"integer\")\n        tm.assert_sp_array_equal(arr, expected)\n        assert arr.dtype == SparseDtype(np.int64)\n        assert arr.fill_value == 0\n\n        arr = SparseArray(\n            data=[1, 2, 3],\n            sparse_index=IntIndex(4, [1, 2, 3]),\n            dtype=np.int64,\n            fill_value=0,\n        )\n        exp = SparseArray([0, 1, 2, 3], dtype=np.int64, fill_value=0)\n        tm.assert_sp_array_equal(arr, exp)\n        assert arr.dtype == SparseDtype(np.int64)\n        assert arr.fill_value == 0\n\n        arr = SparseArray(\n            data=[1, 2], sparse_index=IntIndex(4, [1, 2]), fill_value=0, dtype=np.int64\n        )\n        exp = SparseArray([0, 1, 2, 0], fill_value=0, dtype=np.int64)\n        tm.assert_sp_array_equal(arr, exp)\n        assert arr.dtype == SparseDtype(np.int64)\n        assert arr.fill_value == 0\n\n        arr = SparseArray(\n            data=[1, 2, 3],\n            sparse_index=IntIndex(4, [1, 2, 3]),\n            dtype=None,\n            fill_value=0,\n        )\n        exp = SparseArray([0, 1, 2, 3], dtype=None)\n        tm.assert_sp_array_equal(arr, exp)\n        assert arr.dtype == SparseDtype(np.int64)\n        assert arr.fill_value == 0\n\n    @pytest.mark.parametrize(\"sparse_index\", [None, IntIndex(1, [0])])\n    def test_constructor_spindex_dtype_scalar(self, sparse_index):\n        # scalar input\n        arr = SparseArray(data=1, sparse_index=sparse_index, dtype=None)\n        exp = SparseArray([1], dtype=None)\n        tm.assert_sp_array_equal(arr, exp)\n        assert arr.dtype == SparseDtype(np.int64)\n        assert arr.fill_value == 0\n\n        arr = SparseArray(data=1, sparse_index=IntIndex(1, [0]), dtype=None)\n        exp = SparseArray([1], dtype=None)\n        tm.assert_sp_array_equal(arr, exp)\n        assert arr.dtype == SparseDtype(np.int64)\n        assert arr.fill_value == 0\n\n    def test_constructor_spindex_dtype_scalar_broadcasts(self):\n        arr = SparseArray(\n            data=[1, 2], sparse_index=IntIndex(4, [1, 2]), fill_value=0, dtype=None\n        )\n        exp = SparseArray([0, 1, 2, 0], fill_value=0, dtype=None)\n        tm.assert_sp_array_equal(arr, exp)\n        assert arr.dtype == SparseDtype(np.int64)\n        assert arr.fill_value == 0\n\n    @pytest.mark.parametrize(\n        \"data, fill_value\",\n        [\n            (np.array([1, 2]), 0),\n            (np.array([1.0, 2.0]), np.nan),\n            ([True, False], False),\n            ([pd.Timestamp(\"2017-01-01\")], pd.NaT),\n        ],\n    )\n    def test_constructor_inferred_fill_value(self, data, fill_value):\n        result = SparseArray(data).fill_value\n\n        if pd.isna(fill_value):\n            assert pd.isna(result)\n        else:\n            assert result == fill_value\n\n    @pytest.mark.parametrize(\"format\", [\"coo\", \"csc\", \"csr\"])\n    @pytest.mark.parametrize(\n        \"size\",\n        [pytest.param(0, marks=td.skip_if_np_lt(\"1.16\", reason=\"NumPy-11383\")), 10],\n    )\n    @td.skip_if_no_scipy\n    def test_from_spmatrix(self, size, format):\n        import scipy.sparse\n\n        mat = scipy.sparse.random(size, 1, density=0.5, format=format)\n        result = SparseArray.from_spmatrix(mat)\n\n        result = np.asarray(result)\n        expected = mat.toarray().ravel()\n        tm.assert_numpy_array_equal(result, expected)\n\n    @pytest.mark.parametrize(\"format\", [\"coo\", \"csc\", \"csr\"])\n    @td.skip_if_no_scipy\n    def test_from_spmatrix_including_explicit_zero(self, format):\n        import scipy.sparse\n\n        mat = scipy.sparse.random(10, 1, density=0.5, format=format)\n        mat.data[0] = 0\n        result = SparseArray.from_spmatrix(mat)\n\n        result = np.asarray(result)\n        expected = mat.toarray().ravel()\n        tm.assert_numpy_array_equal(result, expected)\n\n    @td.skip_if_no_scipy\n    def test_from_spmatrix_raises(self):\n        import scipy.sparse\n\n        mat = scipy.sparse.eye(5, 4, format=\"csc\")\n\n        with pytest.raises(ValueError, match=\"not '4'\"):\n            SparseArray.from_spmatrix(mat)\n\n    @pytest.mark.parametrize(\n        \"scalar,dtype\",\n        [\n            (False, SparseDtype(bool, False)),\n            (0.0, SparseDtype(\"float64\", 0)),\n            (1, SparseDtype(\"int64\", 1)),\n            (\"z\", SparseDtype(\"object\", \"z\")),\n        ],\n    )\n    def test_scalar_with_index_infer_dtype(self, scalar, dtype):\n        # GH 19163\n        arr = SparseArray(scalar, index=[1, 2, 3], fill_value=scalar)\n        exp = SparseArray([scalar, scalar, scalar], fill_value=scalar)\n\n        tm.assert_sp_array_equal(arr, exp)\n\n        assert arr.dtype == dtype\n        assert exp.dtype == dtype\n\n    def test_get_item(self):\n\n        assert np.isnan(self.arr[1])\n        assert self.arr[2] == 1\n        assert self.arr[7] == 5\n\n        assert self.zarr[0] == 0\n        assert self.zarr[2] == 1\n        assert self.zarr[7] == 5\n\n        errmsg = re.compile(\"bounds\")\n\n        with pytest.raises(IndexError, match=errmsg):\n            self.arr[11]\n\n        with pytest.raises(IndexError, match=errmsg):\n            self.arr[-11]\n\n        assert self.arr[-1] == self.arr[len(self.arr) - 1]\n\n    def test_take_scalar_raises(self):\n        msg = \"'indices' must be an array, not a scalar '2'.\"\n        with pytest.raises(ValueError, match=msg):\n            self.arr.take(2)\n\n    def test_take(self):\n        exp = SparseArray(np.take(self.arr_data, [2, 3]))\n        tm.assert_sp_array_equal(self.arr.take([2, 3]), exp)\n\n        exp = SparseArray(np.take(self.arr_data, [0, 1, 2]))\n        tm.assert_sp_array_equal(self.arr.take([0, 1, 2]), exp)\n\n    def test_take_all_empty(self):\n        a = pd.array([0, 0], dtype=pd.SparseDtype(\"int64\"))\n        result = a.take([0, 1], allow_fill=True, fill_value=np.nan)\n        tm.assert_sp_array_equal(a, result)\n\n    def test_take_fill_value(self):\n        data = np.array([1, np.nan, 0, 3, 0])\n        sparse = SparseArray(data, fill_value=0)\n\n        exp = SparseArray(np.take(data, [0]), fill_value=0)\n        tm.assert_sp_array_equal(sparse.take([0]), exp)\n\n        exp = SparseArray(np.take(data, [1, 3, 4]), fill_value=0)\n        tm.assert_sp_array_equal(sparse.take([1, 3, 4]), exp)\n\n    def test_take_negative(self):\n        exp = SparseArray(np.take(self.arr_data, [-1]))\n        tm.assert_sp_array_equal(self.arr.take([-1]), exp)\n\n        exp = SparseArray(np.take(self.arr_data, [-4, -3, -2]))\n        tm.assert_sp_array_equal(self.arr.take([-4, -3, -2]), exp)\n\n    @pytest.mark.parametrize(\"fill_value\", [0, None, np.nan])\n    def test_shift_fill_value(self, fill_value):\n        # GH #24128\n        sparse = SparseArray(np.array([1, 0, 0, 3, 0]), fill_value=8.0)\n        res = sparse.shift(1, fill_value=fill_value)\n        if isna(fill_value):\n            fill_value = res.dtype.na_value\n        exp = SparseArray(np.array([fill_value, 1, 0, 0, 3]), fill_value=8.0)\n        tm.assert_sp_array_equal(res, exp)\n\n    def test_bad_take(self):\n        with pytest.raises(IndexError, match=\"bounds\"):\n            self.arr.take([11])\n\n    def test_take_filling(self):\n        # similar tests as GH 12631\n        sparse = SparseArray([np.nan, np.nan, 1, np.nan, 4])\n        result = sparse.take(np.array([1, 0, -1]))\n        expected = SparseArray([np.nan, np.nan, 4])\n        tm.assert_sp_array_equal(result, expected)\n\n        # XXX: test change: fill_value=True -> allow_fill=True\n        result = sparse.take(np.array([1, 0, -1]), allow_fill=True)\n        expected = SparseArray([np.nan, np.nan, np.nan])\n        tm.assert_sp_array_equal(result, expected)\n\n        # allow_fill=False\n        result = sparse.take(np.array([1, 0, -1]), allow_fill=False, fill_value=True)\n        expected = SparseArray([np.nan, np.nan, 4])\n        tm.assert_sp_array_equal(result, expected)\n\n        msg = \"Invalid value in 'indices'\"\n        with pytest.raises(ValueError, match=msg):\n            sparse.take(np.array([1, 0, -2]), allow_fill=True)\n\n        with pytest.raises(ValueError, match=msg):\n            sparse.take(np.array([1, 0, -5]), allow_fill=True)\n\n        msg = \"out of bounds value in 'indices'\"\n        with pytest.raises(IndexError, match=msg):\n            sparse.take(np.array([1, -6]))\n        with pytest.raises(IndexError, match=msg):\n            sparse.take(np.array([1, 5]))\n        with pytest.raises(IndexError, match=msg):\n            sparse.take(np.array([1, 5]), allow_fill=True)\n\n    def test_take_filling_fill_value(self):\n        # same tests as GH 12631\n        sparse = SparseArray([np.nan, 0, 1, 0, 4], fill_value=0)\n        result = sparse.take(np.array([1, 0, -1]))\n        expected = SparseArray([0, np.nan, 4], fill_value=0)\n        tm.assert_sp_array_equal(result, expected)\n\n        # fill_value\n        result = sparse.take(np.array([1, 0, -1]), allow_fill=True)\n        # XXX: behavior change.\n        # the old way of filling self.fill_value doesn't follow EA rules.\n        # It's supposed to be self.dtype.na_value (nan in this case)\n        expected = SparseArray([0, np.nan, np.nan], fill_value=0)\n        tm.assert_sp_array_equal(result, expected)\n\n        # allow_fill=False\n        result = sparse.take(np.array([1, 0, -1]), allow_fill=False, fill_value=True)\n        expected = SparseArray([0, np.nan, 4], fill_value=0)\n        tm.assert_sp_array_equal(result, expected)\n\n        msg = \"Invalid value in 'indices'.\"\n        with pytest.raises(ValueError, match=msg):\n            sparse.take(np.array([1, 0, -2]), allow_fill=True)\n        with pytest.raises(ValueError, match=msg):\n            sparse.take(np.array([1, 0, -5]), allow_fill=True)\n\n        msg = \"out of bounds value in 'indices'\"\n        with pytest.raises(IndexError, match=msg):\n            sparse.take(np.array([1, -6]))\n        with pytest.raises(IndexError, match=msg):\n            sparse.take(np.array([1, 5]))\n        with pytest.raises(IndexError, match=msg):\n            sparse.take(np.array([1, 5]), fill_value=True)\n\n    def test_take_filling_all_nan(self):\n        sparse = SparseArray([np.nan, np.nan, np.nan, np.nan, np.nan])\n        # XXX: did the default kind from take change?\n        result = sparse.take(np.array([1, 0, -1]))\n        expected = SparseArray([np.nan, np.nan, np.nan], kind=\"block\")\n        tm.assert_sp_array_equal(result, expected)\n\n        result = sparse.take(np.array([1, 0, -1]), fill_value=True)\n        expected = SparseArray([np.nan, np.nan, np.nan], kind=\"block\")\n        tm.assert_sp_array_equal(result, expected)\n\n        msg = \"out of bounds value in 'indices'\"\n        with pytest.raises(IndexError, match=msg):\n            sparse.take(np.array([1, -6]))\n        with pytest.raises(IndexError, match=msg):\n            sparse.take(np.array([1, 5]))\n        with pytest.raises(IndexError, match=msg):\n            sparse.take(np.array([1, 5]), fill_value=True)\n\n    def test_set_item(self):\n        def setitem():\n            self.arr[5] = 3\n\n        def setslice():\n            self.arr[1:5] = 2\n\n        with pytest.raises(TypeError, match=\"assignment via setitem\"):\n            setitem()\n\n        with pytest.raises(TypeError, match=\"assignment via setitem\"):\n            setslice()\n\n    def test_constructor_from_too_large_array(self):\n        with pytest.raises(TypeError, match=\"expected dimension <= 1 data\"):\n            SparseArray(np.arange(10).reshape((2, 5)))\n\n    def test_constructor_from_sparse(self):\n        res = SparseArray(self.zarr)\n        assert res.fill_value == 0\n        tm.assert_almost_equal(res.sp_values, self.zarr.sp_values)\n\n    def test_constructor_copy(self):\n        cp = SparseArray(self.arr, copy=True)\n        cp.sp_values[:3] = 0\n        assert not (self.arr.sp_values[:3] == 0).any()\n\n        not_copy = SparseArray(self.arr)\n        not_copy.sp_values[:3] = 0\n        assert (self.arr.sp_values[:3] == 0).all()\n\n    def test_constructor_bool(self):\n        # GH 10648\n        data = np.array([False, False, True, True, False, False])\n        arr = SparseArray(data, fill_value=False, dtype=bool)\n\n        assert arr.dtype == SparseDtype(bool)\n        tm.assert_numpy_array_equal(arr.sp_values, np.array([True, True]))\n        # Behavior change: np.asarray densifies.\n        # tm.assert_numpy_array_equal(arr.sp_values, np.asarray(arr))\n        tm.assert_numpy_array_equal(arr.sp_index.indices, np.array([2, 3], np.int32))\n\n        dense = arr.to_dense()\n        assert dense.dtype == bool\n        tm.assert_numpy_array_equal(dense, data)\n\n    def test_constructor_bool_fill_value(self):\n        arr = SparseArray([True, False, True], dtype=None)\n        assert arr.dtype == SparseDtype(np.bool_)\n        assert not arr.fill_value\n\n        arr = SparseArray([True, False, True], dtype=np.bool_)\n        assert arr.dtype == SparseDtype(np.bool_)\n        assert not arr.fill_value\n\n        arr = SparseArray([True, False, True], dtype=np.bool_, fill_value=True)\n        assert arr.dtype == SparseDtype(np.bool_, True)\n        assert arr.fill_value\n\n    def test_constructor_float32(self):\n        # GH 10648\n        data = np.array([1.0, np.nan, 3], dtype=np.float32)\n        arr = SparseArray(data, dtype=np.float32)\n\n        assert arr.dtype == SparseDtype(np.float32)\n        tm.assert_numpy_array_equal(arr.sp_values, np.array([1, 3], dtype=np.float32))\n        # Behavior change: np.asarray densifies.\n        # tm.assert_numpy_array_equal(arr.sp_values, np.asarray(arr))\n        tm.assert_numpy_array_equal(\n            arr.sp_index.indices, np.array([0, 2], dtype=np.int32)\n        )\n\n        dense = arr.to_dense()\n        assert dense.dtype == np.float32\n        tm.assert_numpy_array_equal(dense, data)\n\n    def test_astype(self):\n        # float -> float\n        arr = SparseArray([None, None, 0, 2])\n        result = arr.astype(\"Sparse[float32]\")\n        expected = SparseArray([None, None, 0, 2], dtype=np.dtype(\"float32\"))\n        tm.assert_sp_array_equal(result, expected)\n\n        dtype = SparseDtype(\"float64\", fill_value=0)\n        result = arr.astype(dtype)\n        expected = SparseArray._simple_new(\n            np.array([0.0, 2.0], dtype=dtype.subtype), IntIndex(4, [2, 3]), dtype\n        )\n        tm.assert_sp_array_equal(result, expected)\n\n        dtype = SparseDtype(\"int64\", 0)\n        result = arr.astype(dtype)\n        expected = SparseArray._simple_new(\n            np.array([0, 2], dtype=np.int64), IntIndex(4, [2, 3]), dtype\n        )\n        tm.assert_sp_array_equal(result, expected)\n\n        arr = SparseArray([0, np.nan, 0, 1], fill_value=0)\n        with pytest.raises(ValueError, match=\"NA\"):\n            arr.astype(\"Sparse[i8]\")\n\n    def test_astype_bool(self):\n        a = SparseArray([1, 0, 0, 1], dtype=SparseDtype(int, 0))\n        result = a.astype(bool)\n        expected = SparseArray([True, 0, 0, True], dtype=SparseDtype(bool, 0))\n        tm.assert_sp_array_equal(result, expected)\n\n        # update fill value\n        result = a.astype(SparseDtype(bool, False))\n        expected = SparseArray(\n            [True, False, False, True], dtype=SparseDtype(bool, False)\n        )\n        tm.assert_sp_array_equal(result, expected)\n\n    def test_astype_all(self, any_real_dtype):\n        vals = np.array([1, 2, 3])\n        arr = SparseArray(vals, fill_value=1)\n        typ = np.dtype(any_real_dtype)\n        res = arr.astype(typ)\n        assert res.dtype == SparseDtype(typ, 1)\n        assert res.sp_values.dtype == typ\n\n        tm.assert_numpy_array_equal(np.asarray(res.to_dense()), vals.astype(typ))\n\n    @pytest.mark.parametrize(\n        \"array, dtype, expected\",\n        [\n            (\n                SparseArray([0, 1]),\n                \"float\",\n                SparseArray([0.0, 1.0], dtype=SparseDtype(float, 0.0)),\n            ),\n            (SparseArray([0, 1]), bool, SparseArray([False, True])),\n            (\n                SparseArray([0, 1], fill_value=1),\n                bool,\n                SparseArray([False, True], dtype=SparseDtype(bool, True)),\n            ),\n            pytest.param(\n                SparseArray([0, 1]),\n                \"datetime64[ns]\",\n                SparseArray(\n                    np.array([0, 1], dtype=\"datetime64[ns]\"),\n                    dtype=SparseDtype(\"datetime64[ns]\", pd.Timestamp(\"1970\")),\n                ),\n                marks=[pytest.mark.xfail(reason=\"NumPy-7619\")],\n            ),\n            (\n                SparseArray([0, 1, 10]),\n                str,\n                SparseArray([\"0\", \"1\", \"10\"], dtype=SparseDtype(str, \"0\")),\n            ),\n            (SparseArray([\"10\", \"20\"]), float, SparseArray([10.0, 20.0])),\n            (\n                SparseArray([0, 1, 0]),\n                object,\n                SparseArray([0, 1, 0], dtype=SparseDtype(object, 0)),\n            ),\n        ],\n    )\n    def test_astype_more(self, array, dtype, expected):\n        result = array.astype(dtype)\n        tm.assert_sp_array_equal(result, expected)\n\n    def test_astype_nan_raises(self):\n        arr = SparseArray([1.0, np.nan])\n        with pytest.raises(ValueError, match=\"Cannot convert non-finite\"):\n            arr.astype(int)\n\n    def test_set_fill_value(self):\n        arr = SparseArray([1.0, np.nan, 2.0], fill_value=np.nan)\n        arr.fill_value = 2\n        assert arr.fill_value == 2\n\n        arr = SparseArray([1, 0, 2], fill_value=0, dtype=np.int64)\n        arr.fill_value = 2\n        assert arr.fill_value == 2\n\n        # XXX: this seems fine? You can construct an integer\n        # sparsearray with NaN fill value, why not update one?\n        # coerces to int\n        # msg = \"unable to set fill_value 3\\\\.1 to int64 dtype\"\n        # with pytest.raises(ValueError, match=msg):\n        arr.fill_value = 3.1\n        assert arr.fill_value == 3.1\n\n        # msg = \"unable to set fill_value nan to int64 dtype\"\n        # with pytest.raises(ValueError, match=msg):\n        arr.fill_value = np.nan\n        assert np.isnan(arr.fill_value)\n\n        arr = SparseArray([True, False, True], fill_value=False, dtype=np.bool_)\n        arr.fill_value = True\n        assert arr.fill_value\n\n        # coerces to bool\n        # msg = \"unable to set fill_value 0 to bool dtype\"\n        # with pytest.raises(ValueError, match=msg):\n        arr.fill_value = 0\n        assert arr.fill_value == 0\n\n        # msg = \"unable to set fill_value nan to bool dtype\"\n        # with pytest.raises(ValueError, match=msg):\n        arr.fill_value = np.nan\n        assert np.isnan(arr.fill_value)\n\n    @pytest.mark.parametrize(\"val\", [[1, 2, 3], np.array([1, 2]), (1, 2, 3)])\n    def test_set_fill_invalid_non_scalar(self, val):\n        arr = SparseArray([True, False, True], fill_value=False, dtype=np.bool_)\n        msg = \"fill_value must be a scalar\"\n\n        with pytest.raises(ValueError, match=msg):\n            arr.fill_value = val\n\n    def test_copy(self):\n        arr2 = self.arr.copy()\n        assert arr2.sp_values is not self.arr.sp_values\n        assert arr2.sp_index is self.arr.sp_index\n\n    def test_values_asarray(self):\n        tm.assert_almost_equal(self.arr.to_dense(), self.arr_data)\n\n    @pytest.mark.parametrize(\n        \"data,shape,dtype\",\n        [\n            ([0, 0, 0, 0, 0], (5,), None),\n            ([], (0,), None),\n            ([0], (1,), None),\n            ([\"A\", \"A\", np.nan, \"B\"], (4,), object),\n        ],\n    )\n    def test_shape(self, data, shape, dtype):\n        # GH 21126\n        out = SparseArray(data, dtype=dtype)\n        assert out.shape == shape\n\n    @pytest.mark.parametrize(\n        \"vals\",\n        [\n            [np.nan, np.nan, np.nan, np.nan, np.nan],\n            [1, np.nan, np.nan, 3, np.nan],\n            [1, np.nan, 0, 3, 0],\n        ],\n    )\n    @pytest.mark.parametrize(\"fill_value\", [None, 0])\n    def test_dense_repr(self, vals, fill_value):\n        vals = np.array(vals)\n        arr = SparseArray(vals, fill_value=fill_value)\n\n        res = arr.to_dense()\n        tm.assert_numpy_array_equal(res, vals)\n\n        res2 = arr._internal_get_values()\n\n        tm.assert_numpy_array_equal(res2, vals)\n\n    def test_getitem(self):\n        def _checkit(i):\n            tm.assert_almost_equal(self.arr[i], self.arr.to_dense()[i])\n\n        for i in range(len(self.arr)):\n            _checkit(i)\n            _checkit(-i)\n\n    def test_getitem_arraylike_mask(self):\n        arr = SparseArray([0, 1, 2])\n        result = arr[[True, False, True]]\n        expected = SparseArray([0, 2])\n        tm.assert_sp_array_equal(result, expected)\n\n    def test_getslice(self):\n        result = self.arr[:-3]\n        exp = SparseArray(self.arr.to_dense()[:-3])\n        tm.assert_sp_array_equal(result, exp)\n\n        result = self.arr[-4:]\n        exp = SparseArray(self.arr.to_dense()[-4:])\n        tm.assert_sp_array_equal(result, exp)\n\n        # two corner cases from Series\n        result = self.arr[-12:]\n        exp = SparseArray(self.arr)\n        tm.assert_sp_array_equal(result, exp)\n\n        result = self.arr[:-12]\n        exp = SparseArray(self.arr.to_dense()[:0])\n        tm.assert_sp_array_equal(result, exp)\n\n    def test_getslice_tuple(self):\n        dense = np.array([np.nan, 0, 3, 4, 0, 5, np.nan, np.nan, 0])\n\n        sparse = SparseArray(dense)\n        res = sparse[\n            4:,\n        ]  # noqa: E231\n        exp = SparseArray(dense[4:,])  # noqa: E231\n        tm.assert_sp_array_equal(res, exp)\n\n        sparse = SparseArray(dense, fill_value=0)\n        res = sparse[\n            4:,\n        ]  # noqa: E231\n        exp = SparseArray(dense[4:,], fill_value=0)  # noqa: E231\n        tm.assert_sp_array_equal(res, exp)\n\n        msg = \"too many indices for array\"\n        with pytest.raises(IndexError, match=msg):\n            sparse[4:, :]\n\n        with pytest.raises(IndexError, match=msg):\n            # check numpy compat\n            dense[4:, :]\n\n    def test_boolean_slice_empty(self):\n        arr = SparseArray([0, 1, 2])\n        res = arr[[False, False, False]]\n        assert res.dtype == arr.dtype\n\n    @pytest.mark.parametrize(\"op\", [\"add\", \"sub\", \"mul\", \"truediv\", \"floordiv\", \"pow\"])\n    def test_binary_operators(self, op):\n        op = getattr(operator, op)\n        data1 = np.random.randn(20)\n        data2 = np.random.randn(20)\n\n        data1[::2] = np.nan\n        data2[::3] = np.nan\n\n        arr1 = SparseArray(data1)\n        arr2 = SparseArray(data2)\n\n        data1[::2] = 3\n        data2[::3] = 3\n        farr1 = SparseArray(data1, fill_value=3)\n        farr2 = SparseArray(data2, fill_value=3)\n\n        def _check_op(op, first, second):\n            res = op(first, second)\n            exp = SparseArray(\n                op(first.to_dense(), second.to_dense()), fill_value=first.fill_value\n            )\n            assert isinstance(res, SparseArray)\n            tm.assert_almost_equal(res.to_dense(), exp.to_dense())\n\n            res2 = op(first, second.to_dense())\n            assert isinstance(res2, SparseArray)\n            tm.assert_sp_array_equal(res, res2)\n\n            res3 = op(first.to_dense(), second)\n            assert isinstance(res3, SparseArray)\n            tm.assert_sp_array_equal(res, res3)\n\n            res4 = op(first, 4)\n            assert isinstance(res4, SparseArray)\n\n            # Ignore this if the actual op raises (e.g. pow).\n            try:\n                exp = op(first.to_dense(), 4)\n                exp_fv = op(first.fill_value, 4)\n            except ValueError:\n                pass\n            else:\n                tm.assert_almost_equal(res4.fill_value, exp_fv)\n                tm.assert_almost_equal(res4.to_dense(), exp)\n\n        with np.errstate(all=\"ignore\"):\n            for first_arr, second_arr in [(arr1, arr2), (farr1, farr2)]:\n                _check_op(op, first_arr, second_arr)\n\n    def test_pickle(self):\n        def _check_roundtrip(obj):\n            unpickled = tm.round_trip_pickle(obj)\n            tm.assert_sp_array_equal(unpickled, obj)\n\n        _check_roundtrip(self.arr)\n        _check_roundtrip(self.zarr)\n\n    def test_generator_warnings(self):\n        sp_arr = SparseArray([1, 2, 3])\n        with warnings.catch_warnings(record=True) as w:\n            warnings.filterwarnings(action=\"always\", category=DeprecationWarning)\n            warnings.filterwarnings(action=\"always\", category=PendingDeprecationWarning)\n            for _ in sp_arr:\n                pass\n            assert len(w) == 0\n\n    def test_fillna(self):\n        s = SparseArray([1, np.nan, np.nan, 3, np.nan])\n        res = s.fillna(-1)\n        exp = SparseArray([1, -1, -1, 3, -1], fill_value=-1, dtype=np.float64)\n        tm.assert_sp_array_equal(res, exp)\n\n        s = SparseArray([1, np.nan, np.nan, 3, np.nan], fill_value=0)\n        res = s.fillna(-1)\n        exp = SparseArray([1, -1, -1, 3, -1], fill_value=0, dtype=np.float64)\n        tm.assert_sp_array_equal(res, exp)\n\n        s = SparseArray([1, np.nan, 0, 3, 0])\n        res = s.fillna(-1)\n        exp = SparseArray([1, -1, 0, 3, 0], fill_value=-1, dtype=np.float64)\n        tm.assert_sp_array_equal(res, exp)\n\n        s = SparseArray([1, np.nan, 0, 3, 0], fill_value=0)\n        res = s.fillna(-1)\n        exp = SparseArray([1, -1, 0, 3, 0], fill_value=0, dtype=np.float64)\n        tm.assert_sp_array_equal(res, exp)\n\n        s = SparseArray([np.nan, np.nan, np.nan, np.nan])\n        res = s.fillna(-1)\n        exp = SparseArray([-1, -1, -1, -1], fill_value=-1, dtype=np.float64)\n        tm.assert_sp_array_equal(res, exp)\n\n        s = SparseArray([np.nan, np.nan, np.nan, np.nan], fill_value=0)\n        res = s.fillna(-1)\n        exp = SparseArray([-1, -1, -1, -1], fill_value=0, dtype=np.float64)\n        tm.assert_sp_array_equal(res, exp)\n\n        # float dtype's fill_value is np.nan, replaced by -1\n        s = SparseArray([0.0, 0.0, 0.0, 0.0])\n        res = s.fillna(-1)\n        exp = SparseArray([0.0, 0.0, 0.0, 0.0], fill_value=-1)\n        tm.assert_sp_array_equal(res, exp)\n\n        # int dtype shouldn't have missing. No changes.\n        s = SparseArray([0, 0, 0, 0])\n        assert s.dtype == SparseDtype(np.int64)\n        assert s.fill_value == 0\n        res = s.fillna(-1)\n        tm.assert_sp_array_equal(res, s)\n\n        s = SparseArray([0, 0, 0, 0], fill_value=0)\n        assert s.dtype == SparseDtype(np.int64)\n        assert s.fill_value == 0\n        res = s.fillna(-1)\n        exp = SparseArray([0, 0, 0, 0], fill_value=0)\n        tm.assert_sp_array_equal(res, exp)\n\n        # fill_value can be nan if there is no missing hole.\n        # only fill_value will be changed\n        s = SparseArray([0, 0, 0, 0], fill_value=np.nan)\n        assert s.dtype == SparseDtype(np.int64, fill_value=np.nan)\n        assert np.isnan(s.fill_value)\n        res = s.fillna(-1)\n        exp = SparseArray([0, 0, 0, 0], fill_value=-1)\n        tm.assert_sp_array_equal(res, exp)\n\n    def test_fillna_overlap(self):\n        s = SparseArray([1, np.nan, np.nan, 3, np.nan])\n        # filling with existing value doesn't replace existing value with\n        # fill_value, i.e. existing 3 remains in sp_values\n        res = s.fillna(3)\n        exp = np.array([1, 3, 3, 3, 3], dtype=np.float64)\n        tm.assert_numpy_array_equal(res.to_dense(), exp)\n\n        s = SparseArray([1, np.nan, np.nan, 3, np.nan], fill_value=0)\n        res = s.fillna(3)\n        exp = SparseArray([1, 3, 3, 3, 3], fill_value=0, dtype=np.float64)\n        tm.assert_sp_array_equal(res, exp)\n\n    def test_nonzero(self):\n        # Tests regression #21172.\n        sa = SparseArray([float(\"nan\"), float(\"nan\"), 1, 0, 0, 2, 0, 0, 0, 3, 0, 0])\n        expected = np.array([2, 5, 9], dtype=np.int32)\n        (result,) = sa.nonzero()\n        tm.assert_numpy_array_equal(expected, result)\n\n        sa = SparseArray([0, 0, 1, 0, 0, 2, 0, 0, 0, 3, 0, 0])\n        (result,) = sa.nonzero()\n        tm.assert_numpy_array_equal(expected, result)\n\n\nclass TestSparseArrayAnalytics:\n    @pytest.mark.parametrize(\n        \"data,pos,neg\",\n        [\n            ([True, True, True], True, False),\n            ([1, 2, 1], 1, 0),\n            ([1.0, 2.0, 1.0], 1.0, 0.0),\n        ],\n    )\n    def test_all(self, data, pos, neg):\n        # GH 17570\n        out = SparseArray(data).all()\n        assert out\n\n        out = SparseArray(data, fill_value=pos).all()\n        assert out\n\n        data[1] = neg\n        out = SparseArray(data).all()\n        assert not out\n\n        out = SparseArray(data, fill_value=pos).all()\n        assert not out\n\n    @pytest.mark.parametrize(\n        \"data,pos,neg\",\n        [\n            ([True, True, True], True, False),\n            ([1, 2, 1], 1, 0),\n            ([1.0, 2.0, 1.0], 1.0, 0.0),\n        ],\n    )\n    @td.skip_if_np_lt(\"1.15\")  # prior didn't dispatch\n    def test_numpy_all(self, data, pos, neg):\n        # GH 17570\n        out = np.all(SparseArray(data))\n        assert out\n\n        out = np.all(SparseArray(data, fill_value=pos))\n        assert out\n\n        data[1] = neg\n        out = np.all(SparseArray(data))\n        assert not out\n\n        out = np.all(SparseArray(data, fill_value=pos))\n        assert not out\n\n        # raises with a different message on py2.\n        msg = \"the 'out' parameter is not supported\"\n        with pytest.raises(ValueError, match=msg):\n            np.all(SparseArray(data), out=np.array([]))\n\n    @pytest.mark.parametrize(\n        \"data,pos,neg\",\n        [\n            ([False, True, False], True, False),\n            ([0, 2, 0], 2, 0),\n            ([0.0, 2.0, 0.0], 2.0, 0.0),\n        ],\n    )\n    def test_any(self, data, pos, neg):\n        # GH 17570\n        out = SparseArray(data).any()\n        assert out\n\n        out = SparseArray(data, fill_value=pos).any()\n        assert out\n\n        data[1] = neg\n        out = SparseArray(data).any()\n        assert not out\n\n        out = SparseArray(data, fill_value=pos).any()\n        assert not out\n\n    @pytest.mark.parametrize(\n        \"data,pos,neg\",\n        [\n            ([False, True, False], True, False),\n            ([0, 2, 0], 2, 0),\n            ([0.0, 2.0, 0.0], 2.0, 0.0),\n        ],\n    )\n    @td.skip_if_np_lt(\"1.15\")  # prior didn't dispatch\n    def test_numpy_any(self, data, pos, neg):\n        # GH 17570\n        out = np.any(SparseArray(data))\n        assert out\n\n        out = np.any(SparseArray(data, fill_value=pos))\n        assert out\n\n        data[1] = neg\n        out = np.any(SparseArray(data))\n        assert not out\n\n        out = np.any(SparseArray(data, fill_value=pos))\n        assert not out\n\n        msg = \"the 'out' parameter is not supported\"\n        with pytest.raises(ValueError, match=msg):\n            np.any(SparseArray(data), out=out)\n\n    def test_sum(self):\n        data = np.arange(10).astype(float)\n        out = SparseArray(data).sum()\n        assert out == 45.0\n\n        data[5] = np.nan\n        out = SparseArray(data, fill_value=2).sum()\n        assert out == 40.0\n\n        out = SparseArray(data, fill_value=np.nan).sum()\n        assert out == 40.0\n\n    @pytest.mark.parametrize(\n        \"arr\",\n        [\n            np.array([0, 1, np.nan, 1]),\n            np.array([0, 1, 1]),\n            np.array([True, True, False]),\n        ],\n    )\n    @pytest.mark.parametrize(\"fill_value\", [0, 1, np.nan, True, False])\n    @pytest.mark.parametrize(\"min_count, expected\", [(3, 2), (4, np.nan)])\n    def test_sum_min_count(self, arr, fill_value, min_count, expected):\n        # https://github.com/pandas-dev/pandas/issues/25777\n        sparray = SparseArray(arr, fill_value=fill_value)\n        result = sparray.sum(min_count=min_count)\n        if np.isnan(expected):\n            assert np.isnan(result)\n        else:\n            assert result == expected\n\n    def test_numpy_sum(self):\n        data = np.arange(10).astype(float)\n        out = np.sum(SparseArray(data))\n        assert out == 45.0\n\n        data[5] = np.nan\n        out = np.sum(SparseArray(data, fill_value=2))\n        assert out == 40.0\n\n        out = np.sum(SparseArray(data, fill_value=np.nan))\n        assert out == 40.0\n\n        msg = \"the 'dtype' parameter is not supported\"\n        with pytest.raises(ValueError, match=msg):\n            np.sum(SparseArray(data), dtype=np.int64)\n\n        msg = \"the 'out' parameter is not supported\"\n        with pytest.raises(ValueError, match=msg):\n            np.sum(SparseArray(data), out=out)\n\n    @pytest.mark.parametrize(\n        \"data,expected\",\n        [\n            (\n                np.array([1, 2, 3, 4, 5], dtype=float),  # non-null data\n                SparseArray(np.array([1.0, 3.0, 6.0, 10.0, 15.0])),\n            ),\n            (\n                np.array([1, 2, np.nan, 4, 5], dtype=float),  # null data\n                SparseArray(np.array([1.0, 3.0, np.nan, 7.0, 12.0])),\n            ),\n        ],\n    )\n    @pytest.mark.parametrize(\"numpy\", [True, False])\n    def test_cumsum(self, data, expected, numpy):\n        cumsum = np.cumsum if numpy else lambda s: s.cumsum()\n\n        out = cumsum(SparseArray(data))\n        tm.assert_sp_array_equal(out, expected)\n\n        out = cumsum(SparseArray(data, fill_value=np.nan))\n        tm.assert_sp_array_equal(out, expected)\n\n        out = cumsum(SparseArray(data, fill_value=2))\n        tm.assert_sp_array_equal(out, expected)\n\n        if numpy:  # numpy compatibility checks.\n            msg = \"the 'dtype' parameter is not supported\"\n            with pytest.raises(ValueError, match=msg):\n                np.cumsum(SparseArray(data), dtype=np.int64)\n\n            msg = \"the 'out' parameter is not supported\"\n            with pytest.raises(ValueError, match=msg):\n                np.cumsum(SparseArray(data), out=out)\n        else:\n            axis = 1  # SparseArray currently 1-D, so only axis = 0 is valid.\n            msg = re.escape(f\"axis(={axis}) out of bounds\")\n            with pytest.raises(ValueError, match=msg):\n                SparseArray(data).cumsum(axis=axis)\n\n    def test_mean(self):\n        data = np.arange(10).astype(float)\n        out = SparseArray(data).mean()\n        assert out == 4.5\n\n        data[5] = np.nan\n        out = SparseArray(data).mean()\n        assert out == 40.0 / 9\n\n    def test_numpy_mean(self):\n        data = np.arange(10).astype(float)\n        out = np.mean(SparseArray(data))\n        assert out == 4.5\n\n        data[5] = np.nan\n        out = np.mean(SparseArray(data))\n        assert out == 40.0 / 9\n\n        msg = \"the 'dtype' parameter is not supported\"\n        with pytest.raises(ValueError, match=msg):\n            np.mean(SparseArray(data), dtype=np.int64)\n\n        msg = \"the 'out' parameter is not supported\"\n        with pytest.raises(ValueError, match=msg):\n            np.mean(SparseArray(data), out=out)\n\n    def test_ufunc(self):\n        # GH 13853 make sure ufunc is applied to fill_value\n        sparse = SparseArray([1, np.nan, 2, np.nan, -2])\n        result = SparseArray([1, np.nan, 2, np.nan, 2])\n        tm.assert_sp_array_equal(abs(sparse), result)\n        tm.assert_sp_array_equal(np.abs(sparse), result)\n\n        sparse = SparseArray([1, -1, 2, -2], fill_value=1)\n        result = SparseArray([1, 2, 2], sparse_index=sparse.sp_index, fill_value=1)\n        tm.assert_sp_array_equal(abs(sparse), result)\n        tm.assert_sp_array_equal(np.abs(sparse), result)\n\n        sparse = SparseArray([1, -1, 2, -2], fill_value=-1)\n        result = SparseArray([1, 2, 2], sparse_index=sparse.sp_index, fill_value=1)\n        tm.assert_sp_array_equal(abs(sparse), result)\n        tm.assert_sp_array_equal(np.abs(sparse), result)\n\n        sparse = SparseArray([1, np.nan, 2, np.nan, -2])\n        result = SparseArray(np.sin([1, np.nan, 2, np.nan, -2]))\n        tm.assert_sp_array_equal(np.sin(sparse), result)\n\n        sparse = SparseArray([1, -1, 2, -2], fill_value=1)\n        result = SparseArray(np.sin([1, -1, 2, -2]), fill_value=np.sin(1))\n        tm.assert_sp_array_equal(np.sin(sparse), result)\n\n        sparse = SparseArray([1, -1, 0, -2], fill_value=0)\n        result = SparseArray(np.sin([1, -1, 0, -2]), fill_value=np.sin(0))\n        tm.assert_sp_array_equal(np.sin(sparse), result)\n\n    def test_ufunc_args(self):\n        # GH 13853 make sure ufunc is applied to fill_value, including its arg\n        sparse = SparseArray([1, np.nan, 2, np.nan, -2])\n        result = SparseArray([2, np.nan, 3, np.nan, -1])\n        tm.assert_sp_array_equal(np.add(sparse, 1), result)\n\n        sparse = SparseArray([1, -1, 2, -2], fill_value=1)\n        result = SparseArray([2, 0, 3, -1], fill_value=2)\n        tm.assert_sp_array_equal(np.add(sparse, 1), result)\n\n        sparse = SparseArray([1, -1, 0, -2], fill_value=0)\n        result = SparseArray([2, 0, 1, -1], fill_value=1)\n        tm.assert_sp_array_equal(np.add(sparse, 1), result)\n\n    @pytest.mark.parametrize(\"fill_value\", [0.0, np.nan])\n    def test_modf(self, fill_value):\n        # https://github.com/pandas-dev/pandas/issues/26946\n        sparse = SparseArray([fill_value] * 10 + [1.1, 2.2], fill_value=fill_value)\n        r1, r2 = np.modf(sparse)\n        e1, e2 = np.modf(np.asarray(sparse))\n        tm.assert_sp_array_equal(r1, SparseArray(e1, fill_value=fill_value))\n        tm.assert_sp_array_equal(r2, SparseArray(e2, fill_value=fill_value))\n\n    def test_nbytes_integer(self):\n        arr = SparseArray([1, 0, 0, 0, 2], kind=\"integer\")\n        result = arr.nbytes\n        # (2 * 8) + 2 * 4\n        assert result == 24\n\n    def test_nbytes_block(self):\n        arr = SparseArray([1, 2, 0, 0, 0], kind=\"block\")\n        result = arr.nbytes\n        # (2 * 8) + 4 + 4\n        # sp_values, blocs, blengths\n        assert result == 24\n\n    def test_asarray_datetime64(self):\n        s = SparseArray(pd.to_datetime([\"2012\", None, None, \"2013\"]))\n        np.asarray(s)\n\n    def test_density(self):\n        arr = SparseArray([0, 1])\n        assert arr.density == 0.5\n\n    def test_npoints(self):\n        arr = SparseArray([0, 1])\n        assert arr.npoints == 1\n\n\nclass TestAccessor:\n    @pytest.mark.parametrize(\"attr\", [\"npoints\", \"density\", \"fill_value\", \"sp_values\"])\n    def test_get_attributes(self, attr):\n        arr = SparseArray([0, 1])\n        ser = pd.Series(arr)\n\n        result = getattr(ser.sparse, attr)\n        expected = getattr(arr, attr)\n        assert result == expected\n\n    @td.skip_if_no_scipy\n    def test_from_coo(self):\n        import scipy.sparse\n\n        row = [0, 3, 1, 0]\n        col = [0, 3, 1, 2]\n        data = [4, 5, 7, 9]\n        sp_array = scipy.sparse.coo_matrix((data, (row, col)))\n        result = pd.Series.sparse.from_coo(sp_array)\n\n        index = pd.MultiIndex.from_arrays([[0, 0, 1, 3], [0, 2, 1, 3]])\n        expected = pd.Series([4, 9, 7, 5], index=index, dtype=\"Sparse[int]\")\n        tm.assert_series_equal(result, expected)\n\n    @td.skip_if_no_scipy\n    def test_to_coo(self):\n        import scipy.sparse\n\n        ser = pd.Series(\n            [1, 2, 3],\n            index=pd.MultiIndex.from_product([[0], [1, 2, 3]], names=[\"a\", \"b\"]),\n            dtype=\"Sparse[int]\",\n        )\n        A, _, _ = ser.sparse.to_coo()\n        assert isinstance(A, scipy.sparse.coo.coo_matrix)\n\n    def test_non_sparse_raises(self):\n        ser = pd.Series([1, 2, 3])\n        with pytest.raises(AttributeError, match=\".sparse\"):\n            ser.sparse.density\n\n\ndef test_setting_fill_value_fillna_still_works():\n    # This is why letting users update fill_value / dtype is bad\n    # astype has the same problem.\n    arr = SparseArray([1.0, np.nan, 1.0], fill_value=0.0)\n    arr.fill_value = np.nan\n    result = arr.isna()\n    # Can't do direct comparison, since the sp_index will be different\n    # So let's convert to ndarray and check there.\n    result = np.asarray(result)\n\n    expected = np.array([False, True, False])\n    tm.assert_numpy_array_equal(result, expected)\n\n\ndef test_setting_fill_value_updates():\n    arr = SparseArray([0.0, np.nan], fill_value=0)\n    arr.fill_value = np.nan\n    # use private constructor to get the index right\n    # otherwise both nans would be un-stored.\n    expected = SparseArray._simple_new(\n        sparse_array=np.array([np.nan]),\n        sparse_index=IntIndex(2, [1]),\n        dtype=SparseDtype(float, np.nan),\n    )\n    tm.assert_sp_array_equal(arr, expected)\n\n\n@pytest.mark.parametrize(\n    \"arr, loc\",\n    [\n        ([None, 1, 2], 0),\n        ([0, None, 2], 1),\n        ([0, 1, None], 2),\n        ([0, 1, 1, None, None], 3),\n        ([1, 1, 1, 2], -1),\n        ([], -1),\n    ],\n)\ndef test_first_fill_value_loc(arr, loc):\n    result = SparseArray(arr)._first_fill_value_loc()\n    assert result == loc\n\n\n@pytest.mark.parametrize(\n    \"arr\", [[1, 2, np.nan, np.nan], [1, np.nan, 2, np.nan], [1, 2, np.nan]]\n)\n@pytest.mark.parametrize(\"fill_value\", [np.nan, 0, 1])\ndef test_unique_na_fill(arr, fill_value):\n    a = SparseArray(arr, fill_value=fill_value).unique()\n    b = pd.Series(arr).unique()\n    assert isinstance(a, SparseArray)\n    a = np.asarray(a)\n    tm.assert_numpy_array_equal(a, b)\n\n\ndef test_unique_all_sparse():\n    # https://github.com/pandas-dev/pandas/issues/23168\n    arr = SparseArray([0, 0])\n    result = arr.unique()\n    expected = SparseArray([0])\n    tm.assert_sp_array_equal(result, expected)\n\n\ndef test_map():\n    arr = SparseArray([0, 1, 2])\n    expected = SparseArray([10, 11, 12], fill_value=10)\n\n    # dict\n    result = arr.map({0: 10, 1: 11, 2: 12})\n    tm.assert_sp_array_equal(result, expected)\n\n    # series\n    result = arr.map(pd.Series({0: 10, 1: 11, 2: 12}))\n    tm.assert_sp_array_equal(result, expected)\n\n    # function\n    result = arr.map(pd.Series({0: 10, 1: 11, 2: 12}))\n    expected = SparseArray([10, 11, 12], fill_value=10)\n    tm.assert_sp_array_equal(result, expected)\n\n\ndef test_map_missing():\n    arr = SparseArray([0, 1, 2])\n    expected = SparseArray([10, 11, None], fill_value=10)\n\n    result = arr.map({0: 10, 1: 11})\n    tm.assert_sp_array_equal(result, expected)\n\n\n@pytest.mark.parametrize(\"fill_value\", [np.nan, 1])\ndef test_dropna(fill_value):\n    # GH-28287\n    arr = SparseArray([np.nan, 1], fill_value=fill_value)\n    exp = SparseArray([1.0], fill_value=fill_value)\n    tm.assert_sp_array_equal(arr.dropna(), exp)\n\n    df = pd.DataFrame({\"a\": [0, 1], \"b\": arr})\n    expected_df = pd.DataFrame({\"a\": [1], \"b\": exp}, index=pd.Int64Index([1]))\n    tm.assert_equal(df.dropna(), expected_df)\n"
    },
    {
      "filename": "pandas/tests/extension/base/getitem.py",
      "content": "import numpy as np\nimport pytest\n\nimport pandas as pd\n\nfrom .base import BaseExtensionTests\n\n\nclass BaseGetitemTests(BaseExtensionTests):\n    \"\"\"Tests for ExtensionArray.__getitem__.\"\"\"\n\n    def test_iloc_series(self, data):\n        ser = pd.Series(data)\n        result = ser.iloc[:4]\n        expected = pd.Series(data[:4])\n        self.assert_series_equal(result, expected)\n\n        result = ser.iloc[[0, 1, 2, 3]]\n        self.assert_series_equal(result, expected)\n\n    def test_iloc_frame(self, data):\n        df = pd.DataFrame({\"A\": data, \"B\": np.arange(len(data), dtype=\"int64\")})\n        expected = pd.DataFrame({\"A\": data[:4]})\n\n        # slice -> frame\n        result = df.iloc[:4, [0]]\n        self.assert_frame_equal(result, expected)\n\n        # sequence -> frame\n        result = df.iloc[[0, 1, 2, 3], [0]]\n        self.assert_frame_equal(result, expected)\n\n        expected = pd.Series(data[:4], name=\"A\")\n\n        # slice -> series\n        result = df.iloc[:4, 0]\n        self.assert_series_equal(result, expected)\n\n        # sequence -> series\n        result = df.iloc[:4, 0]\n        self.assert_series_equal(result, expected)\n\n        # GH#32959 slice columns with step\n        result = df.iloc[:, ::2]\n        self.assert_frame_equal(result, df[[\"A\"]])\n        result = df[[\"B\", \"A\"]].iloc[:, ::2]\n        self.assert_frame_equal(result, df[[\"B\"]])\n\n    def test_iloc_frame_single_block(self, data):\n        # GH#32959 null slice along index, slice along columns with single-block\n        df = pd.DataFrame({\"A\": data})\n\n        result = df.iloc[:, :]\n        self.assert_frame_equal(result, df)\n\n        result = df.iloc[:, :1]\n        self.assert_frame_equal(result, df)\n\n        result = df.iloc[:, :2]\n        self.assert_frame_equal(result, df)\n\n        result = df.iloc[:, ::2]\n        self.assert_frame_equal(result, df)\n\n        result = df.iloc[:, 1:2]\n        self.assert_frame_equal(result, df.iloc[:, :0])\n\n        result = df.iloc[:, -1:]\n        self.assert_frame_equal(result, df)\n\n    def test_loc_series(self, data):\n        ser = pd.Series(data)\n        result = ser.loc[:3]\n        expected = pd.Series(data[:4])\n        self.assert_series_equal(result, expected)\n\n        result = ser.loc[[0, 1, 2, 3]]\n        self.assert_series_equal(result, expected)\n\n    def test_loc_frame(self, data):\n        df = pd.DataFrame({\"A\": data, \"B\": np.arange(len(data), dtype=\"int64\")})\n        expected = pd.DataFrame({\"A\": data[:4]})\n\n        # slice -> frame\n        result = df.loc[:3, [\"A\"]]\n        self.assert_frame_equal(result, expected)\n\n        # sequence -> frame\n        result = df.loc[[0, 1, 2, 3], [\"A\"]]\n        self.assert_frame_equal(result, expected)\n\n        expected = pd.Series(data[:4], name=\"A\")\n\n        # slice -> series\n        result = df.loc[:3, \"A\"]\n        self.assert_series_equal(result, expected)\n\n        # sequence -> series\n        result = df.loc[:3, \"A\"]\n        self.assert_series_equal(result, expected)\n\n    def test_loc_iloc_frame_single_dtype(self, data):\n        # GH#27110 bug in ExtensionBlock.iget caused df.iloc[n] to incorrectly\n        #  return a scalar\n        df = pd.DataFrame({\"A\": data})\n        expected = pd.Series([data[2]], index=[\"A\"], name=2, dtype=data.dtype)\n\n        result = df.loc[2]\n        self.assert_series_equal(result, expected)\n\n        expected = pd.Series(\n            [data[-1]], index=[\"A\"], name=len(data) - 1, dtype=data.dtype\n        )\n        result = df.iloc[-1]\n        self.assert_series_equal(result, expected)\n\n    def test_getitem_scalar(self, data):\n        result = data[0]\n        assert isinstance(result, data.dtype.type)\n\n        result = pd.Series(data)[0]\n        assert isinstance(result, data.dtype.type)\n\n    def test_getitem_scalar_na(self, data_missing, na_cmp, na_value):\n        result = data_missing[0]\n        assert na_cmp(result, na_value)\n\n    def test_getitem_empty(self, data):\n        # Indexing with empty list\n        result = data[[]]\n        assert len(result) == 0\n        assert isinstance(result, type(data))\n\n        expected = data[np.array([], dtype=\"int64\")]\n        self.assert_extension_array_equal(result, expected)\n\n    def test_getitem_mask(self, data):\n        # Empty mask, raw array\n        mask = np.zeros(len(data), dtype=bool)\n        result = data[mask]\n        assert len(result) == 0\n        assert isinstance(result, type(data))\n\n        # Empty mask, in series\n        mask = np.zeros(len(data), dtype=bool)\n        result = pd.Series(data)[mask]\n        assert len(result) == 0\n        assert result.dtype == data.dtype\n\n        # non-empty mask, raw array\n        mask[0] = True\n        result = data[mask]\n        assert len(result) == 1\n        assert isinstance(result, type(data))\n\n        # non-empty mask, in series\n        result = pd.Series(data)[mask]\n        assert len(result) == 1\n        assert result.dtype == data.dtype\n\n    def test_getitem_mask_raises(self, data):\n        mask = np.array([True, False])\n        with pytest.raises(IndexError):\n            data[mask]\n\n        mask = pd.array(mask, dtype=\"boolean\")\n        with pytest.raises(IndexError):\n            data[mask]\n\n    def test_getitem_boolean_array_mask(self, data):\n        mask = pd.array(np.zeros(data.shape, dtype=\"bool\"), dtype=\"boolean\")\n        result = data[mask]\n        assert len(result) == 0\n        assert isinstance(result, type(data))\n\n        result = pd.Series(data)[mask]\n        assert len(result) == 0\n        assert result.dtype == data.dtype\n\n        mask[:5] = True\n        expected = data.take([0, 1, 2, 3, 4])\n        result = data[mask]\n        self.assert_extension_array_equal(result, expected)\n\n        expected = pd.Series(expected)\n        result = pd.Series(data)[mask]\n        self.assert_series_equal(result, expected)\n\n    def test_getitem_boolean_na_treated_as_false(self, data):\n        # https://github.com/pandas-dev/pandas/issues/31503\n        mask = pd.array(np.zeros(data.shape, dtype=\"bool\"), dtype=\"boolean\")\n        mask[:2] = pd.NA\n        mask[2:4] = True\n\n        result = data[mask]\n        expected = data[mask.fillna(False)]\n\n        self.assert_extension_array_equal(result, expected)\n\n        s = pd.Series(data)\n\n        result = s[mask]\n        expected = s[mask.fillna(False)]\n\n        self.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"idx\",\n        [[0, 1, 2], pd.array([0, 1, 2], dtype=\"Int64\"), np.array([0, 1, 2])],\n        ids=[\"list\", \"integer-array\", \"numpy-array\"],\n    )\n    def test_getitem_integer_array(self, data, idx):\n        result = data[idx]\n        assert len(result) == 3\n        assert isinstance(result, type(data))\n        expected = data.take([0, 1, 2])\n        self.assert_extension_array_equal(result, expected)\n\n        expected = pd.Series(expected)\n        result = pd.Series(data)[idx]\n        self.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"idx\",\n        [[0, 1, 2, pd.NA], pd.array([0, 1, 2, pd.NA], dtype=\"Int64\")],\n        ids=[\"list\", \"integer-array\"],\n    )\n    def test_getitem_integer_with_missing_raises(self, data, idx):\n        msg = \"Cannot index with an integer indexer containing NA values\"\n        with pytest.raises(ValueError, match=msg):\n            data[idx]\n\n        # FIXME: dont leave commented-out\n        # TODO: this raises KeyError about labels not found (it tries label-based)\n        # import pandas._testing as tm\n        # s = pd.Series(data, index=[tm.rands(4) for _ in range(len(data))])\n        # with pytest.raises(ValueError, match=msg):\n        #    s[idx]\n\n    def test_getitem_slice(self, data):\n        # getitem[slice] should return an array\n        result = data[slice(0)]  # empty\n        assert isinstance(result, type(data))\n\n        result = data[slice(1)]  # scalar\n        assert isinstance(result, type(data))\n\n    def test_get(self, data):\n        # GH 20882\n        s = pd.Series(data, index=[2 * i for i in range(len(data))])\n        assert s.get(4) == s.iloc[2]\n\n        result = s.get([4, 6])\n        expected = s.iloc[[2, 3]]\n        self.assert_series_equal(result, expected)\n\n        result = s.get(slice(2))\n        expected = s.iloc[[0, 1]]\n        self.assert_series_equal(result, expected)\n\n        assert s.get(-1) is None\n        assert s.get(s.index.max() + 1) is None\n\n        s = pd.Series(data[:6], index=list(\"abcdef\"))\n        assert s.get(\"c\") == s.iloc[2]\n\n        result = s.get(slice(\"b\", \"d\"))\n        expected = s.iloc[[1, 2, 3]]\n        self.assert_series_equal(result, expected)\n\n        result = s.get(\"Z\")\n        assert result is None\n\n        assert s.get(4) == s.iloc[4]\n        assert s.get(-1) == s.iloc[-1]\n        assert s.get(len(s)) is None\n\n        # GH 21257\n        s = pd.Series(data)\n        s2 = s[::2]\n        assert s2.get(1) is None\n\n    def test_take_sequence(self, data):\n        result = pd.Series(data)[[0, 1, 3]]\n        assert result.iloc[0] == data[0]\n        assert result.iloc[1] == data[1]\n        assert result.iloc[2] == data[3]\n\n    def test_take(self, data, na_value, na_cmp):\n        result = data.take([0, -1])\n        assert result.dtype == data.dtype\n        assert result[0] == data[0]\n        assert result[1] == data[-1]\n\n        result = data.take([0, -1], allow_fill=True, fill_value=na_value)\n        assert result[0] == data[0]\n        assert na_cmp(result[1], na_value)\n\n        with pytest.raises(IndexError, match=\"out of bounds\"):\n            data.take([len(data) + 1])\n\n    def test_take_empty(self, data, na_value, na_cmp):\n        empty = data[:0]\n\n        result = empty.take([-1], allow_fill=True)\n        assert na_cmp(result[0], na_value)\n\n        with pytest.raises(IndexError):\n            empty.take([-1])\n\n        with pytest.raises(IndexError, match=\"cannot do a non-empty take\"):\n            empty.take([0, 1])\n\n    def test_take_negative(self, data):\n        # https://github.com/pandas-dev/pandas/issues/20640\n        n = len(data)\n        result = data.take([0, -n, n - 1, -1])\n        expected = data.take([0, 0, n - 1, n - 1])\n        self.assert_extension_array_equal(result, expected)\n\n    def test_take_non_na_fill_value(self, data_missing):\n        fill_value = data_missing[1]  # valid\n        na = data_missing[0]\n\n        array = data_missing._from_sequence(\n            [na, fill_value, na], dtype=data_missing.dtype\n        )\n        result = array.take([-1, 1], fill_value=fill_value, allow_fill=True)\n        expected = array.take([1, 1])\n        self.assert_extension_array_equal(result, expected)\n\n    def test_take_pandas_style_negative_raises(self, data, na_value):\n        with pytest.raises(ValueError):\n            data.take([0, -2], fill_value=na_value, allow_fill=True)\n\n    @pytest.mark.parametrize(\"allow_fill\", [True, False])\n    def test_take_out_of_bounds_raises(self, data, allow_fill):\n        arr = data[:3]\n        with pytest.raises(IndexError):\n            arr.take(np.asarray([0, 3]), allow_fill=allow_fill)\n\n    def test_take_series(self, data):\n        s = pd.Series(data)\n        result = s.take([0, -1])\n        expected = pd.Series(\n            data._from_sequence([data[0], data[len(data) - 1]], dtype=s.dtype),\n            index=[0, len(data) - 1],\n        )\n        self.assert_series_equal(result, expected)\n\n    def test_reindex(self, data, na_value):\n        s = pd.Series(data)\n        result = s.reindex([0, 1, 3])\n        expected = pd.Series(data.take([0, 1, 3]), index=[0, 1, 3])\n        self.assert_series_equal(result, expected)\n\n        n = len(data)\n        result = s.reindex([-1, 0, n])\n        expected = pd.Series(\n            data._from_sequence([na_value, data[0], na_value], dtype=s.dtype),\n            index=[-1, 0, n],\n        )\n        self.assert_series_equal(result, expected)\n\n        result = s.reindex([n, n + 1])\n        expected = pd.Series(\n            data._from_sequence([na_value, na_value], dtype=s.dtype), index=[n, n + 1]\n        )\n        self.assert_series_equal(result, expected)\n\n    def test_reindex_non_na_fill_value(self, data_missing):\n        valid = data_missing[1]\n        na = data_missing[0]\n\n        array = data_missing._from_sequence([na, valid], dtype=data_missing.dtype)\n        ser = pd.Series(array)\n        result = ser.reindex([0, 1, 2], fill_value=valid)\n        expected = pd.Series(\n            data_missing._from_sequence([na, valid, valid], dtype=data_missing.dtype)\n        )\n\n        self.assert_series_equal(result, expected)\n\n    def test_loc_len1(self, data):\n        # see GH-27785 take_nd with indexer of len 1 resulting in wrong ndim\n        df = pd.DataFrame({\"A\": data})\n        res = df.loc[[0], \"A\"]\n        assert res._mgr._block.ndim == 1\n\n    def test_item(self, data):\n        # https://github.com/pandas-dev/pandas/pull/30175\n        s = pd.Series(data)\n        result = s[:1].item()\n        assert result == data[0]\n\n        msg = \"can only convert an array of size 1 to a Python scalar\"\n        with pytest.raises(ValueError, match=msg):\n            s[:0].item()\n\n        with pytest.raises(ValueError, match=msg):\n            s.item()\n"
    },
    {
      "filename": "pandas/tests/extension/test_sparse.py",
      "content": "import numpy as np\nimport pytest\n\nfrom pandas.errors import PerformanceWarning\n\nfrom pandas.core.dtypes.common import is_object_dtype\n\nimport pandas as pd\nfrom pandas import SparseDtype\nimport pandas._testing as tm\nfrom pandas.arrays import SparseArray\nfrom pandas.tests.extension import base\n\n\ndef make_data(fill_value):\n    if np.isnan(fill_value):\n        data = np.random.uniform(size=100)\n    else:\n        data = np.random.randint(1, 100, size=100)\n        if data[0] == data[1]:\n            data[0] += 1\n\n    data[2::3] = fill_value\n    return data\n\n\n@pytest.fixture\ndef dtype():\n    return SparseDtype()\n\n\n@pytest.fixture(params=[0, np.nan])\ndef data(request):\n    \"\"\"Length-100 PeriodArray for semantics test.\"\"\"\n    res = SparseArray(make_data(request.param), fill_value=request.param)\n    return res\n\n\n@pytest.fixture\ndef data_for_twos(request):\n    return SparseArray(np.ones(100) * 2)\n\n\n@pytest.fixture(params=[0, np.nan])\ndef data_missing(request):\n    \"\"\"Length 2 array with [NA, Valid]\"\"\"\n    return SparseArray([np.nan, 1], fill_value=request.param)\n\n\n@pytest.fixture(params=[0, np.nan])\ndef data_repeated(request):\n    \"\"\"Return different versions of data for count times\"\"\"\n\n    def gen(count):\n        for _ in range(count):\n            yield SparseArray(make_data(request.param), fill_value=request.param)\n\n    yield gen\n\n\n@pytest.fixture(params=[0, np.nan])\ndef data_for_sorting(request):\n    return SparseArray([2, 3, 1], fill_value=request.param)\n\n\n@pytest.fixture(params=[0, np.nan])\ndef data_missing_for_sorting(request):\n    return SparseArray([2, np.nan, 1], fill_value=request.param)\n\n\n@pytest.fixture\ndef na_value():\n    return np.nan\n\n\n@pytest.fixture\ndef na_cmp():\n    return lambda left, right: pd.isna(left) and pd.isna(right)\n\n\n@pytest.fixture(params=[0, np.nan])\ndef data_for_grouping(request):\n    return SparseArray([1, 1, np.nan, np.nan, 2, 2, 1, 3], fill_value=request.param)\n\n\nclass BaseSparseTests:\n    def _check_unsupported(self, data):\n        if data.dtype == SparseDtype(int, 0):\n            pytest.skip(\"Can't store nan in int array.\")\n\n    @pytest.mark.xfail(reason=\"SparseArray does not support setitem\")\n    def test_ravel(self, data):\n        super().test_ravel(data)\n\n\nclass TestDtype(BaseSparseTests, base.BaseDtypeTests):\n    def test_array_type_with_arg(self, data, dtype):\n        assert dtype.construct_array_type() is SparseArray\n\n\nclass TestInterface(BaseSparseTests, base.BaseInterfaceTests):\n    def test_no_values_attribute(self, data):\n        pytest.skip(\"We have values\")\n\n    def test_copy(self, data):\n        # __setitem__ does not work, so we only have a smoke-test\n        data.copy()\n\n    def test_view(self, data):\n        # __setitem__ does not work, so we only have a smoke-test\n        data.view()\n\n\nclass TestConstructors(BaseSparseTests, base.BaseConstructorsTests):\n    pass\n\n\nclass TestReshaping(BaseSparseTests, base.BaseReshapingTests):\n    def test_concat_mixed_dtypes(self, data):\n        # https://github.com/pandas-dev/pandas/issues/20762\n        # This should be the same, aside from concat([sparse, float])\n        df1 = pd.DataFrame({\"A\": data[:3]})\n        df2 = pd.DataFrame({\"A\": [1, 2, 3]})\n        df3 = pd.DataFrame({\"A\": [\"a\", \"b\", \"c\"]}).astype(\"category\")\n        dfs = [df1, df2, df3]\n\n        # dataframes\n        result = pd.concat(dfs)\n        expected = pd.concat(\n            [x.apply(lambda s: np.asarray(s).astype(object)) for x in dfs]\n        )\n        self.assert_frame_equal(result, expected)\n\n    def test_concat_columns(self, data, na_value):\n        self._check_unsupported(data)\n        super().test_concat_columns(data, na_value)\n\n    def test_concat_extension_arrays_copy_false(self, data, na_value):\n        self._check_unsupported(data)\n        super().test_concat_extension_arrays_copy_false(data, na_value)\n\n    def test_align(self, data, na_value):\n        self._check_unsupported(data)\n        super().test_align(data, na_value)\n\n    def test_align_frame(self, data, na_value):\n        self._check_unsupported(data)\n        super().test_align_frame(data, na_value)\n\n    def test_align_series_frame(self, data, na_value):\n        self._check_unsupported(data)\n        super().test_align_series_frame(data, na_value)\n\n    def test_merge(self, data, na_value):\n        self._check_unsupported(data)\n        super().test_merge(data, na_value)\n\n\nclass TestGetitem(BaseSparseTests, base.BaseGetitemTests):\n    def test_get(self, data):\n        s = pd.Series(data, index=[2 * i for i in range(len(data))])\n        if np.isnan(s.values.fill_value):\n            assert np.isnan(s.get(4)) and np.isnan(s.iloc[2])\n        else:\n            assert s.get(4) == s.iloc[2]\n        assert s.get(2) == s.iloc[1]\n\n    def test_reindex(self, data, na_value):\n        self._check_unsupported(data)\n        super().test_reindex(data, na_value)\n\n\n# Skipping TestSetitem, since we don't implement it.\n\n\nclass TestMissing(BaseSparseTests, base.BaseMissingTests):\n    def test_isna(self, data_missing):\n        expected_dtype = SparseDtype(bool, pd.isna(data_missing.dtype.fill_value))\n        expected = SparseArray([True, False], dtype=expected_dtype)\n\n        result = pd.isna(data_missing)\n        self.assert_equal(result, expected)\n\n        result = pd.Series(data_missing).isna()\n        expected = pd.Series(expected)\n        self.assert_series_equal(result, expected)\n\n        # GH 21189\n        result = pd.Series(data_missing).drop([0, 1]).isna()\n        expected = pd.Series([], dtype=expected_dtype)\n        self.assert_series_equal(result, expected)\n\n    def test_fillna_limit_pad(self, data_missing):\n        with tm.assert_produces_warning(PerformanceWarning):\n            super().test_fillna_limit_pad(data_missing)\n\n    def test_fillna_limit_backfill(self, data_missing):\n        with tm.assert_produces_warning(PerformanceWarning):\n            super().test_fillna_limit_backfill(data_missing)\n\n    def test_fillna_series_method(self, data_missing):\n        with tm.assert_produces_warning(PerformanceWarning):\n            super().test_fillna_limit_backfill(data_missing)\n\n    @pytest.mark.skip(reason=\"Unsupported\")\n    def test_fillna_series(self):\n        # this one looks doable.\n        pass\n\n    def test_fillna_frame(self, data_missing):\n        # Have to override to specify that fill_value will change.\n        fill_value = data_missing[1]\n\n        result = pd.DataFrame({\"A\": data_missing, \"B\": [1, 2]}).fillna(fill_value)\n\n        if pd.isna(data_missing.fill_value):\n            dtype = SparseDtype(data_missing.dtype, fill_value)\n        else:\n            dtype = data_missing.dtype\n\n        expected = pd.DataFrame(\n            {\n                \"A\": data_missing._from_sequence([fill_value, fill_value], dtype=dtype),\n                \"B\": [1, 2],\n            }\n        )\n\n        self.assert_frame_equal(result, expected)\n\n\nclass TestMethods(BaseSparseTests, base.BaseMethodsTests):\n    def test_combine_le(self, data_repeated):\n        # We return a Series[SparseArray].__le__ returns a\n        # Series[Sparse[bool]]\n        # rather than Series[bool]\n        orig_data1, orig_data2 = data_repeated(2)\n        s1 = pd.Series(orig_data1)\n        s2 = pd.Series(orig_data2)\n        result = s1.combine(s2, lambda x1, x2: x1 <= x2)\n        expected = pd.Series(\n            SparseArray(\n                [a <= b for (a, b) in zip(list(orig_data1), list(orig_data2))],\n                fill_value=False,\n            )\n        )\n        self.assert_series_equal(result, expected)\n\n        val = s1.iloc[0]\n        result = s1.combine(val, lambda x1, x2: x1 <= x2)\n        expected = pd.Series(\n            SparseArray([a <= val for a in list(orig_data1)], fill_value=False)\n        )\n        self.assert_series_equal(result, expected)\n\n    def test_fillna_copy_frame(self, data_missing):\n        arr = data_missing.take([1, 1])\n        df = pd.DataFrame({\"A\": arr})\n\n        filled_val = df.iloc[0, 0]\n        result = df.fillna(filled_val)\n\n        assert df.values.base is not result.values.base\n        assert df.A._values.to_dense() is arr.to_dense()\n\n    def test_fillna_copy_series(self, data_missing):\n        arr = data_missing.take([1, 1])\n        ser = pd.Series(arr)\n\n        filled_val = ser[0]\n        result = ser.fillna(filled_val)\n\n        assert ser._values is not result._values\n        assert ser._values.to_dense() is arr.to_dense()\n\n    @pytest.mark.skip(reason=\"Not Applicable\")\n    def test_fillna_length_mismatch(self, data_missing):\n        pass\n\n    def test_where_series(self, data, na_value):\n        assert data[0] != data[1]\n        cls = type(data)\n        a, b = data[:2]\n\n        ser = pd.Series(cls._from_sequence([a, a, b, b], dtype=data.dtype))\n\n        cond = np.array([True, True, False, False])\n        result = ser.where(cond)\n\n        new_dtype = SparseDtype(\"float\", 0.0)\n        expected = pd.Series(\n            cls._from_sequence([a, a, na_value, na_value], dtype=new_dtype)\n        )\n        self.assert_series_equal(result, expected)\n\n        other = cls._from_sequence([a, b, a, b], dtype=data.dtype)\n        cond = np.array([True, False, True, True])\n        result = ser.where(cond, other)\n        expected = pd.Series(cls._from_sequence([a, b, b, b], dtype=data.dtype))\n        self.assert_series_equal(result, expected)\n\n    def test_combine_first(self, data):\n        if data.dtype.subtype == \"int\":\n            # Right now this is upcasted to float, just like combine_first\n            # for Series[int]\n            pytest.skip(\"TODO(SparseArray.__setitem__ will preserve dtype.\")\n        super().test_combine_first(data)\n\n    def test_searchsorted(self, data_for_sorting, as_series):\n        with tm.assert_produces_warning(PerformanceWarning):\n            super().test_searchsorted(data_for_sorting, as_series)\n\n    def test_shift_0_periods(self, data):\n        # GH#33856 shifting with periods=0 should return a copy, not same obj\n        result = data.shift(0)\n\n        data._sparse_values[0] = data._sparse_values[1]\n        assert result._sparse_values[0] != result._sparse_values[1]\n\n    @pytest.mark.parametrize(\n        \"method\", [\"argmax\", \"argmin\"],\n    )\n    def test_argmin_argmax_all_na(self, method, data, na_value):\n        # overriding because Sparse[int64, 0] cannot handle na_value\n        self._check_unsupported(data)\n        super().test_argmin_argmax_all_na(method, data, na_value)\n\n    @pytest.mark.parametrize(\"box\", [pd.array, pd.Series, pd.DataFrame])\n    def test_equals(self, data, na_value, as_series, box):\n        self._check_unsupported(data)\n        super().test_equals(data, na_value, as_series, box)\n\n\nclass TestCasting(BaseSparseTests, base.BaseCastingTests):\n    def test_astype_object_series(self, all_data):\n        # Unlike the base class, we do not expect the resulting Block\n        #  to be ObjectBlock\n        ser = pd.Series(all_data, name=\"A\")\n        result = ser.astype(object)\n        assert is_object_dtype(result._data.blocks[0].dtype)\n\n    def test_astype_object_frame(self, all_data):\n        # Unlike the base class, we do not expect the resulting Block\n        #  to be ObjectBlock\n        df = pd.DataFrame({\"A\": all_data})\n\n        result = df.astype(object)\n        assert is_object_dtype(result._data.blocks[0].dtype)\n\n        # FIXME: these currently fail; dont leave commented-out\n        # check that we can compare the dtypes\n        # comp = result.dtypes.equals(df.dtypes)\n        # assert not comp.any()\n\n    def test_astype_str(self, data):\n        result = pd.Series(data[:5]).astype(str)\n        expected_dtype = pd.SparseDtype(str, str(data.fill_value))\n        expected = pd.Series([str(x) for x in data[:5]], dtype=expected_dtype)\n        self.assert_series_equal(result, expected)\n\n    @pytest.mark.xfail(raises=TypeError, reason=\"no sparse StringDtype\")\n    def test_astype_string(self, data):\n        super().test_astype_string(data)\n\n\nclass TestArithmeticOps(BaseSparseTests, base.BaseArithmeticOpsTests):\n    series_scalar_exc = None\n    frame_scalar_exc = None\n    divmod_exc = None\n    series_array_exc = None\n\n    def _skip_if_different_combine(self, data):\n        if data.fill_value == 0:\n            # arith ops call on dtype.fill_value so that the sparsity\n            # is maintained. Combine can't be called on a dtype in\n            # general, so we can't make the expected. This is tested elsewhere\n            raise pytest.skip(\"Incorrected expected from Series.combine\")\n\n    def test_error(self, data, all_arithmetic_operators):\n        pass\n\n    def test_arith_series_with_scalar(self, data, all_arithmetic_operators):\n        self._skip_if_different_combine(data)\n        super().test_arith_series_with_scalar(data, all_arithmetic_operators)\n\n    def test_arith_series_with_array(self, data, all_arithmetic_operators):\n        self._skip_if_different_combine(data)\n        super().test_arith_series_with_array(data, all_arithmetic_operators)\n\n\nclass TestComparisonOps(BaseSparseTests, base.BaseComparisonOpsTests):\n    def _compare_other(self, s, data, op_name, other):\n        op = self.get_op_from_name(op_name)\n\n        # array\n        result = pd.Series(op(data, other))\n        # hard to test the fill value, since we don't know what expected\n        # is in general.\n        # Rely on tests in `tests/sparse` to validate that.\n        assert isinstance(result.dtype, SparseDtype)\n        assert result.dtype.subtype == np.dtype(\"bool\")\n\n        with np.errstate(all=\"ignore\"):\n            expected = pd.Series(\n                SparseArray(\n                    op(np.asarray(data), np.asarray(other)),\n                    fill_value=result.values.fill_value,\n                )\n            )\n\n        tm.assert_series_equal(result, expected)\n\n        # series\n        s = pd.Series(data)\n        result = op(s, other)\n        tm.assert_series_equal(result, expected)\n\n\nclass TestPrinting(BaseSparseTests, base.BasePrintingTests):\n    @pytest.mark.xfail(reason=\"Different repr\", strict=True)\n    def test_array_repr(self, data, size):\n        super().test_array_repr(data, size)\n\n\nclass TestParsing(BaseSparseTests, base.BaseParsingTests):\n    @pytest.mark.parametrize(\"engine\", [\"c\", \"python\"])\n    def test_EA_types(self, engine, data):\n        expected_msg = r\".*must implement _from_sequence_of_strings.*\"\n        with pytest.raises(NotImplementedError, match=expected_msg):\n            super().test_EA_types(engine, data)\n"
    },
    {
      "filename": "pandas/tests/frame/indexing/test_sparse.py",
      "content": "import numpy as np\nimport pytest\n\nimport pandas.util._test_decorators as td\n\nimport pandas as pd\nimport pandas._testing as tm\nfrom pandas.arrays import SparseArray\nfrom pandas.core.arrays.sparse import SparseDtype\n\n\nclass TestSparseDataFrameIndexing:\n    def test_getitem_sparse_column(self):\n        # https://github.com/pandas-dev/pandas/issues/23559\n        data = SparseArray([0, 1])\n        df = pd.DataFrame({\"A\": data})\n        expected = pd.Series(data, name=\"A\")\n        result = df[\"A\"]\n        tm.assert_series_equal(result, expected)\n\n        result = df.iloc[:, 0]\n        tm.assert_series_equal(result, expected)\n\n        result = df.loc[:, \"A\"]\n        tm.assert_series_equal(result, expected)\n\n    @pytest.mark.parametrize(\"spmatrix_t\", [\"coo_matrix\", \"csc_matrix\", \"csr_matrix\"])\n    @pytest.mark.parametrize(\"dtype\", [np.int64, np.float64, complex])\n    @td.skip_if_no_scipy\n    def test_locindexer_from_spmatrix(self, spmatrix_t, dtype):\n        import scipy.sparse\n\n        spmatrix_t = getattr(scipy.sparse, spmatrix_t)\n\n        # The bug is triggered by a sparse matrix with purely sparse columns.  So the\n        # recipe below generates a rectangular matrix of dimension (5, 7) where all the\n        # diagonal cells are ones, meaning the last two columns are purely sparse.\n        rows, cols = 5, 7\n        spmatrix = spmatrix_t(np.eye(rows, cols, dtype=dtype), dtype=dtype)\n        df = pd.DataFrame.sparse.from_spmatrix(spmatrix)\n\n        # regression test for #34526\n        itr_idx = range(2, rows)\n        result = df.loc[itr_idx].values\n        expected = spmatrix.toarray()[itr_idx]\n        tm.assert_numpy_array_equal(result, expected)\n\n        # regression test for #34540\n        result = df.loc[itr_idx].dtypes.values\n        expected = np.full(cols, SparseDtype(dtype, fill_value=0))\n        tm.assert_numpy_array_equal(result, expected)\n\n    def test_reindex(self):\n        # https://github.com/pandas-dev/pandas/issues/35286\n        df = pd.DataFrame(\n            {\"A\": [0, 1], \"B\": pd.array([0, 1], dtype=pd.SparseDtype(\"int64\", 0))}\n        )\n        result = df.reindex([0, 2])\n        expected = pd.DataFrame(\n            {\n                \"A\": [0.0, np.nan],\n                \"B\": pd.array([0.0, np.nan], dtype=pd.SparseDtype(\"float64\", 0.0)),\n            },\n            index=[0, 2],\n        )\n        tm.assert_frame_equal(result, expected)\n\n    def test_all_sparse(self):\n        df = pd.DataFrame({\"A\": pd.array([0, 0], dtype=pd.SparseDtype(\"int64\"))})\n        result = df.loc[[0, 1]]\n        tm.assert_frame_equal(result, df)\n"
    }
  ],
  "questions": [
    "Can you give an example of how/why that would happen? I don't understand quite how we should get a NaN when the fill_value is not nan.",
    "Here's my proposed solution:\r\n\r\nReplace\r\n```\r\n    @property\r\n    def fill_value(self):\r\n        # Used in reindex_indexer\r\n        return self.values.dtype.na_value\r\n```\r\nfrom https://github.com/pandas-dev/pandas/blob/master/pandas/core/internals/blocks.py#L1730 with\r\n```\r\n    @property\r\n    def fill_value(self):\r\n        # Used in reindex_indexer\r\n        try:\r\n            return self.values.dtype.fill_value\r\n        except AttributeError:\r\n            return self.values.dtype.na_value\r\n```\r\nThoughts?",
    "Pretty sure my dataset is showing this. Seems to apply to some columns but not all.\r\n[strange_test.pickle.gz](https://github.com/pandas-dev/pandas/files/3958518/strange_test.pickle.gz)\r\n\r\nIf anyone wants a large real-world set to test this on:\r\n```python\r\nimport pandas as pd\r\ntest = pd.read_pickle('strange_test.pickle.gz')\r\nany(test.DistinctSKUs_SEPB.isna())\r\n> False\r\nany(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\n> True # !?!\r\n```",
    "@akdor1154 can you try this monkey patch and see if it solves your issue?\r\n\r\n```\r\ndef fill_value(self):\r\n    # Used in reindex_indexer\r\n    try:\r\n        return self.values.dtype.fill_value\r\n    except AttributeError:\r\n        return self.values.dtype.na_value\r\n\r\nfrom pandas.core.internals.blocks import ExtensionBlock\r\n\r\nsetattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n```"
  ],
  "golden_answers": [
    "Here's my proposed solution:\r\n\r\nReplace\r\n```\r\n    @property\r\n    def fill_value(self):\r\n        # Used in reindex_indexer\r\n        return self.values.dtype.na_value\r\n```\r\nfrom https://github.com/pandas-dev/pandas/blob/master/pandas/core/internals/blocks.py#L1730 with\r\n```\r\n    @property\r\n    def fill_value(self):\r\n        # Used in reindex_indexer\r\n        try:\r\n            return self.values.dtype.fill_value\r\n        except AttributeError:\r\n            return self.values.dtype.na_value\r\n```\r\nThoughts?",
    "Pretty sure my dataset is showing this. Seems to apply to some columns but not all.\r\n[strange_test.pickle.gz](https://github.com/pandas-dev/pandas/files/3958518/strange_test.pickle.gz)\r\n\r\nIf anyone wants a large real-world set to test this on:\r\n```python\r\nimport pandas as pd\r\ntest = pd.read_pickle('strange_test.pickle.gz')\r\nany(test.DistinctSKUs_SEPB.isna())\r\n> False\r\nany(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\n> True # !?!\r\n```",
    "@akdor1154 can you try this monkey patch and see if it solves your issue?\r\n\r\n```\r\ndef fill_value(self):\r\n    # Used in reindex_indexer\r\n    try:\r\n        return self.values.dtype.fill_value\r\n    except AttributeError:\r\n        return self.values.dtype.na_value\r\n\r\nfrom pandas.core.internals.blocks import ExtensionBlock\r\n\r\nsetattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n```",
    "Pretty sure my monkey patch works. I can write a PR if I can get approval from @TomAugspurger or @jorisvandenbossche \r\n\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> df1 = pd.DataFrame({\"A\": pd.arrays.SparseArray([0, 0, 0]), 'B': [1,2,3]})\r\n>>> df1.loc[df1['B'] != 2]\r\n    A  B\r\n0 NaN  1\r\n2 NaN  3\r\n>>> def fill_value(self):\r\n...     # Used in reindex_indexer\r\n...     try:\r\n...         return self.values.dtype.fill_value\r\n...     except AttributeError:\r\n...         return self.values.dtype.na_value\r\n...\r\n>>> from pandas.core.internals.blocks import ExtensionBlock\r\n>>>\r\n>>> setattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n>>> df1.loc[df1['B'] != 2]\r\n   A  B\r\n0  0  1\r\n2  0  3\r\n```\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> X = pd.DataFrame([[0,1,0], [1,0,0], [1,1,0]]).astype(\r\n...     pd.SparseDtype(float, fill_value=0.0))\r\n>>> X.loc[[0,1]]\r\n     0    1   2\r\n0  0.0  1.0 NaN\r\n1  1.0  0.0 NaN\r\n>>> def fill_value(self):\r\n...     # Used in reindex_indexer\r\n...     try:\r\n...         return self.values.dtype.fill_value\r\n...     except AttributeError:\r\n...         return self.values.dtype.na_value\r\n...\r\n>>> from pandas.core.internals.blocks import ExtensionBlock\r\n>>>\r\n>>> setattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n>>> X.loc[[0,1]]\r\n     0    1    2\r\n0  0.0  1.0  0.0\r\n1  1.0  0.0  0.0\r\n```\r\n\r\n```\r\n>>> import pandas as pd\r\n>>> test = pd.read_pickle('strange_test.pickle.gz')\r\n>>> any(test.DistinctSKUs_SEPB.isna())\r\nFalse\r\n>>> any(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\nTrue\r\n>>> def fill_value(self):\r\n...     # Used in reindex_indexer\r\n...     try:\r\n...         return self.values.dtype.fill_value\r\n...     except AttributeError:\r\n...         return self.values.dtype.na_value\r\n...\r\n>>> from pandas.core.internals.blocks import ExtensionBlock\r\n>>>\r\n>>> setattr(ExtensionBlock, \"fill_value\", property(fill_value))\r\n>>> any(test.loc[lambda _: _.IsBTS].DistinctSKUs_SEPB.isna())\r\nFalse\r\n```"
  ],
  "questions_generated": [
    "What is the primary issue caused by filtering a DataFrame with an all-zero sparse column in the pandas-dev/pandas repository?",
    "How does the SparseArray.take method contribute to the issue when filtering DataFrames?",
    "What is a naive solution proposed to fix the SparseArray.take method issue?",
    "In the context of the pandas-dev/pandas issue, what is the expected behavior when filtering DataFrames with sparse columns?",
    "What version of pandas introduced the regression related to filtering DataFrames with sparse columns?",
    "Why is it important for the SparseArray.take method to handle fill values correctly in the pandas library?"
  ],
  "golden_answers_generated": [
    "The primary issue is that filtering a DataFrame with an all-zero sparse column results in NA values in the sparse column, even when there are no missing values initially. This behavior is inconsistent with the expected outcome where non-missing data should not lead to missing data after filtering.",
    "The issue arises in the SparseArray.take method, specifically when allow_fill=True. In this scenario, the method incorrectly uses NaN as the fill value instead of the actual fill value, which is 0 in the all-zero sparse column case. This leads to NA values appearing in the filtered DataFrame.",
    "A naive solution proposed is to modify the _take_with_fill method to use self.fill_value if the fill_value parameter is None. This change would ensure that the correct fill value is used when allow_fill=True, preventing NA values from appearing erroneously in the filtered DataFrame.",
    "The expected behavior is that filtering a DataFrame with non-missing data should maintain the integrity of the data without introducing NA values. Both DataFrames, regardless of the initial values in the sparse column, should have consistent results after filtering, without any missing data.",
    "The regression related to filtering DataFrames with sparse columns appeared in pandas version 0.25.0. This version introduced a change in behavior compared to pandas 0.23, where SparseSeries did not exhibit this issue.",
    "Correct handling of fill values in the SparseArray.take method is crucial because it ensures data integrity and consistency when performing operations like filtering. Incorrect fill values can lead to unintended NA values, which can distort data analysis and result in incorrect conclusions being drawn from the data."
  ]
}