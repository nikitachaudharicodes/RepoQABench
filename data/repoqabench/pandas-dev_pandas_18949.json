{
  "repo_name": "pandas-dev_pandas",
  "issue_id": "18949",
  "issue_description": "# LINT: pandas/scripts\n\nneed to lint these & add a rule in ci/lint.sh to check.",
  "issue_comments": [
    {
      "id": 354084227,
      "user": "gfyoung",
      "body": "Yeah, that directory could certainly benefit from a lot of clean-up."
    },
    {
      "id": 355645084,
      "user": "bhavybarca",
      "body": "I would like to work on this as my first issue"
    },
    {
      "id": 355652015,
      "user": "gfyoung",
      "body": "@bhavybarca : Go for it!"
    },
    {
      "id": 355765233,
      "user": "bhavybarca",
      "body": "what all do i need to do, i am a beginner \r\n"
    },
    {
      "id": 355765695,
      "user": "jreback",
      "body": "you need to add a add ``pandas/scripts/`` to the ci/lint.sh script (make it a separate step)\r\nthen run this ``export LINT=1; ci/lint.sh`` and see what turns up, which needs fixing. you can use ``autopep8`` to fix most"
    },
    {
      "id": 355766998,
      "user": "bhavybarca",
      "body": "so first i need to make ci/lint.sh script and then copy the files to it ?\r\n"
    },
    {
      "id": 355767240,
      "user": "jreback",
      "body": "itâ€™s aleady there just edit "
    },
    {
      "id": 355767873,
      "user": "bhavybarca",
      "body": "can you please simplify all this a bit am i just starting \r\n"
    },
    {
      "id": 355768998,
      "user": "TomAugspurger",
      "body": "This file: https://github.com/pandas-dev/pandas/blob/master/ci/lint.sh\r\n\r\nYou'll add another `flake8` line like the others, but for `scripts/` directory.\r\n\r\n@bhavybarca are you on Windows or Mac or Linux? You'll set the `LINT` environment variable and then run `ci/lint.sh`, which will print out a list of errors that need to be fixed.\r\n\r\n"
    },
    {
      "id": 355769051,
      "user": "TomAugspurger",
      "body": "It'll be roughly like\r\n\r\n```\r\n$ flake8 scripts/\r\nscripts/api_rst_coverage.py:24:1: E302 expected 2 blank lines, found 1\r\nscripts/api_rst_coverage.py:64:80: E501 line too long (90 > 79 characters)\r\nscripts/api_rst_coverage.py:70:80: E501 line too long (93 > 79 characters)\r\nscripts/api_rst_coverage.py:78:80: E501 line too long (88 > 79 characters)\r\n...\r\n```"
    },
    {
      "id": 355769492,
      "user": "gfyoung",
      "body": "@TomAugspurger : I prefer to use `flake8 scripts/`.  No need to lint the entire repository for this issue."
    },
    {
      "id": 355769578,
      "user": "TomAugspurger",
      "body": "yeah, agreed."
    },
    {
      "id": 355775860,
      "user": "datapythonista",
      "body": "I think it makes sense to include as part of this ticket, couple of other things related to the scripts:\r\n* Fix PEP-8 issues, and add check to lint (as originally requested on the ticket)\r\n* Add documentation to scripts that don't have it (like find_undoc_args.py)\r\n* Make all the scripts executable (chmod a+x and #! header)\r\n* Create a section in contributing.rst with the available script and what they are used for\r\n* Make the scripts work regardless of the current directory (announce.py needs to be called from the pandas home, api_rst_coverage.py from the scripts directory...)\r\n* Follow the same convention in script names (rename merge-pr.py to merge_pr.py)\r\n\r\nWhat do you think?"
    },
    {
      "id": 355775873,
      "user": "bhavybarca",
      "body": "@TomAugspurger  i m on linux 16.04, all this makes a lot of sense now thanks ,just last thing how do i setup lint environment to run lint.sh"
    },
    {
      "id": 355789090,
      "user": "gfyoung",
      "body": "@datapythonista : All of these are excellent ideas!  I think linting is a pretty big step as it is though, so might be worthwhile to just open another issue."
    },
    {
      "id": 359401179,
      "user": "datapythonista",
      "body": "@bhavybarca I'll take this, if that's all right"
    },
    {
      "id": 359402167,
      "user": "bhavybarca",
      "body": "@datapythonista yeah sure!!\r\n"
    },
    {
      "id": 359544885,
      "user": "gfyoung",
      "body": "As a follow-up, we should document all of these functions (@datapythonista : no need to this in your open PR, but wanted to add that just so that we don't forget :smile: )"
    }
  ],
  "text_context": "# LINT: pandas/scripts\n\nneed to lint these & add a rule in ci/lint.sh to check.\n\nYeah, that directory could certainly benefit from a lot of clean-up.\n\nI would like to work on this as my first issue\n\n@bhavybarca : Go for it!\n\nwhat all do i need to do, i am a beginner \r\n\n\nyou need to add a add ``pandas/scripts/`` to the ci/lint.sh script (make it a separate step)\r\nthen run this ``export LINT=1; ci/lint.sh`` and see what turns up, which needs fixing. you can use ``autopep8`` to fix most\n\nso first i need to make ci/lint.sh script and then copy the files to it ?\r\n\n\nitâ€™s aleady there just edit \n\ncan you please simplify all this a bit am i just starting \r\n\n\nThis file: https://github.com/pandas-dev/pandas/blob/master/ci/lint.sh\r\n\r\nYou'll add another `flake8` line like the others, but for `scripts/` directory.\r\n\r\n@bhavybarca are you on Windows or Mac or Linux? You'll set the `LINT` environment variable and then run `ci/lint.sh`, which will print out a list of errors that need to be fixed.\r\n\r\n\n\nIt'll be roughly like\r\n\r\n```\r\n$ flake8 scripts/\r\nscripts/api_rst_coverage.py:24:1: E302 expected 2 blank lines, found 1\r\nscripts/api_rst_coverage.py:64:80: E501 line too long (90 > 79 characters)\r\nscripts/api_rst_coverage.py:70:80: E501 line too long (93 > 79 characters)\r\nscripts/api_rst_coverage.py:78:80: E501 line too long (88 > 79 characters)\r\n...\r\n```\n\n@TomAugspurger : I prefer to use `flake8 scripts/`.  No need to lint the entire repository for this issue.\n\nyeah, agreed.\n\nI think it makes sense to include as part of this ticket, couple of other things related to the scripts:\r\n* Fix PEP-8 issues, and add check to lint (as originally requested on the ticket)\r\n* Add documentation to scripts that don't have it (like find_undoc_args.py)\r\n* Make all the scripts executable (chmod a+x and #! header)\r\n* Create a section in contributing.rst with the available script and what they are used for\r\n* Make the scripts work regardless of the current directory (announce.py needs to be called from the pandas home, api_rst_coverage.py from the scripts directory...)\r\n* Follow the same convention in script names (rename merge-pr.py to merge_pr.py)\r\n\r\nWhat do you think?\n\n@TomAugspurger  i m on linux 16.04, all this makes a lot of sense now thanks ,just last thing how do i setup lint environment to run lint.sh\n\n@datapythonista : All of these are excellent ideas!  I think linting is a pretty big step as it is though, so might be worthwhile to just open another issue.\n\n@bhavybarca I'll take this, if that's all right\n\n@datapythonista yeah sure!!\r\n\n\nAs a follow-up, we should document all of these functions (@datapythonista : no need to this in your open PR, but wanted to add that just so that we don't forget :smile: )",
  "pr_link": "https://github.com/pandas-dev/pandas/pull/19344",
  "code_context": [
    {
      "filename": "ci/lint.sh",
      "content": "#!/bin/bash\n\necho \"inside $0\"\n\nsource activate pandas\n\nRET=0\n\nif [ \"$LINT\" ]; then\n\n    # pandas/_libs/src is C code, so no need to search there.\n    echo \"Linting *.py\"\n    flake8 pandas --filename=*.py --exclude pandas/_libs/src\n    if [ $? -ne \"0\" ]; then\n        RET=1\n    fi\n    echo \"Linting *.py DONE\"\n\n    echo \"Linting setup.py\"\n    flake8 setup.py\n    if [ $? -ne \"0\" ]; then\n        RET=1\n    fi\n    echo \"Linting setup.py DONE\"\n\n    echo \"Linting asv_bench/benchmarks/\"\n    flake8 asv_bench/benchmarks/  --exclude=asv_bench/benchmarks/*.py --ignore=F811\n    if [ $? -ne \"0\" ]; then\n        RET=1\n    fi\n    echo \"Linting asv_bench/benchmarks/*.py DONE\"\n\n    echo \"Linting scripts/*.py\"\n    flake8 scripts --filename=*.py\n    if [ $? -ne \"0\" ]; then\n        RET=1\n    fi\n    echo \"Linting scripts/*.py DONE\"\n\n    echo \"Linting *.pyx\"\n    flake8 pandas --filename=*.pyx --select=E501,E302,E203,E111,E114,E221,E303,E128,E231,E126,E265,E305,E301,E127,E261,E271,E129,W291,E222,E241,E123,F403\n    if [ $? -ne \"0\" ]; then\n        RET=1\n    fi\n    echo \"Linting *.pyx DONE\"\n\n    echo \"Linting *.pxi.in\"\n    for path in 'src'\n    do\n        echo \"linting -> pandas/$path\"\n        flake8 pandas/$path --filename=*.pxi.in --select=E501,E302,E203,E111,E114,E221,E303,E231,E126,F403\n        if [ $? -ne \"0\" ]; then\n            RET=1\n        fi\n    done\n    echo \"Linting *.pxi.in DONE\"\n\n    echo \"Linting *.pxd\"\n    for path in '_libs'\n    do\n        echo \"linting -> pandas/$path\"\n        flake8 pandas/$path --filename=*.pxd --select=E501,E302,E203,E111,E114,E221,E303,E231,E126,F403\n        if [ $? -ne \"0\" ]; then\n            RET=1\n        fi\n    done\n    echo \"Linting *.pxd DONE\"\n\n    # readability/casting: Warnings about C casting instead of C++ casting\n    # runtime/int: Warnings about using C number types instead of C++ ones\n    # build/include_subdir: Warnings about prefacing included header files with directory\n\n    # We don't lint all C files because we don't want to lint any that are built\n    # from Cython files nor do we want to lint C files that we didn't modify for\n    # this particular codebase (e.g. src/headers, src/klib, src/msgpack). However,\n    # we can lint all header files since they aren't \"generated\" like C files are.\n    echo \"Linting *.c and *.h\"\n    for path in '*.h' 'period_helper.c' 'datetime' 'parser' 'ujson'\n    do\n        echo \"linting -> pandas/_libs/src/$path\"\n        cpplint --quiet --extensions=c,h --headers=h --filter=-readability/casting,-runtime/int,-build/include_subdir --recursive pandas/_libs/src/$path\n        if [ $? -ne \"0\" ]; then\n            RET=1\n        fi\n    done\n    echo \"Linting *.c and *.h DONE\"\n\n    echo \"Check for invalid testing\"\n\n    # Check for the following code in testing:\n    #\n    # np.testing\n    # np.array_equal\n    grep -r -E --include '*.py' --exclude testing.py '(numpy|np)(\\.testing|\\.array_equal)' pandas/tests/\n\n    if [ $? = \"0\" ]; then\n        RET=1\n    fi\n\n    # Check for pytest.warns\n    grep -r -E --include '*.py' 'pytest\\.warns' pandas/tests/\n\n    if [ $? = \"0\" ]; then\n        RET=1\n    fi\n\n    echo \"Check for invalid testing DONE\"\n\n    # Check for imports from pandas.core.common instead\n    # of `import pandas.core.common as com`\n    echo \"Check for non-standard imports\"\n    grep -R --include=\"*.py*\" -E \"from pandas.core.common import \" pandas\n    if [ $? = \"0\" ]; then\n        RET=1\n    fi\n    echo \"Check for non-standard imports DONE\"\n\n    echo \"Check for use of lists instead of generators in built-in Python functions\"\n\n    # Example: Avoid `any([i for i in some_iterator])` in favor of `any(i for i in some_iterator)`\n    #\n    # Check the following functions:\n    # any(), all(), sum(), max(), min(), list(), dict(), set(), frozenset(), tuple(), str.join()\n    grep -R --include=\"*.py*\" -E \"[^_](any|all|sum|max|min|list|dict|set|frozenset|tuple|join)\\(\\[.* for .* in .*\\]\\)\" pandas\n\n    if [ $? = \"0\" ]; then\n        RET=1\n    fi\n    echo \"Check for use of lists instead of generators in built-in Python functions DONE\"\n\n    echo \"Check for incorrect sphinx directives\"\n    SPHINX_DIRECTIVES=$(echo \\\n       \"autosummary|contents|currentmodule|deprecated|function|image|\"\\\n       \"important|include|ipython|literalinclude|math|module|note|raw|\"\\\n       \"seealso|toctree|versionadded|versionchanged|warning\" | tr -d \"[:space:]\")\n    for path in './pandas' './doc/source'\n    do\n        grep -R --include=\"*.py\" --include=\"*.pyx\" --include=\"*.rst\" -E \"\\.\\. ($SPHINX_DIRECTIVES):[^:]\" $path\n        if [ $? = \"0\" ]; then\n            RET=1\n        fi\n    done\n    echo \"Check for incorrect sphinx directives DONE\"\n\n    echo \"Check for deprecated messages without sphinx directive\"\n    grep -R --include=\"*.py\" --include=\"*.pyx\" -E \"(DEPRECATED|DEPRECATE|Deprecated)(:|,|\\.)\" pandas\n\n    if [ $? = \"0\" ]; then\n        RET=1\n    fi\n    echo \"Check for deprecated messages without sphinx directive DONE\"\nelse\n    echo \"NOT Linting\"\nfi\n\nexit $RET\n"
    },
    {
      "filename": "scripts/announce.py",
      "content": "#!/usr/bin/env python\n# -*- encoding:utf-8 -*-\n\"\"\"\nScript to generate contributor and pull request lists\n\nThis script generates contributor and pull request lists for release\nannouncements using Github v3 protocol. Use requires an authentication token in\norder to have sufficient bandwidth, you can get one following the directions at\n`<https://help.github.com/articles/creating-an-access-token-for-command-line-use/>_\nDon't add any scope, as the default is read access to public information. The\ntoken may be stored in an environment variable as you only get one chance to\nsee it.\n\nUsage::\n\n    $ ./scripts/announce.py <token> <revision range>\n\nThe output is utf8 rst.\n\nDependencies\n------------\n\n- gitpython\n- pygithub\n\nSome code was copied from scipy `tools/gh_lists.py` and `tools/authors.py`.\n\nExamples\n--------\n\nFrom the bash command line with $GITHUB token.\n\n    $ ./scripts/announce.py $GITHUB v1.11.0..v1.11.1 > announce.rst\n\n\"\"\"\nfrom __future__ import print_function, division\n\nimport os\nimport re\nimport codecs\nfrom git import Repo\n\nUTF8Writer = codecs.getwriter('utf8')\nthis_repo = Repo(os.path.join(os.path.dirname(__file__), \"..\"))\n\nauthor_msg = \"\"\"\\\nA total of %d people contributed to this release.  People with a \"+\" by their\nnames contributed a patch for the first time.\n\"\"\"\n\npull_request_msg = \"\"\"\\\nA total of %d pull requests were merged for this release.\n\"\"\"\n\n\ndef get_authors(revision_range):\n    pat = u'^.*\\\\t(.*)$'\n    lst_release, cur_release = [r.strip() for r in revision_range.split('..')]\n\n    # authors, in current release and previous to current release.\n    cur = set(re.findall(pat, this_repo.git.shortlog('-s', revision_range),\n                         re.M))\n    pre = set(re.findall(pat, this_repo.git.shortlog('-s', lst_release),\n                         re.M))\n\n    # Homu is the author of auto merges, clean him out.\n    cur.discard('Homu')\n    pre.discard('Homu')\n\n    # Append '+' to new authors.\n    authors = [s + u' +' for s in cur - pre] + [s for s in cur & pre]\n    authors.sort()\n    return authors\n\n\ndef get_pull_requests(repo, revision_range):\n    prnums = []\n\n    # From regular merges\n    merges = this_repo.git.log(\n        '--oneline', '--merges', revision_range)\n    issues = re.findall(u\"Merge pull request \\\\#(\\\\d*)\", merges)\n    prnums.extend(int(s) for s in issues)\n\n    # From Homu merges (Auto merges)\n    issues = re. findall(u\"Auto merge of \\\\#(\\\\d*)\", merges)\n    prnums.extend(int(s) for s in issues)\n\n    # From fast forward squash-merges\n    commits = this_repo.git.log(\n        '--oneline', '--no-merges', '--first-parent', revision_range)\n    issues = re.findall(u'^.*\\\\(\\\\#(\\\\d+)\\\\)$', commits, re.M)\n    prnums.extend(int(s) for s in issues)\n\n    # get PR data from github repo\n    prnums.sort()\n    prs = [repo.get_pull(n) for n in prnums]\n    return prs\n\n\ndef main(revision_range, repo):\n    lst_release, cur_release = [r.strip() for r in revision_range.split('..')]\n\n    # document authors\n    authors = get_authors(revision_range)\n    heading = u\"Contributors\"\n    print()\n    print(heading)\n    print(u\"=\" * len(heading))\n    print(author_msg % len(authors))\n\n    for s in authors:\n        print(u'* ' + s)\n\n\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(description=\"Generate author lists for release\")\n    parser.add_argument('revision_range', help='<revision>..<revision>')\n    parser.add_argument('--repo', help=\"Github org/repository\",\n                        default=\"pandas-dev/pandas\")\n    args = parser.parse_args()\n    main(args.revision_range, args.repo)\n"
    },
    {
      "filename": "scripts/api_rst_coverage.py",
      "content": "#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\nScript to generate a report with the coverage of the API in the docs.\n\nThe output of this script shows the existing methods that are not\nincluded in the API documentation, as well as the methods documented\nthat do not exist. Ideally, no method should be listed. Currently it\nconsiders the methods of Series, DataFrame and Panel.\n\nDeprecated methods are usually removed from the documentation, while\nstill available for three minor versions. They are listed with the\nword deprecated and the version number next to them.\n\nUsage::\n\n    $ PYTHONPATH=.. ./api_rst_coverage.py\n\n\"\"\"\nimport os\nimport re\nimport inspect\nimport pandas as pd\n\n\ndef main():\n    # classes whose members to check\n    classes = [pd.Series, pd.DataFrame, pd.Panel]\n\n    def class_name_sort_key(x):\n        if x.startswith('Series'):\n            # make sure Series precedes DataFrame, and Panel.\n            return ' ' + x\n        else:\n            return x\n\n    def get_docstring(x):\n        class_name, method = x.split('.')\n        obj = getattr(getattr(pd, class_name), method)\n        return obj.__doc__\n\n    def deprecation_version(x):\n        pattern = re.compile('\\.\\. deprecated:: ([0-9]+\\.[0-9]+\\.[0-9]+)')\n        doc = get_docstring(x)\n        match = pattern.search(doc)\n        if match:\n            return match.groups()[0]\n\n    def add_notes(x):\n        # Some methods are not documented in api.rst because they\n        # have been deprecated. Adding a comment to detect them easier.\n        doc = get_docstring(x)\n        note = None\n        if not doc:\n            note = 'no docstring'\n        else:\n            version = deprecation_version(x)\n            if version:\n                note = 'deprecated in {}'.format(version)\n\n        return '{} ({})'.format(x, note) if note else x\n\n    # class members\n    class_members = set()\n    for cls in classes:\n        for member in inspect.getmembers(cls):\n            class_members.add('{cls}.{member}'.format(cls=cls.__name__,\n                                                      member=member[0]))\n\n    # class members referenced in api.rst\n    api_rst_members = set()\n    base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    api_rst_fname = os.path.join(base_path, 'doc', 'source', 'api.rst')\n    class_names = (cls.__name__ for cls in classes)\n    pattern = re.compile('({})\\.(\\w+)'.format('|'.join(class_names)))\n    with open(api_rst_fname, 'r') as f:\n        for line in f:\n            match = pattern.search(line)\n            if match:\n                api_rst_members.add(match.group(0))\n\n    print()\n    print(\"Documented members in api.rst that aren't actual class members:\")\n    for x in sorted(api_rst_members.difference(class_members),\n                    key=class_name_sort_key):\n        print(x)\n\n    print()\n    print(\"Class members (other than those beginning with '_') \"\n          \"missing from api.rst:\")\n    for x in sorted(class_members.difference(api_rst_members),\n                    key=class_name_sort_key):\n        if '._' not in x:\n            print(add_notes(x))\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "filename": "scripts/build_dist_for_release.sh",
      "content": "#!/bin/bash\n\n# this requires cython to be installed\n\n# this builds the release cleanly & is building on the current checkout\nrm -rf dist\ngit clean -xfd\npython setup.py clean --quiet\npython setup.py cython --quiet\npython setup.py sdist --formats=gztar --quiet\n"
    },
    {
      "filename": "scripts/convert_deps.py",
      "content": "\"\"\"\nConvert the conda environment.yaml to a pip requirements.txt\n\"\"\"\nimport yaml\n\nexclude = {'python=3'}\nrename = {'pytables': 'tables'}\n\nwith open(\"ci/environment-dev.yaml\") as f:\n    dev = yaml.load(f)\n\nwith open(\"ci/requirements-optional-conda.txt\") as f:\n    optional = [x.strip() for x in f.readlines()]\n\nrequired = dev['dependencies']\nrequired = [rename.get(dep, dep) for dep in required if dep not in exclude]\noptional = [rename.get(dep, dep) for dep in optional if dep not in exclude]\n\n\nwith open(\"ci/requirements_dev.txt\", 'wt') as f:\n    f.write(\"# This file was autogenerated by scripts/convert_deps.py\\n\")\n    f.write(\"# Do not modify directly\\n\")\n    f.write('\\n'.join(required))\n\n\nwith open(\"ci/requirements-optional-pip.txt\", 'wt') as f:\n    f.write(\"# This file was autogenerated by scripts/convert_deps.py\\n\")\n    f.write(\"# Do not modify directly\\n\")\n    f.write(\"\\n\".join(optional))\n"
    },
    {
      "filename": "scripts/find_commits_touching_func.py",
      "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# copyright 2013, y-p @ github\n\"\"\"\nSearch the git history for all commits touching a named method\n\nYou need the sh module to run this\nWARNING: this script uses git clean -f, running it on a repo with untracked\nfiles will probably erase them.\n\nUsage::\n    $ ./find_commits_touching_func.py  (see arguments below)\n\"\"\"\nfrom __future__ import print_function\nimport logging\nimport re\nimport os\nimport argparse\nfrom collections import namedtuple\nfrom pandas.compat import lrange, map, string_types, text_type, parse_date\ntry:\n    import sh\nexcept ImportError:\n    raise ImportError(\"The 'sh' package is required to run this script.\")\n\n\ndesc = \"\"\"\nFind all commits touching a specified function across the codebase.\n\"\"\".strip()\nargparser = argparse.ArgumentParser(description=desc)\nargparser.add_argument('funcname', metavar='FUNCNAME',\n                       help='Name of function/method to search for changes on')\nargparser.add_argument('-f', '--file-masks', metavar='f_re(,f_re)*',\n                       default=[\"\\.py.?$\"],\n                       help='comma separated list of regexes to match '\n                       'filenames against\\ndefaults all .py? files')\nargparser.add_argument('-d', '--dir-masks', metavar='d_re(,d_re)*',\n                       default=[],\n                       help='comma separated list of regexes to match base '\n                       'path against')\nargparser.add_argument('-p', '--path-masks', metavar='p_re(,p_re)*',\n                       default=[],\n                       help='comma separated list of regexes to match full '\n                       'file path against')\nargparser.add_argument('-y', '--saw-the-warning',\n                       action='store_true', default=False,\n                       help='must specify this to run, acknowledge you '\n                       'realize this will erase untracked files')\nargparser.add_argument('--debug-level',\n                       default=\"CRITICAL\",\n                       help='debug level of messages (DEBUG, INFO, etc...)')\nargs = argparser.parse_args()\n\n\nlfmt = logging.Formatter(fmt='%(levelname)-8s %(message)s',\n                         datefmt='%m-%d %H:%M:%S')\nshh = logging.StreamHandler()\nshh.setFormatter(lfmt)\nlogger = logging.getLogger(\"findit\")\nlogger.addHandler(shh)\n\nHit = namedtuple(\"Hit\", \"commit path\")\nHASH_LEN = 8\n\n\ndef clean_checkout(comm):\n    h, s, d = get_commit_vitals(comm)\n    if len(s) > 60:\n        s = s[:60] + \"...\"\n    s = s.split(\"\\n\")[0]\n    logger.info(\"CO: %s %s\" % (comm, s))\n\n    sh.git('checkout', comm, _tty_out=False)\n    sh.git('clean', '-f')\n\n\ndef get_hits(defname, files=()):\n    cs = set()\n    for f in files:\n        try:\n            r = sh.git('blame',\n                       '-L',\n                       '/def\\s*{start}/,/def/'.format(start=defname),\n                       f,\n                       _tty_out=False)\n        except sh.ErrorReturnCode_128:\n            logger.debug(\"no matches in %s\" % f)\n            continue\n\n        lines = r.strip().splitlines()[:-1]\n        # remove comment lines\n        lines = [x for x in lines if not re.search(\"^\\w+\\s*\\(.+\\)\\s*#\", x)]\n        hits = set(map(lambda x: x.split(\" \")[0], lines))\n        cs.update(set(Hit(commit=c, path=f) for c in hits))\n\n    return cs\n\n\ndef get_commit_info(c, fmt, sep='\\t'):\n    r = sh.git('log',\n               \"--format={}\".format(fmt),\n               '{}^..{}'.format(c, c),\n               \"-n\",\n               \"1\",\n               _tty_out=False)\n    return text_type(r).split(sep)\n\n\ndef get_commit_vitals(c, hlen=HASH_LEN):\n    h, s, d = get_commit_info(c, '%H\\t%s\\t%ci', \"\\t\")\n    return h[:hlen], s, parse_date(d)\n\n\ndef file_filter(state, dirname, fnames):\n    if (args.dir_masks and\n            not any(re.search(x, dirname) for x in args.dir_masks)):\n        return\n    for f in fnames:\n        p = os.path.abspath(os.path.join(os.path.realpath(dirname), f))\n        if (any(re.search(x, f) for x in args.file_masks) or\n                any(re.search(x, p) for x in args.path_masks)):\n            if os.path.isfile(p):\n                state['files'].append(p)\n\n\ndef search(defname, head_commit=\"HEAD\"):\n    HEAD, s = get_commit_vitals(\"HEAD\")[:2]\n    logger.info(\"HEAD at %s: %s\" % (HEAD, s))\n    done_commits = set()\n    # allhits = set()\n    files = []\n    state = dict(files=files)\n    os.walk('.', file_filter, state)\n    # files now holds a list of paths to files\n\n    # seed with hits from q\n    allhits = set(get_hits(defname, files=files))\n    q = set([HEAD])\n    try:\n        while q:\n            h = q.pop()\n            clean_checkout(h)\n            hits = get_hits(defname, files=files)\n            for x in hits:\n                prevc = get_commit_vitals(x.commit + \"^\")[0]\n                if prevc not in done_commits:\n                    q.add(prevc)\n            allhits.update(hits)\n            done_commits.add(h)\n\n            logger.debug(\"Remaining: %s\" % q)\n    finally:\n        logger.info(\"Restoring HEAD to %s\" % HEAD)\n        clean_checkout(HEAD)\n    return allhits\n\n\ndef pprint_hits(hits):\n    SUBJ_LEN = 50\n    PATH_LEN = 20\n    hits = list(hits)\n    max_p = 0\n    for hit in hits:\n        p = hit.path.split(os.path.realpath(os.curdir) + os.path.sep)[-1]\n        max_p = max(max_p, len(p))\n\n    if max_p < PATH_LEN:\n        SUBJ_LEN += PATH_LEN - max_p\n        PATH_LEN = max_p\n\n    def sorter(i):\n        h, s, d = get_commit_vitals(hits[i].commit)\n        return hits[i].path, d\n\n    print(('\\nThese commits touched the %s method in these files '\n           'on these dates:\\n') % args.funcname)\n    for i in sorted(lrange(len(hits)), key=sorter):\n        hit = hits[i]\n        h, s, d = get_commit_vitals(hit.commit)\n        p = hit.path.split(os.path.realpath(os.curdir) + os.path.sep)[-1]\n\n        fmt = \"{:%d} {:10} {:<%d} {:<%d}\" % (HASH_LEN, SUBJ_LEN, PATH_LEN)\n        if len(s) > SUBJ_LEN:\n            s = s[:SUBJ_LEN - 5] + \" ...\"\n        print(fmt.format(h[:HASH_LEN], d.isoformat()[:10], s, p[-20:]))\n\n    print(\"\\n\")\n\n\ndef main():\n    if not args.saw_the_warning:\n        argparser.print_help()\n        print(\"\"\"\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nWARNING:\nthis script uses git clean -f, running it on a repo with untracked files.\nIt's recommended that you make a fresh clone and run from its root directory.\nYou must specify the -y argument to ignore this warning.\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\"\"\")\n        return\n    if isinstance(args.file_masks, string_types):\n        args.file_masks = args.file_masks.split(',')\n    if isinstance(args.path_masks, string_types):\n        args.path_masks = args.path_masks.split(',')\n    if isinstance(args.dir_masks, string_types):\n        args.dir_masks = args.dir_masks.split(',')\n\n    logger.setLevel(getattr(logging, args.debug_level))\n\n    hits = search(args.funcname)\n    pprint_hits(hits)\n\n\nif __name__ == \"__main__\":\n    import sys\n    sys.exit(main())\n"
    },
    {
      "filename": "scripts/find_undoc_args.py",
      "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nScript that compares the signature arguments with the ones in the docsting\nand returns the differences in plain text or GitHub task list format.\n\nUsage::\n    $ ./find_undoc_args.py  (see arguments below)\n\"\"\"\nfrom __future__ import print_function\nimport sys\nfrom collections import namedtuple\nimport types\nimport os\nimport re\nimport argparse\nimport inspect\n\n\nparser = argparse.ArgumentParser(description='Program description.')\nparser.add_argument('-p', '--path', metavar='PATH', type=str, required=False,\n                    default=None, action='store',\n                    help='full path relative to which paths wills be reported')\nparser.add_argument('-m', '--module', metavar='MODULE', type=str,\n                    required=True, action='store',\n                    help='name of package to import and examine')\nparser.add_argument('-G', '--github_repo', metavar='REPO', type=str,\n                    required=False, default=None, action='store',\n                    help='github project where the code lives, '\n                    'e.g. \"pandas-dev/pandas\"')\nargs = parser.parse_args()\n\nEntry = namedtuple('Entry',\n                   'func path lnum undoc_names missing_args '\n                   'nsig_names ndoc_names')\n\n\ndef entry_gen(root_ns, module_name):\n    \"\"\"Walk and yield all methods and functions in the module root_ns and\n    submodules.\"\"\"\n    q = [root_ns]\n    seen = set()\n    while q:\n        ns = q.pop()\n        for x in dir(ns):\n            cand = getattr(ns, x)\n            if (isinstance(cand, types.ModuleType) and\n                    cand.__name__ not in seen and\n                    cand.__name__.startswith(module_name)):\n                seen.add(cand.__name__)\n                q.insert(0, cand)\n            elif (isinstance(cand, (types.MethodType, types.FunctionType)) and\n                  cand not in seen and cand.__doc__):\n                seen.add(cand)\n                yield cand\n\n\ndef cmp_docstring_sig(f):\n    \"\"\"Return an `Entry` object describing the differences between the\n    arguments in the signature and the documented ones.\"\"\"\n    def build_loc(f):\n        path = f.__code__.co_filename.split(args.path, 1)[-1][1:]\n        return dict(path=path, lnum=f.__code__.co_firstlineno)\n\n    sig_names = set(inspect.getargspec(f).args)\n    # XXX numpydoc can be used to get the list of parameters\n    doc = f.__doc__.lower()\n    doc = re.split('^\\s*parameters\\s*', doc, 1, re.M)[-1]\n    doc = re.split('^\\s*returns*', doc, 1, re.M)[0]\n    doc_names = {x.split(\":\")[0].strip() for x in doc.split('\\n')\n                 if re.match('\\s+[\\w_]+\\s*:', x)}\n    sig_names.discard('self')\n    doc_names.discard('kwds')\n    doc_names.discard('kwargs')\n    doc_names.discard('args')\n    return Entry(func=f, path=build_loc(f)['path'], lnum=build_loc(f)['lnum'],\n                 undoc_names=sig_names.difference(doc_names),\n                 missing_args=doc_names.difference(sig_names),\n                 nsig_names=len(sig_names), ndoc_names=len(doc_names))\n\n\ndef format_id(i):\n    return i\n\n\ndef format_item_as_github_task_list(i, item, repo):\n    tmpl = ('- [ ] {id_}) [{fname}:{lnum} ({func_name}())]({link}) -  '\n            '__Missing__[{nmissing}/{total_args}]: {undoc_names}')\n    link_tmpl = \"https://github.com/{repo}/blob/master/{file}#L{lnum}\"\n    link = link_tmpl.format(repo=repo, file=item.path, lnum=item.lnum)\n    s = tmpl.format(id_=i, fname=item.path, lnum=item.lnum,\n                    func_name=item.func.__name__, link=link,\n                    nmissing=len(item.undoc_names),\n                    total_args=item.nsig_names,\n                    undoc_names=list(item.undoc_names))\n    if item.missing_args:\n        s += '    __Extra__(?): %s' % list(item.missing_args)\n    return s\n\n\ndef format_item_as_plain(i, item):\n    tmpl = ('+{lnum} {path} {func_name}(): '\n            'Missing[{nmissing}/{total_args}]={undoc_names}')\n    s = tmpl.format(path=item.path, lnum=item.lnum,\n                    func_name=item.func.__name__,\n                    nmissing=len(item.undoc_names),\n                    total_args=item.nsig_names,\n                    undoc_names=list(item.undoc_names))\n    if item.missing_args:\n        s += ' Extra(?)=%s' % list(item.missing_args)\n    return s\n\n\ndef main():\n    module = __import__(args.module)\n    if not args.path:\n        args.path = os.path.dirname(module.__file__)\n    collect = [cmp_docstring_sig(e)\n               for e in entry_gen(module, module.__name__)]\n    # only include if there are missing arguments in the docstring\n    # (fewer false positives) and there are at least some documented arguments\n    collect = [e for e in collect\n               if e.undoc_names and len(e.undoc_names) != e.nsig_names]\n    collect.sort(key=lambda x: x.path)\n\n    if args.github_repo:\n        for i, item in enumerate(collect, 1):\n            print(format_item_as_github_task_list(i, item, args.github_repo))\n    else:\n        for i, item in enumerate(collect, 1):\n            print(format_item_as_plain(i, item))\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n"
    },
    {
      "filename": "scripts/merge-pr.py",
      "content": "#!/usr/bin/env python\n\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Utility for creating well-formed pull request merges and pushing them to\n# Apache.\n#   usage: ./apache-pr-merge.py    (see config env vars below)\n#\n# Lightly modified from version of this script in incubator-parquet-format\nfrom __future__ import print_function\n\nfrom subprocess import check_output\nfrom requests.auth import HTTPBasicAuth\nimport requests\n\nimport os\nimport six\nimport sys\nimport textwrap\n\nfrom six.moves import input\n\nPANDAS_HOME = '.'\nPROJECT_NAME = 'pandas'\nprint(\"PANDAS_HOME = \" + PANDAS_HOME)\n\n# Remote name with the PR\nPR_REMOTE_NAME = os.environ.get(\"PR_REMOTE_NAME\", \"upstream\")\n\n# Remote name where results pushed\nPUSH_REMOTE_NAME = os.environ.get(\"PUSH_REMOTE_NAME\", \"upstream\")\n\nGITHUB_BASE = \"https://github.com/pandas-dev/\" + PROJECT_NAME + \"/pull\"\nGITHUB_API_BASE = \"https://api.github.com/repos/pandas-dev/\" + PROJECT_NAME\n\n# Prefix added to temporary branches\nBRANCH_PREFIX = \"PR_TOOL\"\n\nos.chdir(PANDAS_HOME)\n\nauth_required = False\n\nif auth_required:\n    GITHUB_USERNAME = os.environ['GITHUB_USER']\n    import getpass\n    GITHUB_PASSWORD = getpass.getpass('Enter github.com password for %s:'\n                                      % GITHUB_USERNAME)\n\n    def get_json_auth(url):\n        auth = HTTPBasicAuth(GITHUB_USERNAME, GITHUB_PASSWORD)\n        req = requests.get(url, auth=auth)\n        return req.json()\n\n    get_json = get_json_auth\nelse:\n    def get_json_no_auth(url):\n        req = requests.get(url)\n        return req.json()\n\n    get_json = get_json_no_auth\n\n\ndef fail(msg):\n    print(msg)\n    clean_up()\n    sys.exit(-1)\n\n\ndef run_cmd(cmd):\n    if isinstance(cmd, six.string_types):\n        cmd = cmd.split(' ')\n\n    output = check_output(cmd)\n\n    if isinstance(output, six.binary_type):\n        output = output.decode('utf-8')\n    return output\n\n\ndef continue_maybe(prompt):\n    result = input(\"\\n%s (y/n): \" % prompt)\n    if result.lower() != \"y\":\n        fail(\"Okay, exiting\")\n\n\ndef continue_maybe2(prompt):\n    result = input(\"\\n%s (y/n): \" % prompt)\n    if result.lower() != \"y\":\n        return False\n    else:\n        return True\n\n\noriginal_head = run_cmd(\"git rev-parse HEAD\")[:8]\n\n\ndef clean_up():\n    print(\"Restoring head pointer to %s\" % original_head)\n    run_cmd(\"git checkout %s\" % original_head)\n\n    branches = run_cmd(\"git branch\").replace(\" \", \"\").split(\"\\n\")\n\n    for branch in [b for b in branches if b.startswith(BRANCH_PREFIX)]:\n        print(\"Deleting local branch %s\" % branch)\n        run_cmd(\"git branch -D %s\" % branch)\n\n\n# Merge the requested PR and return the merge hash\ndef merge_pr(pr_num, target_ref):\n\n    pr_branch_name = \"%s_MERGE_PR_%s\" % (BRANCH_PREFIX, pr_num)\n    target_branch_name = \"%s_MERGE_PR_%s_%s\" % (BRANCH_PREFIX, pr_num,\n                                                target_ref.upper())\n    run_cmd(\"git fetch %s pull/%s/head:%s\" % (PR_REMOTE_NAME, pr_num,\n                                              pr_branch_name))\n    run_cmd(\"git fetch %s %s:%s\" % (PUSH_REMOTE_NAME, target_ref,\n                                    target_branch_name))\n    run_cmd(\"git checkout %s\" % target_branch_name)\n\n    had_conflicts = False\n    try:\n        run_cmd(['git', 'merge', pr_branch_name, '--squash'])\n    except Exception as e:\n        msg = (\"Error merging: %s\\nWould you like to manually fix-up \"\n               \"this merge?\" % e)\n        continue_maybe(msg)\n        msg = (\"Okay, please fix any conflicts and 'git add' \"\n               \"conflicting files... Finished?\")\n        continue_maybe(msg)\n        had_conflicts = True\n\n    commit_authors = run_cmd(['git', 'log', 'HEAD..%s' % pr_branch_name,\n                             '--pretty=format:%an <%ae>']).split(\"\\n\")\n    distinct_authors = sorted(set(commit_authors),\n                              key=lambda x: commit_authors.count(x),\n                              reverse=True)\n    primary_author = distinct_authors[0]\n    commits = run_cmd(['git', 'log', 'HEAD..%s' % pr_branch_name,\n                      '--pretty=format:%h [%an] %s']).split(\"\\n\\n\")\n\n    merge_message_flags = []\n\n    merge_message_flags += [\"-m\", title]\n    if body is not None:\n        merge_message_flags += [\"-m\", '\\n'.join(textwrap.wrap(body))]\n\n    authors = \"\\n\".join(\"Author: %s\" % a for a in distinct_authors)\n\n    merge_message_flags += [\"-m\", authors]\n\n    if had_conflicts:\n        committer_name = run_cmd(\"git config --get user.name\").strip()\n        committer_email = run_cmd(\"git config --get user.email\").strip()\n        message = (\"This patch had conflicts when merged, \"\n                   \"resolved by\\nCommitter: %s <%s>\"\n                   % (committer_name, committer_email))\n        merge_message_flags += [\"-m\", message]\n\n    # The string \"Closes #%s\" string is required for GitHub to correctly close\n    # the PR\n    merge_message_flags += [\n        \"-m\",\n        \"Closes #%s from %s and squashes the following commits:\"\n        % (pr_num, pr_repo_desc)]\n    for c in commits:\n        merge_message_flags += [\"-m\", c]\n\n    run_cmd(['git', 'commit', '--author=\"%s\"' % primary_author] +\n            merge_message_flags)\n\n    continue_maybe(\"Merge complete (local ref %s). Push to %s?\" % (\n        target_branch_name, PUSH_REMOTE_NAME))\n\n    try:\n        run_cmd('git push %s %s:%s' % (PUSH_REMOTE_NAME, target_branch_name,\n                                       target_ref))\n    except Exception as e:\n        clean_up()\n        fail(\"Exception while pushing: %s\" % e)\n\n    merge_hash = run_cmd(\"git rev-parse %s\" % target_branch_name)[:8]\n    clean_up()\n    print(\"Pull request #%s merged!\" % pr_num)\n    print(\"Merge hash: %s\" % merge_hash)\n    return merge_hash\n\n\ndef update_pr(pr_num, user_login, base_ref):\n\n    pr_branch_name = \"%s_MERGE_PR_%s\" % (BRANCH_PREFIX, pr_num)\n\n    run_cmd(\"git fetch %s pull/%s/head:%s\" % (PR_REMOTE_NAME, pr_num,\n                                              pr_branch_name))\n    run_cmd(\"git checkout %s\" % pr_branch_name)\n\n    continue_maybe(\"Update ready (local ref %s)? Push to %s/%s?\" % (\n        pr_branch_name, user_login, base_ref))\n\n    push_user_remote = \"https://github.com/%s/pandas.git\" % user_login\n\n    try:\n        run_cmd('git push %s %s:%s' % (push_user_remote, pr_branch_name,\n                                       base_ref))\n    except Exception as e:\n\n        if continue_maybe2(\"Force push?\"):\n            try:\n                run_cmd(\n                    'git push -f %s %s:%s' % (push_user_remote, pr_branch_name,\n                                              base_ref))\n            except Exception as e:\n                fail(\"Exception while pushing: %s\" % e)\n                clean_up()\n        else:\n            fail(\"Exception while pushing: %s\" % e)\n            clean_up()\n\n    clean_up()\n    print(\"Pull request #%s updated!\" % pr_num)\n\n\ndef cherry_pick(pr_num, merge_hash, default_branch):\n    pick_ref = input(\"Enter a branch name [%s]: \" % default_branch)\n    if pick_ref == \"\":\n        pick_ref = default_branch\n\n    pick_branch_name = \"%s_PICK_PR_%s_%s\" % (BRANCH_PREFIX, pr_num,\n                                             pick_ref.upper())\n\n    run_cmd(\"git fetch %s %s:%s\" % (PUSH_REMOTE_NAME, pick_ref,\n                                    pick_branch_name))\n    run_cmd(\"git checkout %s\" % pick_branch_name)\n    run_cmd(\"git cherry-pick -sx %s\" % merge_hash)\n\n    continue_maybe(\"Pick complete (local ref %s). Push to %s?\" % (\n        pick_branch_name, PUSH_REMOTE_NAME))\n\n    try:\n        run_cmd('git push %s %s:%s' % (PUSH_REMOTE_NAME, pick_branch_name,\n                                       pick_ref))\n    except Exception as e:\n        clean_up()\n        fail(\"Exception while pushing: %s\" % e)\n\n    pick_hash = run_cmd(\"git rev-parse %s\" % pick_branch_name)[:8]\n    clean_up()\n\n    print(\"Pull request #%s picked into %s!\" % (pr_num, pick_ref))\n    print(\"Pick hash: %s\" % pick_hash)\n    return pick_ref\n\n\ndef fix_version_from_branch(branch, versions):\n    #  Note: Assumes this is a sorted (newest->oldest) list of un-released\n    #  versions\n    if branch == \"master\":\n        return versions[0]\n    else:\n        branch_ver = branch.replace(\"branch-\", \"\")\n        return filter(lambda x: x.name.startswith(branch_ver), versions)[-1]\n\n\npr_num = input(\"Which pull request would you like to merge? (e.g. 34): \")\npr = get_json(\"%s/pulls/%s\" % (GITHUB_API_BASE, pr_num))\n\nurl = pr[\"url\"]\ntitle = pr[\"title\"]\nbody = pr[\"body\"]\ntarget_ref = pr[\"base\"][\"ref\"]\nuser_login = pr[\"user\"][\"login\"]\nbase_ref = pr[\"head\"][\"ref\"]\npr_repo_desc = \"%s/%s\" % (user_login, base_ref)\n\nif pr[\"merged\"] is True:\n    print(\"Pull request {0} has already been merged, please backport manually\"\n          .format(pr_num))\n    sys.exit(0)\n\nif not bool(pr[\"mergeable\"]):\n    msg = (\"Pull request {0} is not mergeable in its current form.\\n\"\n           \"Continue? (experts only!)\".format(pr_num))\n    continue_maybe(msg)\n\nprint(\"\\n=== Pull Request #%s ===\" % pr_num)\n\n# we may have un-printable unicode in our title\ntry:\n    title = title.encode('raw_unicode_escape')\nexcept Exception:\n    pass\n\nprint(\"title\\t{title}\\nsource\\t{source}\\ntarget\\t{target}\\nurl\\t{url}\".format(\n    title=title, source=pr_repo_desc, target=target_ref, url=url))\n\n\nmerged_refs = [target_ref]\n\nprint(\"\\nProceed with updating or merging pull request #%s?\" % pr_num)\nupdate = input(\"Update PR and push to remote (r), merge locally (l), \"\n               \"or do nothing (n) ?\")\nupdate = update.lower()\n\nif update == 'r':\n    merge_hash = update_pr(pr_num, user_login, base_ref)\nelif update == 'l':\n    merge_hash = merge_pr(pr_num, target_ref)\n"
    }
  ],
  "questions": [
    "This file: https://github.com/pandas-dev/pandas/blob/master/ci/lint.sh\r\n\r\nYou'll add another `flake8` line like the others, but for `scripts/` directory.\r\n\r\n@bhavybarca are you on Windows or Mac or Linux? You'll set the `LINT` environment variable and then run `ci/lint.sh`, which will print out a list of errors that need to be fixed."
  ],
  "golden_answers": [
    "It'll be roughly like\r\n\r\n```\r\n$ flake8 scripts/\r\nscripts/api_rst_coverage.py:24:1: E302 expected 2 blank lines, found 1\r\nscripts/api_rst_coverage.py:64:80: E501 line too long (90 > 79 characters)\r\nscripts/api_rst_coverage.py:70:80: E501 line too long (93 > 79 characters)\r\nscripts/api_rst_coverage.py:78:80: E501 line too long (88 > 79 characters)\r\n...\r\n```"
  ],
  "questions_generated": [
    "What is the purpose of editing the ci/lint.sh script in the pandas-dev/pandas repository?",
    "How can a developer set up the environment to run the modified ci/lint.sh script on a Linux system?",
    "What are some potential areas of improvement for the scripts located in 'pandas/scripts/' as suggested in the discussion context?",
    "Why might the contributor choose to use 'flake8 scripts/' instead of linting the entire repository?",
    "Describe the steps a beginner should take to address the issue of linting the 'pandas/scripts/' directory in the pandas-dev/pandas repository."
  ],
  "golden_answers_generated": [
    "The purpose of editing the ci/lint.sh script is to add a linting rule for the 'pandas/scripts/' directory. This involves adding a specific 'flake8' command for the directory to ensure that all Python scripts within 'pandas/scripts/' adhere to coding standards, thus maintaining code quality and consistency across the repository.",
    "To set up the environment on a Linux system, the developer needs to activate the appropriate Python environment using 'source activate pandas'. Then, they should set the 'LINT' environment variable by executing 'export LINT=1'. After these steps, running the 'ci/lint.sh' script will perform the linting checks specified within the script.",
    "The discussion suggests several improvements for the scripts in 'pandas/scripts/': fixing PEP-8 issues, adding documentation to undocumented scripts, making scripts executable by adding '#!' headers, documenting the scripts in 'contributing.rst', ensuring scripts work regardless of the current directory, and following consistent naming conventions for script files.",
    "The contributor might choose to use 'flake8 scripts/' to focus the linting process specifically on the 'scripts/' directory. This approach ensures targeted linting, which is more efficient and relevant for the issue at hand, as opposed to linting the entire repository, which could be time-consuming and may include unrelated files.",
    "A beginner should first edit the 'ci/lint.sh' script to include a 'flake8' command for the 'pandas/scripts/' directory. Next, set the 'LINT' environment variable and run 'ci/lint.sh' to get a list of linting errors. Use tools like 'autopep8' to fix these errors. The beginner should then test the changes by re-running the script to ensure all issues are resolved before committing and pushing the modifications."
  ]
}